Issue Type,Issue key,Issue id,Summary,Description,Assignee,Reporter,Priority,Status,Resolution,Created,Updated
Bug,ZOOKEEPER-1351,12537256,invalid test verification in MultiTransactionTest,"tests such as org.apache.zookeeper.test.MultiTransactionTest.testWatchesTriggered() are incorrect. Two issues I see

1) zk.sync is async, there is no guarantee that the watcher will be called subsequent to sync returning

{noformat}
        zk.sync(""/"", null, null);
        assertTrue(watcher.triggered); /// incorrect assumption
{noformat}

The callback needs to be implemented, only once the callback is called can we verify the trigger.

2) trigger is not declared as volatile, even though it will be set in the context of a different thread (eventthread)

See https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-solaris/91/testReport/junit/org.apache.zookeeper.test/MultiTransactionTest/testWatchesTriggered/
for an example of a false positive failure

{noformat}
junit.framework.AssertionFailedError
	at org.apache.zookeeper.test.MultiTransactionTest.testWatchesTriggered(MultiTransactionTest.java:236)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}
",phunt,phunt,Major,Resolved,Fixed,04/Jan/12 22:55,16/Jan/12 03:56
Bug,ZOOKEEPER-1352,12537386,server.InvalidSnapshotTest is using connection timeouts that are too short,"InvalidSnapshotTest is using connection timeouts that are too short, see this false failure:
https://builds.apache.org/job/ZooKeeper_branch33_solaris/65/testReport/junit/org.apache.zookeeper.server/InvalidSnapshotTest/testInvalidSnapshot/

{noformat}
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /invalidsnap-0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:643)
	at org.apache.zookeeper.server.InvalidSnapshotTest.testInvalidSnapshot(InvalidSnapshotTest.java:71)
{noformat}

Also in looking at the test itself it could use some cleanup (reuse features from ClientBase test utils)",phunt,phunt,Major,Resolved,Fixed,05/Jan/12 19:12,06/Feb/12 10:58
Bug,ZOOKEEPER-1353,12537601,C client test suite fails consistently,"When the c client test suite, zktest-mt, is run, it fails with this:

tests/TestZookeeperInit.cc:233: Assertion: equality assertion failed [Expected: 2, Actual  : 22]

This was also reported in 3.3.1 here:

http://www.mail-archive.com/zookeeper-dev@hadoop.apache.org/msg08914.html

The C client tests are making some assumptions that are not valid. getaddrinfo may have, at one time, returned ENOENT instead of EINVAL for the host given in the test. The assertion should simply be that EINVAL | ENOENT are given, so that builds on platforms which return ENOENT for this are not broken.

",spamaps,spamaps,Minor,Resolved,Fixed,06/Jan/12 21:42,06/Feb/12 10:58
Bug,ZOOKEEPER-1354,12537617,AuthTest.testBadAuthThenSendOtherCommands fails intermittently,"I'm seeing the following intermittent failure:

{noformat}
junit.framework.AssertionFailedError: Should have called my watcher expected:<1> but was:<0>
	at org.apache.zookeeper.test.AuthTest.testBadAuthThenSendOtherCommands(AuthTest.java:89)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}

The following commit introduced this test:

bq. ZOOKEEPER-1152. Exceptions thrown from handleAuthentication can cause buffer corruption issues in NIOServer. (camille via breed)

+            Assert.assertEquals(""Should have called my watcher"",
+                    1, authFailed.get());

I think it's due to either a) the code is not waiting for the
notification to be propagated, or 2) the message doesn't make it back
from the server to the client prior to the socket or the clientcnxn
being closed.

What do you think, should I just wait for the notification to arrive? or do you think it's 2). ?

",phunt,phunt,Major,Resolved,Fixed,07/Jan/12 01:10,02/Mar/12 03:21
Bug,ZOOKEEPER-1357,12537831,"Zab1_0Test uses hard-wired port numbers. Specifically, it uses the same port for leader in two different tests. The second test periodically fails complaining that the port is still in use.","Here's what I get:


Testcase: testLeaderInConnectingFollowers took 34.117 sec
Testcase: testLastAcceptedEpoch took 0.047 sec                    <----- new test added in ZK-1343
Testcase: testLeaderInElectingFollowers took 0.004 sec
        Caused an ERROR
Address already in use
java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:383)
        at java.net.ServerSocket.bind(ServerSocket.java:328)
        at java.net.ServerSocket.<init>(ServerSocket.java:194)
        at java.net.ServerSocket.<init>(ServerSocket.java:106)
        at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:220)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.createLeader(Zab1_0Test.java:711)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testLeaderInElectingFollowers(Zab1_0Test.java:225)

Testcase: testNormalFollowerRun took 29.128 sec
Testcase: testNormalRun took 25.158 sec
Testcase: testLeaderBehind took 25.148 sec
Testcase: testAbandonBeforeACKEpoch took 34.029 sec


My guess is that testLastAcceptedEpoch doesn't properly close the connection before testLeaderInElectingFollowers starts.
I propose to add 

if (leadThread != null) {
                leadThread.interrupt();
                leadThread.join();
}       

to the test.


In addition, I propose to change the hard-wired ports in Zab1_0Test to use Portassignment.unique() as done in other tests. If I understand correctly the static counter used in unique() to assign ports is initialized once per test file, so it would also prevent the problem I'm seeing here of two tests in the same file trying to use the same port. 

The error can be reproduced using the attached patch (for some reason I don't see the problem in the trunk).

",shralex,shralex,Minor,Resolved,Fixed,09/Jan/12 23:04,14/Apr/14 22:31
Bug,ZOOKEEPER-1358,12537850,"In StaticHostProviderTest.java, testNextDoesNotSleepForZero tests that hostProvider.next(0) doesn't sleep by checking that the latency of this call is less than 10sec","should check for something smaller, perhaps 1ms or 5ms",shralex,shralex,Trivial,Resolved,Fixed,10/Jan/12 01:46,16/Jan/12 03:56
Bug,ZOOKEEPER-1361,12538340,Leader.lead iterates over 'learners' set without proper synchronisation,"This block:

{code}
HashSet<Long> followerSet = new HashSet<Long>();
for(LearnerHandler f : learners)
    followerSet.add(f.getSid());
{code}

is executed without holding the lock on learners, so if there were ever a condition where a new learner was added during the initial sync phase, I'm pretty sure we'd see a concurrent modification exception. Certainly other parts of the code are very careful to lock on learners when iterating. 

It would be nice to use a {{ConcurrentHashMap}} to hold the learners instead, but I can't convince myself that this wouldn't introduce some correctness bugs. For example the following:

Learners contains A, B, C, D
Thread 1 iterates over learners, and gets as far as B.
Thread 2 removes A, and adds E.
Thread 1 continues iterating and sees a learner view of A, B, C, D, E

This may be a bug if Thread 1 is counting the number of synced followers for a quorum count, since at no point was A, B, C, D, E a correct view of the quorum.

In practice, I think this is actually ok, because I don't think ZK makes any strong ordering guarantees on learners joining or leaving (so we don't need a strong serialisability guarantee on learners) but I don't think I'll make that change for this patch. Instead I want to clean up the locking protocols on the follower / learner sets - to avoid another easy deadlock like the one we saw in ZOOKEEPER-1294 - and to do less with the lock held; i.e. to copy and then iterate over the copy rather than iterate over a locked set. ",henryr,henryr,Major,Resolved,Fixed,13/Jan/12 17:43,17/Sep/12 05:04
Bug,ZOOKEEPER-1366,12538929,Zookeeper should be tolerant of clock adjustments,"If you want to wreak havoc on a ZK based system just do [date -s ""+1hour""] and watch the mayhem as all sessions expire at once.

This shouldn't happen.  Zookeeper could easily know handle elapsed times as elapsed times rather than as differences between absolute times.  The absolute times are subject to adjustment when the clock is set while a timer is not subject to this problem.  In Java, System.currentTimeMillis() gives you absolute time while System.nanoTime() gives you time based on a timer from an arbitrary epoch.

I have done this and have been running tests now for some tens of minutes with no failures.  I will set up a test machine to redo the build again on Ubuntu and post a patch here for discussion.",hdeng,tdunning,Critical,Resolved,Fixed,19/Jan/12 07:00,11/May/17 20:34
Bug,ZOOKEEPER-1367,12539194,Data inconsistencies and unexpired ephemeral nodes after cluster restart,"In one of our tests, we have a cluster of three ZooKeeper servers.  We kill all three, and then restart just two of them.  Sometimes we notice that on one of the restarted servers, ephemeral nodes from previous sessions do not get deleted, while on the other server they do.  We are effectively running 3.4.2, though technically we are running 3.4.1 with the patch manually applied for ZOOKEEPER-1333 and a C client for 3.4.1 with the patches for ZOOKEEPER-1163.

I noticed that when I connected using zkCli.sh to the first node (90.0.0.221, zkid 84), I saw only one znode in a particular path:

{quote}
[zk: 90.0.0.221:2888(CONNECTED) 0] ls /election/zkrsm
[nominee0000000011]
[zk: 90.0.0.221:2888(CONNECTED) 1] get /election/zkrsm/nominee0000000011
90.0.0.222:7777 
cZxid = 0x400000027
ctime = Thu Jan 19 08:18:24 UTC 2012
mZxid = 0x400000027
mtime = Thu Jan 19 08:18:24 UTC 2012
pZxid = 0x400000027
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xa234f4f3bc220001
dataLength = 16
numChildren = 0
{quote}

However, when I connect zkCli.sh to the second server (90.0.0.222, zkid 251), I saw three znodes under that same path:

{quote}
[zk: 90.0.0.222:2888(CONNECTED) 2] ls /election/zkrsm
nominee0000000006   nominee0000000010   nominee0000000011
[zk: 90.0.0.222:2888(CONNECTED) 2] get /election/zkrsm/nominee0000000011
90.0.0.222:7777 
cZxid = 0x400000027
ctime = Thu Jan 19 08:18:24 UTC 2012
mZxid = 0x400000027
mtime = Thu Jan 19 08:18:24 UTC 2012
pZxid = 0x400000027
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xa234f4f3bc220001
dataLength = 16
numChildren = 0
[zk: 90.0.0.222:2888(CONNECTED) 3] get /election/zkrsm/nominee0000000010
90.0.0.221:7777 
cZxid = 0x30000014c
ctime = Thu Jan 19 07:53:42 UTC 2012
mZxid = 0x30000014c
mtime = Thu Jan 19 07:53:42 UTC 2012
pZxid = 0x30000014c
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xa234f4f3bc220000
dataLength = 16
numChildren = 0
[zk: 90.0.0.222:2888(CONNECTED) 4] get /election/zkrsm/nominee0000000006
90.0.0.223:7777 
cZxid = 0x200000cab
ctime = Thu Jan 19 08:00:30 UTC 2012
mZxid = 0x200000cab
mtime = Thu Jan 19 08:00:30 UTC 2012
pZxid = 0x200000cab
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x5434f5074e040002
dataLength = 16
numChildren = 0
{quote}

These never went away for the lifetime of the server, for any clients connected directly to that server.  Note that this cluster is configured to have all three servers still, the third one being down (90.0.0.223, zkid 162).

I captured the data/snapshot directories for the the two live servers.  When I start single-node servers using each directory, I can briefly see that the inconsistent data is present in those logs, though the ephemeral nodes seem to get (correctly) cleaned up pretty soon after I start the server.

I will upload a tar containing the debug logs and data directories from the failure.  I think we can reproduce it regularly if you need more info.",breed,strib,Blocker,Resolved,Fixed,20/Jan/12 18:48,28/Aug/13 22:20
Bug,ZOOKEEPER-1370,12539546,Add logging changes in Release Notes needed for clients because of ZOOKEEPER-850.,,mahadev,mahadev,Major,Resolved,Fixed,24/Jan/12 00:28,06/Feb/12 10:29
Bug,ZOOKEEPER-1371,12539547,Remove dependency on log4j in the source code.,"ZOOKEEPER-850 added slf4j to ZK. We still depend on log4j in our codebase. We should remove the dependency on log4j so that we can make logging pluggable.
",arshad.mohammad,mahadev,Major,Closed,Fixed,24/Jan/12 00:30,02/May/23 17:31
Bug,ZOOKEEPER-1373,12539563,Hardcoded SASL login context name clashes with Hadoop security configuration override,"I'm trying to configure a process with Hadoop security (Hive metastore server) to talk to ZooKeeper 3.4.2 with Kerberos authentication. In this scenario Hadoop controls the SASL configuration (org.apache.hadoop.security.UserGroupInformation.HadoopConfiguration), instead of setting up the ZooKeeper ""Client"" loginContext via jaas.conf and system property 

{{-Djava.security.auth.login.config}}

Using the Hadoop configuration would work, except that ZooKeeper client code expects the loginContextName to be ""Client"" while Hadoop security will use  ""hadoop-keytab-kerberos"". I verified that by changing the name in the debugger the SASL authentication succeeds while otherwise the login configuration cannot be resolved and the connection to ZooKeeper is unauthenticated. 

To integrate with Hadoop, the following in ZooKeeperSaslClient would need to change to make the name configurable:

     {{login = new Login(""Client"",new ClientCallbackHandler(null));}}
",ekoontz,thw,Major,Resolved,Fixed,24/Jan/12 03:40,02/May/13 02:29
Bug,ZOOKEEPER-1374,12539622,C client multi-threaded test suite fails to compile on ARM architectures.,"The multi-threaded test suite fails to build on ARM architectures:

g++ -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -D_FORTIFY_SOURCE=2 -DUSE_STATIC_LIB -DTHREADED -DZKSERVER_CMD=""\""./tests/zkServer.sh\"""" -Wall -g -MT zktest_mt-ThreadingUtil.o -MD -MP -MF .deps/zktest_mt-ThreadingUtil.Tpo -c -o zktest_mt-ThreadingUtil.o `test -f 'tests/ThreadingUtil.cc' || echo './'`tests/ThreadingUtil.cc
/tmp/ccqJWQRC.s: Assembler messages:
/tmp/ccqJWQRC.s:373: Error: bad instruction `lock xaddl r4,[r3,#0]'
/tmp/ccqJWQRC.s:425: Error: bad instruction `lock xchgl r4,[r3,#0]'

gcc does provide alternative primitives (_sync_*) which provide better cross platform compatibility; but that does make the assumption that a) gcc is being used or b) the primitives are provided by alternative compilers.

Tracked in Ubuntu here: https://bugs.launchpad.net/ubuntu/+source/zookeeper/+bug/920871",javacruft,javacruft,Minor,Resolved,Fixed,24/Jan/12 15:22,28/Jun/16 08:37
Bug,ZOOKEEPER-1376,12539987,zkServer.sh does not correctly check for $SERVER_JVMFLAGS,"It will always include it even if not defined, although not much harm.

if [ ""x$SERVER_JVMFLAGS"" ]
then
JVMFLAGS=""$SERVER_JVMFLAGS $JVMFLAGS""
fi

should use the std idiom.",skye,phunt,Minor,Resolved,Fixed,27/Jan/12 01:39,24/Sep/12 18:29
Bug,ZOOKEEPER-1379,12540327,"'printwatches, redo, history and connect '. client commands always print usage. This is not necessary","while executing the commands:
'printwatches, redo, history and connect usage is getting print 
.basically we are printing usage if user has entered the command 
wrong but in these commands case every time usage is getting print.
eg
{noformat}
[zk: localhost:2181(CONNECTED) 0] printwatches
printwatches is on
ZooKeeper -server host:port cmd args
	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	delquota [-n|-b] path
	quit 
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close 
	ls2 path [watch]
	history 
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path
{noformat}",eribeiro,kavita sharma,Minor,Closed,Fixed,30/Jan/12 08:43,13/Mar/14 18:17
Bug,ZOOKEEPER-1380,12540397,zkperl: _zk_release_watch doesn't remove items properly from the watch list,"The doubly linked list of watches is not updated properly if a watch is taken out from the middle of the chain.
The item after the item which is taken out will receive null pointer for the previous element! This will make the doubly linked list inconsistent and can lead to segfault or infinite loop when the doubly linked list is iterated later.",botond.hejj,botond.hejj,Major,Resolved,Fixed,30/Jan/12 17:14,07/Sep/12 11:01
Bug,ZOOKEEPER-1382,12540477,Zookeeper server holds onto dead/expired session ids in the watch data structures,"I've observed that zookeeper server holds onto expired session ids in the watcher data structures. The result is the wchp command reports session ids that cannot be found through cons/dump and those expired session ids sit there maybe until the server is restarted. Here are snippets from the client and the server logs that lead to this state, for one particular session id 0x134485fd7bcb26f -

There are 4 servers in the zookeeper cluster - 223, 224, 225 (leader), 226 and I'm using ZkClient to connect to the cluster

From the application log -

application.log.2012-01-26-325.gz:2012/01/26 04:56:36.177 INFO [ClientCnxn] [main-SendThread(223.prod:12913)] [application Session establishment complete on server 223.prod/172.17.135.38:12913, sessionid = 0x134485fd7bcb26f, negotiated timeout = 6000
application.log.2012-01-27.gz:2012/01/27 09:52:37.714 INFO [ClientCnxn] [main-SendThread(223.prod:12913)] [application] Client session timed out, have not heard from server in 9827ms for sessionid 0x134485fd7bcb26f, closing socket connection and attempting reconnect
application.log.2012-01-27.gz:2012/01/27 09:52:38.191 INFO [ClientCnxn] [main-SendThread(226.prod:12913)] [application] Unable to reconnect to ZooKeeper service, session 0x134485fd7bcb26f has expired, closing socket connection

On the leader zk, 225 -

zookeeper.log.2012-01-27-leader-225.gz:2012-01-27 09:52:34,010 - INFO  [SessionTracker:ZooKeeperServer@314] - Expiring session 0x134485fd7bcb26f, timeout of 6000ms exceeded
zookeeper.log.2012-01-27-leader-225.gz:2012-01-27 09:52:34,010 - INFO  [ProcessThread:-1:PrepRequestProcessor@391] - Processed session termination for sessionid: 0x134485fd7bcb26f

On the server, the client was initially connected to, 223 -

zookeeper.log.2012-01-26-223.gz:2012-01-26 04:56:36,173 - INFO  [CommitProcessor:1:NIOServerCnxn@1580] - Established session 0x134485fd7bcb26f with negotiated timeout 6000 for client /172.17.136.82:45020
zookeeper.log.2012-01-27-223.gz:2012-01-27 09:52:34,018 - INFO  [CommitProcessor:1:NIOServerCnxn@1435] - Closed socket connection for client /172.17.136.82:45020 which had sessionid 0x134485fd7bcb26f

Here are the log snippets from 226, which is the server, the client reconnected to, before getting session expired event -

2012-01-27 09:52:38,190 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12913:NIOServerCnxn@770] - Client attempting to renew session 0x134485fd7bcb26f at /172.17.136.82:49367
2012-01-27 09:52:38,191 - INFO  [QuorumPeer:/0.0.0.0:12913:NIOServerCnxn@1573] - Invalid session 0x134485fd7bcb26f for client /172.17.136.82:49367, probably expired
2012-01-27 09:52:38,191 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12913:NIOServerCnxn@1435] - Closed socket connection for client /172.17.136.82:49367 which had sessionid 0x134485fd7bcb26f

wchp output from 226, taken on 01/30 -

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *226.*wchp* | wc -l
3

wchp output from 223, taken on 01/30 -

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *223.*wchp* | wc -l
0

cons output from 223 and 226, taken on 01/30 -

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *226.*cons* | wc -l
0

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *223.*cons* | wc -l
0

So, what seems to have happened is that the client was able to re-register the watches on the new server (226), after it got disconnected from 223, inspite of having an expired session id. 


In NIOServerCnxn, I saw that after suspecting that a session is expired, a server removes the cnxn and its watches from its internal data structures. But before that it allows more requests to be processed even if the session is expired -

            // Now that the session is ready we can start receiving packets
            synchronized (this.factory) {
                sk.selector().wakeup();
                enableRecv();
            }
        } catch (Exception e) {
            LOG.warn(""Exception while establishing session, closing"", e);
            close();
        }

I wonder if the client somehow sneaked in the set watches, right after the server removed the connection through removeCnxn() API ?
",abranzyck,nehanarkhede,Critical,Closed,Fixed,31/Jan/12 01:06,14/Oct/16 05:47
Bug,ZOOKEEPER-1384,12540662,test-cppunit overrides LD_LIBRARY_PATH and fails if gcc is in non-standard location,"On Linux with gcc installed in /usr/local and the libs in /usr/local/lib64, test-core-cppunit fails because zktest-st is unable to find the right libstdc++.

build.xml is overriding the environment LD_LIBRARY_PATH instead of appending to it. This should be changed to match the treatment of PATH by appending the desired extra path.",shrauner,shrauner,Minor,Resolved,Fixed,01/Feb/12 00:51,19/Mar/12 11:00
Bug,ZOOKEEPER-1386,12541121,"avoid flaky URL redirection in ""ant javadoc"" : replace ""http://java.sun.com/javase/6/docs/api/"" with ""http://download.oracle.com/javase/6/docs/api/"" ","It seems that the current javadoc.link.java value, http://java.sun.com/javase/6/docs/api/, redirects (via HTTP 301) to http://download.oracle.com/javase/6/docs/api/. This redirect does not always work apparently, causing the URL fetch to fail. This causes an additional javadoc warning: 

javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list

which can in turn cause Jenkins to give a -1 to an otherwise OK build (see e.g. https://issues.apache.org/jira/browse/ZOOKEEPER-1373?focusedCommentId=13199456&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13199456). 

",ekoontz,ekoontz,Minor,Resolved,Fixed,03/Feb/12 19:14,28/Feb/12 00:23
Bug,ZOOKEEPER-1387,12541374,Wrong epoch file created,"It looks like line 443 in QuorumPeer [1] may need to change from:

writeLongToFile(CURRENT_EPOCH_FILENAME, acceptedEpoch);

to

writeLongToFile(ACCEPTED_EPOCH_FILENAME, acceptedEpoch);

I only noticed this reading the code, so I may be wrong and I don't know yet if/how this affects the runtime.

[1] https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L443",breed,busjaeger,Minor,Closed,Fixed,06/Feb/12 05:57,13/Mar/14 18:16
Bug,ZOOKEEPER-1388,12541563,Client side 'PathValidation' is missing for the multi-transaction api.,"Multi ops: Op.create(path,..), Op.delete(path, ..), Op.setData(path, ..), 
Op.check(path, ...) apis are not performing the client side path validation and the call will go to the server side and is throwing exception back to the client. 

It would be good to provide ZooKeeper client side path validation for the multi transaction apis. Presently its getting err codes from the server, which is also not properly conveying the cause.

For example: When specified invalid znode path in Op.create, it giving the following exception. This will not be useful to know the actual cause.
{code}
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1174)
	at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:1115)
{code}",rakeshr,rakeshr,Major,Closed,Fixed,07/Feb/12 05:17,13/Mar/14 18:17
Bug,ZOOKEEPER-1392,12542343,Should not allow to read ACL when not authorized to read node,"Not authorized to read, yet still able to list ACL:

[zk: localhost:2181(CONNECTED) 0] getAcl /sasltest/n4
'sasl,'notme@EXAMPLE.COM
: cdrwa
[zk: localhost:2181(CONNECTED) 1] get /sasltest/n4
Exception in thread ""main"" org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /sasltest/n4
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1131)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1160)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:711)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:593)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:365)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)
",lovesf,thw,Major,Closed,Fixed,13/Feb/12 01:45,02/Apr/19 10:40
Bug,ZOOKEEPER-1395,12543551,node-watcher double-free redux,"This is basically the same issue as ZOOKEEPER-888 and ZOOKEEPER-740 (the latter is open as I write this, but it was superseded by the fix that went in with 888). The problem still exists after the ZOOKEEPER-888 patch, however; it's just more difficult to trigger:

1) Zookeeper notices connection loss, schedules watcher_dispatch
2) Zookeeper notices session loss, schedules watcher_dispatch
3) watcher_dispatch runs for connection loss
4) pywatcher is freed due to is_unrecoverable being true
5) watcher_dispatch runs for session loss
6) PyObject_CallObject attempts to run freed pywatcher with varying bad results

The fix is easy, the dispatcher should act on the state it is given, not the state of the world when it runs. (Patch attached). Reliably triggering the crash is tricky due to the race, but it's not theoretical.",novas0x2a,novas0x2a,Critical,Resolved,Fixed,21/Feb/12 20:01,25/Apr/12 23:37
Bug,ZOOKEEPER-1403,12544916,zkCli.sh script quoting issue,"The zkCli.sh script included with zookeeper doesn't quote its parameters
correctly when passing them on to the java program. 

This causes issues with arguments with spaces and such.
",javacruft,javacruft,Minor,Resolved,Fixed,02/Mar/12 11:12,18/Mar/12 11:00
Bug,ZOOKEEPER-1404,12545168,leader election pseudo code probably incorrect,"The pseudo code for leader election in the recipes.html page of 3.4.3 documentation is the following...

{quote}
Let ELECTION be a path of choice of the application. To volunteer to be a leader: 

1.Create znode z with path ""ELECTION/guid-n_"" with both SEQUENCE and EPHEMERAL flags;

2.Let C be the children of ""ELECTION"", and i be the sequence number of z;

3.Watch for changes on ""ELECTION/guid-n_j"", where j is the {color:red}*smallest*{color} sequence number such that j < i and n_j is a znode in C;

Upon receiving a notification of znode deletion: 

1.Let C be the new set of children of ELECTION; 

2.If z is the smallest node in C, then execute leader procedure;

3.Otherwise, watch for changes on ""ELECTION/guid-n_j"", where j is the {color:red}*smallest*{color} sequence number such that j < i and n_j is a znode in C; 
{quote}


I think, in both third steps *highest* should appear instead of {color:red}*smallest*{color}.
",,robvarga,Major,Resolved,Fixed,05/Mar/12 12:05,14/Dec/12 22:11
Bug,ZOOKEEPER-1406,12545302,dpkg init scripts don't restart - missing check_priv_sep_dir,"The included init.d script for dpkg creation doesn't restart.

It exits with the following error:

{quote}
\# /etc/init.d/zookeeper restart
/etc/init.d/zookeeper: 127: check_privsep_dir: not found
{quote}

Also the actual zkServer.sh line in restart has a path of .../bin/ rather than .../sbin/

",ericthecat,ericthecat,Major,Resolved,Fixed,06/Mar/12 09:14,18/Mar/12 11:00
Bug,ZOOKEEPER-1412,12546069,java client watches inconsistently triggered on reconnect,"I've observed an inconsistent behavior in java client watches. The inconsistency relates to the behavior after the client reconnects to the zookeeper ensemble.

After the client reconnects to the ensemble only those watches should trigger which should have been triggered also if the connections was not lost. This means if I watch for changes in node /foo and there is no change there than my watch should not be triggered on reconnecting to the ensemble.
This is not always the case in the java client.

I've debugged the issues and I could locate the case when the watch is always triggered on reconnect. This is consistently happening if I connect to a follower in the ensemble and I don't do any operation which goes through the leader.
Looking at the code I see that the client stores the lastzxid and sends that with its request. This is 0 on startup and will be updated everytime from the server replies. This lastzxid is also sent to the server after reconnect together with watches. The server decides which watch to trigger based on this lastzxid probably because that should mean the last known state of the client. If this lastzxid is 0 than all the watches are triggered.
I've checked why is this lastzxid 0. I thought it shouldn't be since there was already a request to the server to set the watch and in the reply the server could have sent back the zxid but it turns out that it sends just 0. Looking at the server code I see that for requests which doesn't go through the leader the follower server just sends back the same zxid that the client sent.",phunt,botond.hejj,Blocker,Resolved,Fixed,12/Mar/12 09:13,04/Jun/12 23:33
Bug,ZOOKEEPER-1417,12546618,investigate differences in client last zxid handling btw c and java clients,"In ZOOKEEPER-1412 it was identified that the c and java clients handle updating the last zxid seen a bit differently. ZOOKEEPER-1412 fixed a bug associated with this, however there are still some differences that should be investigated.",thawan,phunt,Major,Resolved,Fixed,15/Mar/12 17:05,06/Jun/13 17:21
Bug,ZOOKEEPER-1419,12546694,Leader election never settles for a 5-node cluster,"We have a situation where it seems to my untrained eye that leader election never finishes for a 5-node cluster.  In this test, all nodes are ZK 3.4.3 and running on the same server (listening on different ports, of course).  The nodes have server IDs of 0, 1, 2, 3, 4.  The test brings up the cluster in different configurations, adding in a new node each time.  We embed ZK in our application, so when we shut a node down and restart it with a new configuration, it all happens in a single JVM process.  Here's our server startup code (for the case where there's more than one node in the cluster):

{code}
if (servers.size() > 1) {
    _log.debug(""Starting Zookeeper server in quorum server mode"");

    _quorum_peer = new QuorumPeer();
    synchronized(_quorum_peer) {
        _quorum_peer.setClientPortAddress(clientAddr);
        _quorum_peer.setTxnFactory(log);
        _quorum_peer.setQuorumPeers(servers);
        _quorum_peer.setElectionType(_election_alg);
        _quorum_peer.setMyid(_server_id);
        _quorum_peer.setTickTime(_tick_time);
        _quorum_peer.setInitLimit(_init_limit);
        _quorum_peer.setSyncLimit(_sync_limit);
        QuorumVerifier quorumVerifier =
            new QuorumMaj(servers.size());
        _quorum_peer.setQuorumVerifier(quorumVerifier);
        _quorum_peer.setCnxnFactory(_cnxn_factory);
        _quorum_peer.setZKDatabase(new ZKDatabase(log));
        _quorum_peer.start();
    }
} else {
    _log.debug(""Starting Zookeeper server in single server mode"");
    _zk_server = new ZooKeeperServer();
    _zk_server.setTxnLogFactory(log);
    _zk_server.setTickTime(_tick_time);
    _cnxn_factory.startup(_zk_server);
}
{code}

And here's our shutdown code:

{code}
if (_quorum_peer != null) {
    synchronized(_quorum_peer) {
        _quorum_peer.shutdown();
        FastLeaderElection fle =
            (FastLeaderElection) _quorum_peer.getElectionAlg();
        fle.shutdown();
        try {
            _quorum_peer.getTxnFactory().commit();
        } catch (java.nio.channels.ClosedChannelException e) {
            // ignore
        }
    }
} else {
    _cnxn_factory.shutdown();
    _zk_server.getTxnLogFactory().commit();
}
{code}

The test steps through the following scenarios in quick succession:

Run 1: Start a 1-node cluster, servers=[0]
Run 2: Start a 2-node cluster, servers=[0,3]
Run 3: Start a 3-node cluster, servers=[0,1,3]
Run 4: Start a 4-node cluster, servers=[0,1,2,3]
Run 5: Start a 5-node cluster, servers=[0,1,2,3,4]

It appears that run 5 never elects a leader -- the nodes just keep spewing messages like this (example from node 0):

{noformat}
2012-03-14 16:23:12,775 13308 [WorkerSender[myid=0]] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 2
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Sending Notification: 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), 3 (recipient), 0 (myid), 0x2 (n.peerEpoch)
2012-03-14 16:23:12,776 13309 [WorkerSender[myid=0]] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 3
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Sending Notification: 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), 4 (recipient), 0 (myid), 0x2 (n.peerEpoch)
2012-03-14 16:23:12,776 13309 [WorkerSender[myid=0]] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 4
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 0
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 4 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x0 (n.peerEPoch), LOOKING (my state)
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 0
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x2 (n.peerEPoch), LOOKING (my state)
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Adding vote: from=1, proposed leader=3, proposed zxid=0x0, proposed election epoch=0x1
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 3, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 3, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 3, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 4, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 4, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
{noformat}

I'm guessing this means that nodes 3 and 4 are fighting over leadership, but I don't know enough about the leader election code to debug this any further.  Attaching a tarball with the logs for each run and the data directories for each node (though I don't think any data is being written to ZK during the test).",fpj,strib,Blocker,Resolved,Fixed,16/Mar/12 00:07,20/Mar/12 01:19
Bug,ZOOKEEPER-1427,12547093,Writing to local files is done non-atomically,"Currently, the writeLongToFile() function opens the file for truncate, writes the new data, syncs, and then closes. If the process crashes after opening the file but before writing the new data, the file may be left empty, causing ZK to ""forget"" an earlier promise. Instead, it should use RandomAccessFile to avoid truncating.",phunt,tlipcon,Critical,Resolved,Fixed,19/Mar/12 21:25,18/Jul/12 11:01
Bug,ZOOKEEPER-1431,12547757,zkpython: async calls leak memory,"I'm seeing a memory leakage when using the ""aget"" method.

It leaks tuples and dicts, both containing ""stats"".

",kapilt,jrydberg,Major,Resolved,Fixed,23/Mar/12 07:11,19/Jun/12 11:00
Bug,ZOOKEEPER-1437,12548598,Client uses session before SASL authentication complete,"Found issue in the context of hbase region server startup, but can be reproduced w/ zkCli alone.

getData may occur prior to SaslAuthenticated and fail with NoAuth. This is not expected behavior when the client is configured to use SASL.
",ekoontz,thw,Major,Resolved,Fixed,29/Mar/12 02:09,18/Feb/16 12:31
Bug,ZOOKEEPER-1439,12548989,c sdk: core in log_env for lack of checking the output argument *pwp* of getpwuid_r,"Man of getpwuid_r ""return a pointer to a passwd structure, or NULL if the matching entry is not found or an error occurs"",
""The getpwnam_r() and getpwuid_r() functions return zero on success."", it means entry may not be found when getpwuid_r success.

In log_env of zookeeper.c in c sdk:
  {{if (!getpwuid_r(uid, &pw, buf, sizeof(buf), &pwp)) {}}
    {{LOG_INFO((""Client environment:user.home=%s"", pw.pw_dir));}}
  {{}}}
pwp is not checked to ensure entry is found, pw.pw_dir is not initialized in this case, core happens in LOG_INFO.",yin.yubing@gmail.com,yin.yubing@gmail.com,Major,Resolved,Fixed,01/Apr/12 08:15,27/Apr/12 11:00
Bug,ZOOKEEPER-1440,12549021,Spurious log error messages when QuorumCnxManager is shutting down,"When shutting down the QuroumPeer, ZK server logs unnecessary errors. See QuorumCnxManager.Listener.run() - ss.accept() will throw an exception when it is closed. The catch (IOException e) will log errors. It should first check the shutdown field to see if the Listener is being shutdown. If it is, the exception is correct and no errors should be logged.",randgalt,randgalt,Minor,Resolved,Fixed,01/Apr/12 20:28,12/Mar/14 23:32
Bug,ZOOKEEPER-1448,12551259,Node+Quota creation in transaction log can crash leader startup,"Hi,

I've found a bug in zookeeper related to quota creation which can shutdown zookeeper leader on startup.

Steps to reproduce:
1. create /quota_bug
2. setquota -n 10000 /quota_bug
3. stop the whole ensemble (the previous operations should be in the transaction log)
4. start all the servers
5. the elected leader will shutdown with an exception (Missing stat node for count /zookeeper/quota/quota_bug/zookeeper_
stats)

I've debugged a bit what happening and I found the following problem:
On startup each server loads the last snapshot and replays the last transaction log. While doing this it fills up the pTrie variable of the DataTree with the path of the nodes which have quota.
After the leader is elected the leader servers loads the snapshot and last transaction log but it doesn't clean up the pTrie variable. This means it still contains the ""/quota_bug"" path. Now when the ""create /quota_bug"" is processed from the transaction log the DataTree already thinks that the quota nodes (""/zookeeper/quota/quota_bug/zookeeper_limits"" and ""/zookeeper/quota/quota_bug/zookeeper_stats"") are created but those node creation actually comes later in the transaction log. This leads to the missing stat node exception.

I think clearing the pTrie should solve this problem.

",fpj,botond.hejj,Critical,Closed,Fixed,17/Apr/12 16:15,13/Mar/14 18:17
Bug,ZOOKEEPER-1451,12551652,C API improperly logs getaddrinfo failures on Linux when using glibc,"This is how the code currently logs getaddrinfo errors:

{quote}
                errno = getaddrinfo_errno(rc);
                LOG_ERROR((""getaddrinfo: %s\n"", strerror(errno)));
{quote}

On Linux, specifically when using glibc, there is a better function for logging getaddrinfo errors called gai_strerror. An example:

{quote}
                LOG_ERROR((""getaddrinfo: %s\n"", gai_strerror(rc)));
{quote}

It doesn't miss a lot of cases like the errno based version does.",tyree731,tyree731,Trivial,Resolved,Fixed,19/Apr/12 18:04,25/Apr/12 20:29
Bug,ZOOKEEPER-1460,12553409,IPv6 literal address not supported for quorum members,"Via code inspection, I see that the ""server.nnn"" configuration key does not support literal IPv6 addresses because the property value is split on "":"". In v3.4.3, the problem is in QuorumPeerConfig:

{noformat}
String parts[] = value.split("":"");
InetSocketAddress addr = new InetSocketAddress(parts[0],
                        Integer.parseInt(parts[1]));
{noformat}

In the current trunk (http://svn.apache.org/viewvc/zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java?view=markup) this code has been refactored into QuorumPeer.QuorumServer, but the bug remains:

{noformat}
String serverClientParts[] = addressStr.split("";"");
String serverParts[] = serverClientParts[0].split("":"");
addr = new InetSocketAddress(serverParts[0],
                        Integer.parseInt(serverParts[1]));
{noformat}

This bug probably affects very few users because most will naturally use a hostname rather than a literal IP address. But given that IPv6 addresses are supported for clients via ZOOKEEPER-667 it seems that server support should be fixed too.",joe@kafsemo.org,cdolan,Major,Closed,Fixed,30/Apr/12 19:49,21/Jul/16 20:18
Bug,ZOOKEEPER-1462,12553696,Read-only server does not initialize database properly,"Brief Description:
When a participant or observer get partitioned and restart as Read-only server. ZkDb doesn't get reinitialized. This causes the RO server to drop any incoming request with zxid > 0 

Error message:
Refusing session request for client /xx.xx.xx.xx:39875 
as it has seen zxid 0x2e00405fd9 our last zxid is 0x0 client must try another server

Steps to reproduce:
Start an RO-enabled observer connecting to an ensemble. Kill the ensemble and wait until the observer restart in RO mode. Zxid of this observer should be 0.

Description:
Before a server transition into LOOKING state, its database get closed as part of shutdown sequence. The database of leader, follower and observer get initialized as a side effect of participating in leader election protocol. (eg. observer will call registerWithLeader() and call getLastLoggedZxid() which initialize the db if not already).

However, RO server does not participate in this protocol so its DB doesn't get initialized properly
 ",thawan,thawan,Critical,Closed,Fixed,03/May/12 01:37,13/Mar/14 18:16
Bug,ZOOKEEPER-1465,12554690,Cluster availability following new leader election takes a long time with large datasets - is correlated to dataset size,"When re-electing a new leader of a cluster, it takes a long time for the cluster to become available if the dataset is large

Test Data
----------
650mb snapshot size
20k nodes of varied size 
3 member cluster 

On 3.4.x branch (http://svn.apache.org/repos/asf/zookeeper/branches/branch-3.4?r=1244779)
------------------------------------------------------------------------------------------

Takes 3-4 minutes to bring up a cluster from cold 
Takes 40-50 secs to recover from a leader failure 
Takes 10 secs for a new follower to join the cluster 

Using the 3.3.5 release on the same hardware with the same dataset
-----------------------------------------------------------------

Takes 10-20 secs to bring up a cluster from cold 
Takes 10 secs to recover from a leader failure 
Takes 10 secs for a new follower to join the cluster 

I can see from the logs in 3.4.x that once a new leader is elected, it pushes a new snapshot to each of the followers who need to save it before they ack the leader who can then mark the cluster as available. 

The kit being used is a low spec vm so the times taken are not relevant per se - more the fact that a snapshot is always sent even through there is no difference between the persisted state on each peer.
No data is being added to the cluster while the peers are being restarted.






",fournc,alexgvozdenovic,Critical,Resolved,Fixed,10/May/12 14:47,18/Jul/12 00:33
Bug,ZOOKEEPER-1466,12555834,QuorumCnxManager.shutdown missing synchronization,org.apache.zookeeper.server.quorum.QuorumCnxManager.shutdown is not being synchronized even though it's accessed by multiple threads.,phunt,phunt,Blocker,Resolved,Fixed,15/May/12 17:21,30/Jun/12 11:01
Bug,ZOOKEEPER-1471,12556625,Jute generates invalid C++ code,"There are 2 issues with the current jute generated C++ code.

1. Variable declaration for JRecord is incorrect. It looks something like this:
{code} 
    Id id;
{code}
It should be like this instead:
{code} 
 org::apache::zookeeper::data::Id mid;
{code}

2. The header file declares all the variables (except for JRecord ones) with ""m"" prefix, but the .cc file doesn't use the prefix. ",michim,michim,Minor,Resolved,Fixed,21/May/12 01:10,30/Jun/12 11:01
Bug,ZOOKEEPER-1474,12558607,Cannot build Zookeeper with IBM Java: use of Sun MXBean classes,"zookeeper.server.NIOServerCnxn and zookeeper.server.NettyServerCnxn imports com.sun.management.UnixOperatingSystemMXBean . This OperatingSystemMXBean class is not implemented by IBM or open java. 

In my case, I need IBM Java so I can run zookeeper in Power ppc64 servers.",pvital,adalbas,Major,Closed,Fixed,30/May/12 14:31,13/Mar/14 18:17
Bug,ZOOKEEPER-1478,12559070,Small bug in QuorumTest.testFollowersStartAfterLeader( ),"The following code appears in QuorumTest.testFollowersStartAfterLeader( ):

for (int i = 0; i < 30; i++) {
    try {
       zk.create(""/test"", ""test"".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,
                 CreateMode.PERSISTENT);
       break;
     } catch(KeeperException.ConnectionLossException e) {
       Thread.sleep(1000);
     }
    // test fails if we still can't connect to the quorum after 30 seconds.
    Assert.fail(""client could not connect to reestablished quorum: giving up after 30+ seconds."");
}

From the comment it looks like the intention was to try to reconnect 30 times and only then trigger the Assert, but that's not what this does.
After we fail to connect once and Thread.sleep is executed, Assert.fail will be executed without retrying create. ",shralex,shralex,Minor,Closed,Fixed,03/Jun/12 03:03,13/Mar/14 18:16
Bug,ZOOKEEPER-1483,12560240,Fix leader election recipe documentation,"The leader election recipe documentation suggest that to avoid the herd effect a client process volunteering for leadership via child znode [i] under the leader election path [/leader] must only watch the the SMALLEST znode [j] from a different client process such that [j < i]. 

This will NOT avoid the herd effect as many clients will end up watching the same znode[j] where j is the next-in-sequence number greater than the number of the current leader.

Specifically in Step 3 of the Election procedure here http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection

This ""where j is the SMALLEST sequence number"" should be changed to this
""where j is the LARGEST sequence number""
",michim,ankur,Major,Resolved,Fixed,11/Jun/12 21:09,14/Dec/12 22:11
Bug,ZOOKEEPER-1489,12594918,Data loss after truncate on transaction log,"The truncate method on the transaction log in the class org.apache.zookeeper.server.persistence.FileTxnLog will reduce the file size to the required amount without either closing or re-positioning the logStream (which could also be dangerous since the truncate method is not synchronized against concurrent writes to the log).

This causes the next append to that log to create a small ""hole"" in the file which java would interpret as binary zeroes when reading it. This then causes to the FileTxnIterator.next() implementation to detect the end of the log file too early.

I'll attach a small maven project with one junit test which can be used to reproduce the issue. Due to the blackbox nature of the test it will run for roughly 50 seconds unfortunately. 

Steps to reproduce:
- Start an ensemble of zookeeper servers with at least 3 participants
- Create one entry and the remove one of the servers from the ensemble temporarily (e.g. zk-2)
- Create another entry which is hence only reflected on zk-1 and zk-3
- Take zk-1 out of the ensemble without shutting it down (that is important, I did that by interrupting the network connection to that node) and clean zk-3
- Bring back zk-2 and zk-3 so that they form a quorum
- Allow zk-1 to connect again
- zk-1 will receive a TRUNC message from zk-2 since zk-1 is now a minority knowing about that second node creation event
- Create a third node
- Force zk-1 to become master somehow
- That third node will be gone
",phunt,christianz,Blocker,Resolved,Fixed,18/Jun/12 09:09,18/Jul/12 11:01
Bug,ZOOKEEPER-1490,12595049, If the configured log directory does not exist zookeeper will not start. Better to create the directory and start,"if the configured log directory does not exists zookeeper will not start. Better to create the directory and start
in zkEnv.sh we can change as follows

if [ ""x${ZOO_LOG_DIR}"" = ""x"" ]
then
    ZOO_LOG_DIR="".""
   else
    if [ ! -w ""$ZOO_LOG_DIR"" ] ; then
        mkdir -p ""$ZOO_LOG_DIR""
    fi
fi
 

",suja,suja,Minor,Resolved,Fixed,19/Jun/12 04:10,30/Jun/12 11:01
Bug,ZOOKEEPER-1493,12595339,C Client: zookeeper_process doesn't invoke completion callback if zookeeper_close has been called,"In ZOOKEEPER-804, we added a check in zookeeper_process() to see if zookeeper_close() has been called. This was to avoid calling assert(cptr) on a NULL pointer, as dequeue_completion() returns NULL if the sent_requests queue has been cleared by free_completion() from zookeeper_close(). However, we should still call the completion if it is not NULL. ",michim,michim,Major,Resolved,Fixed,20/Jun/12 20:47,21/Nov/12 10:12
Bug,ZOOKEEPER-1494,12595623,C client: socket leak after receive timeout in zookeeper_interest(),"In zookeeper_interest(), we set zk->fd to -1 without closing it when timeout happens. Instead we should let handle_socket_error_msg() function take care of closing the socket properly.

--Michi",michim,michim,Major,Resolved,Fixed,22/Jun/12 19:56,10/Sep/12 11:01
Bug,ZOOKEEPER-1495,12596218,ZK client hangs when using a function not available on the server.,"This happens for example when using zk#multi with a 3.4 client but a 3.3 server.

The issue seems to be on the server side: the servers drops the packets with an unknown OpCode in ZooKeeperServer#submitRequest
{noformat}
public void submitRequest(Request si) {
    // snip
    try {
        touch(si.cnxn);
        boolean validpacket = Request.isValid(si.type); // ===> Check on case OpCode.*
        if (validpacket) {
            // snip
        } else {
            LOG.warn(""Dropping packet at server of type "" + si.type);
            // if invalid packet drop the packet.
        }
    } catch (MissingSessionException e) {
        if (LOG.isDebugEnabled()) {
            LOG.debug(""Dropping request: "" + e.getMessage());
        }
    }
}
{noformat}

The solution discussed in ZOOKEEPER-1381 would be to get an exception on the client side then & close the session.",nkeywal,nkeywal,Minor,Closed,Fixed,28/Jun/12 07:51,13/Mar/14 18:16
Bug,ZOOKEEPER-1496,12596235,Ephemeral node not getting cleared even after client has exited,"In one of the tests we performed, came across a case where the ephemeral node was not getting cleared from zookeeper though the client exited.

Zk version: 3.4.3

Ephemeral node still exists in Zookeeper: 

HOST-xx-xx-xx-55:/home/Jun25_LR/install/zookeeper/bin # date 

Tue Jun 26 16:07:04 IST 2012 
HOST-xx-xx-xx-55:/home/Jun25_LR/install/zookeeper/bin # ./zkCli.sh -server xx.xx.xx.55:2182 
Connecting to xx.xx.xx.55:2182 
Welcome to ZooKeeper! 
JLine support is enabled 
[zk: xx.xx.xx.55:2182(CONNECTING) 0] 
WATCHER:: 

WatchedEvent state:SyncConnected type:None path:null 

[zk: xx.xx.xx.55:2182(CONNECTED) 0] get /hadoop-ha/hacluster/ActiveStandbyElectorLock 

haclusternn2HOSt-xx-xx-xx-102 ï¿½ï¿½ 
cZxid = 0x200000075 
ctime = Tue Jun 26 13:10:19 IST 2012 
mZxid = 0x200000075 
mtime = Tue Jun 26 13:10:19 IST 2012 
pZxid = 0x200000075 
cversion = 0 
dataVersion = 0 
aclVersion = 0 
ephemeralOwner = 0x1382791d4e50004 
dataLength = 42 
numChildren = 0 
[zk: xx.xx.xx.55:2182(CONNECTED) 1] 

Grepped logs at ZK side for session ""0x1382791d4e50004"" - close session and later create coming before closesession processed. 

HOSt-xx-xx-xx-91:/home/Jun25_LR/install/zookeeper/logs # grep -E ""/hadoop-ha/hacluster/ActiveStandbyElectorLock|0x1382791d4e50004"" *|grep 0x200000074 
2012-06-26 13:10:18,834 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::CommitProcessor@171] - Processing request:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 
2012-06-26 13:10:19,892 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::Leader@716] - Proposing:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 
2012-06-26 13:10:19,919 [myid:3] - DEBUG [LearnerHandler-/xx.xx.xx.102:13846:CommitProcessor@161] - Committing request:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 
2012-06-26 13:10:20,608 [myid:3] - DEBUG [CommitProcessor:3:FinalRequestProcessor@88] - Processing request:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 

HOSt-xx-xx-xx-91:/home/Jun25_LR/install/zookeeper/logs # grep -E ""/hadoop-ha/hacluster/ActiveStandbyElectorLock|0x1382791d4e50004"" *|grep 0x200000075 
2012-06-26 13:10:19,893 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::CommitProcessor@171] - Processing request:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 
2012-06-26 13:10:19,920 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::Leader@716] - Proposing:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 
2012-06-26 13:10:20,278 [myid:3] - DEBUG [LearnerHandler-/xx.xx.xx.102:13846:CommitProcessor@161] - Committing request:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 
2012-06-26 13:10:20,752 [myid:3] - DEBUG [CommitProcessor:3:FinalRequestProcessor@88] - Processing request:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 


 Close session and create requests coming almost parallely. 


Env:
Hadoop setup.
We were using Namenode HA with bookkeeper as shared storage and auto failover enabled.
NN102 was active and NN55 was standby. 
FailoverController at 102 got shut down due to ZK connection error. 
The lock-ActiveStandbyElectorLock created (ephemeral node) by this failovercontroller is not cleared from ZK
",rakeshr,suja,Critical,Resolved,Fixed,28/Jun/12 10:24,17/Sep/12 11:02
Bug,ZOOKEEPER-1499,12597361,clientPort config changes not backwards-compatible,"With the new reconfig logic, clientPort=2181 in the zoo.cfg file no longer gets read, and clients can't connect without adding ;2181 to the end of their server lines. ",shralex,fournc,Blocker,Resolved,Fixed,03/Jul/12 22:33,24/Oct/13 11:08
Bug,ZOOKEEPER-1501,12597491,Nagios plugin always returns OK when it cannot connect to zookeeper,Returning OK under such conditions is really not good...,jinty,jinty,Major,Resolved,Fixed,04/Jul/12 17:14,07/Sep/12 11:01
Bug,ZOOKEEPER-1513,12599557,"""Unreasonable length"" exception while starting a server.","The server is allowing a client to set data larger than the server can then later read:

{noformat}
2012-07-18 14:28:12,555 - FATAL [main:QuorumPeer@400] - Unable to load database on disk 
java.io.IOException: Unreasonable length = 1048583 
at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:100) 
at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:232) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:602) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504) 
at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341) 
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:131) 
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222) 
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76) 
2012-07-18 14:28:12,555 - FATAL [main:QuorumPeerMain@87] - Unexpected exception, exiting abnormally 
java.lang.RuntimeException: Unable to run quorum server 
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:401) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76) 
Caused by: java.io.IOException: Unreasonable length = 1048583 
at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:100) 
at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:232) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:602) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504) 
at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341) 
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:131) 
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222) 
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398) 
... 3 more
{noformat}

Notice the size is 0x100007 - 7 bytes beyond.

The SetDataTxn contains the client data + a couple extra fields. On ingest the server is applying the jute.maxbuffer size to the data (expected) but not handling the fact that the data plus these extra fields may exceed the jute.maxbuffer check when reading from disk.

Workaround was simple here: set the jute.maxbuffer size a bit higher (and fix the mis-behaving client, expectation was not that the data would grow this large).
",skye,phunt,Major,Closed,Fixed,20/Jul/12 00:38,13/Mar/14 18:16
Bug,ZOOKEEPER-1514,12599561,FastLeaderElection - leader ignores the round information when joining a quorum,"In the following case we have a 3 server ensemble.

Initially all is well, zk3 is the leader.

However zk3 fails, restarts, and rejoins the quorum as the new leader (was the old leader, still the leader after re-election)

The existing two followers, zk1 and zk2 rejoin the new quorum again as followers of zk3.

zk1 then fails, the datadirectory is deleted (so it has no state whatsoever) and restarted. However zk1 can never rejoin the quorum (even after an hour). During this time zk2 and zk3 are serving properly.

Later all three servers are later restarted and properly form a functional quourm.


Here are some interesting log snippets. Nothing else of interest was seen in the logs during this time:

zk3. This is where it becomes the leader after failing initially (as the leader). Notice the ""round"" is ahead of zk1 and zk2:

{noformat}
2012-07-18 17:19:35,423 - INFO  [QuorumPeer:/0.0.0.0:2181:FastLeaderElection@663] - New election. My id =  3, Proposed zxid = 77309411648
2012-07-18 17:19:35,423 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 77309411648 (n.zxid), 832 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-18 17:19:35,424 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 73014444480 (n.zxid), 831 (n.round), FOLLOWING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-18 17:19:35,424 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 73014444480 (n.zxid), 831 (n.round), FOLLOWING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-18 17:19:35,424 - INFO  [QuorumPeer:/0.0.0.0:2181:QuorumPeer@655] - LEADING
{noformat}

zk1 which won't come back. Notice that zk3 is reporting the round as 831, while zk2 thinks that the round is 832:

{noformat}
2012-07-18 17:31:12,015 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 77309411648 (n.zxid), 1 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-18 17:31:12,016 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 73014444480 (n.zxid), 831 (n.round), LEADING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-18 17:31:12,017 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 77309411648 (n.zxid), 832 (n.round), FOLLOWING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-18 17:31:15,219 - INFO  [QuorumPeer:/0.0.0.0:2181:FastLeaderElection@697] - Notification time out: 6400
{noformat}
",fpj,phunt,Critical,Resolved,Fixed,20/Jul/12 00:52,03/Aug/12 10:55
Bug,ZOOKEEPER-1518,12600136,Mailing List link is broken in the Zookeeper documentation,"Mailing List link under Miscellaneous section from the Zookeeper documentation is broken.
Following is the link:
http://zookeeper.apache.org/mailing_lists.html",phunt,kiran_bc,Major,Resolved,Fixed,25/Jul/12 09:28,01/Aug/12 19:09
Bug,ZOOKEEPER-1521,12600335,LearnerHandler initLimit/syncLimit problems specifying follower socket timeout limits,"branch 3.3: The leader is expecting the follower to initialize in syncLimit time rather than initLimit. In LearnerHandler run line 395 (branch33) we look for the ack from the follower with a timeout of syncLimit.

branch 3.4+: seems like ZOOKEEPER-1136 introduced a regression while attempting to fix the problem. It sets the timeout as initLimit however it never sets the timeout to syncLimit once the ack is received.",phunt,phunt,Critical,Resolved,Fixed,26/Jul/12 15:44,29/Jul/12 11:02
Bug,ZOOKEEPER-1522,12600741,intermittent failures in Zab test due to NPE in recursiveDelete test function,"The jdk7 test job on jenkins is failing intermittently with 

{noformat}
java.lang.NullPointerException
	at org.apache.zookeeper.server.quorum.Zab1_0Test.recursiveDelete(Zab1_0Test.java:917)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.recursiveDelete(Zab1_0Test.java:918)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.recursiveDelete(Zab1_0Test.java:918)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.testPopulatedLeaderConversation(Zab1_0Test.java:419)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.testUnnecessarySnap(Zab1_0Test.java:483)
{noformat}

Seems to not be handling the case where the file is deleted out from under. Also the recursive deletes should be at the very end of the finally I would think.",phunt,phunt,Major,Resolved,Fixed,30/Jul/12 22:27,01/Aug/12 21:12
Bug,ZOOKEEPER-1533,12603185,Correct the documentation of the args for the JavaExample doc.,Small doc fix in the JavaExample doc.,wturkal,wturkal,Minor,Resolved,Fixed,13/Aug/12 06:50,03/Mar/16 01:35
Bug,ZOOKEEPER-1535,12603538,ZK Shell/Cli re-executes last command on exit,"In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell. In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run. 
{noformat}
[zk: zookeeper.example.com:2181(CONNECTED) 0] ls /blah
[foo]
[zk: zookeeper.example.com:2181(CONNECTED) 1] <ctrl+d> [foo]
$
{noformat}",eribeiro,stuhood,Major,Closed,Fixed,14/Aug/12 23:56,20/May/17 23:07
Bug,ZOOKEEPER-1536,12603682,c client : memory leak in winport.c,"At line 99 in winport.c, use windows API ""InitializeCriticalSection"" but never call ""DeleteCriticalSection""",brook,brook,Major,Resolved,Fixed,16/Aug/12 03:13,31/Aug/12 11:02
Bug,ZOOKEEPER-1538,12604306,Improve space handling in zkServer.sh and zkEnv.sh,"Running `bin/zkServer.sh start` from a freshly-built copy of trunk fails if the source code is checked-out to a directory with spaces in the name. I'll include a small fix to fix this problem.

thanks!",adferguson,adferguson,Trivial,Resolved,Fixed,22/Aug/12 00:00,25/Jun/13 18:07
Bug,ZOOKEEPER-1540,12604651,ZOOKEEPER-1411 breaks backwards compatibility,"There is a one-line bug in ZOOKEEPER-1411 which breaks backwards compatibility for sites which are using separate configuration files for each server. The bug is with the handling of the clientPort option.

One line fix to follow shortly.

thanks!
Andrew",adferguson,adferguson,Major,Resolved,Fixed,23/Aug/12 17:51,03/Mar/16 01:34
Bug,ZOOKEEPER-1550,12609272,ZooKeeperSaslClient does not finish anonymous login on OpenJDK,"On OpenJDK, {{javax.security.auth.login.Configuration.getConfiguration}} does not throw an exception.  {{ZooKeeperSaslClient.clientTunneledAuthenticationInProgress}} uses an exception from that method as a proxy for ""this client is not configured to use SASL"" and as a result no commands can be sent, since it is still waiting for auth to complete.

[Link to mailing list discussion|http://comments.gmane.org/gmane.comp.java.zookeeper.user/2667]

The relevant bit of logs from OpenJDK and Oracle versions of 'connect and do getChildren(""/"")':

{code:title=OpenJDK}
INFO [main] 2012-09-25 14:02:24,545 com.socrata.Main Waiting for connection...
DEBUG [main] 2012-09-25 14:02:24,548 com.socrata.zookeeper.ZooKeeperProvider Waiting for connected-state...
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,576 org.apache.zookeeper.ClientCnxn Opening socket connection to server mike.local/10.0.2.106:2181. Will not attempt to authenticate using SASL (unknown error)
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,584 org.apache.zookeeper.ClientCnxn Socket connection established to mike.local/10.0.2.106:2181, initiating session
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,586 org.apache.zookeeper.ClientCnxn Session establishment request sent on mike.local/10.0.2.106:2181
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,600 org.apache.zookeeper.ClientCnxn Session establishment complete on server mike.local/10.0.2.106:2181, sessionid = 0x139ff2e85b60005, negotiated timeout = 40000
DEBUG [main-EventThread] 2012-09-25 14:02:24,614 com.socrata.zookeeper.ZooKeeperProvider ConnectionStateChanged(Connected)
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,636 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:37,923 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:37,924 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:37,924 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,260 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,260 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,261 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,261 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,261 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,265 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,265 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,266 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,266 org.apache.zookeeper.ClientCnxn Client session timed out, have not heard from server in 26668ms for sessionid 0x139ff2e85b60005, closing socket connection and attempting reconnect
DEBUG [main-EventThread] 2012-09-25 14:02:51,377 com.socrata.zookeeper.ZooKeeperProvider ConnectionStateChanged(Disconnected)
{code}

{code:title=Oracle}
INFO [main] 2012-09-25 14:03:16,315 com.socrata.Main Waiting for connection...
DEBUG [main] 2012-09-25 14:03:16,319 com.socrata.zookeeper.ZooKeeperProvider Waiting for connected-state...
INFO [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,335 org.apache.zookeeper.ClientCnxn Opening socket connection to server 10.0.2.106/10.0.2.106:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
INFO [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,344 org.apache.zookeeper.ClientCnxn Socket connection established to 10.0.2.106/10.0.2.106:2181, initiating session
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,346 org.apache.zookeeper.ClientCnxn Session establishment request sent on 10.0.2.106/10.0.2.106:2181
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,347 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,351 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
INFO [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,368 org.apache.zookeeper.ClientCnxn Session establishment complete on server 10.0.2.106/10.0.2.106:2181, sessionid = 0x139ff2e85b60006, negotiated timeout = 40000
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,371 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,371 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-EventThread] 2012-09-25 14:03:16,385 com.socrata.zookeeper.ZooKeeperProvider ConnectionStateChanged(Connected)
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,417 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,417 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,417 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,418 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,418 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,431 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,438 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,443 org.apache.zookeeper.ClientCnxn Reading reply sessionid:0x139ff2e85b60006, packet:: clientPath:/ serverPath:/ finished:false header:: 1,12  replyHeader:: 1,8292982,0  request:: '/,F  response:: v{'ro,'row-index,'zkbtest,'consumers,'reindex,'hotstandby,'bigdir,'vs,'orestes,'eurybates,'shardedcly,'row-locks,'id-counter,'zookeeper,'cly,'locks,'rwlocks,'tickets,'brokers},s{0,0,0,0,0,61,0,0,0,19,8292893}
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,444 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
OK(Set(cly, row-locks, hotstandby, locks, tickets, bigdir, zkbtest, row-index, reindex, id-counter, eurybates, vs, rwlocks, shardedcly, brokers, consumers, zookeeper, orestes, ro),0,0,0,0,0,61,0,0,0,19,8292893)
{code}",ekoontz,rmac,Blocker,Resolved,Fixed,26/Sep/12 16:32,16/Jan/13 19:00
Bug,ZOOKEEPER-1551,12609708,Observers ignore txns that come after snapshot and UPTODATE ,"In Learner.java, txns which comes after the learner has taken the snapshot (after NEWLEADER packet) are stored in packetsNotCommitted. The follower has special logic to apply these txns at the end of syncWithLeader() method. However, the observer will ignore these txns completely, causing data inconsistency. ",thawan,thawan,Blocker,Closed,Fixed,01/Oct/12 00:57,13/Mar/14 18:17
Bug,ZOOKEEPER-1553,12609819,Findbugs configuration is missing some dependencies,"While updating the findbugs configuration to account for a change in log4j versions I noticed findbugs complaining about access to the netty and slf4j classes.

Steps to reproduce:

# install findbugs to $FINDBUGS_HOME
# run ant -Dfindbugs.home=""$FINDBUGS_HOME"" findbugs

",busbey,busbey,Minor,Closed,Fixed,01/Oct/12 22:00,13/Mar/14 18:16
Bug,ZOOKEEPER-1554,12610086,Can't use zookeeper client without SASL,"The ZooKeeperSaslClient correctly detects that it should not use SASL when nothing is configured, however the SendThread waits forever because clientTunneledAuthenticationInProgress() returns true instead of false.",,gnodet,Blocker,Closed,Fixed,03/Oct/12 15:35,13/Mar/14 18:17
Bug,ZOOKEEPER-1557,12610426,jenkins jdk7 test failure in testBadSaslAuthNotifiesWatch,"Failure of testBadSaslAuthNotifiesWatch on the jenkins jdk7 job:

https://builds.apache.org/job/ZooKeeper-trunk-jdk7/407/

haven't seen this before.",ekoontz,phunt,Major,Closed,Fixed,04/Oct/12 23:03,13/Mar/14 18:17
Bug,ZOOKEEPER-1560,12611273,Zookeeper client hangs on creation of large nodes,"To reproduce, try creating a node with 0.5M of data using java client. The test will hang waiting for a response from the server. See the attached patch for the test that reproduces the issue.

It seems that ZOOKEEPER-1437 introduced a few issues to {{ClientCnxnSocketNIO.doIO}} that prevent {{ClientCnxnSocketNIO}} from sending large packets that require several invocations of {{SocketChannel.write}} to complete. The first issue is that the call to {{outgoingQueue.removeFirstOccurrence(p);}} removes the packet from the queue even if the packet wasn't completely sent yet.  It looks to me that this call should be moved under {{if (!pbb.hasRemaining())}} The second issue is that {{p.createBB()}} is reinitializing {{ByteBuffer}} on every iteration, which confuses {{SocketChannel.write}}. And the third issue is caused by extra calls to {{cnxn.getXid()}} that increment xid on every iteration and confuse the server.
",skye,imotov,Major,Resolved,Fixed,10/Oct/12 23:45,31/Oct/12 23:00
Bug,ZOOKEEPER-1562,12611660,Memory leaks in zoo_multi API,"Valgrind is reporting memory leak for zoo_multi operations.

==4056== 2,240 (160 direct, 2,080 indirect) bytes in 1 blocks are definitely lost in loss record 18 of 24
==4056==    at 0x4A04A28: calloc (vg_replace_malloc.c:467)
==4056==    by 0x504D822: create_completion_entry (zookeeper.c:2322)
==4056==    by 0x5052833: zoo_amulti (zookeeper.c:3141)
==4056==    by 0x5052A8B: zoo_multi (zookeeper.c:3240)

It looks like completion entries for individual operations in multiupdate transaction are not getting freed. My observation is that memory leak size depends on the number of operations in single mutlipupdate transaction",djagtap,djagtap,Trivial,Closed,Fixed,13/Oct/12 01:03,13/Mar/14 18:16
Bug,ZOOKEEPER-1563,12611701,Wrong solution - unable to build under Windows with Visual Studio,"When I try to open zookeeper.sln the VS wants me to convert the project. While the convertion is taking place I'm getting a message:

""A file with the name: ""[path]\zookeeper.vcxproj"" already exists on disk.
Do you want to overwrite the project and its imported property sheets""

And after it I get next message with same text but it is about Cli.vxproj

No matter If I click yes or no the coverting process fails, both projects (Cli and zookeeper) are marked as unavailable.

If I close VS and open the zookeeper.sln once again it wants me to convert but now if I answer yes the projects are again unavailable but if I answer no the projects are available but are empty.",,kuebk,Major,Resolved,Fixed,13/Oct/12 14:55,28/Dec/12 12:35
Bug,ZOOKEEPER-1573,12614446,Unable to load database due to missing parent node,"While replaying txnlog on data tree, the server has a code to detect missing parent node. This code block was last modified as part of ZOOKEEPER-1333. In our production, we found a case where this check is return false positive.

The sequence of txns is as follows:

zxid 1:  create /prefix/a
zxid 2:  create /prefix/a/b
zxid 3:  delete /prefix/a/b
zxid 4:  delete /prefix/a

The server start capturing snapshot at zxid 1. However, by the time it traversing the data tree down to /prefix, txn 4 is already applied and /prefix have no children. 

When the server restore from snapshot, it process txnlog starting from zxid 2. This txn generate missing parent error and the server refuse to start up.

The same check allow me to discover bug in ZOOKEEPER-1551, but I don't know if we have any option beside removing this check to solve this issue.  ",vinayakumarb,thawan,Critical,Closed,Fixed,01/Nov/12 23:13,13/Mar/14 18:17
Bug,ZOOKEEPER-1575,12615059,adding .gitattributes to prevent CRLF and LF mismatches for source and text files,adding .gitattributes to prevent CRLF and LF mismatches for source and text files,raja@cmbasics.com,raja@cmbasics.com,Major,Resolved,Fixed,07/Nov/12 01:31,04/Apr/14 11:12
Bug,ZOOKEEPER-1576,12615140,Zookeeper cluster - failed to connect to cluster if one of the provided IPs causes java.net.UnknownHostException,"Using a cluster of three 3.4.3 zookeeper servers.
All the servers are up, but on the client machine, the firewall is blocking one of the  servers.
The following exception is happening, and the client is not connected to any of the other cluster members.

The exception:Nov 02, 2012 9:54:32 PM com.netflix.curator.framework.imps.CuratorFrameworkImpl logError
SEVERE: Background exception was not retry-able or retry gave up
java.net.UnknownHostException: scnrmq003.myworkday.com
at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$1.lookupAllHostAddr(Unknown Source)
at java.net.InetAddress.getAddressesFromNameService(Unknown Source)
at java.net.InetAddress.getAllByName0(Unknown Source)
at java.net.InetAddress.getAllByName(Unknown Source)
at java.net.InetAddress.getAllByName(Unknown Source)
at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:440)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:375)

The code at the org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60) is :
public StaticHostProvider(Collection<InetSocketAddress> serverAddresses) throws UnknownHostException {
for (InetSocketAddress address : serverAddresses) {
InetAddress resolvedAddresses[] = InetAddress.getAllByName(address
.getHostName());
for (InetAddress resolvedAddress : resolvedAddresses) { this.serverAddresses.add(new InetSocketAddress(resolvedAddress .getHostAddress(), address.getPort())); }
}
......

The for-loop is not trying to resolve the rest of the servers on the list if there is an UnknownHostException at the InetAddress.getAllByName(address.getHostName()); 
and it fails the client connection creation.


I was expecting the connection will be created for the other members of the cluster. 
Also, InetAddress is a blocking command, and if it takes very long time,  (longer than the defined timeout) - that also should allow us to continue to try and connect to the other servers on the list.
Assuming this will be fixed, and we will get connection to the current available servers, I think the zookeeper should continue to retry to connect to the not-connected server of the cluster, so it will be able to use it later when it is back.
If one of the servers on the list is not available during the connection creation, then it should be retried every x time despite the fact that we 

",eribeiro,tally.tsabary,Major,Resolved,Fixed,07/Nov/12 11:11,10/May/23 13:58
Bug,ZOOKEEPER-1578,12615285,org.apache.zookeeper.server.quorum.Zab1_0Test failed due to hard code with 33556 port,"org.apache.zookeeper.server.quorum.Zab1_0Test was failed both with SUN JDK and open JDK.

    [junit] Running org.apache.zookeeper.server.quorum.Zab1_0Test
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 18.334 sec
    [junit] Test org.apache.zookeeper.server.quorum.Zab1_0Test FAILED 


Zab1_0Test log:
Zab1_0Test log:
2012-07-11 23:17:15,579 [myid:] - INFO  [main:Leader@427] - Shutdown called
java.lang.Exception: shutdown Leader! reason: end of test
        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:427)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testLastAcceptedEpoch(Zab1_0Test.java:211)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48)


2012-07-11 23:17:15,584 [myid:] - ERROR [main:Leader@139] - Couldn't bind to port 33556
java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:402)
        at java.net.ServerSocket.bind(ServerSocket.java:328)
        at java.net.ServerSocket.bind(ServerSocket.java:286)
        at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:137)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.createLeader(Zab1_0Test.java:810)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testLeaderInElectingFollowers(Zab1_0Test.java:224)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

2012-07-11 23:17:20,202 [myid:] - ERROR [LearnerHandler-bdvm039.svl.ibm.com/9.30.122.48:40153:LearnerHandler@559] - Unex
pected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:291)
2012-07-11 23:17:20,203 [myid:] - WARN  [LearnerHandler-bdvm039.svl.ibm.com/9.30.122.48:40153:LearnerHandler@569] - ****
*** GOODBYE bdvm039.svl.ibm.com/9.30.122.48:40153 ********
2012-07-11 23:17:20,204 [myid:] - INFO  [Thread-20:Leader@421] - Shutting down
2012-07-11 23:17:20,204 [myid:] - INFO  [Thread-20:Leader@427] - Shutdown called
java.lang.Exception: shutdown Leader! reason: lead ended

this failure seems 33556 port is already used, but it is not in use with command check in fact. There is a hard code in unit test, we can improve it with code patch.",michelle,michelle,Major,Closed,Fixed,08/Nov/12 03:55,13/Mar/14 18:17
Bug,ZOOKEEPER-1580,12615319,QuorumPeer.setRunning is not used,"setRunning is a public method and a search did not indicate that it is used anywhere, not even in tests. In fact, I believe we should not change ""running"" freely and we should only do it when calling shutdown. ",maoling,fpj,Minor,Resolved,Fixed,08/Nov/12 11:13,30/Jan/18 10:44
Bug,ZOOKEEPER-1581,12615338,change copyright in notice to 2012,it's 2012 so the copyright in notice.txt should end with 2012,breed,breed,Major,Closed,Fixed,08/Nov/12 14:56,13/Mar/14 18:17
Bug,ZOOKEEPER-1585,12616685,make dist for src/c broken in trunk,make dist from trunk is failing because of a wrong reference to src/zookeeper_log.h (which exists in include/). ,rgs,rgs,Major,Resolved,Fixed,19/Nov/12 06:04,03/Mar/16 01:34
Bug,ZOOKEEPER-1590,12617869,Patch to add zk.updateServerList(newServerList) broke the build,"Here is the related output of jenkins:

{noformat}
validate-xdocs:
     [exec] /home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml:578:5: The element type ""para"" must be terminated by the matching end-tag ""</para>"".
     [exec] 
     [exec] BUILD FAILED
     [exec] /home/jenkins/tools/forrest/latest/main/targets/validate.xml:135: Could not validate document /home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml
     [exec] 
{noformat}",fpj,fpj,Blocker,Resolved,Fixed,27/Nov/12 20:08,28/Nov/12 11:07
Bug,ZOOKEEPER-1591,12617900,Windows build is broken because inttypes.h doesn't exist,"addrvec.h includes inttypes.h, but it is not present in the windows build environment.

https://builds.apache.org/job/ZooKeeper-trunk-WinVS2008/596/console

f:\hudson\hudson-slave\workspace\zookeeper-trunk-winvs2008\trunk\src\c\src\addrvec.h(22): fatal error C1083: Cannot open include file: 'inttypes.h': No such file or directory ",marshall,michim,Major,Resolved,Fixed,27/Nov/12 23:32,01/Dec/12 11:03
Bug,ZOOKEEPER-1596,12618655,Zab1_0Test should ensure that the file is closed,"Zab1_0Test fails on windows with: 
{code}
java.io.IOException: Could not rename temporary file C:\Users\ADMINI~1\AppData\Local\Temp\2\test6831881113551099349dir\version-2\acceptedEpoch.tmp to C:\Users\A
DMINI~1\AppData\Local\Temp\2\test6831881113551099349dir\version-2\acceptedEpoch
        at org.apache.zookeeper.common.AtomicFileOutputStream.close(AtomicFileOutputStream.java:82)
        at org.apache.zookeeper.server.quorum.QuorumPeer.writeLongToFile(QuorumPeer.java:1121)
        at org.apache.zookeeper.server.quorum.QuorumPeer.setAcceptedEpoch(QuorumPeer.java:1148)
        at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:281)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:72)
        at org.apache.zookeeper.server.quorum.Zab1_0Test$1.run(Zab1_0Test.java:450)
{code}

The file handlers currentEpoch and acceptedEpoch are not closed, so delete fails on windows. ",enis,enis,Major,Closed,Fixed,03/Dec/12 23:34,13/Mar/14 18:17
Bug,ZOOKEEPER-1597,12618704,Windows build failing,"Seems to be related to C client changes done for ZK-1355.
We're not sure why these build failures happen on Windows.

###################################################################################
########################## LAST 60 LINES OF THE CONSOLE ###########################
[...truncated 376 lines...]
  .\src\zookeeper.c(768): error C2224: left of '.count' must have struct/union type
  .\src\zookeeper.c(768): error C2065: 'i' : undeclared identifier
  .\src\zookeeper.c(770): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(770): error C2224: left of '.data' must have struct/union type
  .\src\zookeeper.c(770): error C2065: 'i' : undeclared identifier
  .\src\zookeeper.c(773): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(774): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(780): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(781): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(788): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(789): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(792): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(792): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(794): error C2065: 'found_current' : undeclared identifier
  .\src\zookeeper.c(797): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(797): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(814): error C2065: 'found_current' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(825): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(825): error C2440: '=' : cannot convert from 'int' to 'addrvec_t'
  .\src\zookeeper.c(843): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(843): error C2224: left of '.data' must have struct/union type
  .\src\zookeeper.c(845): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(848): error C2065: 'hosts' : undeclared identifier
  .\src\zookeeper.c(849): error C2065: 'hosts' : undeclared identifier
  .\src\zookeeper.c(850): error C2065: 'hosts' : undeclared identifier
  .\src\zookeeper.c(853): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1177): error C2143: syntax error : missing ';' before 'const'
  .\src\zookeeper.c(1179): error C2065: 'endpoint_info' : undeclared identifier
  .\src\zookeeper.c(1883): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(1884): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1885): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1916): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(1920): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(1927): error C2065: 'ssoresult' : undeclared identifier
  .\src\zookeeper.c(1927): error C2065: 'enable_tcp_nodelay' : undeclared identifier
  .\src\zookeeper.c(1927): error C2065: 'enable_tcp_nodelay' : undeclared identifier
  .\src\zookeeper.c(1928): error C2065: 'ssoresult' : undeclared identifier
  .\src\zookeeper.c(1944): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1949): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1962): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1963): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(2004): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(2004): fatal error C1003: error count exceeds 100; stopping compilation

    38 Warning(s)
    102 Error(s)",michim,shralex,Major,Closed,Fixed,04/Dec/12 08:28,13/Mar/14 18:17
Bug,ZOOKEEPER-1602,12624078,a change to QuorumPeerConfig's API broke compatibility with HBase,"The following patch broke an API that's in use by HBase. Otherwise current trunk compiles fine when used by hbase:

bq. ZOOKEEPER-1411. Consolidate membership management, distinguish between static and dynamic configuration parameters (Alex Shraer via breed)

Considering it a blocker even though it's not really a ""public"" API. If possible we should add back ""getServers"" method on QuorumPeerConfig to reduce friction for the hbase team.
",shralex,phunt,Blocker,Resolved,Fixed,15/Dec/12 00:40,16/Dec/12 11:04
Bug,ZOOKEEPER-1603,12624159,StaticHostProviderTest testUpdateClientMigrateOrNot hangs,"StaticHostProviderTest method testUpdateClientMigrateOrNot hangs forever.

On my laptop getHostName for 10.10.10.* takes 5+ seconds per call. As a result this method effectively runs forever.

Every time I run this test it hangs. Consistent.",fpj,phunt,Blocker,Closed,Fixed,16/Dec/12 08:27,13/Mar/14 18:17
Bug,ZOOKEEPER-1606,12625060,intermittent failures in ZkDatabaseCorruptionTest on jenkins,"ZkDatabaseCorruptionTest is failing intermittently on jenkins with:

""Error Message: the last server is not the leader""

Seeing this on jdk7/openjdk7/solaris - 3 times in the last month.

https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-openjdk7/2/testReport/junit/org.apache.zookeeper.test/ZkDatabaseCorruptionTest/testCorruption/",infgeoax,phunt,Major,Closed,Fixed,21/Dec/12 22:19,13/Mar/14 18:17
Bug,ZOOKEEPER-1610,12625275,Some classes are using == or != to compare Long/String objects instead of .equals(),"The classes org.apache.zookeeper.client.ZooKeeperSaslClient.java and 
org.apache.zookeeper.server.quorum.flexible.QuorumHierarchical.java compare Strings and/or Longs using referential equality.

Usually, this is not a problem because the Longs are cached and Strings are interned, but I myself  had problems with those kind of comparisons in the past because one production JVM didn't reused the objects.",eribeiro,eribeiro,Critical,Closed,Fixed,26/Dec/12 17:31,13/Mar/14 18:17
Bug,ZOOKEEPER-1613,12625536,The documentation still points to 2008 in the copyright notice,While fiddling with docbook to solve the broken links of ZOOKEEPER-1488 I noted that all the documentation's copyright notice still has the year 2008 only. I am submitting a patch a fix this.,eribeiro,eribeiro,Trivial,Closed,Fixed,30/Dec/12 23:47,13/Mar/14 18:16
Bug,SPARK-583,12704925,Failures in BlockStore may lead to infinite loops of task failures,"Summary: failures in BlockStore may lead to infinite loops of task failures.

I ran into a situation where a block manager operation failed:
{code}
12/10/20 21:25:13 ERROR storage.BlockManagerWorker: Exception handling buffer message
com.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492)
	at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78)
	at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58)
	at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73)
	at spark.storage.BlockManager.dataSerialize(BlockManager.scala:834)
	at spark.storage.MemoryStore.getBytes(MemoryStore.scala:72)
	at spark.storage.BlockManager.getLocalBytes(BlockManager.scala:311)
	at spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:79)
	at spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:58)
	at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33)
	at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.Iterator$class.foreach(Iterator.scala:772)
	at scala.collection.IndexedSeqLike$Elements.foreach(IndexedSeqLike.scala:54)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:73)
	at spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:12)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at spark.storage.BlockMessageArray.map(BlockMessageArray.scala:12)
	at spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:33)
	at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23)
	at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23)
	at spark.network.ConnectionManager.spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:276)
	at spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
{code}

This failure appears to have been detected via a fetch failure in the following stage:

{code}
12/10/20 21:25:12 INFO scheduler.DAGScheduler: Marking Stage 2 (mapPartitions at Operator.scala:197) for resubmision due to a fetch failure
12/10/20 21:25:12 INFO scheduler.DAGScheduler: The failed fetch was from Stage 3 (mapPartitions at Operator.scala:197); marking it for resubmission
{code}

The failed task was retried on the same machine, and it executed without exceptions.

However, the job is unable to make forward progress; the scheduler gets stuck in an infinite loop of the form
{code}
12/10/20 22:23:08 INFO spark.CacheTrackerActor: Asked for current cache locations
12/10/20 22:23:08 INFO scheduler.DAGScheduler: Resubmitting Stage 3 (mapPartitions at Operator.scala:197) because some of its tasks had failed: 220
12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting Stage 3 (mapPartitions at Operator.scala:197), which has no missing parents
12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 3
12/10/20 22:23:08 INFO cluster.ClusterScheduler: Adding task set 3.4080 with 1 tasks
12/10/20 22:23:08 INFO cluster.TaskSetManager: Starting task 3.4080:0 as TID 5484 on slave 201210202106-1093469194-5050-5222-43: domU-12-31-39-14-5E-5E.compute-1.internal (preferred)
12/10/20 22:23:08 INFO cluster.TaskSetManager: Serialized task 3.4080:0 as 7605 bytes in 0 ms
12/10/20 22:23:09 INFO cluster.TaskSetManager: Finished TID 5484 in 820 ms (progress: 1/1)
12/10/20 22:23:09 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(3, 220)
12/10/20 22:23:09 INFO scheduler.DAGScheduler: ShuffleMapTask finished with host domU-12-31-39-14-5E-5E.compute-1.internal
12/10/20 22:23:09 INFO scheduler.DAGScheduler: Stage 3 (mapPartitions at Operator.scala:197) finished; looking for newly runnable stages
12/10/20 22:23:09 INFO scheduler.DAGScheduler: running: Set()
12/10/20 22:23:09 INFO scheduler.DAGScheduler: waiting: Set(Stage 2, Stage 1)
12/10/20 22:23:09 INFO scheduler.DAGScheduler: failed: Set()
12/10/20 22:23:09 INFO spark.CacheTrackerActor: Asked for current cache locations
{code}

If I look at the worker that is running these tasks, I see infinite loop of the form
{code}
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_0 already exists on this machine; not re-adding it
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_1 already exists on this machine; not re-adding it
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_2 already exists on this machine; not re-adding it
[..]
12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_199 already exists on this machine; not re-adding it
12/10/20 21:29:29 INFO executor.Executor: Serialized size of result for 1677 is 350
12/10/20 21:29:29 INFO executor.Executor: Finished task ID 1677
12/10/20 21:29:29 INFO executor.Executor: Running task ID 1678
12/10/20 21:29:29 INFO executor.Executor: Its generation is 3
12/10/20 21:29:29 INFO spark.CacheTracker: Cache key is rdd_4_220
12/10/20 21:29:29 INFO spark.CacheTracker: Found partition in cache!
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Running Pre-Shuffle Group-By
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Mapside hash aggregation enabled
12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5
{code}

I'm not sure of the exact cause of this behavior, but I have a guess:

During the original failed execution, the task's output blocks were not stored but their block ids were added to the BlockManager's {{blockInfo}} map.  This prevents these blocks from being recomputed and causes the ""{{Block shuffle_*_*_* already exists on this machine; not re-adding it}}"" warnings.  As a result, the block is never stored and the master is never informed of its location.

This causes the DAG scheduler to repeatedly launch the same task in an infinite loop, saying that it is ""{{Resubmitting Stage * because some of its tasks had failed: *}}"".",woggle,joshrosen,Major,Resolved,Fixed,20/Oct/12 15:54,19/May/13 13:31
Bug,SPARK-585,12705349,Mesos may not work with mesos:// URLs,According to some users. I guess we should strip the mesos:// at the front.,matei,matei,Major,Resolved,Fixed,23/Oct/12 12:01,21/Nov/12 11:43
Bug,SPARK-587,12705183,Test issue,,,matei,Major,Closed,Fixed,23/Oct/12 21:53,23/Oct/12 21:56
Bug,SPARK-589,12704953,MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos,,,patrick,Major,Resolved,Fixed,24/Oct/12 09:57,10/Aug/13 16:31
Bug,SPARK-590,12705408,Log task size when it's too large on master,"For now, log once per job if the closure is > 100Kb. ",,patrick,Major,Resolved,Fixed,24/Oct/12 09:58,07/Dec/13 13:04
Bug,SPARK-597,12704982,HashPartitioner incorrectly partitions RDD[Array[_]],"Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1].  As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result.

This was the cause of a bug in PySpark, where I hash partition PairRDDs with {{Array[Byte]}} keys.  In PySpark, I fixed this by using a custom {{Partitioner}} that calls {{Arrays.hashCode(byte[])}} when passed an {{Array[Byte]}} [2].

I would like to address this issue more generally in Spark.

We could add logic to {{HashPartitioner}} to perform special handling for arrays, but I'm not sure whether the additional branching would add a significant performance overhead.  The logic could become messy because the {{Arrays}} module defines {{Arrays.hashCode()}} for primitive arrays and {{Arrays.deepHashCode()}} for Object arrays.  Perhaps Guava or Apache Commons has an implementation of this.

An alternative would be to keep the current {{HashPartitioner}} and add logic to print warnings (or to fail with an error) when shuffling an {{RDD[Array[_]]}} using the default {{HashPartitioner}}.


[1] http://stackoverflow.com/questions/744735/java-array-hashcode-implementation
[2] https://github.com/JoshRosen/spark/commit/2ccf3b665280bf5b0919e3801d028126cb070dbd",joshrosen,joshrosen,Major,Resolved,Fixed,28/Oct/12 22:51,08/Oct/14 22:27
Bug,SPARK-599,12705373,OutOfMemoryErrors can cause workers to hang indefinitely,"While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java.lang.OutOfMemoryError : GC overhead limit exceeded}}.  This caused that Java process to hang at 100% CPU, spending all of its time in the garbage collector.  This failure wasn't detected by the master, causing the entire job to hang.

Handling and reporting failures due to {{OutOfMemoryError}} can be complicated because the {{OutOfMemoryError}} exception can be raised at many different locations, depending on which allocation caused the error.

I'm not sure that it's safe to recover from {{OutOfMemoryError}}, so worker processes should probably die once they raise that error.  We might be able to do this in an uncaught exception handler.",,joshrosen,Major,Resolved,Fixed,30/Oct/12 22:06,26/Nov/12 12:32
Bug,SPARK-600,12705060,SparkContext.stop and clearJars delete local JAR files,"If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext.",,matei,Major,Resolved,Fixed,31/Oct/12 14:43,06/Nov/14 06:58
Bug,SPARK-601,12704935,PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None,"If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed:

  scala> val rdd = sc.parallelize(List((1, 'a'), (1, 'b'), (2, 'c')))
  rdd: spark.RDD[(Int, Char)] = spark.ParallelCollection@73f4117b

  scala> rdd.lookup(1)
  java.lang.UnsupportedOperationException: lookup() called on an RDD without a partitioner
  	  at spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:315)

At a minimum, a filter.map.collect over the whole RDD works when the optimized path using a partitioner is not available:

         case None =>
-          throw new UnsupportedOperationException(""lookup() called on an RDD without a partitioner"")
+          self.filter(kv => kv._1 == key).map(kv => kv._2).collect",markhamstra,,Minor,Resolved,Fixed,31/Oct/12 16:02,06/Aug/13 23:20
Bug,SPARK-607,12705071,Timeout while fetching map statuses may cause job to hang,"Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker.

I ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster.

After applying the attached patch to generate random timeout failures, my groupByKey job lost a task due to the timeout.  It looks like the master is notified of the failure, since it appears in its log:

{code}
12/11/06 10:19:39 INFO TaskSetManager: Serialized task 0.0:7 as 3095 bytes in 1 ms
12/11/06 10:19:39 INFO TaskSetManager: Lost TID 10 (task 0.0:2)
12/11/06 10:19:40 INFO TaskSetManager: Loss was due to spark.SparkException: Error communicating with MapOutputTracker
	at spark.MapOutputTracker.askTracker(MapOutputTracker.scala:78)
	at spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:154)
	at spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:14)
	at spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:38)
	at spark.RDD.iterator(RDD.scala:161)
	at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:18)
	at spark.RDD.iterator(RDD.scala:161)
	at spark.scheduler.ResultTask.run(ResultTask.scala:18)
	at spark.executor.Executor$TaskRunner.run(Executor.scala:76)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
12/11/06 10:19:40 INFO TaskSetManager: Starting task 0.0:2 as TID 16 on slave worker-20121106101845-128.32.130.156-50931: 128.32.130.156 (preferred)
12/11/06 10:19:40 INFO TaskSetManager: Serialized task 0.0:2 as 3095 bytes in 1 ms
{code}

The job hangs here; perhaps the failure leaves some inconsistent state on the worker.",joshrosen,joshrosen,Major,Resolved,Fixed,06/Nov/12 10:32,16/Jan/13 21:54
Bug,SPARK-613,12705254,Standalone web UI links to internal IPs when running on EC2,"When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e.g. http://10.159.2.115:8081/) instead of externally-accessible addresses (e.g. http://ec2-*-*-*-*.compute-1.amazonaws.com/).  This makes it hard to view the worker logs.",joshrosen,joshrosen,Major,Resolved,Fixed,07/Nov/12 23:01,25/May/13 23:42
Bug,SPARK-617,12704785,Driver program can crash when a standalone worker is lost,Seems to be due to an uncaught communication timeout in Akka.,matei,matei,Major,Resolved,Fixed,10/Nov/12 22:46,11/Nov/12 21:21
Bug,SPARK-619,12705244,Hadoop MapReduce should be configured to use all local disks for shuffle on AMI,"It used to be, but that got lost at some point.",,matei,Minor,Resolved,Fixed,11/Nov/12 11:22,06/Nov/14 07:00
Bug,SPARK-623,12705264,Don't hardcode log location for standalone UI,,cgrothaus,dennybritz,Minor,Resolved,Fixed,14/Nov/12 15:41,13/May/13 14:37
Bug,SPARK-625,12704941,Client hangs when connecting to standalone cluster using wrong address,"I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128.32.*.*:7077).  If I try to connect spark-shell to the master using ""spark://0.0.0.0:7077"", it successfully brings up a Scala prompt but hangs when I try to run a job.

From the standalone master's log, it looks like the client's messages are being dropped without the client discovering that the connection has failed:

{code}
12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message RegisterJob(JobDescription(Spark shell)) for non-local recipient akka://spark@0.0.0.0:7077/user/Master at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077
12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message DaemonMsgWatch(Actor[akka://spark@128.32.*.*:57518/user/$a],Actor[akka://spark@0.0.0.0:7077/user/Master]) for non-local recipient akka://spark@0.0.0.0:7077/remote at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077
{code}",,joshrosen,Minor,Resolved,Fixed,27/Nov/12 14:13,07/Feb/15 22:48
Bug,SPARK-626,12705385,deleting security groups gives me a 400 error,"Filing on behalf of Shivaram:


Deleting security group tinytasks-test-zoo
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>
Traceback (most recent call last):
  File ""./spark_ec2.py"", line 637, in <module>
    main()
  File ""./spark_ec2.py"", line 571, in main
    conn.delete_security_group(group.name)
  File ""/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py"", line 2039, in delete_security_group
  File ""/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py"", line 944, in get_status
boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request
<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>",pas256,rxin,Major,Resolved,Fixed,29/Nov/12 21:40,11/Dec/12 12:13
Bug,SPARK-629,12705443,Standalone job details page has strange value for number of cores,"When I view the job details page of a job running on a standalone cluster, I see the following strange output:

{code}
Cores: 2147483647 (400 Granted )
{code}

I'm not sure where 2147483647 is coming from (it's Integer.MAX_VALUE).

Looking at the code for this job details page, this is generated by the following:

{code}
        <li><strong>Cores:</strong>                                             
          @job.desc.cores                                                       
          (@job.coresGranted Granted                                            
          @if(job.desc.cores == Integer.MAX_VALUE) {                            
                                                                                
          } else {                                                              
            , @job.coresLeft                                                    
          }                                                                     
          )                                                                     
        </li>     
{code}

I'm not sure what this is supposed to do; is the idea to display something like ""Cores: totalCores (x granted, y pending)""?  Does Integer.MAX_VALUE have any special significance when used as the number of cores in a JobDescription?",joshrosen,joshrosen,Minor,Resolved,Fixed,04/Dec/12 14:50,04/May/13 22:52
Bug,SPARK-630,12705421,Master web UI shows some finished/killed executors as running,"When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING.  However, when I view any of the workers' pages, the same executor appears under the ""Finished Executors"" list.",joshrosen,joshrosen,Minor,Resolved,Fixed,04/Dec/12 18:15,04/May/13 22:52
Bug,SPARK-631,12705277,SPARK_LOCAL_IP environment variable should also affect spark.master.host,So that we can have a single variable for configuring the IP address that Spark uses.,matei,matei,Major,Resolved,Fixed,06/Dec/12 18:10,08/Dec/12 01:11
Bug,SPARK-632,12705311,Akka system names need to be normalized (since they are case-sensitive),"The ""system"" name of the Akka full path is case-sensitive (see http://akka.io/faq/#what_is_the_name_of_a_remote_actor).

Since DNS names are case-insensitive and we're using them in the ""system"" name, we need to normalize them (e.g. make them all lowercase).  Otherwise, users will find the ""workers"" will not be able to connect with the ""master"" even though the URI appears to be correct.

For example, Berkeley DNS occasionally uses names e.g. foo.Berkley.EDU. If I used foo.berkeley.edu as the master adddress, the workers would write to their logs that they are connecting to foo.berkeley.edu but failed to. They never show up in the master UI.  If use the foo.Berkeley.EDU address, everything works as it should. ",,massie,Major,Closed,Fixed,07/Dec/12 11:44,11/Nov/14 09:13
Bug,SPARK-638,12705003,Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting,"spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves.sh script but not in start-master.sh.  This causes the workers to connect to the master on the wrong address, which fails.",joshrosen,joshrosen,Major,Resolved,Fixed,13/Dec/12 17:04,13/Dec/12 18:08
Bug,SPARK-641,12705394,spark-ec2 standalone launch should create ~/mesos-ec2/slaves,"When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script).  We should fix this, since users may still wish to use scripts like copy-dir to copy files.",shivaram,joshrosen,Minor,Resolved,Fixed,14/Dec/12 10:05,05/Apr/13 19:45
Bug,SPARK-642,12705363,spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS,"spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS.  This is done automatically when running under Mesos mode by using the scripts included in the AMI.

We should either have feature parity between the --cluster-type modes or remove the feature if it's too difficult to support both modes.",shivaram,joshrosen,Minor,Resolved,Fixed,14/Dec/12 10:47,05/Apr/13 19:48
Bug,SPARK-643,12704451,Standalone master crashes during actor restart,"The standalone master will crash if it restarts due to an exception:

{code}
12/12/15 03:10:47 ERROR master.Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.
spark.SparkException: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.
        at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:103)
        at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:62)
        at akka.actor.Actor$class.apply(Actor.scala:318)
        at spark.deploy.master.Master.apply(Master.scala:17)
        at akka.actor.ActorCell.invoke(ActorCell.scala:626)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)
        at akka.dispatch.Mailbox.run(Mailbox.scala:179)
        at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)
        at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
        at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
        at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
        at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
12/12/15 03:10:47 INFO master.Master: Starting Spark master at spark://ip-10-226-87-193:7077
12/12/15 03:10:47 INFO io.IoWorker: IoWorker thread 'spray-io-worker-1' started
12/12/15 03:10:47 ERROR master.Master: Failed to create web UI
akka.actor.InvalidActorNameException:actor name HttpServer is not unique!
[05aed000-4665-11e2-b361-12313d316833]
        at akka.actor.ActorCell.actorOf(ActorCell.scala:392)
        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.liftedTree1$1(ActorRefProvider.scala:394)
        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:394)
        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:392)
        at akka.actor.Actor$class.apply(Actor.scala:318)
        at akka.actor.LocalActorRefProvider$Guardian.apply(ActorRefProvider.scala:388)
        at akka.actor.ActorCell.invoke(ActorCell.scala:626)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)
        at akka.dispatch.Mailbox.run(Mailbox.scala:179)
        at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)
        at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
        at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
        at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
        at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
{code}

When the Master actor restarts, Akka calls the {{postRestart}} hook.  [By default|http://doc.akka.io/docs/akka/snapshot/general/supervision.html#supervision-restart], this calls {{preStart}}.  The standalone master's {{preStart}} method tries to start the webUI but crashes because it is already running.

I ran into this after a job failed more than 11 times, which causes the Master to throw a SparkException from its {{receive}} method.

The solution is to implement a custom {{postRestart}} hook.",joshrosen,joshrosen,Major,Resolved,Fixed,14/Dec/12 19:19,06/Nov/14 17:33
Bug,SPARK-644,12705396,Jobs canceled due to repeated executor failures may hang,"In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github.com/mesos/spark/pull/210).  Currently, the master crashes when aborting jobs (this is the issue that uncovered SPARK-643).  If we fix the crash, which involves removing a {{throw}} from the actor's {{receive}} method, then these failures can lead to a hang because they cause the job to be removed from the master's scheduler, but the upstream scheduler components aren't notified of the failure and will wait for the job to finish.

I've considered fixing this by adding additional callbacks to propagate the failure to the higher-level schedulers.  It might be cleaner to move the decision to abort the job into the higher-level layers of the scheduler, sending an {{AbortJob(jobId)}} method to the Master.  The Client is already notified of executor state changes, so it may be able to make the decision to abort (or defer that decision to a higher layer).",joshrosen,joshrosen,Major,Resolved,Fixed,20/Dec/12 23:55,06/Nov/14 17:33
Bug,SPARK-645,12705256,Calling distinct() without parentheses fails,"While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd.distinct().persist and not rdd.distinct.persist.  The without-parentheses form should be allowed -- as it is for persist().",markhamstra,markhamstra,Trivial,Resolved,Fixed,24/Dec/12 01:47,24/Dec/12 08:05
Bug,SPARK-646,12704952,Floating point overflow/underflow in LR examples,"The SparkLR examples call scala.math.exp() with very large or small exponents, causing its result to be rounded to 0 or Infinity.  Is this a bug?

I discovered this while porting the LR example to Python, because Python's math.exp() function rounds very small results to 0 but raises OverflowError for large results.

In Scala:

{code}
scala> import math.exp
import math.exp

scala> math.exp(10000)
res4: Double = Infinity

scala> math.exp(-10000)
res5: Double = 0.0
{code}

Python:

{code}
from math import exp
exp(10000)
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
OverflowError: math range error
exp(-10000)
0.0
{code}

I added a call to println("""" + (-p.y * (w dot p.x))) in the map UDF in SparkLR and SparkHdfsLR to log the exponents, and in both cases I saw small exponents like
{code}
-4.967736504527945
-1.0153344192159428
0.4639647012587064
{code}

in the first round and huge exponents like

{code}
-3731.0565020800145
469.3852842964799
-2838.8348220771445
{code}

in all later rounds.

The examples calculate the gradients using

{code}
(1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y * p.x.
{code}

The exponent (w dot p.x) grows rapidly because the magnitudes of w's components grow rapidly.

I'm not familiar enough with logistic regression to know whether this is common or how to fix this.

This could be a problem because a model whose weights have large magnitudes would always make predictions with extremely high confidence (e.g. p(y = 1 | x) is always 0 or 1, due to rounding).",matei,joshrosen,Major,Resolved,Fixed,29/Dec/12 17:59,06/Aug/13 23:24
Bug,MAPREDUCE-3614,12537113, finalState UNDEFINED if AM is killed by hand,"Courtesy [~dcapwell]

{quote}
If the AM is running and you kill the process (sudo kill #pid), the State in Yarn would be FINISHED and FinalStatus is UNDEFINED.  The Tracking UI would say ""History"" and point to the proxy url (which will redirect to the history server).

The state should be more descriptive that the job failed and the tracker url shouldn't point to the history server.
{quote}",raviprak,raviprak,Major,Resolved,Fixed,03/Jan/12 23:18,04/Mar/12 13:57
Bug,MAPREDUCE-3615,12537190,mapred ant test failures,"The following mapred ant tests are failing.  This started on December 22nd.


    [junit] Running org.apache.hadoop.mapred.TestTrackerBlacklistAcrossJobs
    [junit] Running org.apache.hadoop.mapred.TestMiniMRDFSSort
    [junit] Running org.apache.hadoop.mapred.TestBadRecords
    [junit] Running org.apache.hadoop.mapred.TestClusterMRNotification
    [junit] Running org.apache.hadoop.mapred.TestDebugScript
    [junit] Running org.apache.hadoop.mapred.TestJobCleanup
    [junit] Running org.apache.hadoop.mapred.TestJobClient
    [junit] Running org.apache.hadoop.mapred.TestJobHistory
    [junit] Running org.apache.hadoop.mapred.TestJobInProgressListener
    [junit] Running org.apache.hadoop.mapred.TestJobKillAndFail
    [junit] Running org.apache.hadoop.mapred.TestJvmReuse
    [junit] Running org.apache.hadoop.mapred.TestKillSubProcesses
    [junit] Running org.apache.hadoop.mapred.TestNodeRefresh
    [junit] Running org.apache.hadoop.mapred.TestSetupAndCleanupFailure
    [junit] Running org.apache.hadoop.mapred.TestTaskFail
    [junit] Running org.apache.hadoop.mapred.TestTaskOutputSize
    [junit] Running org.apache.hadoop.mapred.TestTaskTrackerSlotManagement
    [junit] Running org.apache.hadoop.mapreduce.TestMRJobClient
    [junit] Running org.apache.hadoop.mapreduce.lib.db.TestDBJob
    [junit] Running org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
",tgraves,tgraves,Blocker,Closed,Fixed,04/Jan/12 14:30,05/Mar/12 02:49
Bug,MAPREDUCE-3617,12537262,Remove yarn default values for resource manager and nodemanager principal,Default values should be empty since no use can be made of them without correct values defined.,jeagles,jeagles,Major,Closed,Fixed,04/Jan/12 23:27,10/Mar/15 04:32
Bug,MAPREDUCE-3621,12537340,TestDBJob and TestDataDrivenDBInputFormat ant tests fail,"The following mapred ant tests fail and have been failing for a very long time:

[junit] Running org.apache.hadoop.mapreduce.lib.db.TestDBJob
[junit] Running org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat",raviprak,tgraves,Major,Closed,Fixed,05/Jan/12 15:16,11/Oct/12 17:48
Bug,MAPREDUCE-3624,12537376,bin/yarn script adds jdk tools.jar to the classpath.,"Thanks to Roman for pointing it out. Looks like we have the following lines in bin/yarn:

{code}
CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
{code}

We dont really have a dependency on the tools jar. We should remove this.",mahadev,mahadev,Major,Closed,Fixed,05/Jan/12 18:31,05/Mar/12 02:49
Bug,MAPREDUCE-3625,12537400,CapacityScheduler web-ui display of queue's used capacity is broken,"The display of the queue's used capacity at runtime is broken because it display's 'used' relative to the queue's capacity and not the parent's capacity as shown in the above attachment.

The display should be relative to parent's capacity and not leaf queues as everything else in the display is relative to parent's capacity.
",jlowe,acmurthy,Critical,Closed,Fixed,05/Jan/12 19:53,05/Mar/12 02:49
Bug,MAPREDUCE-3632,12537466,Need better error message on the Web UI when NM can't find the container logs instead of NPEno,"If for some reason NM could not find container logs, then an NPE is seen while trying to access from web UI. Instead an error message should be displayed.",ravidotg,ravidotg,Major,Resolved,Fixed,06/Jan/12 04:24,09/Mar/15 21:56
Bug,MAPREDUCE-3634,12537564,All daemons should crash instead of hanging around when their EventHandlers get exceptions,We should make sure that the daemons crash in case the dispatchers get exceptions and stop processing. That way we will be debugging RM/NM/AM crashes instead of hard-to-track hanging jobs. ,vinodkv,vinodkv,Major,Resolved,Fixed,06/Jan/12 18:39,29/Mar/17 22:29
Bug,MAPREDUCE-3639,12537623,TokenCache likely broken for FileSystems which don't issue delegation tokens,Ref HADOOP-7963.,sseth,sseth,Blocker,Closed,Fixed,07/Jan/12 04:21,05/Mar/12 02:48
Bug,MAPREDUCE-3644,12537806,Snapshot builds have confusing jar file names in share/hadoop/mapreduce in tarball,"If you build a Hadoop tarball with a non-release version, the moduleSet used in hadoop-assemblies/src/main/resources/assemblies/hadoop-mapreduce-dist.xml results in jar files going into share/hadoop/mapreduce with unique snapshot versions - i.e., the timestamp they were built. This isn't an issue in release builds. It can be fixed by adding "" <outputFileNameMapping>${module.artifactId}-${project.version}${dashClassifier?}.${module.extension}</outputFileNameMapping>"" to the binaries tag of the moduleSet.",,abayer,Major,Resolved,Fixed,09/Jan/12 20:08,17/Mar/16 16:31
Bug,MAPREDUCE-3645,12537820,TestJobHistory fails,"TestJobHistory fails.

>>> org.apache.hadoop.mapred.TestJobHistory.testDoneFolderOnHDFS 	
>>> org.apache.hadoop.mapred.TestJobHistory.testDoneFolderNotOnDefaultFileSystem 	
>>> org.apache.hadoop.mapred.TestJobHistory.testHistoryFolderOnHDFS 	
>>> org.apache.hadoop.mapred.TestJobHistory.testJobHistoryFile 

It looks like this was introduced by MAPREDUCE-3349 and the issue is that the test expects the hostname to be in the format rackname/hostname, but with 3349 it split those apart into 2 different fields.",tgraves,tgraves,Blocker,Closed,Fixed,09/Jan/12 21:43,05/Mar/12 02:48
Bug,MAPREDUCE-3646,12537825,"Remove redundant URL info from ""mapred job"" output","The URL information to track the job is printed for all the ""mapred job""mrv2 commands. This information is redundant and has to be removed.

E.g:
{noformat}
-bash-3.2$ mapred job -list 

Total jobs:3
JobId   State   StartTime       UserName        Queue   Priority        Maps    Reduces UsedContainers  RsvdContainers  UsedMem RsvdMem NeededMem       AM info
12/01/09 22:20:15 INFO mapred.ClientServiceDelegate: The url to track the job: <RM host>:8088/proxy/<application ID 1>/
<job ID 1>  RUNNING 1326147596446   ramya  default NORMAL  10      10      21      0       22528M  0M      22528M  <RM host>:8088/proxy/<application ID 1>/
12/01/09 22:20:15 INFO mapred.ClientServiceDelegate: The url to track the job: <RM host>:8088/proxy/<application ID 2>/
<job ID 2>  RUNNING 1326147603726   ramya  default NORMAL  10      10      11      0       12288M  0M      12288M  <RM host>:8088/proxy/<application ID 2>/
12/01/09 22:20:16 INFO mapred.ClientServiceDelegate: The url to track the job: <RM host>:8088/proxy/<application ID 3>/
<job ID 3>  RUNNING 1326147520126   ramya  default NORMAL  10      10      21      0       22528M  0M      22528M  <RM host>:8088/proxy/<application ID 3>/
{noformat}
",jeagles,rramya,Major,Closed,Fixed,09/Jan/12 22:25,05/Mar/12 02:49
Bug,MAPREDUCE-3648,12537833,TestJobConf failing,"TestJobConf is failing:



testFindContainingJar 
testFindContainingJarWithPlus 

java.lang.ClassNotFoundException: ClassWithNoPackage
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:247)
	at org.apache.hadoop.mapred.TestJobConf.testJarAtPath(TestJobConf.java:78)
	at org.apache.hadoop.mapred.TestJobConf.testFindContainingJar(TestJobConf.java:44)


Looks like perhaps a classpath issue.


TestQueueManagerRefresh also has failures and I'm wondering might be related as it doesn't seem to pick up a config file written out to build/test/extraconf",tgraves,tgraves,Blocker,Closed,Fixed,09/Jan/12 23:48,05/Mar/12 02:48
Bug,MAPREDUCE-3649,12537834,Job End notification gives an error on calling back.,"When calling job end notification for oozie the AM fails with the following trace:

{noformat}
2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed
java.net.UnknownServiceException: no content-type
	at java.net.URLConnection.getContentHandler(URLConnection.java:1192)
	at java.net.URLConnection.getContent(URLConnection.java:689)
	at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)
	at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)
{noformat}",raviprak,mahadev,Blocker,Closed,Fixed,09/Jan/12 23:53,05/Mar/12 02:49
Bug,MAPREDUCE-3650,12537855,testGetTokensForHftpFS() fails," org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForHftpFS  fails.

Looks like it may have been introduced with HADOOP-7808",raviprak,tgraves,Blocker,Closed,Fixed,10/Jan/12 02:20,07/Sep/12 21:03
Bug,MAPREDUCE-3651,12537908,TestQueueManagerRefresh fails,"The following tests fail:
org.apache.hadoop.mapred.TestQueueManagerRefresh.testRefreshWithRemovedQueues 
org.apache.hadoop.mapred.TestQueueManagerRefresh.testRefreshOfSchedulerProperties 

It looks like its simply trying to remove one of the queues but the remove is failing.It looks like MAPREDUCE-3328. mapred queue -list output inconsistent and missing child queues - change the getChilren routine to do a new JobQueueInfo on each one when returning it which is making the remove routine fail since they aren't the same object now.",tgraves,tgraves,Blocker,Closed,Fixed,10/Jan/12 16:04,05/Mar/12 02:49
Bug,MAPREDUCE-3652,12537914,org.apache.hadoop.mapred.TestWebUIAuthorization.testWebUIAuthorization fails,"org.apache.hadoop.mapred.TestWebUIAuthorization.testWebUIAuthorization fails.

This is testing the old jsp web interfaces.  I think this test should just be removed.


Any objections?",tgraves,tgraves,Blocker,Closed,Fixed,10/Jan/12 16:28,05/Mar/12 02:49
Bug,MAPREDUCE-3656,12538062,Sort job on 350 scale is consistently failing with latest MRV2 code ,"With the code checked out on last two days. 
Sort Job on 350 node scale with 16800 maps and 680 reduces consistently failing for around last 6 runs
When around 50% of maps are completed, suddenly job jumps to failed state.
On looking at NM log, found RM sent Stop Container Request to NM for AM container.
But at INFO level from RM log not able find why RM is killing AM when job is not killed manually.
One thing found common on failed AM logs is -:
org.apache.hadoop.yarn.state.InvalidStateTransitonException
With with different.
For e.g. One log says -:
{code}
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_UPDATE at ASSIGNED 
{code}
Whereas other logs says -:
{code}
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR
{code}

",sseth,karams,Blocker,Closed,Fixed,11/Jan/12 15:45,05/Mar/12 02:48
Bug,MAPREDUCE-3657,12538076,State machine visualize build fails,Attempting to build the state machine graphs with {{mvn -Pvisualize compile}} fails for the resourcemanager and nodemanager projects.  The build fails because org.apache.commons.logging.LogFactory isn't in the classpath.,jlowe,jlowe,Minor,Closed,Fixed,11/Jan/12 17:29,10/Mar/15 04:31
Bug,MAPREDUCE-3664,12538072,HDFS Federation Documentation has incorrect configuration example,"HDFS Federation documentation example (1) has the following

<property>
    <name>dfs.namenode.rpc-address.ns1</name>
    <value>hdfs://nn-host1:rpc-port</value>
</property>

dfs.namenode.rpc-address.* should be set to hostname:port, hdfs:// should not be there.

(1) - http://hadoop.apache.org/common/docs/r0.23.0/hadoop-yarn/hadoop-yarn-site/Federation.html",brandonli,praveensripati,Minor,Closed,Fixed,11/Jan/12 16:47,10/Mar/15 04:31
Bug,MAPREDUCE-3669,12538355,Getting a lot of PriviledgedActionException / SaslException when running a job,"On a secure cluster, when running a job we are seeing a lot of PriviledgedActionException / SaslExceptions.  The job runs fine, its just the jobclient can't connect to the AM to get the progress information.

Its in a very tight loop retrying while getting the exceptions.

snip of the client log is:
12/01/13 15:33:45 INFO security.SecurityUtil: Acquired token Ident: 00 1c 68 61 64 6f 6f 70 71 61 40 44 45 56 2e 59 47
52 49 44 2e 59 41 48 4f 4f 2e 43 4f 4d 08 6d 61 70 72 65 64 71 61 00 8a 01 34 d7 b3 ff f5 8a 01 34 fb c0 83 f5 08 02,
Kind: HDFS_DELEGATION_TOKEN, Service: 10.10.10.10:8020
12/01/13 15:33:45 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 8 for user1 on 10.10.10.10:8020
12/01/13 15:33:45 INFO security.TokenCache: Got dt for
hdfs://host1.domain.com:8020;uri=10.10.10.10:8020;t.service=10.10.10.10:8020
12/01/13 15:33:45 WARN conf.Configuration: mapred.used.genericoptionsparser is deprecated. Instead, use
mapreduce.client.genericoptionsparser.used
12/01/13 15:33:45 INFO mapreduce.JobSubmitter: number of splits:2
12/01/13 15:33:45 INFO mapred.ResourceMgrDelegate: Submitted application application_1326410042859_0008 to
ResourceManager at rmhost.domain/10.10.10.11:8040
12/01/13 15:33:45 INFO mapreduce.Job: Running job: job_1326410042859_0008
12/01/13 15:33:52 INFO mapred.ClientServiceDelegate: The url to track the job:
rmhost.domain:8088/proxy/application_1326410042859_0008/
12/01/13 15:33:52 ERROR security.UserGroupInformation: PriviledgedActionException as:user1@DEV.YGRID.YAHOO.COM
(auth:SIMPLE) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail
ed to find any
Kerberos tgt)]
12/01/13 15:33:52 WARN ipc.Client: Exception encountered while connecting to the server :
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided
(Mechanism level: Failed to find any Kerberos tgt)]
12/01/13 15:33:52 ERROR security.UserGroupInformation: PriviledgedActionException as:user1@DEV.YGRID.YAHOO.COM
(auth:SIMPLE) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (
Mechanism level:
Failed to find any Kerberos tgt)]
12/01/13 15:33:52 INFO mapred.ClientServiceDelegate: The url to track the job:
rmhost.domain:8088/proxy/application_1326410042859_0008/",mahadev,tgraves,Blocker,Closed,Fixed,13/Jan/12 20:16,05/Mar/12 02:48
Bug,MAPREDUCE-3672,12538376,Killed maps shouldn't be counted towards JobCounter.NUM_FAILED_MAPS,"We count maps that are killed, say by speculator, towards JobCounter.NUM_FAILED_MAPS. We should instead have a separate JobCounter for killed maps.

Same with reduces too.",anupamseth,vinodkv,Major,Closed,Fixed,13/Jan/12 23:46,11/Oct/12 17:48
Bug,MAPREDUCE-3674,12538469,"If invoked with no queueName request param, jobqueue_details.jsp injects a null queue name into schedulers.","When you access /jobqueue_details.jsp manually, instead of via a link, it has queueName set to null internally and this goes for a lookup into the scheduling info maps as well.

As a result, if using FairScheduler, a Pool with String name = null gets created and this brings the scheduler down. I have not tested what happens to the CapacityScheduler, but ideally if no queueName is set in that jsp, it should fall back to 'default'. Otherwise, this brings down the JobTracker completely.

FairScheduler must also add a check to not create a pool with 'null' name.

The following is the strace that ensues:

{code}
ERROR org.mortbay.log: /jobqueue_details.jsp 
java.lang.NullPointerException 
at org.apache.hadoop.mapred.jobqueue_005fdetails_jsp._jspService(jobqueue_005fdetails_jsp.java:71) 
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) 
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) 
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) 
at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:829) 
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) 
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399) 
at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) 
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) 
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) 
at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) 
at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230) 
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) 
at org.mortbay.jetty.Server.handle(Server.java:326) 
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) 
at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) 
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) 
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) 
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) 
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410) 
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) 
INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9001, call heartbeat from XYZ:MNOP: error: java.io.IOException: java.lang.NullPointerException 
java.io.IOException: java.lang.NullPointerException 
at org.apache.hadoop.mapred.SchedulingAlgorithms$FairShareComparator.compare(SchedulingAlgorithms.java:95) 
at org.apache.hadoop.mapred.SchedulingAlgorithms$FairShareComparator.compare(SchedulingAlgorithms.java:68) 
at java.util.Arrays.mergeSort(Unknown Source) 
at java.util.Arrays.sort(Unknown Source) 
at java.util.Collections.sort(Unknown Source) 
at org.apache.hadoop.mapred.FairScheduler.assignTasks(FairScheduler.java:435) 
at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:3226) 
at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) 
at java.lang.reflect.Method.invoke(Unknown Source) 
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127) 
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428)
{code}",qwertymaniac,qwertymaniac,Critical,Closed,Fixed,16/Jan/12 03:49,17/Oct/12 18:27
Bug,MAPREDUCE-3680,12538653,FifoScheduler web service rest API can print out invalid JSON,"running a GET on the scheduler web services rest api (RM:port/ws/cluster/scheduler) with the FifoScheduler configured with no nodemanagers up yet and it prints out invalid json of NaN for the used Capacity:

{""scheduler"":{""schedulerInfo"":{""type"":""fifoScheduler"",""capacity"":1.0,""usedCapacity"":NaN,""qstate"":""RUNNING"",""minQueueMemoryCapacity"":1024,""maxQueueMemoryCapacity"":10240,""numNodes"":0,""usedNodeCapacity"":0,""availNodeCapacity"":0,""totalNodeCapacity"":0,""numContainers"":0}}}",,tgraves,Major,Resolved,Fixed,17/Jan/12 17:00,11/Feb/12 13:16
Bug,MAPREDUCE-3681,12538670,capacity scheduler LeafQueues calculate used capacity wrong,"In the Capacity scheduler if you configure the queues to be hierarchical where you have root -> parent queue -> leaf queue, the leaf queue doesn't calculate the used capacity properly. It seems to be using the entire cluster memory rather then its parents memory capacity. 

In updateResource in LeafQueue:
    setUsedCapacity(
        usedResources.getMemory() / (clusterResource.getMemory() * capacity));

I think the clusterResource.getMemory() should be something like getParentsMemory().",acmurthy,tgraves,Critical,Closed,Fixed,17/Jan/12 18:13,05/Mar/12 02:48
Bug,MAPREDUCE-3682,12538674,Tracker URL says AM tasks run on localhost,"If you look at the task page, it will show you the node the task ran on.  For jobs that run in UberAM they point to http://localhost:9999 and logs points to http://localhost:9999/node/containerlogs/$container_id/

This was run on a multi node cluster.",raviprak,dcapwell,Major,Closed,Fixed,17/Jan/12 18:34,11/Oct/12 17:48
Bug,MAPREDUCE-3683,12538678,Capacity scheduler LeafQueues maximum capacity calculation issues,"In the Capacity scheduler if you configure the queues to be hierarchical where you have root -> parent queue -> leaf queue, the leaf queue doesn't take into account its parents maximum capacity when calculate its own maximum capacity, instead it seems to use the parents capacity.  Looking at the code its using the parents absoluteCapacity and I think it should be using the parents absoluteMaximumCapacity.

It also seems to only use the parents capacity in the leaf queues max capacity calculation when the leaf queue has a max capacity configured. If the leaf queues maximum-capacity is not configured, then it can use 100% of the cluster.  ",acmurthy,tgraves,Blocker,Closed,Fixed,17/Jan/12 18:57,05/Mar/12 02:48
Bug,MAPREDUCE-3684,12538714,LocalDistributedCacheManager does not shut down its thread pool,This was observed by running a Hive job in local mode. The job completed but the client process did not exit for 60 seconds.,tomwhite,tomwhite,Major,Closed,Fixed,17/Jan/12 22:59,05/Mar/12 02:49
Bug,MAPREDUCE-3685,12538742,There are some bugs in implementation of MergeManager,,anty.rao,anty,Critical,Closed,Fixed,18/Jan/12 06:35,27/Aug/13 22:21
Bug,MAPREDUCE-3686,12538836,history server web ui - job counter values for map/reduce not shown properly,"Looking at the job counters page on the history server for a finished job, it shows 3 columns for each counter - map, reduce, total. The total appears correct, but map and reduce columns are always zero.  If you click on a particular column it does show you the map/reduce task and the value of each one.

Going to attach screenshots shortly.",kamesh,tgraves,Critical,Resolved,Fixed,18/Jan/12 16:55,27/Feb/12 05:41
Bug,MAPREDUCE-3687,12538850,"If AM dies before it returns new tracking URL, proxy redirects to http://N/A/ and doesn't return error code","I tried to turn on Uber AM and put 9223372036854775807l (last char is an L) for maxbytes.  This caused a
NumberFormatException in the AM and killed it.

When I try to go to the RM proxy, it redirects me to http://N/A/

curl -i http://resource.manager.example.com:$port/proxy/application_1326504761991_0001/
HTTP/1.1 302 Found
Content-Type: text/plain; charset=utf-8
Location: http://N/A/
Content-Length: 0
Server: Jetty(6.1.26)

Since the AM has no tracker URL, I would expect the return code to be 400~ or 500~ and return an error.",raviprak,dcapwell,Major,Resolved,Fixed,18/Jan/12 18:03,01/Mar/12 13:58
Bug,MAPREDUCE-3689,12538859,RM web UI doesn't handle newline in job name,"a user submitted a mapreduce job with a newline (\n) in the job name. This caused the resource manager web ui to get a javascript exception when loading the application and scheduler pages and the pages were pretty well useless after that since they didn't load everything.  Note that this only happens when the data is returned in the JS_ARRAY, which is when you get over 100 applications.

errors:
Uncaught SyntaxError: Unexpected token ILLEGAL
Uncaught ReferenceError: appsData is not defined

It seems odd that we allow the \n in the job name.  It appears we allow it in 1.0 also, although when I ran a test the job itself failed in taskrunner. 0.23 seems to run the job fine with the \n in the job name. ",tgraves,tgraves,Blocker,Closed,Fixed,18/Jan/12 18:44,05/Mar/12 02:49
Bug,MAPREDUCE-3691,12538878,webservices add support to compress response," The web services currently don't support header 'Accept-Encoding: gzip'

Given that the responses have a lot of duplicate data like the property names in JSON or the tag names in XML, it should
compress very well, and would save on bandwidth and download time when fetching a potentially large response, like the
ones from ws/v1/cluster/apps and ws/v1/history/mapreduce/jobs",tgraves,tgraves,Critical,Closed,Fixed,18/Jan/12 20:53,02/May/13 02:29
Bug,MAPREDUCE-3696,12538977,MR job via oozie does not work on hadoop 23,"NM throws an error on submitting an MR job via oozie on the latest Hadoop 23.
*Courtesy: Mona Chitnis (ooize)",johnvijoe,johnvijoe,Blocker,Closed,Fixed,19/Jan/12 15:00,05/Mar/12 02:49
Bug,MAPREDUCE-3697,12538978,Hadoop Counters API limits Oozie's working across different hadoop versions,"Oozie uses Hadoop Counters API, by invoking Counters.getGroup(). However, in
hadoop 23, org.apache.hadoop.mapred.Counters does not implement getGroup(). Its
parent class AbstractCounters implements it. This is different from hadoop20X.
As a result, Oozie compiled with either hadoop version does not work with the
other version.
A specific scenario, Oozie compiled with .23 and run against 205, does not
update job status owing to a Counters API exception.

Will explicit re-compilation against the relevant hadoop jars be required each
time? This will prevent launching a uniform Oozie version across different
clusters.",mahadev,johnvijoe,Blocker,Closed,Fixed,19/Jan/12 15:04,02/May/13 02:29
Bug,MAPREDUCE-3699,12539068,Default RPC handlers are very low for YARN servers,"Mainly NM has a default of 5, RM has 10 and AM also has 10 irrespective of num-slots, num-nodes and num-tasks respectively. Though ideally we want to scale according to slots/nodes/tasks, for now increasing the defaults should be enough.",hitesh,vinodkv,Major,Closed,Fixed,19/Jan/12 23:39,05/Mar/12 02:48
Bug,MAPREDUCE-3701,12539093,Delete HadoopYarnRPC from 0.23 branch.,HadoopYarnRPC file exists in 0.23 (should have been removed with the new HadoopYarnProtoRPC). Trunk does not have this issue.,mahadev,mahadev,Major,Closed,Fixed,20/Jan/12 07:29,05/Mar/12 02:48
Bug,MAPREDUCE-3702,12539154,internal server error trying access application master via proxy with filter enabled,"I had a hadoop.http.filter.initializers in place to do user authentication, but was purposely trying to let it bypass authentication on certain pages.  One of those was the proxy and the application master main page. When I then tried to go to the application master through the proxy it throws an internal server error:

Problem accessing /mapreduce. Reason:

    INTERNAL_SERVER_ERROR
Caused by:

java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:100)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:940)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)


It looks like the problem is that AmIpFilter doesn't check for null returned from httpReq.getCookies()",tgraves,tgraves,Critical,Closed,Fixed,20/Jan/12 15:38,05/Mar/12 02:49
Bug,MAPREDUCE-3703,12539169,ResourceManager should provide node lists in JMX output,"In 0.20.*, the JMX UI for the JobTracker (http://<JobTrackerHost>:50030/jmx) showed lists of Live and BlackListed Nodes under the JobTrackerInfo section.

In 0.23, the ResourceManager JMX UI shows the number of active, decommissioned, lost, unhealthy, and rebooted nodes under the ClusterMetrics section, but does not give the list of nodes.

At least the list of active nodes is needed in JSON format.",epayne,epayne,Critical,Closed,Fixed,20/Jan/12 16:46,05/Mar/12 02:49
Bug,MAPREDUCE-3705,12539220,ant build fails on 0.23 branch ,"running the ant build in mapreduce on the latest 23 branch fails.  Looks like the ivy properties file still has 0.24.0 and then the gridmix dependencies need to have rumen as dependency.

The gridmix errors look like:
   [javac] /home/tgraves/anttest/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java:249: cannot find symbol
    [javac] symbol  : class JobStoryProducer
    [javac] location: class org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator
    [javac]   int setupGenerateDistCacheData(JobStoryProducer jsp)
    [javac]                                  ^",tgraves,tgraves,Blocker,Closed,Fixed,20/Jan/12 23:14,05/Mar/12 02:49
Bug,MAPREDUCE-3706,12539256,HTTP Circular redirect error on the job attempts page,"submitted job and tried to go to following url:

http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW

This resulted in the following HTTP ERROR:

HTTP ERROR 500

Problem accessing /proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW. Reason:

    Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'

Caused by:

org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'
        at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)
        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)
        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)


Note that if you first go to the proxy at: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/ and then click the links to get here you don't get the error.",revans2,tgraves,Critical,Resolved,Fixed,21/Jan/12 04:01,04/Mar/16 05:51
Bug,MAPREDUCE-3708,12539285,Metrics: Incorrect Apps Submitted Count,"Submitted an application with the following configuration
{code:xml}
<property>
 <name>yarn.resourcemanager.am.max-retries</name>
 <value>2</value>
</property>
{code}
In the above case, application had failed first time. So AM attempted the same application again. 
While attempting the same application, *Apps Submitted* counter also has been incremented.",kamesh,kam_iitkgp,Major,Closed,Fixed,21/Jan/12 14:40,10/Mar/15 04:32
Bug,MAPREDUCE-3709,12539506,TestDistributedShell is failing,"TestDistributedShell#testDSShell is failing the assert on line 90 on branch-23.
",hitesh,eli,Major,Closed,Fixed,23/Jan/12 17:36,05/Mar/12 02:49
Bug,MAPREDUCE-3710,12539514,last split generated by FileInputFormat.getSplits may not have the best locality,"The last split generated by FileInputFormat.getSplits considers {{blkLocations.length-1}} to be the hosts for the split.
The last split may be larger than the rest (SPLIT_SLOP=1.1 by default) - in which case locality is picked up from a smaller block.
e.g. 1027MB file with a 128MB split size. The last split ends up being 131MB. The hosts for locality end up being the nodes containing the 3MB block instead of the 128MB block.
",sseth,sseth,Major,Closed,Fixed,23/Jan/12 19:13,05/Mar/12 02:49
Bug,MAPREDUCE-3712,12539543,The mapreduce tar does not contain the hadoop-mapreduce-client-jobclient-tests.jar. ,"Working MRv1 tests were moved into the maven build as part of MAPREDUCE-3582. Some classes like MRBench, SleepJob, FailJob which are essential for QE got moved to jobclient-tests.jar. However the tar.gz file does not contain this jar.",mahadev,raviprak,Blocker,Closed,Fixed,24/Jan/12 00:07,05/Mar/12 02:49
Bug,MAPREDUCE-3713,12539559,Incorrect headroom reported to jobs,"With multiple jobs submitted per user, and multiple users submitting jobs - the headroom reported to the AppMasters is incorrect (very high).
Leads to a deadlock - reduces started, map tasks not complete... and reduces are not preempted by the AM due to the incorrect headroom.",acmurthy,sseth,Blocker,Closed,Fixed,24/Jan/12 01:54,05/Mar/12 02:49
Bug,MAPREDUCE-3714,12539561,Reduce hangs in a corner case,"[~karams] found this long time back and we(Sid/I) ran into this again.

Logs to follow..",vinodkv,vinodkv,Blocker,Closed,Fixed,24/Jan/12 02:04,05/Mar/12 02:49
Bug,MAPREDUCE-3716,12539632,java.io.File.createTempFile fails in map/reduce tasks,container-launch.sh specifies java option java.io.tmpdir when executing the java process for the child container but fails to create the tmpdir. All uses of createTempFile and other commands relying on java.io.tmpdir will fail when called from child container jvms.,jeagles,jeagles,Blocker,Closed,Fixed,24/Jan/12 17:00,05/Mar/12 02:49
Bug,MAPREDUCE-3717,12539649,JobClient test jar has missing files to run all the test programs.,Looks like MAPREDUCE-3582 forgot to move couple of files from the ant builds. The current test jar from jobclient does not work. ,mahadev,mahadev,Blocker,Closed,Fixed,24/Jan/12 19:02,05/Mar/12 02:49
Bug,MAPREDUCE-3720,12539687,Command line listJobs should not visit each AM,"When the RM has a large number of jobs, {{bin/mapred job -list}} takes a long time as it visits each AM to get information like num-maps, num-reduces etc.

We should move all per-AM information to {{bin/mapred job -status}} and keep the list just a list.",vinodkv,vinodkv,Major,Closed,Fixed,24/Jan/12 22:54,05/Mar/12 02:49
Bug,MAPREDUCE-3721,12539712,Race in shuffle can cause it to hang,"If all current {{Fetcher}}s complete while an in-memory merge is in progress - shuffle could hang. 
Specifically - if the memory freed by an in-memory merge does not bring {{MergeManager.usedMemory}} below {{MergeManager.memoryLimit}} and all current Fetchers complete before the in-memory merge completes, another in-memory merge will not be triggered - and shuffle will hang. (All new fetchers are asked to WAIT).
",sseth,sseth,Blocker,Closed,Fixed,25/Jan/12 04:31,03/Dec/12 21:30
Bug,MAPREDUCE-3723,12539751,TestAMWebServicesJobs & TestHSWebServicesJobs incorrectly asserting tests,"While testing a patch for one of the MR issues, I found TestAMWebServicesJobs & TestHSWebServicesJobs incorrectly asserting tests. 
Moreover tests may fail if
{noformat}
	index of counterGroups > #counters in a particular counterGroup
{noformat}
{code:title=TestAMWebServicesJobs.java|borderStyle=solid}
for (int j = 0; j < counters.length(); j++) {
 JSONObject counter = counters.getJSONObject(i);
{code}

where is *i* is index of outer loop. It should be *j* instead of *i*.",kamesh,kam_iitkgp,Major,Closed,Fixed,25/Jan/12 11:57,10/Mar/15 04:33
Bug,MAPREDUCE-3725,12539812,Hadoop 22 hadoop job -list returns user name as NULL,Hadoop 22 hadoop job -list returns user name as NULL,mayank_bansal,mayank_bansal,Major,Resolved,Fixed,25/Jan/12 19:26,02/Feb/12 13:20
Bug,MAPREDUCE-3727,12539815,jobtoken location property in jobconf refers to wrong jobtoken file,"Oozie launcher job (for MR/Pig/Hive/Sqoop action) reads the location of the jobtoken file from the *HADOOP_TOKEN_FILE_LOCATION* ENV var and seeds it as the *mapreduce.job.credentials.binary* property in the jobconf that will be used to launch the real (MR/Pig/Hive/Sqoop) job.

The MR/Pig/Hive/Sqoop submission code (via Hadoop job submission) uses correctly the injected *mapreduce.job.credentials.binary* property to load the credentials and submit their MR jobs.

The problem is that the *mapreduce.job.credentials.binary* property also makes it to the tasks of the MR/Pig/Hive/Sqoop MR jobs.

If for some reason the MR/Pig/Hive/Sqoop MR code does some logic that triggers the credential loading, because the property is set, the credential loading fails trying to load a jobtoken file of the launcher job which does not exists in the context of the MR/Pig/Hive/Sqoop jobs.

More specifically, we are seeing this happening with certain hive queries that trigger a conditional code within their RowContainer which then uses the FileInputFormat.getSplits() and then the TokenCache tries to load credentials for a file that is for the wrong job.
",tucu00,tucu00,Critical,Closed,Fixed,25/Jan/12 19:33,15/May/13 05:16
Bug,MAPREDUCE-3728,12539819,ShuffleHandler can't access results when configured in a secure mode,"While running the simplest of jobs (Pi) on MR2 in a fully secure configuration I have noticed that the job was failing on the reduce side with the following messages littering the nodemanager logs:

{noformat}
2012-01-19 08:35:32,544 ERROR org.apache.hadoop.mapred.ShuffleHandler: Shuffle error
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find usercache/rvs/appcache/application_1326928483038_0001/output/attempt_1326928483038_0001_m_000003_0/file.out.index in any of the configured local directories
{noformat}

While digging further I found out that the permissions on the files/dirs were prohibiting nodemanager (running under the user yarn) to access these files:

{noformat}
$ ls -l /data/3/yarn/usercache/testuser/appcache/application_1327102703969_0001/output/attempt_1327102703969_0001_m_000001_0
-rw-r----- 1 testuser testuser 28 Jan 20 15:41 file.out
-rw-r----- 1 testuser testuser 32 Jan 20 15:41 file.out.index
{noformat}

Digging even further revealed that the group-sticky bit that was faithfully put on all the subdirectories between testuser and application_1327102703969_0001 was gone from output and attempt_1327102703969_0001_m_000001_0. 

Looking into how these subdirectories are created (org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.initDirs())
{noformat}
      // $x/usercache/$user/appcache/$appId/filecache
      Path appFileCacheDir = new Path(appBase, FILECACHE);
      appsFileCacheDirs[i] = appFileCacheDir.toString();
      lfs.mkdir(appFileCacheDir, null, false);
      // $x/usercache/$user/appcache/$appId/output
      lfs.mkdir(new Path(appBase, OUTPUTDIR), null, false);
{noformat}

Reveals that lfs.mkdir ends up manipulating permissions and thus clears sticky bit from output and filecache.

At this point I'm at a loss about how this is supposed to work. My understanding was
that the whole sequence of events here was predicated on a sticky bit set so
that daemons running under the user yarn (default group yarn) can have access
to the resulting files and subdirectories down at output and below. Please let
me know if I'm missing something or whether this is just a bug that needs to be fixed.

On a related note, when the shuffle side of the Pi job failed the job itself didn't.
It went into the endless loop and only exited when it exhausted all the local storage
for the log files (at which point the nodemanager died and thus the job ended). Perhaps
this is even more serious side effect of this issue that needs to be investigated 
separately.",dyuan3,rvs,Critical,Closed,Fixed,25/Jan/12 19:48,30/Jul/13 00:12
Bug,MAPREDUCE-3732,12539868,CS should only use 'activeUsers with pending requests' for computing user-limits,"CS should only use 'activeUsers with pending requests' for computing user-limits, similar to what is done in hadoop-1.",acmurthy,acmurthy,Blocker,Closed,Fixed,26/Jan/12 07:51,05/Mar/12 02:49
Bug,MAPREDUCE-3733,12539873,Add Apache License Header to hadoop-distcp/pom.xml,Looks like I missed the Apache Headers in the review. Adding it now.,mahadev,mahadev,Major,Closed,Fixed,26/Jan/12 08:19,05/Mar/12 02:49
Bug,MAPREDUCE-3735,12539879,Add distcp jar to the distribution (tar),Distcp jar isnt getting added to the tarball as of now. We need to add it along with archives/streaming and others.,mahadev,mahadev,Blocker,Closed,Fixed,26/Jan/12 09:11,05/Mar/12 02:49
Bug,MAPREDUCE-3736,12539948,Variable substitution depth too large for fs.default.name causes jobs to fail,"I'm seeing the same failure as MAPREDUCE-3462 in downstream projects running against a recent build of branch-23. MR-3462 modified the tests rather than fixing the framework. In that jira Ravi mentioned ""I'm still ignorant of the change which made the tests start to fail. I should probably understand better the reasons for that change before proposing a more generalized fix."" Let's figure out the general fix (rather than require all projects to set mapreduce.job.hdfs-servers in their conf we should fix this in the framework). Perhaps we should not default this config to ""$fs.default.name""?",ahmed.radwan,eli,Blocker,Resolved,Fixed,26/Jan/12 18:36,15/Feb/12 13:53
Bug,MAPREDUCE-3737,12539951,The Web Application Proxy's is not documented very well,"The Web Application Proxy is a security feature, but there is no documentation for what it does, why it does it, and more importantly what attacks it is known not protect against.  This is so that anyone addopting Hadoop can know exactly what they potential security issues they may encounter.",revans2,revans2,Critical,Closed,Fixed,26/Jan/12 18:50,05/Mar/12 02:49
Bug,MAPREDUCE-3738,12539965,NM can hang during shutdown if AppLogAggregatorImpl thread dies unexpectedly,"If an AppLogAggregator thread dies unexpectedly (e.g.: uncaught exception like OutOfMemoryError in the case I saw) then this will lead to a hang during nodemanager shutdown.  The NM calls AppLogAggregatorImpl.join() during shutdown to make sure log aggregation has completed, and that method internally waits for an atomic boolean to be set by the log aggregation thread to indicate it has finished.  Since the thread was killed off earlier due to an uncaught exception, the boolean will never be set and the NM hangs during shutdown repeating something like this every second in the log file:

2012-01-25 22:20:56,366 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: Waiting for aggregation to complete for application_1326848182580_2806",jlowe,jlowe,Critical,Resolved,Fixed,26/Jan/12 20:57,10/Mar/15 04:31
Bug,MAPREDUCE-3740,12540027,Mapreduce Trunk compilation fails,"{code:xml}
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /home/hadoop/hadoop-trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ipc/ProtoOverHadoopRpcEngine.java:[61,7] org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine is not abstract and does not override abstract method 
getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client.ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory) in org.apache.hadoop.ipc.RpcEngine
[INFO] 1 error
[INFO] -------------------------------------------------------------

{code}",devaraj,devaraj,Blocker,Closed,Fixed,27/Jan/12 11:47,10/Mar/15 04:31
Bug,MAPREDUCE-3741,12540138,Conflicting dependency in hadoop-mapreduce-examples,"{code:xml}
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-mapreduce-client-hs</artifactId>
       <scope>provided</scope>
     </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-mapreduce-client-hs</artifactId>
       <scope>test</scope>
     </dependency>
{code}

Are we missing <type> here?",,kihwal,Major,Resolved,Fixed,27/Jan/12 15:23,09/Mar/15 20:34
Bug,MAPREDUCE-3742,12540157,"""yarn logs"" command fails with ClassNotFoundException","Executing ""yarn logs"" at a shell prompt fails with this error:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogDumper
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogDumper
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogDumper.  Program will exit.

Appears to have been caused by the code reorg in MAPREDUCE-3297.",jlowe,jlowe,Blocker,Closed,Fixed,27/Jan/12 17:30,10/Mar/15 04:32
Bug,MAPREDUCE-3744,12540187,"Unable to retrieve application logs via ""yarn logs"" or ""mapred job -logs""","Trying to retrieve application logs via the ""yarn logs"" shell command results in an error similar to this:

Exception in thread ""main"" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.
	at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)
	at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)
	at org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)
	at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)
	at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)
	at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)
	at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)
	at org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)
	at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)
	at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)
	at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)

Trying to grab the logs via the ""mapred jobs -logs"" command results in this error:

2012-01-27 14:05:52,040 INFO  mapred.ClientServiceDelegate (ClientServiceDelegate.java:getProxy(246)) - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
2012-01-27 14:05:52,041 WARN  mapred.ClientServiceDelegate (ClientServiceDelegate.java:checkAndGetHSProxy(257)) - Job History Server is not configured.
Unable to get log information for job: job_1327694122989_0001

Even though the historyserver process is running.",jlowe,jlowe,Blocker,Closed,Fixed,27/Jan/12 20:31,10/Mar/15 04:32
Bug,MAPREDUCE-3747,12540221,Memory Total is not refreshed until an app is launched,"Memory Total on the RM UI is not refreshed until an application is launched. This is a problem when the cluster is started for the first time or when there are any lost/decommissioned NMs.
When the cluster is started for the first time, Active Nodes is > 0 but the Memory Total=0. Also when there are any lost/decommissioned nodes, Memory Total has wrong value.
This is a useful tool for cluster admins and has to be updated correctly without having the need to submit an app each time.",acmurthy,rramya,Major,Closed,Fixed,28/Jan/12 00:29,05/Mar/12 02:49
Bug,MAPREDUCE-3748,12540225,Move CS related nodeUpdate log messages to DEBUG,"Currently, the RM has nodeUpdate logs per NM per second such as the following:
2012-01-27 21:51:32,429 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <nodemanager1>:<port1> clusterResources: memory: 57344
2012-01-27 21:51:32,510 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <nodemanager2>:<port2> clusterResources: memory: 57344
2012-01-27 21:51:33,094 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <nodemanager1>:<port1> clusterResources: memory: 57344

Debugging is difficult with huge amount of logs such as this. These logs need to be moved to DEBUG.",rramya,rramya,Minor,Closed,Fixed,28/Jan/12 00:56,05/Mar/12 02:48
Bug,MAPREDUCE-3749,12540226,ConcurrentModificationException in counter groups,"Iterating over a counter's groups while adding more groups will cause a ConcurrentModificationException.

This was found while running Hive unit tests against a recent 0.23 version.",tomwhite,tomwhite,Blocker,Closed,Fixed,28/Jan/12 01:25,05/Mar/12 02:49
Bug,MAPREDUCE-3752,12540237,Headroom should be capped by queue max-cap,Headroom should be capped by queue max-cap.,acmurthy,acmurthy,Blocker,Closed,Fixed,28/Jan/12 03:16,05/Mar/12 02:49
Bug,MAPREDUCE-3754,12540303,RM webapp should have pages filtered based on App-state,Helps a lot when we have lot of apps. Already having difficulties with gridmix with a single big list of apps of all states.,vinodkv,vinodkv,Major,Closed,Fixed,29/Jan/12 20:00,05/Mar/12 02:48
Bug,MAPREDUCE-3757,12540338,Rumen Folder is not adjusting the shuffleFinished and sortFinished times of reduce task attempts,Rumen Folder is not adjusting the shuffleFinished and sortFinished times of reduce task attempts when it is adjusting the attempt-start-time and attempt-finish-time. This is leading to wrong values which are greater than the attempt-finish-time in trace file.,ravidotg,ravidotg,Major,Closed,Fixed,30/Jan/12 10:07,03/Sep/14 22:45
Bug,MAPREDUCE-3759,12540415,ClassCastException thrown in -list-active-trackers when there are a few unhealthy nodes,"When there are a few blacklisted nodes in the cluster, ""bin/mapred job -list-active-trackers"" throws ""java.lang.ClassCastException: org.apache.hadoop.yarn.server.resourcemanager.resource.Resources$1 cannot be cast to org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl""",vinodkv,rramya,Major,Closed,Fixed,30/Jan/12 19:31,05/Mar/12 02:48
Bug,MAPREDUCE-3760,12540418,Blacklisted NMs should not appear in Active nodes list,"Blacklisted NMs appear in both ""Active Nodes"" and ""Unhealthy nodes"" on the RM UI. This should be fixed.",vinodkv,rramya,Major,Closed,Fixed,30/Jan/12 20:12,05/Mar/12 02:48
Bug,MAPREDUCE-3762,12540441,Resource Manager fails to come up with default capacity scheduler configs.,"Thanks to [~harip] for pointing out the issue. This is the stack trace for bringing up RM with default CS configs:

{code}
java.lang.IllegalArgumentException: Illegal value  of maximumCapacity -0.01 used in call to setMaxCapacity for queue default
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils.checkMaxCapacity(CSQueueUtils.java:28)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs(LeafQueue.java:210)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:184)
{code}",mahadev,mahadev,Critical,Closed,Fixed,30/Jan/12 21:40,05/Mar/12 02:49
Bug,MAPREDUCE-3764,12540456,AllocatedGB etc metrics incorrect if min-allocation-mb isn't a multiple of 1GB,"MutableGaugeInt incremented as {{allocatedGB.incr(res.getMemory() / GB * containers);}}

Setting yarn.scheduler.capacity.minimum-allocation-mb to 1536 - each increment is counted as 1GB.
Trying to analyze the metrics - looks like the cluster is never over 67-68% utilized, depending on high ram requests.",acmurthy,sseth,Critical,Closed,Fixed,30/Jan/12 23:21,05/Mar/12 02:48
Bug,MAPREDUCE-3765,12540457,FifoScheduler does not respect yarn.scheduler.fifo.minimum-allocation-mb setting,FifoScheduler uses default min 1 GB regardless of the configuration value set for minimum memory allocation.,hitesh,hitesh,Minor,Closed,Fixed,30/Jan/12 23:32,10/Mar/15 04:32
Bug,MAPREDUCE-3770,12540503,[Rumen] Zombie.getJobConf() results into NPE,"The error trace is as follows
{code}
java.lang.NullPointerException
        at java.util.Hashtable.put(Hashtable.java:394)
        at java.util.Properties.setProperty(Properties.java:143)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:623)
        at org.apache.hadoop.mapred.JobConf.setJobName(JobConf.java:1322)
        at org.apache.hadoop.tools.rumen.ZombieJob.getJobConf(ZombieJob.java:139)
        at org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator.updateHDFSDistCacheFilesList(DistributedCacheEmulator.java:315)
        at org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator.buildDistCacheFilesList(DistributedCacheEmulator.java:280)
        at org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator.setupGenerateDistCacheData(DistributedCacheEmulator.java:253)
        at org.apache.hadoop.mapred.gridmix.Gridmix.setupDistCacheEmulation(Gridmix.java:528)
        at org.apache.hadoop.mapred.gridmix.Gridmix.setupEmulation(Gridmix.java:501)
        at org.apache.hadoop.mapred.gridmix.Gridmix.start(Gridmix.java:433)
        at org.apache.hadoop.mapred.gridmix.Gridmix.runJob(Gridmix.java:380)
        at org.apache.hadoop.mapred.gridmix.Gridmix.access$000(Gridmix.java:56)
        at org.apache.hadoop.mapred.gridmix.Gridmix$1.run(Gridmix.java:313)
        at org.apache.hadoop.mapred.gridmix.Gridmix$1.run(Gridmix.java:311)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
        at org.apache.hadoop.mapred.gridmix.Gridmix.run(Gridmix.java:311)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.mapred.gridmix.Gridmix.main(Gridmix.java:606)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
{code}

The bug seems to be in {{ZombieJob#getName()}} where a not-null check for jobName.getValue() is missing. ",amar_kamat,amar_kamat,Critical,Closed,Fixed,31/Jan/12 06:54,10/Mar/15 04:31
Bug,MAPREDUCE-3774,12540632,yarn-default.xml should be moved to hadoop-yarn-common.,yarn-default.xml right now resides in hadoop-yarn-server-common jars which is not the right thing to do since this jar might not be needed in some cases when depending upon yarn. We should move it to hadoop-yarn-common which is a required dependency for all the yarn components (client/server).,mahadev,mahadev,Major,Closed,Fixed,31/Jan/12 21:30,05/Mar/12 02:49
Bug,MAPREDUCE-3775,12540642,Change MiniYarnCluster to escape special chars in testname,"When using MiniYarnCluster with the testname set to a nested classname, the ""$"" within the class name creates issues with the container launch scripts as they try to expand the $... within the paths/variables in use.  ",hitesh,hitesh,Minor,Closed,Fixed,31/Jan/12 22:27,10/Mar/15 04:32
Bug,MAPREDUCE-3777,12540660,used mem and util have negative values after a queue addition,"After a queue addition to capacity scheduler and submission of an application, root queue utilization and used memory have negative values. 
",acmurthy,rramya,Major,Resolved,Fixed,01/Feb/12 00:41,09/Mar/15 21:56
Bug,MAPREDUCE-3780,12540768,RM assigns containers to killed applications,RM attempts to assign containers to killed applications. The applications were killed when they were inactive and waiting for AM allocation.,hitesh,rramya,Blocker,Closed,Fixed,01/Feb/12 19:47,05/Mar/12 02:48
Bug,MAPREDUCE-3782,12540775,teragen terasort jobs fail when using webhdfs:// ,"When running a teragen job with a webhdfs:// url the delegation token that is retrieved is an hdfs delegation token. 

And the subsequent terasort job on the output fails with java io exception",jlowe,arpitgupta,Critical,Closed,Fixed,01/Feb/12 20:46,10/Mar/15 04:32
Bug,MAPREDUCE-3784,12540831,maxActiveApplications(|PerUser) per queue is too low for small clusters,"We ran into this issue while testing on small clusters. 
On a 7node cluster with 8G per node,  for a queue with absolute capacity 30%, user limit 100%, maxActiveApplications and maxActiveApplicationsPerUser is calculated to be 1.
This means that even though the queue has 17GB(0.3*8*7), only 1 user can run 1 app at a given time queuing up rest of the apps/users. This hurts performance on small clusters.
",acmurthy,rramya,Major,Closed,Fixed,01/Feb/12 22:46,05/Mar/12 02:49
Bug,MAPREDUCE-3789,12540903,CapacityTaskScheduler may perform unnecessary reservations in heterogenous tracker environments,"Briefly, to reproduce:

* Run JT with CapacityTaskScheduler [Say, Cluster max map = 8G, Cluster map = 2G]
* Run two TTs but with varied capacity, say, one with 4 map slot, another with 3 map slots.
* Run a job with two tasks, each demanding mem worth 4 slots at least (Map mem = 7G or so).
* Job will begin running on TT #1, but will also end up reserving the 3 slots on TT #2 cause it does not check for the maximum limit of slots when reserving (as it goes greedy, and hopes to gain more slots in future).
* Other jobs that could've run on the TT #2 over 3 slots are thereby blocked out due to this illogical reservation.

I've not yet tested MR2 for this so feel free to weigh in if it affects MR2 as well.

For MR1, I've attached a test case initially to indicate this. A fix that checks reservations vs. max slots, to follow.",qwertymaniac,qwertymaniac,Critical,Closed,Fixed,02/Feb/12 10:07,17/Oct/12 18:27
Bug,MAPREDUCE-3790,12540962,Broken pipe on streaming job can lead to truncated output for a successful job,"If a streaming job doesn't consume all of its input then the job can be marked successful even though the job's output is truncated.

Here's a simple setup that can exhibit the problem.  Note that the job output will most likely be truncated compared to the same job run with a zero-length input file.

{code}
$ hdfs dfs -cat in
foo
$ yarn jar ./share/hadoop/tools/lib/hadoop-streaming-0.24.0-SNAPSHOT.jar -Dmapred.map.tasks=1 -Dmapred.reduce.tasks=1 -mapper /bin/env -reducer NONE -input in -output out
{code}

Examining the map task log shows this:

{code:title=Excerpt from map task stdout log}
2012-02-02 11:27:25,054 WARN [main] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Broken pipe
2012-02-02 11:27:25,054 INFO [main] org.apache.hadoop.streaming.PipeMapRed: mapRedFinished
2012-02-02 11:27:25,056 WARN [Thread-12] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Bad file descriptor
2012-02-02 11:27:25,124 INFO [main] org.apache.hadoop.mapred.Task: Task:attempt_1328203555769_0001_m_000000_0 is done. And is in the process of commiting
2012-02-02 11:27:25,127 WARN [Thread-11] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: DFSOutputStream is closed
2012-02-02 11:27:25,199 INFO [main] org.apache.hadoop.mapred.Task: Task attempt_1328203555769_0001_m_000000_0 is allowed to commit now
2012-02-02 11:27:25,225 INFO [main] org.apache.hadoop.mapred.FileOutputCommitter: Saved output of task 'attempt_1328203555769_0001_m_000000_0' to hdfs://localhost:9000/user/somebody/out/_temporary/1
2012-02-02 11:27:27,834 INFO [main] org.apache.hadoop.mapred.Task: Task 'attempt_1328203555769_0001_m_000000_0' done.
{code}

In PipeMapRed.mapRedFinished() we can see it will eat IOExceptions and return without waiting for the output threads or throwing a runtime exception to fail the job.  Net result is that the DFS streams could be shutdown too early if the output threads are still busy and we could lose job output.

Fixing this brings up the bigger question of what *should* happen when a streaming job doesn't consume all of its input.  Should we have grabbed all of the output from the job and still marked it successful or should we have failed the job?  If the former then we need to fix some other places in the code as well, since feeding a much larger input file (e.g.: 600K) to the same sample streaming job results in the job failing with the exception below.  It wouldn't be consistent to fail the job that doesn't consume a lot of input but pass the job that leaves just a few leftovers.

{code}
2012-02-02 10:29:37,220 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1270)) - Running job: job_1328200108174_0001
2012-02-02 10:29:44,354 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1291)) - Job job_1328200108174_0001 running in uber mode : false
2012-02-02 10:29:44,355 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1298)) -  map 0% reduce 0%
2012-02-02 10:29:46,394 INFO  mapreduce.Job (Job.java:printTaskEvents(1386)) - Task Id : attempt_1328200108174_0001_m_000000_0, Status : FAILED
Error: java.io.IOException: Broken pipe
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:282)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:394)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:329)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
{code}

Assuming the job returns a successful exit code, I think we should allow the job to complete successfully even though it doesn't consume all of its inputs.  Part of the reasoning is that there's already this comment in PipeMapper.java that implies we desire that behavior:

{code:title=PipeMapper.java}
        // terminate with success:
        // swallow input records although the stream processor failed/closed
{code}",jlowe,jlowe,Major,Resolved,Fixed,02/Feb/12 18:04,12/Apr/17 19:57
Bug,MAPREDUCE-3791,12540969,can't build site in hadoop-yarn-server-common,"Here's how to reproduce:

{noformat}
$ mvn site site:stage -DskipTests -DskipTest -DskipITs
....
main:
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................ SUCCESS [49.017s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [5.152s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [4.973s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [4.514s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [4.334s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [5.215s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [4.051s]
[INFO] Apache Hadoop Common .............................. SUCCESS [4.111s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [4.198s]
[INFO] Apache Hadoop HDFS ................................ SUCCESS [5.373s]
[INFO] Apache Hadoop HttpFS .............................. SUCCESS [22.549s]
[INFO] Apache Hadoop HDFS Project ........................ SUCCESS [4.440s]
[INFO] hadoop-yarn ....................................... SUCCESS [5.250s]
[INFO] hadoop-yarn-api ................................... SUCCESS [4.579s]
[INFO] hadoop-yarn-common ................................ SUCCESS [4.268s]
[INFO] hadoop-yarn-server ................................ SUCCESS [4.408s]
[INFO] hadoop-yarn-server-common ......................... FAILURE [0.035s]
[INFO] hadoop-yarn-server-nodemanager .................... SKIPPED
[INFO] hadoop-yarn-server-web-proxy ...................... SKIPPED
[INFO] hadoop-yarn-server-resourcemanager ................ SKIPPED
[INFO] hadoop-yarn-server-tests .......................... SKIPPED
[INFO] hadoop-mapreduce-client ........................... SKIPPED
[INFO] hadoop-mapreduce-client-core ...................... SKIPPED
[INFO] hadoop-yarn-applications .......................... SKIPPED
[INFO] hadoop-yarn-applications-distributedshell ......... SKIPPED
[INFO] hadoop-yarn-site .................................. SKIPPED
[INFO] hadoop-mapreduce-client-common .................... SKIPPED
[INFO] hadoop-mapreduce-client-shuffle ................... SKIPPED
[INFO] hadoop-mapreduce-client-app ....................... SKIPPED
[INFO] hadoop-mapreduce-client-hs ........................ SKIPPED
[INFO] hadoop-mapreduce-client-jobclient ................. SKIPPED
[INFO] Apache Hadoop MapReduce Examples .................. SKIPPED
[INFO] hadoop-mapreduce .................................. SKIPPED
[INFO] Apache Hadoop MapReduce Streaming ................. SKIPPED
[INFO] Apache Hadoop Distributed Copy .................... SKIPPED
[INFO] Apache Hadoop Archives ............................ SKIPPED
[INFO] Apache Hadoop Rumen ............................... SKIPPED
[INFO] Apache Hadoop Extras .............................. SKIPPED
[INFO] Apache Hadoop Tools Dist .......................... SKIPPED
[INFO] Apache Hadoop Tools ............................... SKIPPED
[INFO] Apache Hadoop Distribution ........................ SKIPPED
[INFO] Apache Hadoop Client .............................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ........................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:18.427s
[INFO] Finished at: Thu Feb 02 10:31:35 PST 2012
[INFO] Final Memory: 321M/1012M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (default) on project hadoop-yarn-server-common: An Ant BuildException has occured: Warning: Could not find file /home/rvs/src/apache/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/resources/yarn-default.xml to copy. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-yarn-server-common
{noformat}",mahadev,rvs,Major,Closed,Fixed,02/Feb/12 18:34,05/Mar/12 02:48
Bug,MAPREDUCE-3792,12541004,job -list displays only the jobs submitted by a particular user,"""mapred job -list"" lists only the jobs submitted by the user who ran the command. This behavior is different from 1.x. 
",jlowe,rramya,Critical,Resolved,Fixed,02/Feb/12 21:42,04/Mar/12 13:57
Bug,MAPREDUCE-3794,12541013,Support mapred.Task.Counter and mapred.JobInProgress.Counter enums for compatibility,"The new counters are mapreduce.TaskCounter and mapreduce.JobCounter, but we should support the old ones too since they are public in Hadoop 1.x.",tomwhite,tomwhite,Major,Closed,Fixed,02/Feb/12 22:47,05/Mar/12 02:49
Bug,MAPREDUCE-3795,12541022,"""job -status"" command line output is malformed",Misses new lines after numMaps and numReduces. Caused by MAPREDUCE-3720.,vinodkv,vinodkv,Major,Closed,Fixed,02/Feb/12 23:49,05/Mar/12 02:49
Bug,MAPREDUCE-3801,12541111,org.apache.hadoop.mapreduce.v2.app.TestRuntimeEstimators.testExponentialEstimator fails intermittently,"org.apache.hadoop.mapreduce.v2.app.TestRuntimeEstimators,testExponentialEstimator fails intermittently",jlowe,revans2,Major,Resolved,Fixed,03/Feb/12 17:19,18/Sep/18 21:36
Bug,MAPREDUCE-3804,12541138,yarn webapp interface vulnerable to cross scripting attacks,"Yarn webapp interface may be vulnerable to certain cross scripting attacks, injected through URL request.

",davet,davet,Major,Closed,Fixed,03/Feb/12 21:34,05/Mar/12 02:49
Bug,MAPREDUCE-3808,12541206,NPE in FileOutputCommitter when running a 0 reduce job,"This was while running LoadGen.

{noformat}
Error: java.lang.NullPointerException at org.apache.hadoop.fs.Path.<init>(Path.java:67) 
at org.apache.hadoop.fs.Path.<init>(Path.java:56) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getPendingJobAttemptsPath(FileOutputCommitter.java:118) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(FileOutputCommitter.java:167) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(FileOutputCommitter.java:149) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getPendingTaskAttemptsPath(FileOutputCommitter.java:185) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209) 
at org.apache.hadoop.mapred.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:100) 
at org.apache.hadoop.mapred.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:94) 
at org.apache.hadoop.mapred.FileOutputCommitter.needsTaskCommit(FileOutputCommitter.java:176) 
at org.apache.hadoop.mapred.OutputCommitter.needsTaskCommit(OutputCommitter.java:248) 
at org.apache.hadoop.mapred.Task.isCommitRequired(Task.java:955) 
at org.apache.hadoop.mapred.Task.done(Task.java:912) 
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:331) 
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:396) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157) 
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
{noformat}",revans2,sseth,Blocker,Closed,Fixed,04/Feb/12 19:51,10/Mar/15 04:32
Bug,MAPREDUCE-3814,12541365,MR1 compile fails,"$ ant veryclean all-jars -Dversion=0.23.1 -Dresolvers=internal


BUILD FAILED
/grid/0/dev/acm/hadoop-0.23/hadoop-mapreduce-project/build.xml:537: srcdir ""/grid/0/dev/acm/hadoop-0.23/hadoop-mapreduce-project/src/test/mapred/testjar"" does not exist!
",acmurthy,acmurthy,Major,Closed,Fixed,06/Feb/12 03:43,05/Mar/12 02:49
Bug,MAPREDUCE-3816,12541421,capacity scheduler web ui bar graphs for used capacity wrong,"The capacity scheduler web ui has bar graphs showing the capacity/used capacity/max capacity for each queue. The used capacity it is showing is actually the % of its parents queue it is using, which doesn't make sense on the bar graphs when compared to the capacity and max capacity of that particular queue.  The bar graphs should be using utilization so that the user can see that its using x% or the y% allocated to that queue.

I will attach some screen shots showing the issue.",tgraves,tgraves,Critical,Resolved,Fixed,06/Feb/12 15:18,29/Feb/12 13:57
Bug,MAPREDUCE-3817,12541454,bin/mapred command cannot run distcp and archive jobs,,arpitgupta,arpitgupta,Major,Closed,Fixed,06/Feb/12 18:06,10/Mar/15 04:31
Bug,MAPREDUCE-3818,12541471,Trunk MRV1 compilation is broken.,"Seeing this:
{code}
    [javac] /Users/vinodkv/Workspace/eclipse-workspace/apache-git/hadoop-common/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java:155: cannot find symbol
    [javac] symbol  : class ClientNamenodeWireProtocol
    [javac] location: class org.apache.hadoop.mapred.TestSubmitJob
    [javac]       RPC.getProxy(ClientNamenodeWireProtocol.class,
    [javac]                    ^
{code}",sureshms,vinodkv,Blocker,Closed,Fixed,06/Feb/12 19:40,10/Mar/15 04:31
Bug,MAPREDUCE-3822,12541503,TestJobCounters is failing intermittently on trunk and 0.23.,TestJobCounters fails sometimes on trunk. I have tracked it down to stats issue in FileSystem. Still working on it. ,mahadev,mahadev,Critical,Closed,Fixed,06/Feb/12 21:07,05/Mar/12 02:49
Bug,MAPREDUCE-3824,12541523,Distributed caches are not removed properly,Distributed caches are not being properly removed by the TaskTracker when they are expected to be expired. ,tgraves,aw,Critical,Closed,Fixed,06/Feb/12 22:49,05/Apr/12 18:42
Bug,MAPREDUCE-3826,12541535,RM UI when loaded throws a message stating Data Tables warning and then the column sorting stops working,,jeagles,arpitgupta,Major,Closed,Fixed,06/Feb/12 23:30,05/Mar/12 02:49
Bug,MAPREDUCE-3828,12541553,Broken urls: AM tracking url and jobhistory url in a single node setup.,"If the user doesn't explicitly set the yarn.resourcemanager.address conf property, in a single node setup, and tries to connect to the web UI from a remote machine, then all links (AM tracking url and jobhistory url) in the Web UI are broken (pointing to IP address 0.0.0.0).
",sseth,ahmed.radwan,Major,Closed,Fixed,07/Feb/12 02:56,05/Mar/12 02:49
Bug,MAPREDUCE-3829,12541579,[Gridmix] Gridmix should give better error message when input-data directory already exists and -generate option is given,"Instead of throwing exception messages on to the console, Gridmix should give better error message when input-data directory already exists and -generate option is given.",ravidotg,ravidotg,Major,Closed,Fixed,07/Feb/12 08:00,03/Sep/14 22:45
Bug,MAPREDUCE-3833,12541679,Capacity scheduler queue refresh doesn't recompute queue capacities properly,Refreshing the capacity scheduler configuration (e.g.: via yarn rmadmin -refreshQueues) can fail to compute the proper absolute capacity for leaf queues.,jlowe,jlowe,Major,Closed,Fixed,07/Feb/12 19:20,10/Mar/15 04:32
Bug,MAPREDUCE-3834,12541683,"If multiple hosts for a split belong to the same rack, the rack is added multiple times in the AM request table",Should be added only once - so that the RM doesn't think there's multiple rack local requests for that particular rack.,sseth,sseth,Critical,Closed,Fixed,07/Feb/12 19:47,05/Mar/12 02:49
Bug,MAPREDUCE-3840,12541820,JobEndNotifier doesn't use the proxyToUse during connecting,"I stupidly removed the proxyToUse from openConnection() in MAPREDUCE-3649.
",raviprak,raviprak,Blocker,Closed,Fixed,08/Feb/12 15:55,10/Mar/15 04:32
Bug,MAPREDUCE-3843,12542027,Job summary log file found missing on the RM host,"This bug was found by Phil Su as part of our testing.

After MAPREDUCE-3354 went in, the Job summary log file seems to have gone missing on the
RM host.

The job summary log appears to be interspersed in yarn-mapredqa-historyserver-<host>.out. 
e.g. 
12/02/09 15:57:21 INFO jobhistory.JobSummary:
jobId=job_1328658619341_0011,submitTime=1328802904381,launchTime=1328802909977,firstMapTaskLaunchTime=1328802912116,firstReduceTaskLaunchTime=1328802915074,finishTime=1328802933797,resourc
esPerMap=1024,resourcesPerReduce=2048,numMaps=10,numReduces=10,user=hadoopqa,queue=default,status=KILLED,mapSlotSeconds=0,reduceSlotSeconds=0

1) On the RM with older hadoop version where the job summary log does not exist
mapredqa 10903  0.0  1.2 1424404 210240 ?      Sl   Feb07   0:19 /home/gs/java/jdk64/current/bin/java -Xmx1000m
-Djava.net.preferIPv4Stack=true
-Djava.library.path=/home/gs/gridre/theoden/share/hadoop/lib/native/Linux-amd64-64:/
home/gs/gridre/theoden/share/hadoop/lib/native/Linux-amd64-64 -Dhadoop.log.dir=/home/gs/var/log/mapredqa
-Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/gs/gridre/theoden/share/hadoop -Dhadoop.id.str=mapredqa
-Dhadoop
.root.logger=INFO,console
-Djava.library.path=/home/gs/gridre/theoden/share/hadoop/lib/native/Linux-amd64-64:/home/gs/gridre/theoden/share/hadoop/lib/native/Linux-amd64-64:/home/gs/gridre/theoden/share/hadoop/lib/nat
ive -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dmapred.jobsummary.logger=INFO,console
-Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer

2) On the RM with older hadoop version where the job summary log exists
mapredqa 24851  0.0  0.5 1463280 90516 ?       Sl   Jan25   0:37 /home/gs/java/jdk64/current/bin/java
-Dproc_historyserver -Xmx1000m -Dmapred.jobsummary.logger=INFO,JSA -Dyarn.log.dir=/home/gs/var/log/mapredqa
-Dyarn.log.file=yarn.log -Dyarn.home.dir= -Dyarn.id.str= -Dyarn.root.logger=INFO,console
-Djava.library.path=/home/gs/gridre/shelob/share/hadoop/lib/native/Linux-amd64-64
-Dyarn.policy.file=hadoop-policy.xml -Dyarn.log.dir=/home/gs/var/log/mapredqa
-Dyarn.log.file=yarn-mapredqa-historyserver-<host>.log -Dyarn.home.dir= -Dyarn.id.str=mapredqa
-Dyarn.root.logger=INFO,DRFA -Djava.library.path=/home/gs/gridre/shelob/share/hadoop/lib/native/Linux-amd64-64
-Dyarn.policy.file=hadoop-policy.xml -Dmapred.jobsummary.logger=INFO,JSA -Dhadoop.log.dir=/home/gs/var/log/mapredqa
-Dyarn.log.dir=/home/gs/var/log/mapredqa
-Dhadoop.log.file=yarn-mapredqa-historyserver-<host>.log
-Dyarn.log.file=yarn-mapredqa-historyserver-<host>.log
-Dyarn.home.dir=/home/gs/gridre/shelob/share/hadoop -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA
-Djava.library.path=/home/gs/gridre/shelob/share/hadoop/lib/native/Linux-amd64-64 -classpath
/home/gs/gridre/shelob/conf/hadoop:/home/gs/gridre/shelob/conf/hadoop:/home/gs/gridre/shelob/conf/hadoop:/home/gs/gridre/shelob/conf/hadoop:/home/gs/gridre/shelob/share/hadoop/share/hadoop/common/lib/*:/home/gs/gridre/shelob/share/hadoop/share/hadoop/common/*:/home/gs/gridre/shelob/share/hadoop/hadoop-*-capacity-scheduler.jar:/home/gs/gridre/shelob/share/hadoop/hadoop-*-capacity-scheduler.jar:/home/gs/gridre/shelob/share/hadoop/hadoop-*-capacity-scheduler.jar:/home/gs/gridre/shelob/share/hadoop/share/hadoop/hdfs:/home/gs/gridre/shelob/share/hadoop/share/hadoop/hdfs/lib/*:/home/gs/gridre/shelob/share/hadoop/share/hadoop/hdfs/*:/home/gs/gridre/shelob/share/hadoop/share/hadoop/mapreduce/lib/*:/home/gs/gridre/shelob/share/hadoop/share/hadoop/mapreduce/*:/home/gs/java/jdk64/current/lib/tools.jar:/home/gs/gridre/shelob/share/hadoop/share/hadoop/mapreduce/*:/home/gs/gridre/shelob/share/hadoop/share/hadoop/mapreduce/lib/*
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer

1) On the RM with older hadoop version where the job summary log does not exist
jobhistory ps shows using the option:
-Dmapred.jobsummary.logger=INFO,console 

2) On the RM with older hadoop version where the job summary log exists
jobhistory ps shows using the option:
-Dmapred.jobsummary.logger=INFO,JSA 
-Dmapred.jobsummary.logger=INFO,JSA

",anupamseth,anupamseth,Critical,Closed,Fixed,09/Feb/12 20:45,05/Mar/12 02:49
Bug,MAPREDUCE-3851,12542197,Allow more aggressive action on detection of the jetty issue,"MAPREDUCE-2529 added the useful failure detection mechanism. In this jira, I propose we add a periodic check inside TT and configurable action to self-destruct. Blacklisting helps but is not enough. Hung jetty still accepts connection and it takes very long time for clients to fail out. Short jobs are delayed for hours because of this. This feature will be a nice companion to MAPREDUCE-3184.",tgraves,kihwal,Major,Closed,Fixed,10/Feb/12 22:16,31/Aug/12 13:20
Bug,MAPREDUCE-3852,12542198,test TestLinuxResourceCalculatorPlugin failing,"tests are failing:
org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin.testParsingProcStatAndCpuFile
org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin.testParsingProcMemFile 

https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1831/testReport/junit/org.apache.hadoop.yarn.util/TestLinuxResourceCalculatorPlugin/testParsingProcStatAndCpuFile/

both with similar error:
java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/target/test-dir/MEMINFO_238849741 (No such file or directory)

",tgraves,tgraves,Blocker,Resolved,Fixed,10/Feb/12 22:33,10/Mar/15 04:32
Bug,MAPREDUCE-3856,12542417,Instances of RunningJob class givs incorrect job tracking urls when mutiple jobs are submitted from same client jvm.,"When multiple jobs are submitted from the same client JVM, each call to RunningJob.getTrackingURL() always returns the tracking URL from the first job.

This happens even if the jobs are submitted and the client waits for the job to complete before submitting the subsequent job. Each job runs fine and is definitely a new, unique job, but the call to getTrackingURL() still returns the URL for the first job.",epayne,epayne,Critical,Closed,Fixed,13/Feb/12 16:33,05/Mar/12 02:49
Bug,MAPREDUCE-3857,12542448,Grep example ignores mapred.job.queue.name,Grep example creates two jobs as part of its implementation. The first job correctly uses the configuration settings. The second job ignores configuration settings.,jeagles,jeagles,Major,Closed,Fixed,13/Feb/12 22:15,16/May/12 20:45
Bug,MAPREDUCE-3858,12542468,Task attempt failure during commit results in task never completing,"On a terasort job a task attempt failed during the commit phase. Another attempt was rescheduled, but when it tried to commit it failed.

{noformat}
attempt_1329019187148_0083_r_000586_0 already given a go for committing the task output, so killing attempt_1329019187148_0083_r_000586_1
{noformat}

The job hung as new attempts kept getting scheduled only to fail during commit.

",tomwhite,tomwhite,Critical,Closed,Fixed,14/Feb/12 01:21,17/May/13 21:02
Bug,MAPREDUCE-3859,12542496,CapacityScheduler incorrectly utilizes extra-resources of queue for high-memory jobs,"Imagine, we have a queue A with capacity 10 slots and 20 as extra-capacity, jobs which use 3 map slots will never consume more than 9 slots, regardless how many free slots on a cluster.",sergeant,sergeant,Major,Closed,Fixed,14/Feb/12 09:03,11/Oct/13 17:18
Bug,MAPREDUCE-3862,12542720,Nodemanager can appear to hang on shutdown due to lingering DeletionService threads,"When a nodemanager attempts to shutdown cleanly, it's possible for it to appear to hang due to lingering DeletionService threads.  This can occur when yarn.nodemanager.delete.debug-delay-sec is set to a relatively large value and one or more containers executes on the node shortly before the shutdown.

The DeletionService is never calling {{setExecuteExistingDelayedTasksAfterShutdownPolicy()}} on the ScheduledThreadPoolExecutor, and it defaults to waiting for all scheduled tasks to complete before exiting.",jlowe,jlowe,Major,Resolved,Fixed,15/Feb/12 17:31,21/Feb/12 19:19
Bug,MAPREDUCE-3863,12542724,0.22 branch mvn deploy is not publishing hadoop-streaming JAR,Without this JAR Oozie cannot be built/tested against 0.22,benoyantony,tucu00,Critical,Resolved,Fixed,15/Feb/12 18:19,03/May/12 14:07
Bug,MAPREDUCE-3866,12542782,bin/yarn prints the command line unnecessarily,"For commands like rmadmin, version etc, it also prints the whole command line unnecessarily.

This was /me from long time ago, pre alpha :)",vinodkv,vinodkv,Minor,Resolved,Fixed,15/Feb/12 23:02,25/Feb/12 13:57
Bug,MAPREDUCE-3867,12542806,MiniMRYarn/MiniYarn uses fixed ports,"This presents issues if there are other processes using those ports. Also, if multitasking among dev environments using Mini* things start to fail.",tucu00,tucu00,Major,Closed,Fixed,16/Feb/12 00:38,10/Mar/15 04:32
Bug,MAPREDUCE-3869,12542840,Distributed shell application fails with NoClassDefFoundError,"Distributed shell application always fails to start the application master with the following error.
\\

{code:xml}
12/02/16 05:35:25 FATAL distributedshell.ApplicationMaster: Error running ApplicationMaster
java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/ipc/YarnRPC
	at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.<init>(ApplicationMaster.java:252)
	at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.main(ApplicationMaster.java:195)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.ipc.YarnRPC
	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:303)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:316)
	... 2 more
{code}
",devaraj,devaraj,Blocker,Closed,Fixed,16/Feb/12 09:14,14/Nov/12 00:07
Bug,MAPREDUCE-3870,12542867,Invalid App Metrics,I have observed incorrect *Apps Completed* and *Apps Pending* metrics when an application has failed or finished (successful) after multiple attempts(failures).,kam_iitkgp,kam_iitkgp,Major,Closed,Fixed,16/Feb/12 14:35,12/May/16 18:23
Bug,MAPREDUCE-3872,12542968,event handling races in ContainerLauncherImpl and TestContainerLauncher,"TestContainerLauncher is failing intermittently for me.

{noformat}
junit.framework.AssertionFailedError: Expected: <null> but was: Expected 22 but found 21
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertNull(Assert.java:233)
	at junit.framework.Assert.assertNull(Assert.java:226)
	at org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher.testPoolSize(TestContainerLauncher.java:117)
{noformat}

Patch momentarily.",rkanter,phunt,Major,Closed,Fixed,16/Feb/12 23:19,27/Aug/13 22:22
Bug,MAPREDUCE-3873,12543038,Nodemanager is not getting decommisioned if the absolute ip is given in exclude file.,"Configure absolute ip in ""yarn.resourcemanager.nodes.exclude-path"" and try to decommission the node.
It is not getting decommisioned.But if the hostname is given, decommissioning is happening.

I have also given the ip-host mapping of each machine in /etc/hosts.(i,e in every machine the other machines ip-host mapping is specified).",xieguiming,nishan,Minor,Closed,Fixed,17/Feb/12 11:02,11/Oct/12 17:48
Bug,MAPREDUCE-3878,12543109,Null user on filtered jobhistory job page,"If jobhistory/job.* is filtered to bypass acl, resulting page will always show Null user. This differs from 0.20 where filtering on this page, bypasses security to allow all access to the page. essentially passes a null user to AppController where an exception is thrown. If a null user is detected, we should acl checking is disabled on this page.",jeagles,jeagles,Critical,Resolved,Fixed,17/Feb/12 22:15,24/Feb/12 13:57
Bug,MAPREDUCE-3880,12543137,Allow for 32-bit container-executor,Currently we can't pass in -m32 to LCE build.,acmurthy,acmurthy,Blocker,Closed,Fixed,18/Feb/12 06:02,05/Mar/12 02:49
Bug,MAPREDUCE-3884,12543424,PWD should be first in the classpath of MR tasks,"Currently the current directory is not part of the classpath, this is a regression from MR1 and existing applications assuming this fail to work properly.",tucu00,tucu00,Critical,Resolved,Fixed,21/Feb/12 02:19,23/Feb/12 13:55
Bug,MAPREDUCE-3887,12543498,Jenkins mapred commit build tries an unknown target,"I saw the following in the mrv1 ant build portion of Hadoop-Mapreduce-trunk-Commit. The 0.23 build might have the same thing.

{panel}
+ /home/jenkins/tools/ant/latest/bin/ant -Dversion=0.24.0-SNAPSHOT -Dresolvers=internal -Declipse.home=/home/jenkins/tools/eclipse/latest -Dfindbugs.home=/home/jenkins/tools/findbugs/latest -Dforrest.home=/home/jenkins/tools/forrest/latest -Dcompile.c++=true -Dcompile.native=true create-c++-configure binary

....

BUILD FAILED
Target ""binary"" does not exist in the project ""Hadoop"". 
{panel}",,kihwal,Major,Resolved,Fixed,21/Feb/12 15:11,09/Mar/15 22:00
Bug,MAPREDUCE-3889,12543572,"job client tries to use /tasklog interface, but that doesn't exist anymore","if you specify  -Dmapreduce.client.output.filter=SUCCEEDED option when running a job it tries to fetch task logs to print out on the client side from a url like: http://nodemanager:8080/tasklog?plaintext=true&attemptid=attempt_1329857083014_0003_r_000000_0&filter=stdout

It always errors on this request with: Required param job, map and reduce

We saw this error when using distcp and the distcp failed. I'm not sure if it is mandatory for distcp or just informational purposes.  I'm guessing the latter.

",devaraj,tgraves,Critical,Closed,Fixed,21/Feb/12 21:37,12/May/16 18:23
Bug,MAPREDUCE-3893,12543685,allow capacity scheduler configs maximum-applications and maximum-am-resource-percent configurable on a per queue basis,"The capacity scheduler configs for  maximum-applications and maximum-am-resource-percent are currently configured globally and then made proportional to each queue based on its capacity. There are times when this may not work well.  some exampless -  if you have a queue that is running on uberAM jobs, the jobs a queue is running always has a small number of containers, and then you have the opposite where in a queue with very small capacity, you may want to limit the am resources even more so you don't end up deadlocked with all your capacity being used for app masters.

I think we should make those configurable on a per queue basis.",tgraves,tgraves,Critical,Closed,Fixed,22/Feb/12 14:16,12/Nov/12 20:32
Bug,MAPREDUCE-3894,12543705,0.23 and trunk MR builds fail intermittently,"The builds occasionally report ABORTED or FAILURE, which is not caused by the new code change included in the builds. We are not sure since when they have been broken this way, but Bobby's guess is around Feb 10.",,kihwal,Major,Resolved,Fixed,22/Feb/12 16:02,10/Mar/15 04:32
Bug,MAPREDUCE-3896,12543732,pig job through oozie hangs ,running pig job on oozie hangs due to race condition,vinodkv,johnvijoe,Blocker,Resolved,Fixed,22/Feb/12 19:03,03/Mar/12 13:58
Bug,MAPREDUCE-3897,12543744,capacity scheduler - maxActiveApplicationsPerUser calculation can be wrong,"The capacity scheduler calculates the maxActiveApplications and the maxActiveApplicationsPerUser based on the config yarn.scheduler.capacity.maximum-applications or default 10000.  

MaxActiveApplications = max ( ceil ( clusterMemory/minAllocation * maxAMResource% * absoluteMaxCapacity), 1)  

MaxActiveAppsPerUser = max( ceil (maxActiveApplicationsComputedAbove * (userLimit%/100) * userLimitFactor), 1) 

maxActiveApplications is already multiplied by the queue absolute MAXIMUM capacity, so if max capacity > capacity and if you have user limit factor 1 (which is the default) and only 1 user is running, that user will not be allowed to use over the queue capacity, so having it relative to MAX capacity doesn't make sense.  That user could easily end up in a deadlock and all its space used by application masters.
",epayne,tgraves,Critical,Resolved,Fixed,22/Feb/12 21:41,05/Mar/12 13:25
Bug,MAPREDUCE-3903,12543865,no admin override to view jobs on mr app master and job history server,"in 1.0 there was a config mapreduce.cluster.administrators that allowed administrators to view anyones job.  That no longer works on yarn.
yarn has the new config yarn.admin.acl but it appears the mr app master and job history server don't use that.  ",tgraves,tgraves,Critical,Resolved,Fixed,23/Feb/12 17:28,01/Mar/12 13:58
Bug,MAPREDUCE-3904,12543866,[NPE] Job history produced with mapreduce.cluster.acls.enabled false can not be viewed with mapreduce.cluster.acls.enabled true,"Job history page displays 'null'. It looks like job history files only populate job acls when mapreduce.cluster.acls.enabled is true. Upon reading job history files, getAcls can return null, throwing an exception on the HsJobBlock page.",jeagles,jeagles,Major,Resolved,Fixed,23/Feb/12 17:32,25/Feb/12 13:57
Bug,MAPREDUCE-3910,12544012,user not allowed to submit jobs even though queue -showacls shows it allows,User is not allowed to submit applications to a queue even though the queue is configured correctly and mapred queue -showacls shows that the user is allowed to submit,johnvijoe,johnvijoe,Blocker,Resolved,Fixed,24/Feb/12 16:30,26/Feb/12 13:56
Bug,MAPREDUCE-3913,12544020,RM application webpage is unresponsive after 2000 jobs,"After >2000 jobs have been submitted, trying to load the resourcemanager's applications page results in the following exception on the RM:

{code}
java.lang.IllegalArgumentException: No enum cons
t class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState.json
        at java.lang.Enum.valueOf(Enum.java:196)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState.valueOf(RMAppState.java:21)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlock.render(AppsBlock.java:66)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:64)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:74)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:233)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:38)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30345)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:29)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:64)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:74)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:233)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:47)
        at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:843)
        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:54)
....
{code}

",jlowe,jlowe,Critical,Resolved,Fixed,24/Feb/12 17:28,28/Feb/12 14:17
Bug,MAPREDUCE-3914,12544022,Mismatched free() / delete / delete [] in HadoopPipes,"When running valgrind on a simple MapReduce pipes job, valgrind identifies a mismatched new / delete:

==20394== Mismatched free() / delete / delete []
==20394==    at 0x4C27FF2: operator delete(void*) (vg_replace_malloc.c:387)
==20394==    by 0x4328A5: HadoopPipes::runTask(HadoopPipes::Factory const&) (HadoopPipes.cc:1171)
==20394==    by 0x424C33: main (ProcessRow.cpp:118)
==20394==  Address 0x9c5b540 is 0 bytes inside a block of size 131,072 alloc'd
==20394==    at 0x4C2864B: operator new[](unsigned long) (vg_replace_malloc.c:305)
==20394==    by 0x431E5D: HadoopPipes::runTask(HadoopPipes::Factory const&) (HadoopPipes.cc:1121)
==20394==    by 0x424C33: main (ProcessRow.cpp:118)
==20394== 
==20394== Mismatched free() / delete / delete []
==20394==    at 0x4C27FF2: operator delete(void*) (vg_replace_malloc.c:387)
==20394==    by 0x4328AF: HadoopPipes::runTask(HadoopPipes::Factory const&) (HadoopPipes.cc:1172)
==20394==    by 0x424C33: main (ProcessRow.cpp:118)
==20394==  Address 0x9c7b580 is 0 bytes inside a block of size 131,072 alloc'd
==20394==    at 0x4C2864B: operator new[](unsigned long) (vg_replace_malloc.c:305)
==20394==    by 0x431E6A: HadoopPipes::runTask(HadoopPipes::Factory const&) (HadoopPipes.cc:1122)
==20394==    by 0x424C33: main (ProcessRow.cpp:118)

The new [] calls in Lines 1121 and 1122 of HadoopPipes.cc:
        bufin = new char[bufsize];
        bufout = new char[bufsize];
should have matching delete [] calls but are instead bracketed my delete on lines 1171 and 1172:
      delete bufin;
      delete bufout;
So these should be replaced by delete[]
",JoeM,charlescearl,Major,Resolved,Fixed,24/Feb/12 17:42,12/May/16 18:22
Bug,MAPREDUCE-3916,12544027,various issues with running yarn proxyserver,"Seem like yarn proxyserver is not operational when running out of the 0.23.1 RC2 tarball.

# Setting yarn.web-proxy.address to match yarn.resourcemanager.address doesn't disable the proxyserver (althought not setting yarn.web-proxy.address at all correctly disable it and produces a message: org.apache.hadoop.yarn.YarnException: yarn.web-proxy.address is not set so the proxy will not run). This contradicts the documentation provided for yarn.web-proxy.address in yarn-default.xml

# Setting yarn.web-proxy.address and running the service results in the following:

{noformat}
$ ./sbin/yarn-daemon.sh start proxyserver 
starting proxyserver, logging to /tmp/hadoop-0.23.1/logs/yarn-rvs-proxyserver-ahmed-laptop.out
/usr/java/64/jdk1.6.0_22/bin/java -Dproc_proxyserver -Xmx1000m -Dhadoop.log.dir=/tmp/hadoop-0.23.1/logs -Dyarn.log.dir=/tmp/hadoop-0.23.1/logs -Dhadoop.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.home.dir= -Dyarn.id.str=rvs -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA -Djava.library.path=/tmp/hadoop-0.23.1/lib/native -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/tmp/hadoop-0.23.1/logs -Dyarn.log.dir=/tmp/hadoop-0.23.1/logs -Dhadoop.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.home.dir=/tmp/hadoop-0.23.1 -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA -Djava.library.path=/tmp/hadoop-0.23.1/lib/native -classpath /tmp/hadoop-0.23.1/etc/hadoop:/tmp/hadoop-0.23.1/etc/hadoop:/tmp/hadoop-0.23.1/etc/hadoop:/tmp/hadoop-0.23.1/share/hadoop/common/lib/*:/tmp/hadoop-0.23.1/share/hadoop/common/*:/tmp/hadoop-0.23.1/share/hadoop/hdfs:/tmp/hadoop-0.23.1/share/hadoop/hdfs/lib/*:/tmp/hadoop-0.23.1/share/hadoop/hdfs/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/lib/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/lib/* org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer
{noformat}

with the following message found in the logs:

{noformat}
2012-02-24 09:26:31,099 FATAL org.apache.hadoop.yarn.server.webproxy.WebAppProxy: Could not start proxy web server
java.io.FileNotFoundException: webapps/proxy not found in CLASSPATH
        at org.apache.hadoop.http.HttpServer.getWebAppsPath(HttpServer.java:532)
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:224)
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:164)
        at org.apache.hadoop.yarn.server.webproxy.WebAppProxy.start(WebAppProxy.java:85)
        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer.main(WebAppProxyServer.java:76)
{noformat}",devaraj,rvs,Critical,Closed,Fixed,24/Feb/12 18:22,12/May/16 18:22
Bug,MAPREDUCE-3918,12544052,proc_historyserver no longer in command line arguments for HistoryServer,This java arg is missing from the command line and needs to be replaced,jeagles,jeagles,Major,Resolved,Fixed,24/Feb/12 20:43,25/Feb/12 13:57
Bug,MAPREDUCE-3920,12544059,Revise yarn default port number selection,"The default port numbers chosen for nodemanager and resourcemanager are random and widely spread out creating unnecessary overhead in deployments where site operators care, and deploy many clusters.

Current and proposed new default ports are as follows:

Current		New		Config Property	
------- 	---             ---------------
4344		8040		yarn.nodemanager.localizer.address	
45454		8041		yarn.nodemanager.address	
9999		8042		yarn.nodemanager.webapp.address	

8030		8030(NC)	yarn.resourcemanager.scheduler.address
8025		8031		yarn.resourcemanager.resource-tracker.address
8040		8032		yarn.resourcemanager.address
8141		8033		yarn.resourcemanager.admin.address


Affected files include:  embedded defaults (YarnConfiguration.java), yarn-default.xml, documentation and unit tests.
 ",davet,davet,Major,Resolved,Fixed,24/Feb/12 21:46,01/Mar/12 13:58
Bug,MAPREDUCE-3927,12544271,Shuffle hang when set map.failures.percent,"When set mapred.max.map.failures.percent and there does have some failed maps, then shuffle will hang",kam_iitkgp,wangmeng,Critical,Closed,Fixed,27/Feb/12 16:06,10/Mar/15 04:30
Bug,MAPREDUCE-3929,12544322,output of mapred -showacl is not clear,output of 'mapred queue -showacls' is not very clear. This JIRA is aimed at either fixing that or adding something to the document to make it clear.,johnvijoe,johnvijoe,Major,Resolved,Fixed,27/Feb/12 21:54,10/Mar/15 04:31
Bug,MAPREDUCE-3930,12544331,The AM page for a Reducer that has not been launched causes an NPE,"{noformat}
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)
        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)
        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
        at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:120)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:940)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:322)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.util.ConverterUtils.toString(ConverterUtils.java:145)
        at org.apache.hadoop.mapreduce.v2.app.webapp.dao.TaskAttemptInfo.<init>(TaskAttemptInfo.java:69)
        at org.apache.hadoop.mapreduce.v2.app.webapp.dao.TaskAttemptInfo.<init>(TaskAttemptInfo.java:60)
        at org.apache.hadoop.mapreduce.v2.app.webapp.TaskPage$AttemptsBlock.render(TaskPage.java:76)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:64)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:74)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:233)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:47)
        at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:843)
        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:54)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:80)
        at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:210)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.task(AppController.java:250) 
        ... 37 more
{noformat}",revans2,revans2,Critical,Resolved,Fixed,27/Feb/12 22:28,29/Feb/12 13:57
Bug,MAPREDUCE-3931,12544333,MR tasks failing due to changing timestamps on Resources to download,"[~karams] reported this offline. Seems that tasks are randomly failing during gridmix runs:
{code}
2012-02-24 21:03:34,912 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1330116323296_0140_m_003868_0: RemoteTrace:
java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875
       at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)
       at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)
       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)
       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:396)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)
       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
 at LocalTrace:
       org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875
       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)
       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)
       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)
       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)
       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)
       at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)
       at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)
       at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)
       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)
       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:396)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)
{code}",sseth,vinodkv,Major,Resolved,Fixed,27/Feb/12 22:30,29/Feb/12 13:57
Bug,MAPREDUCE-3932,12544334,MR tasks failing and crashing the AM when available-resources/headRoom becomes zero,"[~karams] reported this offline. One reduce task gets preempted because of zero headRoom and crashes the AM.
{code}
2012-02-23 11:30:15,956 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 44544
2012-02-23 11:30:16,959 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 44544
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Assign: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 3
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000006 to attempt_1329995034628_0983_r_000000_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000007 to attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000008 to attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Assign: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:20 AssignedMaps:0 AssignedReduces:3 completedMaps:4 completedReduces:0 containersAllocated:7 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping down all scheduled reduces:20
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Going to preempt 2
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Preempting attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Preempting attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule...
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: completedMapPercent 0.4 totalMemLimit:4608 finalMapMemLimit:2765 finalReduceMemLimit:1843 netScheduledMapMem:9216 netScheduledReduceMem:4608
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping down 0
2012-02-23 11:30:16,968 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host6 to /$rack6
2012-02-23 11:30:16,976 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2012-02-23 11:30:16,976 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host1 to /$rack1
2012-02-23 11:30:16,977 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2012-02-23 11:30:16,981 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host9 to /$rack9
2012-02-23 11:30:16,982 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000002_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2012-02-23 11:30:16,982 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000002_0 TaskAttempt Transitioned from ASSIGNED to KILL_CONTAINER_CLEANUP
2012-02-23 11:30:16,983 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from ASSIGNED to KILL_CONTAINER_CLEANUP
2012-02-23 11:30:16,983 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000000_0
2012-02-23 11:30:16,983 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,983 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000000_0
2012-02-23 11:30:16,984 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,984 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for taskAttempt attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,984 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for taskAttempt attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,987 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,988 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,988 ERROR [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container was killed before it was launched
2012-02-23 11:30:17,061 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1329995034628_0983_r_000000_0 : 53990
2012-02-23 11:30:17,077 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
2012-02-23 11:30:17,077 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1329995034628_0983_r_000001_0: Container was killed before it was launched
2012-02-23 11:30:17,078 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1329995034628_0983_r_000001_0
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)
	at java.lang.Thread.run(Thread.java:619)
2012-02-23 11:30:17,080 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1329995034628_0983_r_000000_0] using containerId: [container_1329995034628_0983_01_000006 on NM: [$host6:51529]
2012-02-23 11:30:17,081 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2012-02-23 11:30:17,207 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1329995034628_0983_r_000002_0 : 47960
2012-02-23 11:30:17,207 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:17,215 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1329995034628_0983Job Transitioned from RUNNING to ERROR
2012-02-23 11:30:17,216 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:657)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:111)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:844)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)
	at java.lang.Thread.run(Thread.java:619)

{code}",revans2,vinodkv,Critical,Closed,Fixed,27/Feb/12 22:39,04/Aug/17 13:36
Bug,MAPREDUCE-3933,12544369,Failures because MALLOC_ARENA_MAX is not set,"We have noticed a bunch of MapReduce test failures on CentOS 6 due to ""running beyond virtual memory limits"".

These tests fail with messages of the form:
{code}
[Node Status Updater] nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:getNodeStatus(254)) - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1330401645767, }, attemptId: 1, }, id: 1, }, state: C_RUNNING, diagnostics: ""Container [pid=16750,containerID=container_1330401645767_0001_01_000001] is running beyond virtual memory limits. Current usage: 220.5mb of 2.0gb physical memory used; 7.1gb of 4.2gb virtual memory used. Killing container
{code}

The failing tests are:
{code}
TestJobCounters
TestJobSysDirWithDFS
TestLazyOutput
TestMiniMRChildTask
TestMiniMRClientCluster
TestReduceFetchFromPartialMem
TestChild
TestMapReduceLazyOutput
TestJobOutputCommitter
TestMRAppWithCombiner
TestMRJobs
TestMRJobsWithHistoryService
TestMROldApiJobs
TestSpeculativeExecution
TestUberAM
{code}

I'll upload a patch momentarily.",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,28/Feb/12 04:57,23/May/12 20:28
Bug,MAPREDUCE-3947,12544707,yarn.app.mapreduce.am.resource.mb not documented,This configuration is useful but doesn't appear to be documented anywhere. ,devaraj,tlipcon,Minor,Closed,Fixed,01/Mar/12 01:18,10/Mar/15 04:33
Bug,MAPREDUCE-3952,12544727,"In MR2, when Total input paths to process == 1, CombinefileInputFormat.getSplits() returns 0 split.","Hive get unexpected result when using MR2(When using MR1, always get expected result).

In MR2, when Total input paths to process == 1, CombinefileInputFormat.getSplits() returns 0 split.

The calling code in Hive, in Hadoop23Shims.java:

InputSplit[] splits = super.getSplits(job, numSplits);

this get splits.length == 0.

In MR1, everything goes fine, the calling code in Hive, in Hadoop20Shims.java:

CombineFileSplit[] splits = (CombineFileSplit[]) super.getSplits(job, numSplits);

this get splits.length == 1.",kam_iitkgp,zhenxiao,Major,Closed,Fixed,01/Mar/12 05:10,10/Mar/15 04:33
Bug,MAPREDUCE-3953,12544746,Gridmix throws NPE and does not simulate a job if the trace contains null taskStatus for a task,"In a trace file, if a succeeded job contains a failed task, then that task's taskStatus will be null. This is causing NPE in Gridmix and then Gridmix is ignoring/not-considering such jobs for simulation. The job could succeed even with failed tasks if the job submitter in original cluster configured that job to tolerate failures using mapreduce.map.failures.maxpercent and mapreduce.reduce.failures.maxpercent.",ravidotg,ravidotg,Major,Closed,Fixed,01/Mar/12 08:25,03/Sep/14 22:45
Bug,MAPREDUCE-3954,12544808,Clean up passing HEAPSIZE to yarn and mapred commands.,"Currently the heap size for all of these is set in yarn-env.sh.  JAVA_HEAP_MAX is set to -Xmx1000m unless YARN_HEAPSIZE is set.  If it is set it will override JAVA_HEAP_MAX.  However, we do not always want to have the RM, NM, and HistoryServer with the exact same heap size.  It would be logical to have inside of yarn and mapred to set JAVA_HEAP_MAX if YARN_RESOURCEMANAGER_HEAPSIZE, YARN_NODEMANAGER_HEAPSIZE or HADOOP_JOB_HISTORYSERVER_HEAPSIZE are set respectively.  This is a bug because it is easy to configure the history server to store more entires then the heap can hold.  It is also a performance issue if we do not allow the history server to cache many entries on a large cluster.  ",revans2,revans2,Blocker,Resolved,Fixed,01/Mar/12 16:54,25/Aug/13 14:32
Bug,MAPREDUCE-3958,12544873,RM: Remove RMNodeState and replace it with NodeState,"RMNodeState is being sent over the wire after MAPREDUCE-3353. This has been done by cloning the enum into NodeState in yarn protocol records.
That makes RMNodeState redundant and it should be replaced with NodeState.",bikassaha,bikassaha,Major,Closed,Fixed,02/Mar/12 01:32,04/Sep/14 01:05
Bug,MAPREDUCE-3960,12544975,web proxy doesn't forward request to AM with configured hostname/IP,"If the host the web proxy is running on has an ip alias or similar and the config file is pointing to the hostname that is the aliased ip of the box, the web proxy will send the request from the base ip rather then the aliased ip and the AM will redirect that request to the proxy again instead of accepting it.",tgraves,tgraves,Critical,Resolved,Fixed,02/Mar/12 19:08,05/Mar/12 13:25
Bug,MAPREDUCE-3961,12545000,Map/ReduceSlotMillis computation incorrect,"Map/ReduceSlot millis are currently computed based on a fixed container size. They should instead be based on the minimum container size offered by the cluster.
There's another jira to rename these Counters - based on the resource type. This jira isn't to do that - just to fix the values. ",sseth,sseth,Major,Resolved,Fixed,02/Mar/12 22:40,07/Mar/12 13:25
Bug,MAPREDUCE-3964,12545017,ResourceManager does not have JVM metrics,ResourceManager is not creating a JvmMetrics instance on startup.,jlowe,jlowe,Critical,Resolved,Fixed,03/Mar/12 01:16,10/Mar/15 04:32
Bug,MAPREDUCE-3974,12545247,TestSubmitJob in MR1 tests doesn't compile after HDFS-1623 merge,"TestSubmitJob in MR1 tests doesn't compile after HDFS-1623 merge.

'ant compile-tests' doesn't work (since it's ant for MR1).",atm,acmurthy,Blocker,Closed,Fixed,05/Mar/12 22:56,23/May/12 20:28
Bug,MAPREDUCE-3975,12545252,Default value not set for Configuration parameter mapreduce.job.local.dir,"mapreduce.job.local.dir (formerly job.local.dir in 0.20) is not set by default. This is a regression from 0.20.205.

In 0.20.205, JobLocalizer.createWorkDir() constructs the ""$mapred.local.dir/taskTracker/$user/jobcache/$jobid/work"" path based on $user and $jobid, and then sets TaskTracker.JOB_LOCAL_DIR in the job's JobConf.

So far, I haven't found where this is done in 0.23. It could be that this is what should be done by LocalJobRunner.setupChildMapredLocalDirs(), but I am still investigating.

",epayne,epayne,Blocker,Resolved,Fixed,05/Mar/12 23:17,08/Mar/12 19:46
Bug,MAPREDUCE-3976,12545270,TestRMContainerAllocator failing,"The following stack trace is being generated
========
org.apache.hadoop.metrics2.MetricsException: Metrics source JvmMetrics already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:126)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:107)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:216)
	at org.apache.hadoop.metrics2.source.JvmMetrics.create(JvmMetrics.java:80)
	at org.apache.hadoop.mapreduce.v2.app.metrics.MRAppMetrics.create(MRAppMetrics.java:58)
	at org.apache.hadoop.mapreduce.v2.app.metrics.MRAppMetrics.create(MRAppMetrics.java:54)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.<init>(MRAppMaster.java:186)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.<init>(MRAppMaster.java:170)
	at org.apache.hadoop.mapreduce.v2.app.MRApp.<init>(MRApp.java:160)
	at org.apache.hadoop.mapreduce.v2.app.TestRMContainerAllocator$1.<init>(TestRMContainerAllocator.java:372)
	at org.apache.hadoop.mapreduce.v2.app.TestRMContainerAllocator.testReportedAppProgress(TestRMContainerAllocator.java:371)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
========

This test fails when git trunk is reset to - commit 6689d99b38c7c562e8cae484207ad30ad7b56eb5
but passes when git trunk is reset to - commit f429fdaf78a02211c4faee54b1ee92822edc5741

",jlowe,bikassaha,Major,Resolved,Fixed,06/Mar/12 02:40,07/Mar/12 13:25
Bug,MAPREDUCE-3977,12545413,LogAggregationService leaks log aggregator objects,LogAggregationService adds log aggregator objects to the {{appLogAggregators}} map but never removes them.,jlowe,jlowe,Critical,Resolved,Fixed,06/Mar/12 22:12,07/Mar/12 13:25
Bug,MAPREDUCE-3982,12545552,TestEmptyJob fails with FileNotFound,TestEmptyJob fails because teh FileOutputCommitter expects a directory to be created that is not created.  The FileOutputCommitter should either ignore the error or create the directory itself.,revans2,revans2,Critical,Resolved,Fixed,07/Mar/12 19:58,10/Mar/15 04:32
Bug,MAPREDUCE-3988,12545703,mapreduce.job.local.dir doesn't point to a single directory on a node.,"After MAPREDUCE-3975, mapreduce.job.local.dir is set correctly for the tasks but it doesn't point to the same directory for all tasks running on the node.

It is a public API. Either we should point to a single directory or point it to all directories and change the documentation to say that it points to all dirs.",epayne,vinodkv,Major,Closed,Fixed,08/Mar/12 19:44,07/Sep/12 21:03
Bug,MAPREDUCE-3990,12545772,MRBench allows Long-sized input-lines value but parses CLI argument as an Integer,"MRBench has the following method:

{code}
public void generateTextFile(FileSystem fs, Path inputFile, long numLines, Order sortOrder) { ... }
{code}

The method is already set to accept a long datatype for numLines, for generating very large amount of data.

However, in {{MRBench#run(...)}}, the inputLines CLI parameter is parsed via an Integer.parseInt, causing numbers passed > Integer.MAX_VALUE to throw NumberFormatExceptions as a result.

The parsing should be Long.parseLong and the inputLines datatype should be switched to the same type as passed to the method (long).",qwertymaniac,qwertymaniac,Trivial,Resolved,Fixed,09/Mar/12 05:11,12/May/16 18:22
Bug,MAPREDUCE-3992,12545793,Reduce fetcher doesn't verify HTTP status code of response,"Currently, the reduce fetch code doesn't check the HTTP status code of the response. This can lead to the following situation:
- the map output servlet gets an IOException after setting the headers but before the first call to flush()
- this causes it to send a response with a non-OK result code, including the exception text as the response body (response.sendError() does this if the response isn't committed)
- it will still include the response headers indicating it's a valid response

In the case of a merge-to-memory, the compression codec might then try to interpret the HTML response as compressed data, resulting in either a huge allocation (OOME) or some other nasty error. This bug seems to be present in MR1, but haven't checked trunk/MR2 yet.",tlipcon,tlipcon,Major,Closed,Fixed,09/Mar/12 08:42,10/Mar/15 04:32
Bug,MAPREDUCE-3993,12545794,Graceful handling of codec errors during decompression,"When using a compression codec for intermediate compression, some cases of corrupt data can cause the codec to throw exceptions other than IOException (eg java.lang.InternalError). This will currently cause the whole reduce task to fail, instead of simply treating it like another case of a failed fetch.",kasha,tlipcon,Major,Closed,Fixed,09/Mar/12 08:44,03/Nov/14 18:05
Bug,MAPREDUCE-3999,12546150,Tracking link gives an error if the AppMaster hasn't started yet,"Courtesy [~sseth]
{quote}
""The MRAppMaster died before writing anything.""

Steps to generate the error:
1. Setup a queue with 1 max active application per user
2. Submit a long running job to this queue.
3. Submit another job to the queue as the same user. Access the tracking URL
for job 2 directly or via Oozie (not via the RM link - which is rewritten once
the app starts).

This would exist in situations where the queue doesn't have enough capacity -
or for the small period of time between app submission and AM start.
{quote}",raviprak,raviprak,Major,Closed,Fixed,12/Mar/12 20:35,07/Sep/12 21:03
Bug,MAPREDUCE-4002,12546229,MultiFileWordCount job fails if the input path is not from default file system,"In the MultiFileWordCount#CombineFileLineRecordReader, filesystem object has been initialized in the following way

{noformat}fs = FileSystem.get(context.getConfiguration());{noformat}

This causes, *fs* to be initialized with default filesystem. Therefore *fs* searchs for the input files on the default file system, which fails if the input path is from different source.",kam_iitkgp,kam_iitkgp,Major,Closed,Fixed,13/Mar/12 13:32,10/Mar/15 04:31
Bug,MAPREDUCE-4003,12546230,log.index (No such file or directory) AND Task process exit with nonzero status of 126,"hello，I have dwelled on this hadoop(cdhu3) problem for 2 days,I have tried every google method.This is the issue: when ran hadoop example ""wordcount"" ,the tasktracker's log in one slave node presented such errors

 1.WARN org.apache.hadoop.mapred.DefaultTaskController: Task wrapper stderr: bash: /var/tmp/mapred/local/ttprivate/taskTracker/hdfs/jobcache/job_201203131751_0003/attempt_201203131751_0003_m_000006_0/taskjvm.sh: Permission denied

2.WARN org.apache.hadoop.mapred.TaskRunner: attempt_201203131751_0003_m_000006_0 : Child Error java.io.IOException: Task process exit with nonzero status of 126.

3.WARN org.apache.hadoop.mapred.TaskLog: Failed to retrieve stdout log for task: attempt_201203131751_0003_m_000003_0 java.io.FileNotFoundException: /usr/lib/hadoop-0.20/logs/userlogs/job_201203131751_0003/attempt_201203131751_0003_m_000003_0/log.index (No such file or directory)

I could not find similar issues in google,just got some posts seem a little relevant ,which suggest: A. the ulimit of hadoop user----but my ulimit is set large enough for this bundled example;B. the memory used by jvm,but my jvm only use Xmx200m,too small to exceed the limit of my machine ;C.the privilege of the mapred.local.dir and logs dir----I set them by ""chmod 777"";D .the disk space is full----there are enough space for hadoop in my log directory and mapred.local.dir.

Thanks for you all,I am really at my wit's end,I have spend days on it. I really appreciate any light!
",knoguchi,zaozaowang,Major,Closed,Fixed,13/Mar/12 13:43,16/May/12 20:45
Bug,MAPREDUCE-4005,12546289,AM container logs URL is broken for completed apps when log aggregation is enabled,"With log aggregation enabled and yarn.log.server.url pointing to the job history server, the AM container logs URL for a completed application fails with the error ""Cannot get container logs without an app owner"".  Looking at the code in the nodemanager to handle redirects to the log server, it appears the AM container log URL is missing a user name for the job.  I verified that tacking on the app's user name after the AM container log URL reported by the RM works.",jlowe,jlowe,Major,Resolved,Fixed,13/Mar/12 20:42,17/Mar/12 13:58
Bug,MAPREDUCE-4006,12546300,history server container log web UI sometimes combines stderr/stdout/syslog contents together,"When log aggregation is enabled, going to the job history server UI for the AM container log can show the log contents combined together.  Examples I've seen are portions of the syslog contents appended to either the stderr or stdout contents.  The log corruption does not occur when using the mapred job -logs command, so this appears to be something specific to the history server web UI.",sseth,jlowe,Major,Resolved,Fixed,13/Mar/12 21:07,17/Mar/12 13:58
Bug,MAPREDUCE-4007,12546317,JobClient getJob(JobID) should return NULL if the job does not exist (for backwards compatibility),To preserve backwards compatibility with MR1 JobClient.getJob() should return NULL if the job does not exist.,tucu00,tucu00,Major,Closed,Fixed,13/Mar/12 22:20,10/Mar/15 04:32
Bug,MAPREDUCE-4008,12546372,ResourceManager throws MetricsException on start up saying QueueMetrics MBean already exists,"{code:xml}
2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default
org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)
	at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)
	at $Proxy6.postStart(Unknown Source)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)
Caused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)
	... 19 more
2012-03-14 15:22:23,090 WARN org.apache.hadoop.metrics2.util.MBeans: Failed to register MBean ""null""
javax.management.RuntimeOperationsException: Exception occurred trying to register the MBean
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)
	at $Proxy6.postStart(Unknown Source)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)
Caused by: java.lang.IllegalArgumentException: No object name specified
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)
	... 20 more
{code}",devaraj,devaraj,Major,Closed,Fixed,14/Mar/12 10:08,12/May/16 18:24
Bug,MAPREDUCE-4010,12546476,TestWritableJobConf fails on trunk,"TestWritableJobConf is currently failing two tests on trunk:

* testEmptyConfiguration
* testNonEmptyConfiguration

Appears to have been caused by HADOOP-8167.",tucu00,jlowe,Critical,Closed,Fixed,14/Mar/12 20:57,10/Mar/15 04:32
Bug,MAPREDUCE-4012,12546498,Hadoop Job setup error leaves no useful info to users (when LinuxTaskController is used),"When distributed cache pull fail on the TaskTracker, job webUI only shows 
{noformat}
Job initialization failed (255)
{noformat}
leaving users confused.  

On the TaskTracker log, there is a log with useful info 
{noformat}
2012-03-14 21:44:17,083 INFO org.apache.hadoop.mapred.TaskController: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: 
Permission denied: user=user1, access=READ, inode=""testfile"":user3:users:rw-------
...
2012-03-14 21:44:17,083 INFO org.apache.hadoop.mapred.TaskController:   at org.apache.hadoop.filecache.TrackerDistributedCacheManager.downloadCacheObject(TrackerDistributedCacheManager.java:415)
...
2012-03-14 21:44:17,083 INFO org.apache.hadoop.mapred.TaskController:   at org.apache.hadoop.mapred.JobLocalizer.main(JobLocalizer.java:530)
{noformat}

	",tgraves,knoguchi,Minor,Closed,Fixed,14/Mar/12 22:57,17/Oct/13 17:50
Bug,MAPREDUCE-4019,12546709,-list-attempt-ids  is not working,"while executing {noformat}bin/mapred  job -list-attempt-ids job_id map running{noformat}, we are getting IllegalArgumentexception.",ashwinshankar77,akumarb2010,Minor,Closed,Fixed,16/Mar/12 02:26,12/May/16 18:22
Bug,MAPREDUCE-4020,12546720,Web services returns incorrect JSON for deep queue tree,"When the capacity scheduler is configured for more than two levels of queues, the web services API returns incorrect JSON for the subQueues field of some parent queues.  The ""subQueues"" field for parent queues should always be an array, but sometimes the field appears multiple times for a queue and as what looks like a CapacityQueueInfo object instead of an array.  Besides the sometimes-an-array-sometimes-not problem, parsing the result into a JSON object causes all but the last ""subQueues"" field to be discarded since they are overwritten by subsequent fields with the same name.",anupamseth,jlowe,Major,Closed,Fixed,16/Mar/12 04:27,12/May/15 08:18
Bug,MAPREDUCE-4024,12546818,RM webservices can't query on finalStatus,The resource manager web service api to get the list of apps doesn't have a query parameter for finalStatus.  It has one for the state but since that isn't what is reported by app master so we really need to be able to query on both state and finalStatus.,tgraves,tgraves,Major,Closed,Fixed,16/Mar/12 16:42,02/May/13 02:29
Bug,MAPREDUCE-4025,12546845,AM can crash if task attempt reports bogus progress value,"If a task attempt reports a bogus progress value (e.g.: something above 1.0) then the AM can crash like this:

{noformat}
java.lang.ArrayIndexOutOfBoundsException: 12
	at org.apache.hadoop.mapred.PeriodicStatsAccumulator.extend(PeriodicStatsAccumulator.java:185)
	at org.apache.hadoop.mapred.WrappedPeriodicStatsAccumulator.extend(WrappedPeriodicStatsAccumulator.java:31)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.updateProgressSplits(TaskAttemptImpl.java:1043)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$4100(TaskAttemptImpl.java:136)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:1509)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:1490)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:931)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:886)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:878)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:74)
	at java.lang.Thread.run(Thread.java:619)
{noformat}",jlowe,jlowe,Blocker,Resolved,Fixed,16/Mar/12 20:58,20/Mar/12 21:52
Bug,MAPREDUCE-4031,12546988,Node Manager hangs on shut down,"I have the MAPREDUCE-3862 changes which fixed this issue earlier and ""yarn.nodemanager.delete.debug-delay-sec"" set to default value but still getting this issue.",devaraj,devaraj,Critical,Closed,Fixed,19/Mar/12 09:02,12/May/16 18:23
Bug,MAPREDUCE-4034,12547135,Unable to view task logs on history server with mapreduce.job.acl-view-job=*,"With log aggregation enabled, users other than the app owner or admins are sometimes unable to view the task logs on the history server even though they are in the ACL for the app.  The same users are able to see the configuration and counters.  Sometimes the users can see some task logs but not other task logs for the same application.",jlowe,jlowe,Blocker,Resolved,Fixed,20/Mar/12 02:19,20/Mar/12 21:52
Bug,MAPREDUCE-4036,12547142,Streaming TestUlimit fails on CentOS 6,"CentOS 6 seems to have higher memory requirements than other distros and together with the new MALLOC library makes the TestUlimit to fail with exit status 134.
",tucu00,tucu00,Major,Closed,Fixed,20/Mar/12 04:36,15/May/13 05:16
Bug,MAPREDUCE-4040,12547203,History links should use hostname rather than IP address.,"While navigating from web page (eg: */cluster/app/<app-id>* ) to HS, browser displays IP address rather than hostname.

{code:title=JobHistoryUtils.java|borderStyle=solid}
    if (address.getAddress().isAnyLocalAddress() || 
        address.getAddress().isLoopbackAddress()) {
      sb.append(InetAddress.getLocalHost().getHostAddress());
    }	
}
{code} 

I *think* it is better to use hostname rather than IP address.",kam_iitkgp,kam_iitkgp,Minor,Closed,Fixed,20/Mar/12 14:29,11/Oct/12 17:48
Bug,MAPREDUCE-4043,12547320,Secret keys set in Credentials are not seen by tasks,"The following scenario works in 0.20.205 but no longer works in 0.23:

1) During job submission, a secret key is set by calling jobConf.getCredentials().addSecretKey(Text, byte[])
2) A map task retrieves the secret key by calling jobConf.getCredentials().getSecretKey(Text)

In 205 the secret key is retrieved successfully but in 0.23 the secret key is missing.",jlowe,jlowe,Blocker,Resolved,Fixed,21/Mar/12 01:39,24/Mar/12 13:58
Bug,MAPREDUCE-4048,12547357,NullPointerException exception while accessing the Application Master UI,"{code:xml}
2012-03-21 10:21:31,838 ERROR [2145015588@qtp-957250718-801] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduce/attempts/job_1332261815858_2_8/m/KILLED
java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)
        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)
        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
        .......
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.NullPointerException
        at com.google.common.base.Joiner.toString(Joiner.java:317)
        at com.google.common.base.Joiner.appendTo(Joiner.java:97)
        at com.google.common.base.Joiner.appendTo(Joiner.java:127)
        at com.google.common.base.Joiner.join(Joiner.java:158)
        at com.google.common.base.Joiner.join(Joiner.java:166)
        at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)
        ... 36 more
{code}",devaraj,devaraj,Major,Closed,Fixed,21/Mar/12 10:01,12/May/16 18:24
Bug,MAPREDUCE-4050,12547385,Invalid node link,"When a task is in *UNASSIGNED* state, node link is displayed as +null+.
But I think it is better to display the link as *N/A* rather than +null+.",kamesh,kam_iitkgp,Major,Closed,Fixed,21/Mar/12 14:07,11/Oct/12 17:48
Bug,MAPREDUCE-4052,12547545,Windows eclipse cannot submit job from Windows client to Linux/Unix Hadoop cluster.,"when I use the eclipse on the windows to submit the job. and the applicationmaster throw the exception:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/v2/app/MRAppMaster
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.v2.app.MRAppMaster
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.hadoop.mapreduce.v2.app.MRAppMaster.  Program will exit.

The reasion is :
class Apps addToEnvironment function, use the
private static final String SYSTEM_PATH_SEPARATOR =
      System.getProperty(""path.separator"");

and will result the MRApplicationMaster classpath use the "";"" separator.

I suggest that nodemanger do the replace.

",jianhe,xieguiming,Major,Closed,Fixed,22/Mar/12 05:01,05/Feb/15 05:17
Bug,MAPREDUCE-4053,12547547,"Counters group names deprecation is wrong, iterating over group names deprecated names don't show up","This is similar to the deprecation of Configuration properties bug HADOOP-8167, interator() retrieval of counter names only returns new names.

Oozie breaks here because it is using the deprecate name and iterating over values (OOZIE-777). While it can be worked around easily in Oozie, this is breaking backwards compatibility.",revans2,tucu00,Major,Closed,Fixed,22/Mar/12 05:14,10/Mar/15 04:32
Bug,MAPREDUCE-4055,12547595,"Job history files are not getting copied from ""intermediate done"" directory to ""done"" directory ","1.Submit job
2.After successful execution of job before the Job history files are copied from intermediate done directory to done directory,NameNode got killed.
3.Restart the NameNode after mapreduce.jobhistory.move.interval-ms time is elapsed(default is 3 min).
Observe that Job history files are not copied from intermediate done directory to done directory and also logs are not updated with any message

Now submit another job observe that Job history files are not copied from intermediate done directory to done directory and also nothing is logged into historyserver logs.

",,nishan,Major,Resolved,Fixed,22/Mar/12 13:43,27/Apr/12 06:20
Bug,MAPREDUCE-4057,12547700,Compilation error in RAID ,"{noformat}
    [javac] Compiling 33 source files to /Users/szetszwo/hadoop/t2/hadoop-mapreduce-project/build/contrib/raid/classes
    [javac] /Users/szetszwo/hadoop/t2/hadoop-mapreduce-project/src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRaidUtil.java:42:
 unreported exception org.apache.hadoop.ipc.StandbyException; must be caught or declared to be thrown
    [javac]     return namesystem.getFileInfo(src, resolveLink);
    [javac]                                  ^
{noformat}
",devaraj,szetszwo,Major,Closed,Fixed,22/Mar/12 22:03,04/Sep/14 01:05
Bug,MAPREDUCE-4060,12547885,Multiple SLF4J binding warning,"This is the MAPREDUCE portion of HADOOP-8005.  We should remove slf4j from the assembly and use the one provided by hadoop-common so we don't end up with multiple binding warnings for SLF4J.
",jlowe,jlowe,Major,Closed,Fixed,23/Mar/12 21:21,07/Sep/12 21:03
Bug,MAPREDUCE-4061,12547888,RM only has 1 AM launcher thread,"The application master launcher has a thread pool that is configured with core size 1, maximum 10.  The thread pool will not create over
the core size thread unless the queue it is using is full. We are using an unbounded queue, so the thread pool will only ever create 1 thread.  We need to have more then 1 AM launch thread.

If that thread becomes hung for some reason, the RM can no longer launch any application masters.  We have seen an instance of this when a NM become unresponsive - something bad happened to host, not sure what yet.  ",tgraves,tgraves,Blocker,Resolved,Fixed,23/Mar/12 21:36,02/Apr/12 18:38
Bug,MAPREDUCE-4062,12547890,AM Launcher thread can hang forever,"We saw an instance where the RM stopped launch Application masters.  We found that the launcher thread was hung because something weird/bad happened to the NM node. Currently there is only 1 launcher thread (jira 4061 to fix that). We need this to not happen.  Even once we increase the number of threads  to > 1 if that many nodes go bad the RM would be stuck.  Note that this was stuck like this for approximately 9 hours.

Stack trace on hung AM launcher:

""pool-1-thread-1"" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()
[0x000000004fad2000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at org.apache.hadoop.ipc.Client.call(Client.java:1076)
    - locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)
    at
org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)
    at $Proxy76.startContainer(Unknown Source)
    at
org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)
    at
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
    at
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)",tgraves,tgraves,Major,Closed,Fixed,23/Mar/12 21:42,07/Sep/12 21:03
Bug,MAPREDUCE-4066,12548066,"To get ""yarn.app.mapreduce.am.staging-dir"" value, should set the default value","when submit the job use the windows eclipse, and the yarn.app.mapreduce.am.staging-dir value is null.
{code:title=MRApps.java|borderStyle=solid}

  public static Path getStagingAreaDir(Configuration conf, String user) {
    return new Path(
        conf.get(MRJobConfig.MR_AM_STAGING_DIR) + 
        Path.SEPARATOR + user + Path.SEPARATOR + STAGING_CONSTANT);
  }
{code}

should modify to:
{code:title=MRApps.java|borderStyle=solid}

  public static Path getStagingAreaDir(Configuration conf, String user) {
    return new Path(
        conf.get(MRJobConfig.MR_AM_STAGING_DIR,""/tmp/hadoop-yarn/staging"") + 
        Path.SEPARATOR + user + Path.SEPARATOR + STAGING_CONSTANT);
  }


{code}
",xieguiming,xieguiming,Minor,Closed,Fixed,26/Mar/12 06:46,23/May/12 20:28
Bug,MAPREDUCE-4067,12548142,Replace YarnRemoteException with IOException in MRv2 APIs,"YarnRemoteException is defined as a generic wrapper for all the exceptions in yarn. I think we should instead throw IOExceptions in the API, which can later be extended for more specialized exceptions without breaking compatibility.",xgong,jnp,Critical,Closed,Fixed,26/Mar/12 16:42,27/Aug/13 22:22
Bug,MAPREDUCE-4068,12548145,Jars in lib subdirectory of the submittable JAR are not added to the classpath,"Prior to hadoop 0.23, users could add third party jars to the lib subdirectory of the submitted job jar and they become available in the task's classpath. I see this functionality was in TaskRunner.java, but I can't see similar functionality in hadoop 0.23 (neither in MapReduceChildJVM.java nor other places).",rkanter,ahmed.radwan,Blocker,Closed,Fixed,26/Mar/12 17:25,11/Oct/12 17:48
Bug,MAPREDUCE-4072,12548360,User set java.library.path seems to overwrite default creating problems native lib loading,"This was found by Peeyush Bishnoi.

While running a distributed cache example with Hadoop-0.23,
tasks are failing as follows:
------------------------------------------------------------------------------------------------------------

Exception from container-launch:
org.apache.hadoop.util.Shell$ExitCodeException: at
org.apache.hadoop.util.Shell.runCommand(Shell.java:261) at
org.apache.hadoop.util.Shell.run(Shell.java:188) at
org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381) at
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:207)
at
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:241)
at
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at
java.util.concurrent.FutureTask.run(FutureTask.java:138) at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619) main : command provided 1 main : user
is <user>
------------------------------------------------------------------------------------------------------------

Same Pig script and command work successfully on 0.20

See this in the stderr:

Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:247)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1179)
    at
org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1149)
    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1238)
    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1264)
    at org.apache.hadoop.security.Groups.(Groups.java:54)
    at
org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:178)
    at
org.apache.hadoop.security.UserGroupInformation.initUGI(UserGroupInformation.java:252)
    at
org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:223)
    at
org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:265)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:75)
Caused by: java.lang.RuntimeException: Bailing out since native library
couldn't be loaded
    at
org.apache.hadoop.security.JniBasedUnixGroupsMapping.(JniBasedUnixGroupsMapping.java:48)
    ... 12 more

Pig command:
$ pig -Dmapred.job.queue.name=<queue> -Dmapred.cache.archives=<archives> -Dmapred.child.java.opts=""-Djava.library.path=./ygeo/lib
-Dip2geo.preLoadLibraries=<some other libs>"" -Djava.io.tmpdir=/grid/0/tmp -Dmapred.create.symlink=yes -Dmapred.job.map.memory.mb=3072 piggeoscript.pig

",anupamseth,anupamseth,Major,Closed,Fixed,27/Mar/12 18:16,07/Sep/12 21:03
Bug,MAPREDUCE-4073,12548364,CS assigns multiple off-switch containers when using multi-level-queues,"CS is supposed to be allocating a single off-switch container per node heartbeat (MAPREDUCE-3641). This works for queues directly under root, but not in the case of multi-level queues.
",sseth,sseth,Critical,Closed,Fixed,27/Mar/12 19:32,07/Sep/12 21:03
Bug,MAPREDUCE-4074,12548466,Client continuously retries to RM When RM goes down before launching Application Master,"Client continuously tries to RM and logs the below messages when the RM goes down before launching App Master. 

I feel exception should be thrown or break the loop after finite no of retries.

{code:xml}
28/03/12 07:15:03 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 0 time(s).
28/03/12 07:15:04 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 1 time(s).
28/03/12 07:15:05 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 2 time(s).
28/03/12 07:15:06 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 3 time(s).
28/03/12 07:15:07 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 4 time(s).
28/03/12 07:15:08 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 5 time(s).
28/03/12 07:15:09 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 6 time(s).
28/03/12 07:15:10 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 7 time(s).
28/03/12 07:15:11 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 8 time(s).
28/03/12 07:15:12 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 9 time(s).
28/03/12 07:15:13 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 0 time(s).
28/03/12 07:15:14 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 1 time(s).
28/03/12 07:15:15 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 2 time(s).
28/03/12 07:15:16 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 3 time(s).
28/03/12 07:15:17 INFO ipc.Client: Retrying connect to server: linux-f330.site/10.18.40.182:8032. Already tried 4 time(s).
{code}",xieguiming,devaraj,Major,Closed,Fixed,28/Mar/12 10:33,11/Oct/12 17:48
Bug,MAPREDUCE-4076,12548490,Stream job fails with ZipException when use yarn jar command,"Stream job fails with ZipException when use yarn jar command and executes successfully with hadoop jar command.

{code:xml}
linux-f330:/home/devaraj/hadoop/trunk/hadoop-0.24.0-SNAPSHOT/bin # ./yarn jar ../share/hadoop/tools/lib/hadoop-streaming-0.24.0-SNAPSHOT.jar -input /hadoop -output /test/output/1 -mapper cat -reducer wc
packageJobJar: [] [/home/devaraj/hadoop/trunk/hadoop-0.24.0-SNAPSHOT/bin/$%7Bhadoop.home.dir%7D/hadoop-$%7Buser.name%7D/hadoop-unjar4241129353499211360/] /tmp/streamjob7683981905208294893.jar tmpDir=null
Exception in thread ""main"" java.io.IOException: java.util.zip.ZipException: ZIP file must have at least one entry
        at org.apache.hadoop.streaming.JarBuilder.merge(JarBuilder.java:82)
        at org.apache.hadoop.streaming.StreamJob.packageJobJar(StreamJob.java:707)
        at org.apache.hadoop.streaming.StreamJob.setJobConf(StreamJob.java:948)
        at org.apache.hadoop.streaming.StreamJob.run(StreamJob.java:127)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:50)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
{code}
",devaraj,devaraj,Blocker,Closed,Fixed,28/Mar/12 13:35,12/May/16 18:23
Bug,MAPREDUCE-4081,12548678,TestMROutputFormat.java does not compile,"[ERROR] /hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMROutputFormat.java:[36,7] class TestConfInCheckSpec is public, should be declared in a file named TestConfInCheckSpec.java
",jlowe,jlowe,Blocker,Closed,Fixed,29/Mar/12 14:25,10/Mar/15 04:30
Bug,MAPREDUCE-4082,12548682,hadoop-mapreduce-client-app's mrapp-generated-classpath file should not be in the module JAR,"Currently the mrapp-generated-classpath file containing the 'built' classpath, which only makes sense during building/testing in the machine where the build happens, is bundled in the hadoop-mapreduce-client-app JAR.

Because the file is bundled in the hadoop-mapreduce-client-app JAR, its contents are added to the classpath of all MR jobs. 

All this entries are useless and just pollute the classpath.

This file should not be bundled in the hadoop-mapreduce-client-app JAR.

As an example, the contents of this file in my local built are:

{code}
/Users/tucu/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/tucu/.m2/repository/asm/asm/3.2/asm-3.2.jar:/Users/tucu/.m2/repository/com/cenqua/clover/clover/3.0.2/clover-3.0.2.jar:/Users/tucu/.m2/repository/com/google/guava/guava/r09/guava-r09.jar:/Users/tucu/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/Users/tucu/.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/Users/tucu/.m2/repository/com/google/protobuf/protobuf-java/2.4.0a/protobuf-java-2.4.0a.jar:/Users/tucu/.m2/repository/com/googlecode/json-simple/json-simple/1.1/json-simple-1.1.jar:/Users/tucu/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-client/1.8/jersey-client-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-grizzly2/1.8/jersey-grizzly2-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.8/jersey-guice-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-test-framework/jersey-test-framework-core/1.8/jersey-test-framework-core-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-test-framework/jersey-test-framework-grizzly2/1.8/jersey-test-framework-grizzly2-1.8.jar:/Users/tucu/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/tucu/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/tucu/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/tucu/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/tucu/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/tucu/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/Users/tucu/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/tucu/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/tucu/.m2/repository/commons-daemon/commons-daemon/1.0.3/commons-daemon-1.0.3.jar:/Users/tucu/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/tucu/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/tucu/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/tucu/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar:/Users/tucu/.m2/repository/commons-lang/commons-lang/2.5/commons-lang-2.5.jar:/Users/tucu/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/Users/tucu/.m2/repository/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar:/Users/tucu/.m2/repository/commons-net/commons-net/1.4.1/commons-net-1.4.1.jar:/Users/tucu/.m2/repository/hsqldb/hsqldb/1.8.0.7/hsqldb-1.8.0.7.jar:/Users/tucu/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/tucu/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/tucu/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/tucu/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/tucu/.m2/repository/javax/xml/bind/jaxb-api/2.1/jaxb-api-2.1.jar:/Users/tucu/.m2/repository/jdiff/jdiff/1.0.9/jdiff-1.0.9.jar:/Users/tucu/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/Users/tucu/.m2/repository/junit/junit/4.8.2/junit-4.8.2.jar:/Users/tucu/.m2/repository/log4j/log4j/1.2.15/log4j-1.2.15.jar:/Users/tucu/.m2/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/tucu/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/Users/tucu/.m2/repository/org/apache/avro/avro/1.5.4/avro-1.5.4.jar:/Users/tucu/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-annotations/0.23.1-cdh4b2-SNAPSHOT/hadoop-annotations-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-auth/0.23.1-cdh4b2-SNAPSHOT/hadoop-auth-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-common/0.23.1-cdh4b2-SNAPSHOT/hadoop-common-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-common/0.23.1-cdh4b2-SNAPSHOT/hadoop-common-0.23.1-cdh4b2-SNAPSHOT-tests.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-hdfs/0.23.1-cdh4b2-SNAPSHOT/hadoop-hdfs-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/0.23.1-cdh4b2-SNAPSHOT/hadoop-mapreduce-client-common-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/0.23.1-cdh4b2-SNAPSHOT/hadoop-mapreduce-client-core-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/0.23.1-cdh4b2-SNAPSHOT/hadoop-mapreduce-client-shuffle-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-api/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-api-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-common/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-common-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-common/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-common-0.23.1-cdh4b2-SNAPSHOT-tests.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-server-common-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-server-nodemanager/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-server-nodemanager-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-server-resourcemanager-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-server-resourcemanager-0.23.1-cdh4b2-SNAPSHOT-tests.jar:/Users/tucu/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/0.23.1-cdh4b2-SNAPSHOT/hadoop-yarn-server-web-proxy-0.23.1-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/zookeeper/zookeeper/3.4.3-cdh4b2-SNAPSHOT/zookeeper-3.4.3-cdh4b2-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/aspectj/aspectjrt/1.6.5/aspectjrt-1.6.5.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/Users/tucu/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/tucu/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/tucu/.m2/repository/org/glassfish/javax.servlet/3.0/javax.servlet-3.0.jar:/Users/tucu/.m2/repository/org/glassfish/external/management-api/3.0.0-b012/management-api-3.0.0-b012.jar:/Users/tucu/.m2/repository/org/glassfish/gmbal/gmbal-api-only/3.0.0-b023/gmbal-api-only-3.0.0-b023.jar:/Users/tucu/.m2/repository/org/glassfish/grizzly/grizzly-framework/2.1.1/grizzly-framework-2.1.1-tests.jar:/Users/tucu/.m2/repository/org/glassfish/grizzly/grizzly-framework/2.1.1/grizzly-framework-2.1.1.jar:/Users/tucu/.m2/repository/org/glassfish/grizzly/grizzly-http/2.1.1/grizzly-http-2.1.1.jar:/Users/tucu/.m2/repository/org/glassfish/grizzly/grizzly-http-server/2.1.1/grizzly-http-server-2.1.1.jar:/Users/tucu/.m2/repository/org/glassfish/grizzly/grizzly-http-servlet/2.1.1/grizzly-http-servlet-2.1.1.jar:/Users/tucu/.m2/repository/org/glassfish/grizzly/grizzly-rcm/2.1.1/grizzly-rcm-2.1.1.jar:/Users/tucu/.m2/repository/org/jboss/netty/netty/3.2.3.Final/netty-3.2.3.Final.jar:/Users/tucu/.m2/repository/org/mockito/mockito-all/1.8.5/mockito-all-1.8.5.jar:/Users/tucu/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/Users/tucu/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/tucu/.m2/repository/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar:/Users/tucu/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar:/Users/tucu/.m2/repository/org/xerial/snappy/snappy-java/1.0.3.2/snappy-java-1.0.3.2.jar:/Users/tucu/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/tucu/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/tucu/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/tucu/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/tucu/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar
{code}",tucu00,tucu00,Critical,Closed,Fixed,29/Mar/12 15:01,07/Sep/12 21:03
Bug,MAPREDUCE-4083,12548685,GridMix emulated job tasks.resource-usage emulator for CPU usage throws NPE when Trace contains cumulativeCpuUsage value of 0 at attempt level,GridMix emulated job tasks.resource-usage emulator for CPU usage throws NPE when Trace contains cumulativeCpuUsage value of 0 at attempt level,amar_kamat,karams,Major,Closed,Fixed,29/Mar/12 15:19,03/Sep/14 22:45
Bug,MAPREDUCE-4087,12548807,[Gridmix] GenerateDistCacheData job of Gridmix can become slow in some cases,"In map() method of GenerateDistCacheData job of Gridmix, val.setSize() is done every time based on the bytes to be written to a distributed cache file. When we try to write data to next distributed cache file in the same map task, the size of random data generated in each iteration can become small based on the particular case. This can make this dist cache data generation slow.",ravidotg,ravidotg,Major,Closed,Fixed,30/Mar/12 10:39,03/Sep/14 22:47
Bug,MAPREDUCE-4088,12548841,Task stuck in JobLocalizer prevented other tasks on the same node from committing,"We saw that as a result of HADOOP-6963, one task was stuck in this

Thread 23668: (state = IN_NATIVE)
 - java.io.UnixFileSystem.getBooleanAttributes0(java.io.File) @bci=0 (Compiled frame; information may be imprecise)
 - java.io.UnixFileSystem.getBooleanAttributes(java.io.File) @bci=2, line=228 (Compiled frame)
 - java.io.File.exists() @bci=20, line=733 (Compiled frame)
 - org.apache.hadoop.fs.FileUtil.getDU(java.io.File) @bci=3, line=446 (Compiled frame)
 - org.apache.hadoop.fs.FileUtil.getDU(java.io.File) @bci=52, line=455 (Compiled frame)
 - org.apache.hadoop.fs.FileUtil.getDU(java.io.File) @bci=52, line=455 (Compiled frame)
....
.... TONS MORE OF THIS SAME LINE
 - org.apache.hadoop.fs.FileUtil.getDU(java.io.File) @bci=52, line=455 (Compiled frame)
.....
.....
 - org.apache.hadoop.fs.FileUtil.getDU(java.io.File) @bci=52, line=455 (Compiled frame)
 - org.apache.hadoop.fs.FileUtil.getDU(java.io.File) @bci=52, line=455 (Interpreted frame)
ne=451 (Interpreted frame)
 - org.apache.hadoop.mapred.JobLocalizer.downloadPrivateCacheObjects(org.apache.hadoop.conf.Configuration, java.net.URI[], org.apache.hadoop.fs.Path[], long[], boolean[], boolean) @bci=150, line=324 (Interpreted frame)
 - org.apache.hadoop.mapred.JobLocalizer.downloadPrivateCache(org.apache.hadoop.conf.Configuration) @bci=40, line=349 (Interpreted frame) 51, line=383 (Interpreted frame)
 - org.apache.hadoop.mapred.JobLocalizer.runSetup(java.lang.String, java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.mapred.TaskUmbilicalProtocol) @bci=46, line=477 (Interpreted frame)
 - org.apache.hadoop.mapred.JobLocalizer$3.run() @bci=20, line=534 (Interpreted frame)
 - org.apache.hadoop.mapred.JobLocalizer$3.run() @bci=1, line=531 (Interpreted frame)
 - java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Interpreted frame)
 - javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=396 (Interpreted frame)
 - org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1082 (Interpreted frame)
 - org.apache.hadoop.mapred.JobLocalizer.main(java.lang.String[]) @bci=266, line=530 (Interpreted frame)

While all other tasks on the same node were stuck in 
Thread 32141: (state = BLOCKED)
 - java.lang.Thread.sleep(long) @bci=0 (Interpreted frame)
 - org.apache.hadoop.mapred.Task.commit(org.apache.hadoop.mapred.TaskUmbilicalProtocol, org.apache.hadoop.mapred.Task$TaskReporter, org.apache.hadoop.mapreduce.OutputCommitter) @bci=24, line=980 (Compiled frame)
 - org.apache.hadoop.mapred.Task.done(org.apache.hadoop.mapred.TaskUmbilicalProtocol, org.apache.hadoop.mapred.Task$TaskReporter) @bci=146, line=871 (Interpreted frame)
 - org.apache.hadoop.mapred.ReduceTask.run(org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.TaskUmbilicalProtocol) @bci=470, line=423 (Interpreted frame)
 - org.apache.hadoop.mapred.Child$4.run() @bci=29, line=255 (Interpreted frame)
 - java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Interpreted frame)
 - javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=396 (Interpreted frame)
 - org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1082 (Interpreted frame)
 - org.apache.hadoop.mapred.Child.main(java.lang.String[]) @bci=738, line=249 (Interpreted frame)

This should never happen. A stuck task should never prevent other tasks from different jobs on the same node from committing.",raviprak,raviprak,Critical,Closed,Fixed,30/Mar/12 16:15,30/Oct/12 20:55
Bug,MAPREDUCE-4089,12548855,Hung Tasks never time out. ,"The AM will timeout a task through mapreduce.task.timeout only when it does not hear from the task within the given timeframe.  On 1.0 a task must be making progress, either by reading input from HDFS, writing output to HDFS, writing to a log, or calling a special method to inform it that it is still making progress.

This is because on 0.23 a status update which happens every 3 seconds is counted as progress.",revans2,revans2,Blocker,Closed,Fixed,30/Mar/12 18:06,10/Mar/15 04:30
Bug,MAPREDUCE-4091,12548873,tools testcases failing because of MAPREDUCE-4082,"MAPREDUCE-4082 moved the generated-classpath file used by MRApp from the main classpath to the test classpath.

The objective of MAPREDUCE-4082 was to remove the generated-classpath file from the hadoop-mapreduce-client-app JAR. I've thought that moving it to the test-classpath would do the trick.

This is breaking tools testcases (most likely) because of different classloader being used by maven for main classpath and test classpath.

",tucu00,tucu00,Critical,Closed,Fixed,30/Mar/12 21:08,07/Sep/12 21:03
Bug,MAPREDUCE-4092,12548879,commitJob Exception does not fail job (regression in 0.23 vs 0.20),"If commitJob throws an exception JobImpl will swallow the exception with a warning and succeed the Job. This is a break from 0.20 and 1.0 where commitJob exception will fail the job

Exception logged in the AM as WARN
  org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Could not do commit for Job
Job still finishes as succeeded",jeagles,jeagles,Blocker,Closed,Fixed,30/Mar/12 22:02,07/Sep/12 21:03
Bug,MAPREDUCE-4095,12548905,TestJobInProgress#testLocality uses a bogus topology,"The following in TestJobInProgress#testLocality:

{code}
    Node r2n4 = new NodeBase(""/default/rack2/s1/node4"");
    nt.add(r2n4);
{code}

violates the check introduced by HADOOP-8159:

{noformat}
Testcase: testLocality took 0.005 sec
        Caused an ERROR
Invalid network topology. You cannot have a rack and a non-rack node at the same level of the network topology.
org.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Invalid network topology. You cannot have a rack and a non-rack node at the same level of the network topology.
        at org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:349)
        at org.apache.hadoop.mapred.TestJobInProgress.testLocality(TestJobInProgress.java:232)
{noformat}",cmccabe,eli2,Major,Closed,Fixed,31/Mar/12 02:28,10/Mar/15 04:32
Bug,MAPREDUCE-4097,12549222,tools testcases fail because missing mrapp-generated-classpath file in classpath,"The mrapp-generated-classpath file is created in hadoop-mapreduce-client-apptarget/classes/ dir but it is excluded from the JAR.

When running tools testcases from root level, mvn uses hadoop-mapreduce-client-app/target/classes/ dir to create the classpath.

When running tools testcases from tools level, mvn uses the hadoop-mapreduce-client-app JAR from M2 cache to create the classpath.

In the later the mrapp-generated-classpath is not present.",rvs,tucu00,Major,Closed,Fixed,02/Apr/12 20:38,07/Sep/12 21:03
Bug,MAPREDUCE-4098,12549302,TestMRApps testSetClasspath fails,"The assertion of this test is testing for equality, as the generated classpath file is in the classpath the test fails.

Instead, the test should test for the expected path elements to be in the classspath.",tucu00,tucu00,Major,Closed,Fixed,03/Apr/12 10:31,23/May/12 20:28
Bug,MAPREDUCE-4099,12549329,ApplicationMaster may fail to remove staging directory,"When the ApplicationMaster shuts down it's supposed to remove the staging directory, assuming properties weren't set to override this behavior. During shutdown the AM tells the ResourceManager that it has finished before it cleans up the staging directory.  However upon hearing the AM has finished, the RM turns right around and kills the AM container.  If the AM is too slow, the AM will be killed before the staging directory is removed.

We're seeing the AM lose this race fairly consistently on our clusters, and the lack of staging directory cleanup quickly leads to filesystem quota issues for some users.",jlowe,jlowe,Critical,Closed,Fixed,03/Apr/12 14:00,02/Apr/15 00:42
Bug,MAPREDUCE-4100,12549347,Sometimes gridmix emulates data larger much larger then acutal counter for map only jobs,"While running 1400+ jobs trace I encountered this issue.
For map-only jobs, observed that some Maps generating data of around 9 GB (From HDFS_BYTES_WRITTEN) whereas actual value is around 5GB in trace.
This can sometimes also cause jobs to fail intermittently.

Other GridMix version coming be Hadoop-1.1.X and above might also effected ",amar_kamat,karams,Minor,Closed,Fixed,03/Apr/12 15:06,03/Sep/14 22:45
Bug,MAPREDUCE-4102,12549389,job counters not available in Jobhistory webui for killed jobs,"Run a simple wordcount or sleep, and kill the job before it finishes.  Go to the job history web ui and click the ""Counters"" link for that job. It displays ""500 error"".

The job history log has:

Caused by: com.google.inject.ProvisionException: Guice provision errors:

2012-04-03 19:42:53,148 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /jobhistory/jobcounters/job_1333482028750_0001
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
...
..
...

1) Error injecting constructor, java.lang.NullPointerException
  at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)
  while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock
...
..
...
Caused by: java.lang.NullPointerException    at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)
    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)
    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)


There are task counters available if you drill down into successful tasks though.",kamesh,tgraves,Major,Closed,Fixed,03/Apr/12 19:51,07/Mar/13 17:43
Bug,MAPREDUCE-4105,12549605,Yarn RackResolver ignores rack configurations,"Incorrect mappings because the Yarn RackResolver ignores rack configurations. This can be verified by inspecting the resource manager web ui that lists all the nodes, all of them show up with /default-rack regardless of the output from the script specified using net.topology.script.file.name configuration property.",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,05/Apr/12 02:16,23/May/12 20:28
Bug,MAPREDUCE-4117,12549907,mapred job -status throws NullPointerException,"{code:xml}
dev@ubuntudev-linux:~/hadoop/hadoop-trunk/bin$ ./mapred job -status job_1333408894669_0001

Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.hadoop.mapreduce.Job.getTaskFailureEventString(Job.java:512)
    at org.apache.hadoop.mapreduce.Job.toString(Job.java:463)
    at java.lang.String.valueOf(String.java:2838)
    at java.io.PrintStream.println(PrintStream.java:788)
    at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:255)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
    at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1244)
{code}",devaraj,devaraj,Critical,Closed,Fixed,06/Apr/12 17:44,12/May/16 18:24
Bug,MAPREDUCE-4123,12550205,./mapred groups gives NoClassDefFoundError,"linux-168:/home/v2/hadoop-3.0.0-SNAPSHOT/bin # ./mapred groups
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/tools/GetGroups
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.tools.GetGroups
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:303)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:316)
Could not find the main class: org.apache.hadoop.mapred.tools.GetGroups.  Program will exit.
",devaraj,nishan,Critical,Closed,Fixed,09/Apr/12 09:33,12/May/16 18:22
Bug,MAPREDUCE-4128,12550252,AM Recovery expects all attempts of a completed task to also be completed.,"The AM seems to assume that all attempts of a completed task (from a previous AM incarnation) would also be completed. There is at least one case in which this does not hold. Case being cancellation of a completed task resulting in a new running attempt.
",bikassaha,bikassaha,Major,Closed,Fixed,09/Apr/12 18:02,12/May/16 18:23
Bug,MAPREDUCE-4129,12550296,Lots of unneeded counters log messages,"Huge number of the same WARN messages are written. We only need to write each distinct message once. The messages are of the form:

{code}
2012-04-05 03:55:04,166 WARN mapreduce.Counters: Group {oldGroup} is deprecated. Use {newGroup} instead
{code}",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,09/Apr/12 23:45,07/Sep/12 21:03
Bug,MAPREDUCE-4133,12550429,MR over viewfs is broken,"After the changes in HADOOP-8014 went in, MR programs using viewfs broke. This is because, viewfs now expects getDefaultBlockSize, getDefaultReplication, and getServerDefaults to pass in a {{path}} as an argument. In the existing MR source, these are called with no arguments.",johnvijoe,johnvijoe,Major,Closed,Fixed,10/Apr/12 19:26,07/Sep/12 21:03
Bug,MAPREDUCE-4139,12550588,Potential ResourceManager deadlock when SchedulerEventDispatcher is stopped,"When the main thread calls ResourceManager$SchedulerEventDispatcher.stop() it grabs a lock on the object, kicks the event processor thread, and then waits for the thread to exit.  However the interrupted event processor thread can end up trying to call the synchronized getConfig() method which results in deadlock.",jlowe,jlowe,Major,Closed,Fixed,11/Apr/12 21:30,11/Oct/12 17:48
Bug,MAPREDUCE-4140,12550608,"mapreduce classes incorrectly importing ""clover.org.apache.*"" classes","A number of classes in mapreduce are importing clover.org.apache.* classes

e.g. hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/PartialJob.java",phunt,phunt,Major,Closed,Fixed,12/Apr/12 00:16,11/Oct/12 17:48
Bug,MAPREDUCE-4141,12550611,"clover integration broken, also mapreduce poms are pulling in clover as a dependency","Some of the poms are specifying clover as a dependency, rather than exclusively as a plugin.

Also I tried running with the clover profile on trunk and the build is failing due to some issue with protobufs code generation.",phunt,phunt,Major,Resolved,Fixed,12/Apr/12 00:50,10/Mar/15 04:30
Bug,MAPREDUCE-4144,12550708,ResourceManager NPE while handling NODE_UPDATE,"The RM on one of our clusters has exited twice in the past few days because of an NPE while trying to handle a NODE_UPDATE:

{noformat}
2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
 [ResourceManager Event Processor]java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

This is very similar to the failure reported in MAPREDUCE-3005.",jlowe,jlowe,Critical,Closed,Fixed,12/Apr/12 16:28,11/Oct/12 17:48
Bug,MAPREDUCE-4147,12550736,YARN should not have a compile-time dependency on HDFS,"YARN doesn't (and shouldn't) use any HDFS-specific APIs, so it should not declare HDFS as a compile-time dependency.",tomwhite,tomwhite,Major,Closed,Fixed,12/Apr/12 20:23,23/May/12 20:28
Bug,MAPREDUCE-4148,12550759,MapReduce should not have a compile-time dependency on HDFS,MapReduce depends on HDFS's DelegationTokenIdentifier (for printing token debug information). We should remove this dependency and MapReduce's compile-time dependency on HDFS.,tomwhite,tomwhite,Major,Closed,Fixed,12/Apr/12 23:44,11/Oct/12 17:48
Bug,MAPREDUCE-4149,12550778,Rumen fails to parse certain counter strings,"If a counter name contains ""{"" or ""}"", Rumen is not able to parse it and throws ParseException.",ravidotg,ravidotg,Major,Closed,Fixed,13/Apr/12 06:14,03/Sep/14 22:45
Bug,MAPREDUCE-4152,12550842,map task left hanging after AM dies trying to connect to RM,"We had an instance where the RM went down for more then an hour.  The application master exited with ""Could not contact RM after 360000 milliseconds""

2012-04-11 10:43:36,040 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1333003059741_15999Job Transitioned from RUNNING to ERROR


",tgraves,tgraves,Major,Closed,Fixed,13/Apr/12 15:27,11/Oct/12 17:48
Bug,MAPREDUCE-4154,12550895,streaming MR job succeeds even if the streaming command fails,"Hadoop 1.0.1 behaves as expected - The task fails for streaming MR job if the streaming command fails. But it succeeds in hadoop 1.0.2 .
",ddas,thejas,Major,Closed,Fixed,13/Apr/12 22:58,16/May/12 20:45
Bug,MAPREDUCE-4156,12551071,ant build fails compiling JobInProgress,"The ant build fails trying to compile jobInProgress.  Looks like TaskFinishedEvent has a new parameter (added in MAPREDUCE-4128) so we probably need to update the old mrv1 source so it atleast compiles.


compile-mapred-classes:
[jsp-compile] 2012-04-13 08:43:33,175 WARN  [main] compiler.TldLocationsCache (TldLocationsCache.java:processWebDotXml(284)) - Internal Error: File /WEB-INF/web.xml not found
    [javac] Compiling 86 source files to /home/y/var/builds/thread2/workspace/Cloud-Hadoop-All-0.23.3-Secondary/hadoop-mapreduce-project/build/classes
    [javac] /home/y/var/builds/thread2/workspace/Cloud-Hadoop-All-0.23.3-Secondary/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java:2712: cannot find symbol
    [javac] symbol  : constructor TaskFinishedEvent(org.apache.hadoop.mapred.TaskID,long,org.apache.hadoop.mapreduce.TaskType,java.lang.String,org.apache.hadoop.mapreduce.Counters)
    [javac] location: class org.apache.hadoop.mapreduce.jobhistory.TaskFinishedEvent
    [javac]     TaskFinishedEvent tfe = new TaskFinishedEvent(tip.getTIPId(),
    [javac]                             ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error
",tgraves,tgraves,Major,Closed,Fixed,16/Apr/12 14:39,11/Oct/12 17:48
Bug,MAPREDUCE-4159,12551176,"Job is running in Uber mode after setting ""mapreduce.job.ubertask.maxreduces"" to zero","1.Configure ""mapreduce.job.ubertask.enable"" to true
2.Configure ""mapreduce.job.ubertask.maxreduces"" to 0(zero)
3.Run job such that it has one reducer(more than ""mapreduce.job.ubertask.maxreduces"" value) 

Observe that job is running in Uber mode instead of normal mode(non uber mode)",devaraj,nishan,Major,Closed,Fixed,17/Apr/12 07:19,12/May/16 18:22
Bug,MAPREDUCE-4160,12551245,some mrv1 ant tests fail with timeout - due to 4156,"Looks like the old mrv1 history server is getting null error when trying to parse the a null for the TaskFinishedEvent successfulAttemptId which was added in jira 4156.  This is causing the ant tests to take like 9 hours to run.


[junit] java.io.IOException: java.lang.NullPointerException: null of string in field successfulAttemptId of org.apache.hadoop.mapreduce.jobhistory.TaskFinished of union in field event of org.apache.hadoop.mapreduce.jobhistory.Event
    [junit] 	at org.apache.avro.generic.GenericDatumWriter.npe(GenericDatumWriter.java:92)
    [junit] 	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:86)
    [junit] 	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:57)
    [junit] 	at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:66)
    [junit] 	at org.apache.hadoop.mapreduce.jobhistory.JobHistory$MetaInfo.writeEvent(JobHistory.java:512)
    [junit] 	at org.apache.hadoop.mapreduce.jobhistory.JobHistory.logEvent(JobHistory.java:345)
    [junit] 	at org.apache.hadoop.mapred.JobInProgress.completedTask(JobInProgress.java:2717)
    [junit] 	at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:1221)
",tgraves,tgraves,Major,Closed,Fixed,17/Apr/12 15:25,11/Oct/12 17:48
Bug,MAPREDUCE-4164,12551307,Hadoop 22 Exception thrown after task completion causes its reexecution,"2012-02-28 19:17:08,504 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 1969310 bytes
2012-02-28 19:17:08,694 INFO org.apache.hadoop.mapred.Task: Task:attempt_201202272306_0794_m_000094_0 is done. And is in the process of commiting
2012-02-28 19:18:08,774 INFO org.apache.hadoop.mapred.Task: Communication exception: java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException
at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)
at org.apache.hadoop.ipc.Client.call(Client.java:1062)
at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)
at $Proxy0.statusUpdate(Unknown Source)
at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)
at java.lang.Thread.run(Thread.java:662)
Caused by: java.nio.channels.ClosedByInterruptException
at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)
at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)
at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)
at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
at java.io.DataOutputStream.flush(DataOutputStream.java:106)
at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)
at org.apache.hadoop.ipc.Client.call(Client.java:1040)
... 4 more

2012-02-28 19:18:08,825 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201202272306_0794_m_000094_0' done.


================>>>>>> SHOULD be <++++++++++++++
2012-02-28 19:17:02,214 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 1974104 bytes
2012-02-28 19:17:02,408 INFO org.apache.hadoop.mapred.Task: Task:attempt_201202272306_0794_m_000000_0 is done. And is in the process of commiting
2012-02-28 19:17:02,519 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201202272306_0794_m_000000_0' done. ",mayank_bansal,mayank_bansal,Major,Resolved,Fixed,17/Apr/12 21:30,24/Apr/12 02:09
Bug,MAPREDUCE-4165,12551539,Committing is misspelled as commiting in task logs,,jeagles,jeagles,Trivial,Closed,Fixed,18/Apr/12 21:42,07/Sep/12 21:03
Bug,MAPREDUCE-4169,12551829,Container Logs appear in unsorted order,"container logs (stdout, stderr, syslog) in the nodemanager ui and jobhistory ui appear in unsorted order where the order displayed is based on what file was created first. This jira will have the results be displayed in a consistent order.",jeagles,jeagles,Minor,Closed,Fixed,20/Apr/12 19:18,07/Sep/12 21:03
Bug,MAPREDUCE-4189,12552081,TestContainerManagerSecurity is failing,"{code:xml}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.TestDiskFailures
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.519 sec
Running org.apache.hadoop.yarn.server.TestContainerManagerSecurity
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 10.673 sec <<< FAILURE!

Results :

Tests in error:
  testAuthenticatedUser(org.apache.hadoop.yarn.server.TestContainerManagerSecurity)
  testMaliceUser(org.apache.hadoop.yarn.server.TestContainerManagerSecurity)
  testUnauthorizedUser(org.apache.hadoop.yarn.server.TestContainerManagerSecurity)

Tests run: 5, Failures: 0, Errors: 3, Skipped: 0

{code}",devaraj,devaraj,Critical,Closed,Fixed,23/Apr/12 05:53,10/Mar/15 04:30
Bug,MAPREDUCE-4193,12552193,broken doc link for yarn-default.xml in site.xml,"the link to yarn-default.xml in site.xml is incorrect, generated docs link is broken.",phunt,phunt,Major,Closed,Fixed,23/Apr/12 20:39,20/Sep/12 12:40
Bug,MAPREDUCE-4194,12552439,ConcurrentModificationError in DirectoryCollection,"As found as part of work on MAPREDUCE-4169, it is possible for a ConcurrentModificationException to be thrown upon disk failure. DirectoryCollection hands out its internal list structure that is accessed across multiple threads. Upon disk failure its internal list is modified, invalidating all current iterators to that structure.",jeagles,jeagles,Major,Closed,Fixed,24/Apr/12 21:42,07/Sep/12 21:03
Bug,MAPREDUCE-4195,12552719,"With invalid queueName request param, jobqueue_details.jsp shows NPE","When you access /jobqueue_details.jsp manually, instead of via a link, it has queueName set to null internally and this goes for a lookup into the scheduling info maps as well.

As a result, if using FairScheduler, a Pool with String name = null gets created and this brings the scheduler down. I have not tested what happens to the CapacityScheduler, but ideally if no queueName is set in that jsp, it should fall back to 'default'. Otherwise, this brings down the JobTracker completely.

FairScheduler must also add a check to not create a pool with 'null' name.

The following is the strace that ensues:

{code}
ERROR org.mortbay.log: /jobqueue_details.jsp 
java.lang.NullPointerException 
at org.apache.hadoop.mapred.jobqueue_005fdetails_jsp._jspService(jobqueue_005fdetails_jsp.java:71) 
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) 
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) 
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) 
at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:829) 
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) 
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399) 
at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) 
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) 
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) 
at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) 
at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230) 
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) 
at org.mortbay.jetty.Server.handle(Server.java:326) 
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) 
at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) 
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) 
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) 
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) 
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410) 
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) 
INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9001, call heartbeat from XYZ:MNOP: error: java.io.IOException: java.lang.NullPointerException 
java.io.IOException: java.lang.NullPointerException 
at org.apache.hadoop.mapred.SchedulingAlgorithms$FairShareComparator.compare(SchedulingAlgorithms.java:95) 
at org.apache.hadoop.mapred.SchedulingAlgorithms$FairShareComparator.compare(SchedulingAlgorithms.java:68) 
at java.util.Arrays.mergeSort(Unknown Source) 
at java.util.Arrays.sort(Unknown Source) 
at java.util.Collections.sort(Unknown Source) 
at org.apache.hadoop.mapred.FairScheduler.assignTasks(FairScheduler.java:435) 
at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:3226) 
at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) 
at java.lang.reflect.Method.invoke(Unknown Source) 
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127) 
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428)
{code}",,jira.shegalov,Critical,Closed,Fixed,25/Apr/12 20:11,15/May/13 05:15
Bug,MAPREDUCE-4197,12552746,Include the hsqldb jar in the hadoop-mapreduce tar file,"Courtesy Brahma

{quote}
In the previuos hadoop releases(20.XX) hsqldb was provided.
But in hadoop-2.0.0 it is not present.Is it intentionally deleted or missing?
{quote}

",raviprak,raviprak,Major,Closed,Fixed,25/Apr/12 22:45,11/Oct/12 17:48
Bug,MAPREDUCE-4201,12553033,Getting PID not working on Windows. Termination of Task/TaskJVM's not working,"Child Task not reporting PID because of Linux specific shell script implementation.
Signaling task termination currently disabled by the initial Windows patch.",bikassaha,bikassaha,Major,Resolved,Fixed,26/Apr/12 20:26,27/Apr/12 22:57
Bug,MAPREDUCE-4202,12553040,TestYarnClientProtocolProvider is broken,The test fails because a cluster is unexpectedly created with an empty conf.,daryn,daryn,Major,Closed,Fixed,26/Apr/12 21:13,10/Mar/15 04:32
Bug,MAPREDUCE-4206,12553214,Sorting by Last Health-Update on the RM nodes page sorts does not work correctly,column is mistakenly sorted lexically,jeagles,jeagles,Minor,Closed,Fixed,27/Apr/12 22:18,11/Oct/12 17:48
Bug,MAPREDUCE-4207,12553217,Remove System.out.println() in FileInputFormat,MAPREDUCE-3607 accidentally left the println statement. ,kihwal,kihwal,Major,Closed,Fixed,27/Apr/12 22:52,16/May/12 20:45
Bug,MAPREDUCE-4209,12553275,junit dependency in hadoop-mapreduce-client is missing scope test,pom.xml in hadoop-mapreduce-client has declared junit as compile time dependency while it must be test scope dependency.,,hsn,Major,Closed,Fixed,29/Apr/12 10:52,10/Mar/15 04:30
Bug,MAPREDUCE-4211,12553432,"Error conditions (missing appid, appid not found) are masked in the RM app page",,jeagles,jeagles,Minor,Closed,Fixed,30/Apr/12 22:17,07/Sep/12 21:03
Bug,MAPREDUCE-4215,12553500,RM app page shows 500 error on appid parse error,"For example, cluster/app/application_1335823499485_000a has displays this error",jeagles,jeagles,Major,Closed,Fixed,01/May/12 16:20,11/Oct/12 17:48
Bug,MAPREDUCE-4220,12553661,RM apps page starttime/endtime sorts are incorrect,Sorting by start time and end time sort lexically instead of temporally.,jeagles,jeagles,Minor,Closed,Fixed,02/May/12 20:03,07/Sep/12 21:03
Bug,MAPREDUCE-4224,12553892,TestFifoScheduler throws org.apache.hadoop.metrics2.MetricsException ,"{code:xml}
2012-05-04 15:18:47,180 WARN  [main] util.MBeans (MBeans.java:getMBeanName(95)) - Error creating MBean object name: Hadoop:service=ResourceManager,name=RMNMInfo
org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=RMNMInfo already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)
	at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:93)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)
	at org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo.<init>(RMNMInfo.java:59)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:225)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler.setUp(TestFifoScheduler.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=RMNMInfo already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)
	... 30 more
{code}",devaraj,devaraj,Major,Closed,Fixed,04/May/12 09:53,12/May/16 18:22
Bug,MAPREDUCE-4226,12554002,ConcurrentModificationException in FileSystemCounterGroup,This was seen in a Hive job. I'll attach a failing test case.,tomwhite,tomwhite,Major,Closed,Fixed,04/May/12 21:04,07/Sep/12 21:03
Bug,MAPREDUCE-4228,12554008,mapreduce.job.reduce.slowstart.completedmaps is not working properly to delay the scheduling of the reduce tasks,"If no more map tasks need to be scheduled but not all have completed, the ApplicationMaster will start scheduling reducers even if the number of completed maps has not met the mapreduce.job.reduce.slowstart.completedmaps threshold.  For example, if the property is set to 1.0 all maps should complete before any reducers are scheduled.  However the reducers are scheduled as soon as the last map task is assigned to a container.  For a job with very long-running maps, a cluster with enough capacity to launch all map tasks could cause reducers to launch prematurely and waste cluster resources.

Thanks to Phil Su for discovering this issue.",jlowe,jlowe,Major,Closed,Fixed,04/May/12 21:25,11/Oct/12 17:48
Bug,MAPREDUCE-4231,12554262,Update RAID to not to use FSInodeInfo,FSInodeInfo was removed by HDFS-3363.  We should update RAID.,szetszwo,szetszwo,Major,Closed,Fixed,08/May/12 00:31,23/May/12 20:28
Bug,MAPREDUCE-4233,12554344,NPE can happen in RMNMNodeInfo.,"{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo.getLiveNodeManagers(RMNMInfo.java:96)
        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at javax.management.StandardMBean.getAttribute(StandardMBean.java:358)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
{noformat}

Looks like rmcontext.getRMNodes() is not kept in sync with scheduler.getNodeReport(), so that the report can be null even though the context still knowns about the node.

The simple fix is to add in a null check.",revans2,revans2,Critical,Closed,Fixed,08/May/12 16:29,07/Sep/12 21:03
Bug,MAPREDUCE-4237,12554546,TestNodeStatusUpdater can fail if localhost has a domain associated with it,"On some systems, RHEL where I work, localhost can resolve to localhost.localdomain.  TestNodeStatusUpdater can fail because the nodeid containes .localdomain which is not expected by the hard coded localhost string.",revans2,revans2,Major,Closed,Fixed,09/May/12 19:39,07/Sep/12 21:03
Bug,MAPREDUCE-4238,12554548,mavenize data_join,mavenize the contrib data_join package,tgraves,tgraves,Critical,Closed,Fixed,09/May/12 19:51,09/Nov/12 02:38
Bug,MAPREDUCE-4241,12554776,Pipes examples do not compile on Ubuntu 12.04,-lssl alone won't work for compiling the pipes examples on 12.04. -lcrypto needs to be added explicitly.,abayer,abayer,Major,Closed,Fixed,10/May/12 20:19,17/Oct/12 18:27
Bug,MAPREDUCE-4250,12554957,"hadoop-config.sh missing variable exports, causes Yarn jobs to fail with ClassNotFoundException MRAppMaster","This is the MR side of HADOOP-8393

If you start a pseudo distributed yarn using ""start-yarn.sh"" you need to specify exports for HADOOP_COMMON_HOME, HADOOP_HDFS_HOME, YARN_HOME, YARN_CONF_DIR, and HADOOP_MAPRED_HOME in hadoop-env.sh (or elsewhere), otherwise the spawned node manager will be missing 
these in it's environment. This is due to start-yarn using yarn-daemons. With this fix it's possible to start yarn (etc...) with only HADOOP_CONF_DIR specified in the environment. Took some time to track down this failure, so seems worthwhile to fix.",phunt,phunt,Major,Closed,Fixed,11/May/12 18:54,11/Oct/12 17:48
Bug,MAPREDUCE-4252,12555002,MR2 job never completes with 1 pending task,"This was found by ATM:

bq. I ran a teragen with 1000 map tasks. Many task attempts failed, but after 999 of the tasks had completed, the job is now sitting forever with 1 task ""pending"".",tomwhite,tomwhite,Major,Closed,Fixed,12/May/12 04:39,11/Oct/12 17:48
Bug,MAPREDUCE-4262,12556036,"NM gives wrong log message saying ""Connected to ResourceManager"" before trying to connect","{code:xml}
2012-05-16 18:04:25,844 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Connected to ResourceManager at /xx.xx.xx.xx:8025
2012-05-16 18:04:26,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 0 time(s).
2012-05-16 18:04:27,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 1 time(s).
2012-05-16 18:04:28,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 2 time(s).
2012-05-16 18:04:29,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 3 time(s).
2012-05-16 18:04:30,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 4 time(s).
{code}",devaraj,devaraj,Minor,Closed,Fixed,16/May/12 13:45,12/May/16 18:24
Bug,MAPREDUCE-4263,12556079,Use taskkill /T to terminate tasks on Windows,On Linux setsid is used to link the processes spawned by the tasks into the same session. So termination of the task terminates the entire tree. We need to do the same for Windows. This is not fool proof but should be sufficient until we have a potentially better solution in MAPREDUCE-4260.,,bikassaha,Major,Resolved,Fixed,16/May/12 17:11,11/Jun/12 20:43
Bug,MAPREDUCE-4264,12556108,Got ClassCastException when using mapreduce.history.server.delegationtoken.required=true,"Oozie fails to run on branch-0.23 with the following exception. This only affects branch-0.23 not branch-2 or trunk. 


12/05/16 15:08:45 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/tgraves/.staging/job_1337177706246_0001java.lang.ClassCastException: org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker cannot be cast to org.apache.hadoop.ipc.RpcInvocationHandler        at org.apache.hadoop.ipc.RPC.getConnectionIdForProxy(RPC.java:330)        at org.apache.hadoop.ipc.RPC.getServerAddress(RPC.java:320)        at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getConnectAddress(MRClientProtocolPBClientImpl.java:108)        at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:195)        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:272)        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:385)        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1226)        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1223)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:396)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)


Note that this is caused because oozie passes in the options:  -Dmapreduce.history.server.delegationtoken.required=true -Dmapreduce.history.server.delegationtoken.renewer=""mr token""",tgraves,tgraves,Blocker,Closed,Fixed,16/May/12 19:19,11/Oct/12 17:48
Bug,MAPREDUCE-4267,12556337,mavenize pipes,We are still building pipes out of the old mrv1 directories using ant.  Move it over to the mrv2 dir structure.  ,tgraves,tgraves,Critical,Closed,Fixed,18/May/12 02:07,07/Feb/15 00:11
Bug,MAPREDUCE-4269,12556507,documentation: Gridmix has javadoc warnings in StressJobFactory,,jeagles,jeagles,Major,Closed,Fixed,19/May/12 06:00,11/Oct/12 17:48
Bug,MAPREDUCE-4270,12556558,data_join test classes are in the wrong packge,"There are three Sample*.java files in this directory

http://svn.apache.org/repos/asf/hadoop/common/trunk/hadoop-tools/hadoop-datajoin/src/test/java/

but they should be in 

http://svn.apache.org/repos/asf/hadoop/common/trunk/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/

based on their package.",tgraves,brocknoland,Major,Closed,Fixed,20/May/12 00:27,10/Mar/15 04:30
Bug,MAPREDUCE-4272,12556573,SortedRanges.Range#compareTo is not spec compliant,"SortedRanges.Range#compareTo does not satisfy the requirement of Comparable#compareTo, where ""the implementor must ensure {noformat}sgn(x.compareTo(y)) == -sgn(y.compareTo(x)){noformat} for all x and y.""

This is manifested as TestStreamingBadRecords failures in alternative JDKs.",crystal_gaoyu,vicaya,Major,Closed,Fixed,20/May/12 04:49,03/Sep/14 23:15
Bug,MAPREDUCE-4276,12556906,"Allow setting yarn.nodemanager.delete.debug-delay-sec property to ""-1"" for easier container debugging.","Allow setting yarn.nodemanager.delete.debug-delay-sec property to ""-1"" to have it never clear (like older TT time).",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,22/May/12 19:12,11/Oct/12 17:48
Bug,MAPREDUCE-4278,12556911,cannot run two local jobs in parallel from the same gateway.,"I cannot run two local mode jobs from Pig in parallel from the same gateway, this is a typical use case. If I re-run the tests sequentially, then the test pass. This seems to be a problem from Hadoop.

Additionally, the pig harness, expects to be able to run Pig-version-undertest against Pig-version-stable from the same gateway.


To replicate the error:

I have two clusters running from the same gateway.
If I run the Pig regression suites nightly.conf in local mode in paralell - once on each cluster. Conflicts in M/R local mode result in failures in the tests. 


ERROR1:

org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find
output/file.out in any of the configured local directories
        at
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:429)
        at
org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)
        at
org.apache.hadoop.mapred.MapOutputFile.getOutputFile(MapOutputFile.java:56)
        at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:944)
        at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:924)
        at org.apache.hadoop.mapred.Task.done(Task.java:875)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:374)
---

ERROR2:

2012-05-17 20:25:36,762 [main] INFO
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
-
HadoopJobId: job_local_0001
2012-05-17 20:25:36,778 [Thread-3] INFO  org.apache.hadoop.mapred.Task -
Using ResourceCalculatorPlugin : org.apache.
hadoop.util.LinuxResourceCalculatorPlugin@ffa490e
2012-05-17 20:25:36,837 [Thread-3] WARN
org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getLoadFunc(PigInputFormat.java
:153)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.createRecordReader(PigInputForm
at.java:106)
        at
org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:489)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at
org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
2012-05-17 20:25:41,291 [main] INFO
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
",sandyr,araceli,Major,Closed,Fixed,22/May/12 19:52,23/Jul/13 23:11
Bug,MAPREDUCE-4279,12556919,getClusterStatus() fails with null pointer exception when running jobs in local mode,"While migrating code from 0.20.2 hadoop codebase to 0.23.1 we encountered this issue for jobs run in local mode of execution:
{code}

java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobClient.arrayToStringList(JobClient.java:783)
	at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:138)
	at org.apache.hadoop.mapred.JobClient$4.run(JobClient.java:815)
	at org.apache.hadoop.mapred.JobClient$4.run(JobClient.java:812)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
	at org.apache.hadoop.mapred.JobClient.getClusterStatus(JobClient.java:812)
{code}

We are using cloudera distribution CDH4b2 for testing, however the underlying code is 0.23.1 and I could see no difference in this implementation.",devaraj,rjain7,Major,Closed,Fixed,22/May/12 20:43,12/May/16 18:23
Bug,MAPREDUCE-4286,12558304,TestClientProtocolProviderImpls passes on failure conditions,,devaraj,devaraj,Major,Closed,Fixed,28/May/12 05:55,10/Apr/15 20:19
Bug,MAPREDUCE-4290,12558385,JobStatus.getState() API is giving ambiguous values,For failed job getState() API is giving status as SUCCEEDED if we use JobClient.getAllJobs() for retrieving all jobs info from RM.,devaraj,nishan,Major,Closed,Fixed,29/May/12 04:17,12/May/16 18:22
Bug,MAPREDUCE-4294,12558582,Submitting job by enabling task profiling gives IOException,"{noformat}
java.io.IOException: Server returned HTTP response code: 400 for URL: http://HOST-10-18-52-224:8080/tasklog?plaintext=true&attemptid=attempt_1338370885386_0006_m_000000_0&filter=profile
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1290)
        at org.apache.hadoop.mapreduce.Job.downloadProfile(Job.java:1421)
        at org.apache.hadoop.mapreduce.Job.printTaskEvents(Job.java:1376)
        at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1310)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1247)
        at org.apache.hadoop.examples.WordCount.main(WordCount.java:84)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
{noformat}",devaraj,nishan,Major,Resolved,Fixed,30/May/12 12:05,12/May/16 18:22
Bug,MAPREDUCE-4295,12558611,RM crashes due to DNS issue,"we had a DNS outage and the RM crashed with the following backtrace:

2012-05-29 19:17:34,492 FATAL
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in
handling event type NODE_UPDATE to the scheduler
java.lang.IllegalArgumentException: java.net.UnknownHostException:
host.com        at
org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:430)
        at
org.apache.hadoop.yarn.util.BuilderUtils.newContainerToken(BuilderUtils.java:261)
       at
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1184)
        at
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1167)
        at
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1213)",tgraves,tgraves,Critical,Closed,Fixed,30/May/12 14:41,08/Jun/13 01:46
Bug,MAPREDUCE-4297,12558644,Usersmap file in gridmix should not fail on empty lines,An empty line (e.g. at the end of the file) in the usersmap file will cause gridmix to fail. Empty lines should be silently ignored.,raviprak,raviprak,Major,Closed,Fixed,30/May/12 17:23,11/Oct/12 17:48
Bug,MAPREDUCE-4299,12558861,Terasort hangs with MR2 FifoScheduler,"What happens is that the number of reducers ramp up until they occupy all of the job's containers, at which point the maps no longer make any progress and the job hangs.

When the same job is run with the CapacityScheduler it succeeds, so this looks like a FifoScheduler bug.",tomwhite,tomwhite,Major,Closed,Fixed,31/May/12 20:41,11/Oct/12 17:48
Bug,MAPREDUCE-4300,12558862,OOM in AM can turn it into a zombie.,"It looks like 4 threads in the AM died with OOM but not the one pinging the RM.

stderr for this AM
{noformat}
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
May 30, 2012 4:49:55 AM com.google.inject.servlet.InternalServletModule$BackwardsCompatibleServletContextProvider get
WARNING: You are attempting to use a deprecated API (specifically, attempting to @Inject ServletContext inside an eagerly created singleton. While we allow this for backwards compatibility, be warned that this MAY have unexpected behavior if you have more than one injector (with ServletModule) running in the same JVM. Please consult the Guice documentation at http://code.google.com/p/google-guice/wiki/Servlets for more information.
May 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
INFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver as a provider class
May 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
May 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
INFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices as a root resource class
May 30, 2012 4:49:55 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
INFO: Initiating Jersey application, version 'Jersey: 1.8 06/24/2011 12:17 PM'
May 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
INFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
May 30, 2012 4:49:56 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
May 30, 2012 4:49:56 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
INFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices to GuiceManagedComponentProvider with the scope ""PerRequest""
Exception in thread ""ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308"" java.lang.OutOfMemoryError: Java heap space
	at com.google.protobuf.CodedInputStream.(CodedInputStream.java:538)
	at com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)
	at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)
	at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)
	at org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)
Exception in thread ""DefaultSpeculator background processing"" java.lang.OutOfMemoryError: Java heap space
	at java.util.HashMap.resize(HashMap.java:462)
	at java.util.HashMap.addEntry(HashMap.java:755)
	at java.util.HashMap.put(HashMap.java:385)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)
	at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)
	at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)
	at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)
	at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)
	at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)
	at java.lang.Thread.run(Thread.java:619)
Exception in thread ""Timer for 'MRAppMaster' metrics system"" java.lang.OutOfMemoryError: Java heap space
Exception in thread ""Socket Reader #4 for port 50500"" java.lang.OutOfMemoryError: Java heap space
{noformat}",revans2,revans2,Major,Closed,Fixed,31/May/12 20:44,11/Oct/12 17:48
Bug,MAPREDUCE-4302,12558972,NM goes down if error encountered during log aggregation,"When a container launch request is sent to the NM, if _any_ exception occurs during the init of log aggregation then the NM goes down.  The problem can be induced by situations including, but certainly not limited to: transient rpc connection issues, missing tokens, expired tokens, permissions, full/quota exceeded dfs, etc.  The problem may occur with and without security enabled.

The ramification is an entire cluster can be rather easily brought down either maliciously, accidentally, or via a submission bug.",daryn,daryn,Critical,Closed,Fixed,01/Jun/12 16:48,10/Mar/15 04:30
Bug,MAPREDUCE-4306,12559037,Problem running Distributed Shell applications as a user other than the one started the daemons,"Using the tarball, if you start the yarn daemons using one user and then switch to a different user. You can successfully run MR jobs, but DS jobs fail to run. Only able to run DS jobs using the user who started the daemons.",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,02/Jun/12 06:33,11/Oct/12 17:48
Bug,MAPREDUCE-4307,12559043,TeraInputFormat calls FileSystem.getDefaultBlockSize() without a Path - Failure when using ViewFileSystem,ViewFileSystem.getDefaultBlockSize() throws NotInMountpointException (see HADOOP-8014). I'll upload a patch momentarily.,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,02/Jun/12 11:03,11/Oct/12 17:48
Bug,MAPREDUCE-4311,12559238,Capacity scheduler.xml does not accept decimal values for capacity and maximum-capacity settings,"if capacity scheduler capacity or max capacity set with decimal it errors:

- Error starting ResourceManager

java.lang.NumberFormatException: For input string: ""10.5""
        at
java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Integer.parseInt(Integer.java:458)
        at java.lang.Integer.parseInt(Integer.java:499)
        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:713)
        at
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getCapacity(CapacitySchedulerConfiguration.java:147)
        at
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:147)
        at
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:297)
        at

0.20 used to take decimal and this could be an issue on large clusters that would have queues with small allocations.",kasha,tgraves,Major,Closed,Fixed,04/Jun/12 18:08,03/Nov/14 18:05
Bug,MAPREDUCE-4313,12559307,TestTokenCache doesn't compile due TokenCache.getDelegationToken compilation error,"Saw this on the trunk Jenkins job:

{noformat}
compile-mapred-test:
    [mkdir] Created dir: /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/hadoop-mapreduce-project/build/test/mapred/classes
    [mkdir] Created dir: /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/hadoop-mapreduce-project/build/test/mapred/testjar
    [mkdir] Created dir: /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/hadoop-mapreduce-project/build/test/mapred/testshell
    [javac] Compiling 95 source files to /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/hadoop-mapreduce-project/build/test/mapred/classes
    [javac] /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCache.java:292: incompatible types
    [javac] found   : org.apache.hadoop.security.token.Token<capture#315 of ?>
    [javac] required: org.apache.hadoop.security.token.Token<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier>
    [javac]     Token<DelegationTokenIdentifier> nnt = TokenCache.getDelegationToken(
    [javac]                                                                         ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error
{noformat}",revans2,eli2,Blocker,Closed,Fixed,05/Jun/12 01:24,04/Sep/14 01:00
Bug,MAPREDUCE-4314,12559334,Synchronization in JvmManager for 0.22 branch,Changes to JvmManager due to MR-2178 for branch 0.22.,benoyantony,shv,Major,Resolved,Fixed,05/Jun/12 07:24,05/Jun/12 13:53
Bug,MAPREDUCE-4315,12559336,jobhistory.jsp throws 500 when a .txt file is found in /done,"if a .txt file located in /done the parser throws an 500 error.
Trace:
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.mapred.jobhistory_jsp$2.compare(jobhistory_jsp.java:295)
        at org.apache.hadoop.mapred.jobhistory_jsp$2.compare(jobhistory_jsp.java:279)
        at java.util.Arrays.mergeSort(Arrays.java:1270)
        at java.util.Arrays.mergeSort(Arrays.java:1282)
        at java.util.Arrays.mergeSort(Arrays.java:1282)
        at java.util.Arrays.mergeSort(Arrays.java:1282)
        at java.util.Arrays.mergeSort(Arrays.java:1281)
        at java.util.Arrays.mergeSort(Arrays.java:1281)
        at java.util.Arrays.mergeSort(Arrays.java:1281)
        at java.util.Arrays.mergeSort(Arrays.java:1281)
        at java.util.Arrays.mergeSort(Arrays.java:1281)
        at java.util.Arrays.mergeSort(Arrays.java:1282)
        at java.util.Arrays.mergeSort(Arrays.java:1281)
        at java.util.Arrays.sort(Arrays.java:1210)
        at org.apache.hadoop.mapred.jobhistory_jsp._jspService(jobhistory_jsp.java:279)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:864)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

Reproduce:

cd ../done
touch test.txt
reload jobhistory",sandyr,alo.alt,Major,Closed,Fixed,05/Jun/12 08:02,15/May/13 05:15
Bug,MAPREDUCE-4317,12559383,Job view ACL checks are too permissive,"The class that does view-based checks, JSPUtil.JobWithViewAccessCheck, has the following internal member:

{code}private boolean isViewAllowed = true;{code}

Note that its true.

Now, in the method that sets proper view-allowed rights, has:

{code}
if (user != null && job != null && jt.areACLsEnabled()) {
      final UserGroupInformation ugi =
        UserGroupInformation.createRemoteUser(user);
      try {
        ugi.doAs(new PrivilegedExceptionAction<Void>() {
          public Void run() throws IOException, ServletException {

            // checks job view permission
            jt.getACLsManager().checkAccess(job, ugi,
                Operation.VIEW_JOB_DETAILS);
            return null;
          }
        });
      } catch (AccessControlException e) {
        String errMsg = ""User "" + ugi.getShortUserName() +
            "" failed to view "" + jobid + ""!<br><br>"" + e.getMessage() +
            ""<hr><a href=\""jobtracker.jsp\"">Go back to JobTracker</a><br>"";
        JSPUtil.setErrorAndForward(errMsg, request, response);
        myJob.setViewAccess(false);
      } catch (InterruptedException e) {
        String errMsg = "" Interrupted while trying to access "" + jobid +
        ""<hr><a href=\""jobtracker.jsp\"">Go back to JobTracker</a><br>"";
        JSPUtil.setErrorAndForward(errMsg, request, response);
        myJob.setViewAccess(false);
      }
    }
    return myJob;
{code}

In the above snippet, you can notice that if user==null, which can happen if user is not http-authenticated (as its got via request.getRemoteUser()), can lead to the view being visible since the default is true and we didn't toggle the view to false for user == null case.

Ideally the default of the view job ACL must be false, or we need an else clause that sets the view rights to false in case of a failure to find the user ID.",kasha,qwertymaniac,Major,Closed,Fixed,05/Jun/12 13:51,03/Nov/14 18:06
Bug,MAPREDUCE-4318,12559419,TestRecoveryManager should not use raw and deprecated configuration parameters.,"TestRecoveryManager should not use deprecated config keys, and should use constants for the keys where possible.",benoyantony,shv,Major,Resolved,Fixed,05/Jun/12 17:51,08/Jun/12 05:02
Bug,MAPREDUCE-4320,12559597,gridmix mainClass wrong in pom.xml,"when trying to run gridmix its actually trying to run org.apache.hadoop.tools.HadoopArchives.

the pom.xml needs to be fixed to have correct mainClass: org.apache.hadoop.mapred.gridmix.Gridmix",tgraves,tgraves,Major,Closed,Fixed,06/Jun/12 20:59,11/Oct/12 17:48
Bug,MAPREDUCE-4321,12559601,DefaultTaskController fails to launch tasks on Windows,"DefaultTaskController#launchTask tries to run the child JVM task with the following command line:
{code}cmd.exe /c /c:/some/path.../taskjvm.cmd{code}
And this fails because the given path is prefixed with a forward slash. This also causes a number of tests to fail:

org.apache.hadoop.conf.TestNoDefaultsJobConf
org.apache.hadoop.fs.TestCopyFiles
org.apache.hadoop.mapred.TestBadRecords
org.apache.hadoop.mapred.TestClusterMRNotification
org.apache.hadoop.mapred.TestCompressedEmptyMapOutputs
org.apache.hadoop.mapred.TestControlledMapReduceJob
org.apache.hadoop.mapred.TestCustomOutputCommitter
org.apache.hadoop.mapred.TestEmptyJob
org.apache.hadoop.mapred.TestFileOutputFormat
org.apache.hadoop.mapred.TestIsolationRunner
org.apache.hadoop.mapred.TestJavaSerialization
org.apache.hadoop.mapred.TestJobCleanup
org.apache.hadoop.mapred.TestJobCounters
org.apache.hadoop.mapred.TestJobHistoryServer
org.apache.hadoop.mapred.TestJobInProgressListener
org.apache.hadoop.mapred.TestJobKillAndFail
org.apache.hadoop.mapred.TestJobName
...",ivanmi,ivanmi,Major,Resolved,Fixed,06/Jun/12 21:15,11/Jun/12 19:04
Bug,MAPREDUCE-4322,12559604,Fix command-line length abort issues on Windows,"When a task is started on the tasktracker, it creates a small batch file to invoke java and runs that batch.  Within the batch file, the invocation of Java currently has -classpath ${CLASSPATH} inline to the command.  That line often exceeds 8000 characters.  This is ok for most linux distributions because the line limit env variable is often set much higher than this.  However, for Windows this cause cmd to abort execution.  This surfaces in Hadoop as an unknown failure mode for the task.

I think the easiest and most natural way to fix this is to push the -classpath option into a config file to take the longest variable part of the line and put it somewhere that scales better.",ivanmi,jgordon,Major,Resolved,Fixed,06/Jun/12 21:42,05/Jul/12 07:03
Bug,MAPREDUCE-4336,12560273,Distributed Shell fails when used with the CapacityScheduler,DistributedShell attempts to get queue info without providing a queue name - which ends up in an NPE.,ahmed.radwan,sseth,Major,Closed,Fixed,12/Jun/12 00:49,11/Oct/12 17:48
Bug,MAPREDUCE-4341,12560647,add types to capacity scheduler properties documentation,"MAPREDUCE-4311 is changing capacity/max capacity configuration to be floats. We should document that in the capacity scheduler properties docs (http://hadoop.apache.org/common/docs/r0.23.1/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Configuration).

",kasha,tgraves,Major,Closed,Fixed,14/Jun/12 14:28,03/Nov/14 18:05
Bug,MAPREDUCE-4342,12560840,Distributed Cache gives inconsistent result if cache files get deleted from task tracker ,,mayank_bansal,mayank_bansal,Major,Closed,Fixed,15/Jun/12 22:32,10/Mar/15 04:30
Bug,MAPREDUCE-4356,12595208,Provide access to ParsedTask.obtainTaskAttempts(),Change the access modifier of obtainTaskAttempts() in ParsedTask.java from default to public sothat it is accessible for everyone.,ravidotg,ravidotg,Major,Closed,Fixed,20/Jun/12 05:20,03/Sep/14 22:45
Bug,MAPREDUCE-4359,12595455,Potential deadlock in Counters,"jcarder identified this deadlock in branch-1 (though it may also be present in trunk):
- Counters.size() is synchronized and locks Counters before Group
- Counters.Group.getCounterForName() is synchronized and calls through to Counters.size()

This creates a potential cycle which could cause a deadlock (though probably quite rare in practice)",tomwhite,tlipcon,Major,Closed,Fixed,21/Jun/12 17:27,15/May/13 05:16
Bug,MAPREDUCE-4360,12595493,Capacity Scheduler Hierarchical leaf queue does not honor the max capacity of container queue,,mayank_bansal,mayank_bansal,Major,Resolved,Fixed,21/Jun/12 21:43,30/Jun/12 01:51
Bug,MAPREDUCE-4361,12595502,Fix detailed metrics for protobuf-based RPC on 0.23,RPC detailed metrics for any protobuf-based RPC ports are always zero.  ProtoOverHadoopRpcEngine needs the same detailed metric logic as in WritableRpcEngine.  This is effectively the same change as in HADOOP-8085 except tailored for branch-0.23 which didn't take the full protobuf branch changes that went into branch-2 and trunk.,jlowe,jlowe,Major,Closed,Fixed,21/Jun/12 22:48,11/Oct/12 17:48
Bug,MAPREDUCE-4366,12595798,mapred metrics shows negative count of waiting maps and reduces,"Negative waiting_maps and waiting_reduces count is observed in the mapred metrics.  MAPREDUCE-1238 partially fixed this but it appears there is still issues as we are seeing it, but not as bad.
",sandyr,tgraves,Major,Resolved,Fixed,25/Jun/12 15:33,27/Jul/13 03:44
Bug,MAPREDUCE-4368,12595849,TaskRunner fails to start jars when the java.library.path contains a quoted path with embedded spaces,"TaskRunner splits arguments by space before it adds them back to the vargs list, so it loses all context of quote escaped strings with embedded spaces.  This gets fixed up later by wrapping all arguments with "" -- so you get something like java ""-D<opt>=<value>"".  This is problematic for paths with embedded spaces, where we end up creating ""-D<opt>=<first part"" ""last part>"".  To java, the jar being run is last part.  So with the environment above, you will see ""ClassNoDefFoundError: memorable"" and the jar will fail to start.  In this particular case, we know that java.libarary.path contains paths and the tests often use %PATH% to seed this, so the fix is to remove embedded quotes in listed path elements because we know the aggregate will be quoted when the JVM is started.",jgordon,jgordon,Major,Resolved,Fixed,25/Jun/12 23:53,05/Jul/12 07:15
Bug,MAPREDUCE-4369,12595852,Fix streaming job failures with WindowsResourceCalculatorPlugin,"Some streaming jobs use local mode job runs that do not start tasks trackers. In these cases, the jvm context is not setup and hence local mode execution causes the code to crash.
Fix is to not not use ResourceCalculatorPlugin in such cases or make the local job run creating dummy jvm contexts. Choosing the first option because thats the current implicit behavior in Linux. The ProcfsBasedProcessTree (used inside the LinuxResourceCalculatorPlugin) does no real work when the process pid is not setup correctly. This is what happens when local job mode runs.
",bikassaha,bikassaha,Major,Resolved,Fixed,26/Jun/12 00:49,05/Jul/12 07:05
Bug,MAPREDUCE-4372,12595891,Deadlock in Resource Manager between SchedulerEventDispatcher.EventProcessor and Shutdown hook manager,Please find the attached resource manager thread dump for the issue.,devaraj,devaraj,Major,Closed,Fixed,26/Jun/12 11:00,12/May/16 18:22
Bug,MAPREDUCE-4374,12595956,Fix child task environment variable config and add support for Windows,"In HADOOP-2838, a new feature was introduced to set environment variables via the Hadoop config 'mapred.child.env' for child tasks. There are some further fixes and improvements around this feature, e.g. HADOOP-5981 were a bug fix; MAPREDUCE-478 broke the config into 'mapred.map.child.env' and 'mapred.reduce.child.env'.  However the current implementation is still not complete. It does not match its documentation or original intend as I believe. Also, by using ‘:’ (colon) and ‘;’ (semicolon) in the configuration syntax, we will have problems using them on Windows because ‘:’ appears very often in Windows path as in “C:\”, and environment variables are used very often to hold path names. The Jira is created to fix the problem and provide support on Windows.",chuanliu,chuanliu,Minor,Closed,Fixed,26/Jun/12 18:53,12/May/16 18:22
Bug,MAPREDUCE-4376,12595988,TestClusterMRNotification times out,"The TestClusterMRNotification test is often timing out.  git bisect tests narrowed it down to MAPREDUCE-3921, as the test consistently passes before that change and times out most of the time after picking up that change.",kihwal,jlowe,Major,Closed,Fixed,26/Jun/12 21:46,18/May/15 17:42
Bug,MAPREDUCE-4379,12596091,Node Manager throws java.lang.OutOfMemoryError: Java heap space due to org.apache.hadoop.fs.LocalDirAllocator.contexts,"{code:xml}
Exception in thread ""Container Monitor"" java.lang.OutOfMemoryError: Java heap space
	at java.io.BufferedReader.<init>(BufferedReader.java:80)
	at java.io.BufferedReader.<init>(BufferedReader.java:91)
	at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:410)
	at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:171)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:389)
	Exception in thread ""LocalizerRunner for container_1340690914008_10890_01_000003"" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOfRange(Arrays.java:3209)
	at java.lang.String.<init>(String.java:215)
	at com.sun.org.apache.xerces.internal.xni.XMLString.toString(XMLString.java:185)
	at com.sun.org.apache.xerces.internal.parsers.AbstractDOMParser.characters(AbstractDOMParser.java:1188)
	at com.sun.org.apache.xerces.internal.xinclude.XIncludeHandler.characters(XIncludeHandler.java:1084)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:464)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:235)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:180)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1738)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1689)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1635)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:722)
	at org.apache.hadoop.conf.Configuration.setStrings(Configuration.java:1300)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.initDirs(ContainerLocalizer.java:375)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.runLocalization(ContainerLocalizer.java:127)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.startLocalizer(DefaultContainerExecutor.java:103)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:862)
{code}",devaraj,devaraj,Blocker,Closed,Fixed,27/Jun/12 13:54,12/May/16 18:24
Bug,MAPREDUCE-4380,12596207,Empty Userlogs directory is getting created under logs directory,Empty Userlogs directory is getting created under logs directory.,devaraj,devaraj,Minor,Closed,Fixed,28/Jun/12 04:16,12/May/16 18:23
Bug,MAPREDUCE-4383,12596305,HadoopPipes.cc needs to include unistd.h,"Since MAPREDUCE-4267 I've seen ""mvn -Pnative compile"" failing with:
{quote}
     [exec] /usr/bin/c++    -g -Wall -O2 -D_REENTRANT -D_FILE_OFFSET_BITS=64 -I/home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/utils/api -I/home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/api -I/home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src    -o CMakeFiles/hadooppipes.dir/main/native/pipes/impl/HadoopPipes.cc.o -c /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc: In member function ‘std::string HadoopPipes::BinaryProtocol::createDigest(std::string&, std::string&)’:
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc:439:21: warning: value computed is not used [-Wunused-value]
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc: In function ‘void* HadoopPipes::ping(void*)’:
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc:1049:16: error: ‘sleep’ was not declared in this scope
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc:1067:30: error: ‘close’ was not declared in this scope
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc: In function ‘bool HadoopPipes::runTask(const HadoopPipes::Factory&)’:
     [exec] /home/adi/w/apache-hadoop-trunk/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc:1162:28: error: ‘close’ was not declared in this scope
     [exec] make[2]: *** [CMakeFiles/hadooppipes.dir/main/native/pipes/impl/HadoopPipes.cc.o] Error 1
{quote}

I believe the failure is new simply because I wasn't compiling pipes before.

The fix is pretty simple, just include unistd.h in HadoopPipes.cc.

My environment is debian unstable, amd64, g++ 4.7.0-6, openjdk-6-jdk 6b24-1.11.1-6.",adi2,adi2,Minor,Closed,Fixed,28/Jun/12 19:33,11/Mar/15 20:27
Bug,MAPREDUCE-4384,12596317,Race conditions in IndexCache,"TestIndexCache is intermittently failing due to a race condition. Up on inspection of IndexCache implementation, more potential issues have been discovered.",kihwal,kihwal,Major,Closed,Fixed,28/Jun/12 20:01,11/Oct/12 17:48
Bug,MAPREDUCE-4385,12596320,FairScheduler.maxTasksToAssign() should check for fairscheduler.assignmultiple.maps < TaskTracker.availableSlots,"FairScheduler.maxTasksToAssign() can potentially return a value greater than the available slots. Currently, we rely on canAssignMaps()/canAssignReduces() to reject such requests.

These additional calls can be avoided by check against the available slots in maxTasksToAssign().",kasha,kasha,Major,Closed,Fixed,28/Jun/12 20:36,03/Nov/14 18:33
Bug,MAPREDUCE-4387,12596416,RM gets fatal error and exits during TestRM,"It doesn't happen on my desktop, but it happens frequently during the builds with clover enabled. Surefire will report it as fork failure.

",kihwal,kihwal,Major,Closed,Fixed,29/Jun/12 16:03,11/Oct/12 17:48
Bug,MAPREDUCE-4392,12597376,Counters.makeCompactString() changed behavior from 0.20,"In 0.20, makeCompactString() returned a comma-separated list, but MAPREDUCE-3697 changed it to be equivalent to makeEscapedCompactString().  Users moving from 0.20 to 0.23 are expecting the original behavior from makeCompactString().",jlowe,jlowe,Major,Closed,Fixed,04/Jul/12 00:19,11/Oct/12 17:48
Bug,MAPREDUCE-4395,12597446,Possible NPE at ClientDistributedCacheManager#determineTimestamps,"{code:title=ClientDistributedCacheManager#determineTimestamps|borderStyle=solid}
URI[] tfiles = DistributedCache.getCacheFiles(job);
{code}

It may be possible that tfiles array contains *null* as it's entry, and subsequently leads to NPE.",kam_iitkgp,kam_iitkgp,Critical,Closed,Fixed,04/Jul/12 12:16,10/Mar/15 04:30
Bug,MAPREDUCE-4396,12597546,Make LocalJobRunner work with private distributed cache,Some LocalJobRunner related unit tests fails if user directory permission and/or umask is too restrictive.,crystal_gaoyu,vicaya,Minor,Closed,Fixed,05/Jul/12 09:36,06/Mar/13 09:55
Bug,MAPREDUCE-4399,12597557,Fix performance regression in shuffle ,There is a significant (up to 3x) performance regression in shuffle (vs 0.20.2) in the Hadoop 1.x series. Most noticeable with high-end switches.,vicaya,vicaya,Major,Closed,Fixed,05/Jul/12 10:20,17/Oct/12 18:25
Bug,MAPREDUCE-4400,12597559,Fix performance regression for small jobs/workflows,There is a significant performance regression for small jobs/workflows (vs 0.20.2) in the Hadoop 1.x series. Most noticeable with Hive and Pig jobs. PigMix has an average 40% regression against 0.20.2.,vicaya,vicaya,Major,Closed,Fixed,05/Jul/12 10:25,17/Oct/12 18:27
Bug,MAPREDUCE-4402,12597796,TestFileInputFormat fails intermittently,TestFileInputFormat#testLocality is failing intermittently when verifying each file split has two block locations.,jlowe,jlowe,Major,Closed,Fixed,06/Jul/12 16:22,11/Oct/12 17:48
Bug,MAPREDUCE-4404,12597827,Adding Test case for TestMRJobClient to verify the user name,Adding Test case for TestMRJobClient to verify the user name,mayank_bansal,mayank_bansal,Minor,Resolved,Fixed,06/Jul/12 20:40,31/May/13 21:19
Bug,MAPREDUCE-4406,12597849,Users should be able to specify the MiniCluster ResourceManager and JobHistoryServer ports,"There is use-cases where users may need to specify the ports used for the resource manager and history server for the minicluster.

In the current implementation, the MiniCluster sets these addresses regardless of them being already set by the user in the conf.

Users should be able to add these properties to the conf and in such case the MiniCluster will use the specified addresses. If not specified then the current behavior of the MiniCluster for explicitly setting the addresses will be used.

I'll be uploading a patch momentarily.",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,06/Jul/12 23:10,14/Nov/12 00:07
Bug,MAPREDUCE-4407,12597851,Add hadoop-yarn-server-tests-<version>-tests.jar to hadoop dist package,This change basically adds hadoop-yarn-server-tests-<version>-tests.jar to the package. ,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,06/Jul/12 23:17,11/Oct/12 17:48
Bug,MAPREDUCE-4413,12598058,MR lib dir contains jdiff (which is gpl),"A tarball built from trunk contains the following:

./share/hadoop/mapreduce/lib/jdiff-1.0.9.jar

jdiff is gplv2, we need to exclude it from the build artifact.",nemon,eli,Critical,Resolved,Fixed,09/Jul/12 18:34,20/May/16 03:22
Bug,MAPREDUCE-4416,12598100,Some tests fail if Clover is enabled,"There are number of tests running under hadoop-mapreduce-client-jobclient that fail if Clover is enabled. Whenever a job is launched, AM doesn't start because it can't locate the clover jar file.

I thought MAPREDUCE-4253 had something to do with this, but I can reproduce the issue on an older revision. Although unrelated, MAPREDUCE-4253 does have a problem and it has been reported to the jira.",kihwal,kihwal,Critical,Closed,Fixed,09/Jul/12 21:52,12/May/16 18:24
Bug,MAPREDUCE-4419,12598154,./mapred queue -info <queuename> -showJobs displays all the jobs irrespective of <queuename> ,"./mapred queue -info <queuename> -showJobs shows all the jobs irrespective of <queuename>

In Queue name field all the jobs are showing as default queue but they are submitted to the configured queue(see screenshots attached).",devaraj,nishan,Major,Closed,Fixed,10/Jul/12 10:02,12/May/16 18:23
Bug,MAPREDUCE-4420,12598157,./mapred queue -info <queuename> -showJobs displays containers and memory as zero always,./mapred queue -info <queuename> -showJobs displays containers and memory as zero always.,devaraj,nishan,Major,Resolved,Fixed,10/Jul/12 10:27,23/Jul/12 09:01
Bug,MAPREDUCE-4423,12598200,Potential infinite fetching of map output,"Inside Fetcher.java there are a few cases where an error can happen and the corresponding map task is not marked as a fetch failure.  One of these is if the Shuffle server returns a malformed result.

MAPREDUCE-3992 makes this case a lot less common, but it is still possible.  IF the shuffle handler always returns a malformed result, but a OK response the Fetcher will never stop trying to fetch those results. ",revans2,revans2,Critical,Closed,Fixed,10/Jul/12 15:26,12/May/16 18:24
Bug,MAPREDUCE-4425,12598250,Speculation + Fetch failures can lead to a hung job,"After a task goes to SUCCEEDED, FAILED/KILLED attempts are ignored.
1. attemp1 starts
2. speculative attempt starts
3. attempt 1 completes - Task moves to SUCCEEDED state
4. speculative attempt is KILLED
5. T_ATTEMPT_KILLED is ignored.
6. attemp1 1 fails with TOO_MANY_FETCH_FAILURES
The job will effectively hang, since a new task attempt isn't started.",jlowe,sseth,Critical,Closed,Fixed,10/Jul/12 19:08,03/Sep/14 23:17
Bug,MAPREDUCE-4432,12598547,Confusing warning message when GenericOptionsParser is not used,"The warning that is issued in JobSubmitter -- ""Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same."" -- is confusing and (probably) grammatically incorrect.

This can be improved by having an updated warning message which gives clearer directions on what can be improved in the application to avoid the warning in the future.",,gabriel.reid,Trivial,Closed,Fixed,12/Jul/12 13:24,11/Oct/12 17:48
Bug,MAPREDUCE-4434,12598610,Backport MR-2779 (JobSplitWriter.java can't handle large job.split file) to branch-1,,kasha,kasha,Major,Closed,Fixed,12/Jul/12 20:15,03/Nov/14 18:33
Bug,MAPREDUCE-4437,12598700,Race in MR ApplicationMaster can cause reducers to never be scheduled,If the MR AM is notified of container completion by the RM before the AM receives notification of the container cleanup from the NM then it can fail to schedule reducers indefinitely.  Logs showing the issue to follow.,jlowe,jlowe,Critical,Closed,Fixed,13/Jul/12 14:25,11/Oct/12 17:48
Bug,MAPREDUCE-4440,12598722,Change SchedulerApp & SchedulerNode to be a minimal interface ,Schedulers should manage their own implementations of SchedulerApp and SchedulerNode.,acmurthy,acmurthy,Major,Closed,Fixed,13/Jul/12 17:15,02/May/13 02:30
Bug,MAPREDUCE-4441,12598750,Fix build issue caused by MR-3451,TestFSSchedulerApp is in the wrong package and missing some imports.,kasha,kasha,Blocker,Closed,Fixed,13/Jul/12 20:02,03/Nov/14 18:33
Bug,MAPREDUCE-4444,12598761,nodemanager fails to start when one of the local-dirs is bad,,jlowe,nroberts,Blocker,Closed,Fixed,13/Jul/12 21:44,12/May/16 18:23
Bug,MAPREDUCE-4447,12598999,Remove aop from cruft from the ant build ,The nop aop build dirs and remaining reference (MR ant build) should be removed. ,eli,eli,Major,Closed,Fixed,16/Jul/12 17:04,11/Oct/12 17:48
Bug,MAPREDUCE-4448,12599023,Nodemanager crashes upon application cleanup if aggregation failed to start,"When log aggregation is enabled, the nodemanager can crash if log aggregation for an application failed to start.",jlowe,jlowe,Critical,Closed,Fixed,16/Jul/12 19:12,11/Oct/12 17:48
Bug,MAPREDUCE-4449,12599054,Incorrect MR_HISTORY_STORAGE property name in JHAdminConfig,Just noticed that MR_HISTORY_STORAGE has an extra period in the its name (i.e. the name is mapreduce.jobhistory..store.class).,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,17/Jul/12 00:33,11/Oct/12 17:48
Bug,MAPREDUCE-4451,12599098,fairscheduler fail to init job with kerberos authentication configured,"Using FairScheduler in Hadoop 1.0.3 with kerberos authentication configured. Job initialization fails:

{code}
2012-07-17 15:15:09,220 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:
java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)
        at org.apache.hadoop.ipc.Client.call(Client.java:1097)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
        at $Proxy7.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
        at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)
        at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)
        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)
        at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)
        at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:543)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:488)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:590)
        at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:187)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1228)
        at org.apache.hadoop.ipc.Client.call(Client.java:1072)
        ... 20 more
Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)
        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)
        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:385)
        at org.apache.hadoop.ipc.Client$Connection.access$1200(Client.java:187)
        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:583)
        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:580)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:579)
        ... 23 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)
        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)
        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)
        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)
        ... 32 more

{code}

When a job is submitted, fairscheduler calls JobTracker.initJob, which calls JobInProgress.generateAndStoreTokens to write security keys to hdfs. However, the operation is involved in the server side rpc call path, using UGI created by UserGroupInformation.createRemoteUser in rpc server, which have no tgt. This should be done with UGI used by JobTracker.",erik.fang,erik.fang,Major,Closed,Fixed,17/Jul/12 10:34,15/May/13 05:16
Bug,MAPREDUCE-4456,12599344,LocalDistributedCacheManager can get an ArrayIndexOutOfBounds when creating symlinks,"{noformat}
java.lang.ArrayIndexOutOfBoundsException: 1
        at
org.apache.hadoop.mapred.LocalDistributedCacheManager.setup(LocalDistributedCacheManager.java:194)
        at
org.apache.hadoop.mapred.LocalJobRunner$Job.<init>(LocalJobRunner.java:154)
        at
org.apache.hadoop.mapred.LocalJobRunner.submitJob(LocalJobRunner.java:620)
        at
org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:385)
        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)
        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1212)
        at
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:336)
        at
org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.run(JobControl.java:233)
        at java.lang.Thread.run(Thread.java:619)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:260)
{noformat}",revans2,revans2,Major,Closed,Fixed,18/Jul/12 21:08,12/May/16 18:23
Bug,MAPREDUCE-4457,12599348,mr job invalid transition TA_TOO_MANY_FETCH_FAILURE at FAILED,"we saw a job go into the ERROR state from an invalid state transition.

3,600 INFO [AsyncDispatcher event handler]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:
attempt_1342238829791_2501_m_007743_0 TaskAttempt Transitioned from SUCCEEDED
to FAILED
2012-07-16 08:49:53,600 INFO [AsyncDispatcher event handler]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:
attempt_1342238829791_2501_m_008850_0 TaskAttempt Transitioned from SUCCEEDED
to FAILED
2012-07-16 08:49:53,600 INFO [AsyncDispatcher event handler]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:
attempt_1342238829791_2501_m_017344_1000 TaskAttempt Transitioned from RUNNING
to SUCCESS_CONTAINER_CLEANUP
2012-07-16 08:49:53,601 ERROR [AsyncDispatcher event handler]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this
event at current state for attempt_1342238829791_2501_m_000027_0
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event:
TA_TOO_MANY_FETCH_FAILURE at FAILED
    at
org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
    at
org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
    at
org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
    at
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)
    at
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)
    at
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)
    at
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)
    at
org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
    at
org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)
    at
org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)
    at
org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
    at java.lang.Thread.run(Thread.java:619)
2012-07-16 08:49:53,601 INFO [AsyncDispatcher event handler]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:
attempt_1342238829791_2501_m_029091_1000 TaskAttempt Transitioned from RUNNING
to SUCCESS_CONTAINER_CLEANUP
2012-07-16 08:49:53,601 INFO [IPC Server handler 17 on 47153]
org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from
attempt_1342238829791_2501_r_000461_1000


It looks like we possibly got 2 TA_TOO_MANY_FETCH_FAILURE events. The first one moved it to FAILED and then the second one failed because no valid transition.",revans2,tgraves,Critical,Closed,Fixed,18/Jul/12 21:38,20/Oct/13 21:21
Bug,MAPREDUCE-4463,12599491,JobTracker recovery fails with HDFS permission issue,Recovery fails when the job user is different to the JT owner (i.e. on anything bigger than a pseudo-distributed cluster).,tomwhite,tomwhite,Blocker,Closed,Fixed,19/Jul/12 16:15,15/May/13 05:15
Bug,MAPREDUCE-4465,12599564,Update description of yarn.nodemanager.address property,"The description for the property 'yarn.nodemanager.address' says 'address of node manager IPC.', which is not clear enough. It should be changed to something as 'The address of the container manager in the NM'.",bowang,bowang,Trivial,Closed,Fixed,20/Jul/12 01:20,11/Oct/12 17:48
Bug,MAPREDUCE-4467,12599654,IndexCache failures due to missing synchronization,"TestMRJobs.testSleepJob fails randomly due to synchronization error in IndexCache:

{code}
2012-07-20 19:32:34,627 ERROR [New I/O server worker #2-1] mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(528)) - Shuffle error: 
java.lang.IllegalMonitorStateException
	at java.lang.Object.wait(Native Method)
	at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)
	at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)
	at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)
	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

A related issue is MAPREDUCE-4384. The change introduced there removed ""synchronized"" keyword and hence ""info.wait()"" call fails. Tbis needs to be wrapped into a ""synchronized"" block.",kihwal,aklochkov,Critical,Closed,Fixed,20/Jul/12 18:20,11/Oct/12 17:48
Bug,MAPREDUCE-4470,12599906,Fix TestCombineFileInputFormat.testForEmptyFile,"TestCombineFileInputFormat.testForEmptyFile started failing after HADOOP-8599. 

It expects one split on an empty input file, but with HADOOP-8599 it gets zero. The new behavior seems correct, but is it breaking anything else?",ikatsov,kihwal,Major,Closed,Fixed,23/Jul/12 21:55,02/Feb/13 12:40
Bug,MAPREDUCE-4478,12600119,TaskTracker's heartbeat is out of control,,liangly,liangly,Major,Closed,Fixed,25/Jul/12 05:30,06/Mar/13 09:55
Bug,MAPREDUCE-4479,12600176,Fix parameter order in assertEquals() in TestCombineInputFileFormat.java,,masokan,masokan,Major,Closed,Fixed,25/Jul/12 15:30,12/May/16 18:24
Bug,MAPREDUCE-4483,12600213,2.0 build does not work ,Seems like hadoop-yarn-applications-unmanaged-am-launcher/pom.xml is pointing to the wrong <parent>,johnvijoe,johnvijoe,Major,Closed,Fixed,25/Jul/12 19:54,04/Sep/14 01:00
Bug,MAPREDUCE-4484,12600234,Incorrect IS_MINI_YARN_CLUSTER property name in YarnConfiguration,"Noticed that the IS_MINI_YARN_CLUSTER property name in YarnConfiguration ended up having an extra ""."" after appending to YARN_PREFIX.",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,25/Jul/12 23:15,11/Oct/12 17:48
Bug,MAPREDUCE-4490,12600413,JVM reuse is incompatible with LinuxTaskController (and therefore incompatible with Security),"When using LinuxTaskController, JVM reuse (mapred.job.reuse.jvm.num.tasks > 1) with more map tasks in a job than there are map slots in the cluster will result in immediate task failures for the second task in each JVM (and then the JVM exits). We have investigated this bug and the root cause is as follows. When using LinuxTaskController, the userlog directory for a task attempt (../userlogs/job/task-attempt) is created only on the first invocation (when the JVM is launched) because userlogs directories are created by the task-controller binary which only runs *once* per JVM. Therefore, attempting to create log.index is guaranteed to fail with ENOENT leading to immediate task failure and child JVM exit.

{quote}
2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0
2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child
ENOENT: No such file or directory
        at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)
        at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)
        at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)
        at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)
        at org.apache.hadoop.mapred.Child.main(Child.java:229)
{quote}

The above error occurs in a JVM which runs tasks 6 and 27.  Task6 goes smoothly. Then Task27 starts. The directory /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_0000027_0 is never created so when mapred.Child tries to write the log.index file for Task27, it fails with ENOENT because the attempt_201207241401_0013_m_0000027_0 directory does not exist. Therefore, the second task in each JVM is guaranteed to fail (and then the JVM exits) every time when using LinuxTaskController. Note that this problem does not occur when using the DefaultTaskController because the userlogs directories are created for each task (not just for each JVM as with LinuxTaskController).

For each task, the TaskRunner calls the TaskController's createLogDir method before attempting to write out an index file.

* DefaultTaskController#createLogDir: creates log directory for each task
* LinuxTaskController#createLogDir: does nothing
** task-controller binary creates log directory [create_attempt_directories] (but only for the first task)

Possible Solution: add a new command to task-controller *initialize task* to create attempt directories.  Call that command, with ShellCommandExecutor, in the LinuxTaskController#createLogDir method




",sam liu,datskos,Critical,Resolved,Fixed,27/Jul/12 01:22,13/Jun/14 21:38
Bug,MAPREDUCE-4492,12600471,Configuring total queue capacity between 100.5 and 99.5 at perticular level is sucessfull,"Scenario:
1.Configure a,b queues with capacities 40.0 and 60.5 respectively under root queue
2.Start process
Observe that process is started sucessfully with configured queue capacity though the total capacity is 100.5(40.0+60.5)",mayank_bansal,nishan,Minor,Closed,Fixed,27/Jul/12 14:24,11/Oct/12 17:48
Bug,MAPREDUCE-4493,12600512,Distibuted Cache Compatability Issues,"The distributed cache does not work like it does in 1.0.

mapreduce.job.cache.symlink.create is completely ignored and symlinks are always created no matter what.  Files and archives without a fragment will also have symlinks created.

If two cache archives or cache files happen to have the same name, or same symlink fragment only the last one in the list is localized.

The localCacheArchives and LocalCacheFiles are not set correctly when these duplicates happen causing off by one or more errors for anyone trying to use them.

The reality is that use of symlinking is so common currently that these incompatibilities are not that likely to show up, but we still need to fix them.",revans2,revans2,Critical,Closed,Fixed,27/Jul/12 20:56,23/Apr/18 15:24
Bug,MAPREDUCE-4494,12600573,"TestFifoScheduler failing with Metrics source QueueMetrics,q0=default already exists!","TestFifoScheduler is failing:

{code}
Running org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler
Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.614 sec <<< FAILURE!

Results :

Tests in error: 
  test(org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler): Metrics source QueueMetrics,q0=default already exists!
{code}",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,28/Jul/12 21:06,11/Oct/12 17:48
Bug,MAPREDUCE-4496,12600732,AM logs link is missing user name,The link to the ApplicationMaster's logs on the MRAppMaster's web page is missing the user name.,jlowe,jlowe,Major,Closed,Fixed,30/Jul/12 21:40,11/Oct/12 17:48
Bug,MAPREDUCE-4498,12600881,Remove hsqldb jar from Hadoop runtime classpath,"The hsqldb jar is included in hadoop for the DBCountPageView example only.  Currently the example is using hsqldb version 2.x; however, 2.x is incompatible with 1.8.x -- having this jar in the hadoop class path conflicts with dependent projects like Oozie, Hive, and Pig which still use 1.8.x.  As there are no features hsqldb 2.x that are used by the example, we should remove it from Hadoop's runtime classpath.",rkanter,rkanter,Critical,Closed,Fixed,31/Jul/12 16:27,11/Oct/12 17:48
Bug,MAPREDUCE-4503,12601060,Should throw InvalidJobConfException if duplicates found in cacheArchives or cacheFiles,"in 1.0 if a file was both in a jobs cache archives and cache files, and InvalidJobConfException was thrown.  We should replicate this behavior on mrv2.  We should also extend it so that if a cache archive or cache file is not going to be downloaded at all because of conflicts in the names of the symlinks a similar exception is thrown.",revans2,revans2,Major,Closed,Fixed,01/Aug/12 13:07,03/Jul/18 19:45
Bug,MAPREDUCE-4504,12601072,SortValidator writes to wrong directory,"SortValidator tries to write to jobConf.get(""hadoop.tmp.dir"", ""/tmp""), but it is not intended to be an HDFS directory. it should just be /tmp.",revans2,revans2,Major,Closed,Fixed,01/Aug/12 14:31,12/May/16 18:22
Bug,MAPREDUCE-4510,12601304,"Avoid logging ""Cannot run program getconf"" on Windows",ProcfsbasesProcessTree logs error messages when it cannot run getconf to determine system attributes on linux. this causes a lot of log spew on windows. need to fix this code because linux is not longer the only OS supported for hadoop.,bikassaha,bikassaha,Major,Resolved,Fixed,03/Aug/12 05:27,30/Aug/12 02:28
Bug,MAPREDUCE-4521,12601632,mapreduce.user.classpath.first incompatibility with 0.20/1.x,"In Hadoop 0.20 or 1.x, jobs can specify the user's classpath should appear first by setting the property mapreduce.user.classpath.first to true in the job configuration.  However in Hadoop 0.23 or 2.x, this has no effect, as the corresponding property there is mapreduce.job.user.classpath.first.",raviprak,jlowe,Major,Closed,Fixed,06/Aug/12 22:39,03/Sep/14 23:17
Bug,MAPREDUCE-4545,12602647,Job Credentials are not transmitted if security is turned off,"Credentials (secret keys) can be passed to a job via mapreduce.job.credentials.json or  mapreduce.job.credentials.binary .

These credentials get submitted during job submission and are made available to the task processes. 

In HADOOP 1, these credentials get submitted and routed to task processes even if security was off.

In HADOOP 2 , these credentials are transmitted only when the security is turned on.

This should be fixed for two reasons:

1) It is not backward compatible.
2) Credentials should be passed even if security is turned off .",,benoyantony,Major,Resolved,Fixed,09/Aug/12 20:07,14/Aug/12 17:47
Bug,MAPREDUCE-4549,12603244,Distributed cache conflicts breaks backwards compatability,"I recently put in MAPREDUCE-4503 which went a bit too far, and broke backwards compatibility with 1.0 in distribtued cache entries.  instead of changing the behavior of the distributed cache to more closely match 1.0 behavior I want to just change the exception to a warning message informing the users that it will become an error in 2.0",revans2,revans2,Blocker,Closed,Fixed,13/Aug/12 14:28,03/Jul/18 19:45
Bug,MAPREDUCE-4554,12603276,Job Credentials are not transmitted if security is turned off,"Credentials (secret keys) can be passed to a job via mapreduce.job.credentials.json or mapreduce.job.credentials.binary .

These credentials get submitted during job submission and are made available to the task processes.

In HADOOP 1, these credentials get submitted and routed to task processes even if security was off.
In HADOOP 2 , these credentials are transmitted only when the security is turned on.

This should be changed for two reasons:
1) It is not backward compatible. 
2) Credentials should be passed even if security is turned off .
",benoyantony,benoyantony,Major,Closed,Fixed,13/Aug/12 17:00,03/Sep/14 23:17
Bug,MAPREDUCE-4558,12603626,TestJobTrackerSafeMode is failing,"MAPREDUCE-1906 exposed an issue with this unit test. It has 3 TTs running, but has a check for the TT count to reach exactly 2 (which would be reached with a higher heartbeat interval).

The test ends up getting stuck, with the following message repeated multiple times.
{code}
    [junit] 2012-08-15 11:26:46,299 INFO  mapred.TestJobTrackerSafeMode (TestJobTrackerSafeMode.java:checkTrackers(201)) - Waiting for Initialize all Task Trackers
    [junit] 2012-08-15 11:26:47,301 INFO  mapred.TestJobTrackerSafeMode (TestJobTrackerSafeMode.java:checkTrackers(201)) - Waiting for Initialize all Task Trackers
    [junit] 2012-08-15 11:26:48,302 INFO  mapred.TestJobTrackerSafeMode (TestJobTrackerSafeMode.java:checkTrackers(201)) - Waiting for Initialize all Task Trackers
    [junit] 2012-08-15 11:26:49,303 INFO  mapred.TestJobTrackerSafeMode (TestJobTrackerSafeMode.java:checkTrackers(201)) - Waiting for Initialize all Task Trackers
{code}

",sseth,sseth,Major,Closed,Fixed,15/Aug/12 18:38,17/Oct/12 18:27
Bug,MAPREDUCE-4562,12603692,"Support for ""FileSystemCounter"" legacy counter group name for compatibility reasons is creating incorrect counter name","Hi Guys,
I was investigating issue in Sqoop project(http://sqoop.apache.org/). Problem is that we are reporting number of written filesystem bytes back to the user and on Hadoop 0.23/2.x we're always getting 0. I've noticed that there was some refactorization in FileSystem counter related code and found MAPREDUCE-3542 requesting backward compatibility.

Included patch seems to be adding counter ""FileSystemCounter"":

{code:title=hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java:84}
legacyMap.put(""FileSystemCounter"", FileSystemCounter.class.getName());
{code}

But it appears that original name is ""FileSystemCounters"" (Notice the plural ""s"" at the end of name):
{code:title=src/mapred/org/apache/hadoop/mapred/Task.java:91 (0.20.2)}
protected static final String FILESYSTEM_COUNTER_GROUP = ""FileSystemCounters"";
{code}
{code:title=src/mapred/org/apache/hadoop/mapred/Task.java:109 (1.0.3)}
protected static final String FILESYSTEM_COUNTER_GROUP = ""FileSystemCounters"";
{code}

I therefore believe that this counter should be renamed in order to provide backward compatibility. I might fix this discrepancy in Sqoop, but I believe that other projects/users might also be affected and therefore it would be better to fix it in upstream.",jarcec,jarcec,Major,Closed,Fixed,16/Aug/12 06:46,11/Oct/12 17:48
Bug,MAPREDUCE-4564,12603804,Shell timeout mechanism does not work for processes spawned using winutils,"Upon timeout, Shell calls Java process.destroy() to terminate the spawned process. This would destroy the winutils process but not the real process spawned by winutils.
",bikassaha,bikassaha,Major,Resolved,Fixed,16/Aug/12 23:22,10/Sep/12 14:33
Bug,MAPREDUCE-4569,12604235,TestHsWebServicesJobsQuery fails on jdk7,"TestHsWebServicesJobsQuery fails on jdk7 due to the test order no longer being constant.


testJobsQueryStateNone(org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery)  Time elapsed: 0.279 sec  <<< FAILURE!
java.lang.AssertionError: jobs is not null expected:<null> but was:<{""job"":[{""startTime"":1345559717819,""finishTime"":1345560891194,""id"":""job_1345560632472_0002"",""name"":""RandomWriter"",""queue"":""mockqueue"",""user"":""mock"",""state"":""KILL_WAIT"",""mapsTotal"":0,""mapsCompleted"":0,""reducesTotal"":1,""reducesCompleted"":1}]}>
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.failNotEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:126)
        at org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery.testJobsQueryStateNone(TestHsWebServicesJobsQuery.java:226)

It looks like the issues is that the mock jobs just iterate through the JOBSTATE and since other tests run first we just happen to hit the state KILL_WAIT that we don't expect any jobs to be in.  ",tgraves,tgraves,Major,Closed,Fixed,21/Aug/12 15:23,12/May/16 18:24
Bug,MAPREDUCE-4570,12604249,ProcfsBasedProcessTree#constructProcessInfo() prints a warning if procfsDir/<pid>/stat is not found.,"I think a warning is misleading in this case. What is happening here is that the list of all processes in the system is found, and then later the procfsDir/<pid>/stat file for each is opened. This warning is thrown when the process finishes before the stat file is opened, and hence the file is no longer there. This could normally happen, and shouldn't signify a waring. An info message is sufficient. I'll be uploading a patch momentarily.",ahmed.radwan,ahmed.radwan,Minor,Closed,Fixed,21/Aug/12 16:47,11/Oct/12 17:48
Bug,MAPREDUCE-4571,12604288,TestHsWebServicesJobs fails on jdk7,"TestHsWebServicesJobs fails on jdk7. 

Tests run: 22, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.561 sec <<< FAILURE!testJobIdSlash(org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobs)  Time elapsed: 0.334 sec  <<< FAILURE!
java.lang.AssertionError: mapsTotal incorrect expected:<0> but was:<1>

",tgraves,tgraves,Major,Closed,Fixed,21/Aug/12 22:03,12/May/16 18:23
Bug,MAPREDUCE-4572,12604313,Can not access user logs - Jetty is not configured by default to serve aliases/symlinks,The task log servlet can no longer access user logs because MAPREDUCE-2415 introduce symlinks to the logs and jetty is not configured by default to serve symlinks. ,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,22/Aug/12 01:07,11/Oct/12 17:48
Bug,MAPREDUCE-4574,12604434,Fix TotalOrderParitioner to work with non-WritableComparable key types,"The current TotalOrderPartitioner class will not work with an alternative serialization library such as Avro.

To make it work, we may edit the readPartitions bits in it to support non-WritableComparable keys and also remove the WritableComparable check in the class types definition.

That is, since we do not use the values at all (NullWritable), we may as well do:

{code}
  private K[] readPartitions(FileSystem fs, Path p, Class<K> keyClass,
      Configuration conf) throws IOException {
    …
    while ((key = (K) reader.next(key)) != null) {
      parts.add(key);
      key = ReflectionUtils.newInstance(keyClass, conf);
    }
    …
  }
{code}",qwertymaniac,qwertymaniac,Trivial,Resolved,Fixed,22/Aug/12 13:45,12/May/16 18:23
Bug,MAPREDUCE-4576,12604520,Large dist cache can block tasktracker heartbeat,,revans2,revans2,Major,Closed,Fixed,22/Aug/12 21:24,15/May/13 05:16
Bug,MAPREDUCE-4577,12604488,HDFS-3672 broke TestCombineFileInputFormat.testMissingBlocks() test,"Before HDFS-3672, locally applying MAPREDUCE-4470 made TestCombineFileInputFormat to pass all it tests.

After HDFS-3672, TestCombineFileInputFormat.testMissingBlocks() fails:

{code}
$ mvn clean test -Dtest=TestCombineFileInputFormat

Running org.apache.hadoop.mapred.TestCombineFileInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.698 sec
Running org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.526 sec <<< FAILURE!

Results :

Tests in error: 
  testMissingBlocks(org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat): org.apache.hadoop.fs.BlockLocation

Tests run: 7, Failures: 0, Errors: 1, Skipped: 0
{code}
",atm,tucu00,Minor,Closed,Fixed,22/Aug/12 20:49,11/Oct/12 17:48
Bug,MAPREDUCE-4579,12604650,TestTaskAttempt fails jdk7,"-------------------------------------------------------------------------------
Test set: org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt
-------------------------------------------------------------------------------
Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.205 sec <<< FAILURE!testAttemptContainerRequest(org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt)  Time elapsed: 0.032 sec  <<< ERROR!
java.io.EOFException
        at java.io.DataInputStream.readByte(DataInputStream.java:267)
        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)
        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:329)
        at org.apache.hadoop.io.Text.readFields(Text.java:280)
        at org.apache.hadoop.security.token.Token.readFields(Token.java:165)",tgraves,tgraves,Major,Closed,Fixed,23/Aug/12 17:44,12/May/16 18:22
Bug,MAPREDUCE-4580,12604672,Change MapReduce to use the yarn-client module,Mapreduce needs to be changed to use the yarn-client module added via YARN-29.,vinodkv,vinodkv,Major,Closed,Fixed,23/Aug/12 19:38,11/Oct/12 17:48
Bug,MAPREDUCE-4583,12604872,Wrong paths for CapacityScheduler/FairScheduler jar in documentation,"Both documentations
http://hadoop.apache.org/common/docs/r1.0.3/fair_scheduler.html
http://hadoop.apache.org/common/docs/r1.0.3/capacity_scheduler.html

say that the jar should be copied from the contrib/*scheduler directory.

But that's not the case ; both jars are actually in the lib folder",,bdechoux,Major,Resolved,Fixed,24/Aug/12 16:08,11/Feb/16 22:51
Bug,MAPREDUCE-4595,12604985,TestLostTracker failing - possibly due to a race in JobHistory.JobHistoryFilesManager#run(),"The source for occasional failure of TestLostTracker seems like the following:

On job completion, JobHistoryFilesManager#run() spawns another thread to move history files to done folder. TestLostTracker waits for job completion, before checking the file format of the history file. However, the history files move might be in the process or might not have started in the first place.

The attachment (force-TestLostTracker-failure.patch) helps reproducing the error locally, by increasing the chance of hitting this race.",kasha,kasha,Critical,Closed,Fixed,25/Aug/12 22:16,03/Nov/14 18:33
Bug,MAPREDUCE-4597,12605140,TestKillSubProcesses intermittently fails,The test starts a mapper that spawns subprocesses. The test then checks if sufficient number of subprocesses have been spawned. The check can happen before all the processes have been spawned and can sometimes fail.,bikassaha,bikassaha,Major,Resolved,Fixed,27/Aug/12 21:44,30/Aug/12 02:26
Bug,MAPREDUCE-4598,12605141,Support for node health scripts on Windows,"Currently, it is not possible to have node health scripts on Windows, as NodeHealthCheckerService tries to directly launch (CreateProcess()) .sh scripts. TestNodeHealthService test fails because of this issue also leading to a subsequent TestNodeRefresh test failure.",bikassaha,bikassaha,Major,Resolved,Fixed,27/Aug/12 21:51,30/Aug/12 02:23
Bug,MAPREDUCE-4600,12605249,TestTokenCache.java from MRV1 no longer compiles,"{noformat}
    [javac] hadoop-mapreduce-project/build.xml:569: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 95 source files to hadoop-mapreduce-project/build/test/mapred/classes
    [javac] hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCache.java:291: cannot find symbol
    [javac] symbol  : method getDelegationToken(org.apache.hadoop.security.Credentials,java.lang.String)
    [javac] location: class org.apache.hadoop.mapreduce.security.TokenCache
    [javac]     Token<DelegationTokenIdentifier> nnt = TokenCache.getDelegationToken(
    [javac]                                                      ^
    [javac] hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCache.java:350: cannot find symbol
    [javac] symbol  : method getDelegationTokens(java.lang.String)
    [javac] location: class org.apache.hadoop.hdfs.HftpFileSystem
    [javac]       }}).when(hfs).getDelegationTokens(renewer);
    [javac]                    ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 2 errors
{noformat}",daryn,revans2,Critical,Closed,Fixed,28/Aug/12 16:36,12/May/16 18:24
Bug,MAPREDUCE-4604,12605399,"In mapred-default, mapreduce.map.maxattempts & mapreduce.reduce.maxattempts defaults are set to 4 as well as mapreduce.job.maxtaskfailures.per.tracker. ","This causes the AM to fail the job at the same time as it blacklists a node, thus never actually trying another node.

Marking as critical because we need this in 0.23.3 before it releases.",raviprak,raviprak,Critical,Closed,Fixed,29/Aug/12 16:49,10/Mar/15 04:30
Bug,MAPREDUCE-4607,12605425,Race condition in ReduceTask completion can result in Task being incorrectly failed,"Problem reported by chackaravarthy in MAPREDUCE-4252

This problem has been handled when speculative task launched for map task and other attempt got failed (not killed)
Can the similar kind of scenario can happen in case of reduce task?
Consider the following scenario for reduce task in case of speculation (one attempt got killed):
1. A task attempt is started.
2. A speculative task attempt for the same task is started.
3. The first task attempt completes and causes the task to transition to SUCCEEDED.
4. Then speculative task attempt will be killed because of the completion of first attempt.
As a result, internal error will be thrown from this attempt (TaskImpl.MapRetroactiveKilledTransition) and hence task attempt failure leads to job failure.
TaskImpl.MapRetroactiveKilledTransition
if (!TaskType.MAP.equals(task.getType())) {
        LOG.error(""Unexpected event for REDUCE task "" + event.getType());
        task.internalError(event.getType());
      }
So, do we need to have following code in MapRetroactiveKilledTransition also just like in MapRetroactiveFailureTransition.
if (event instanceof TaskTAttemptEvent) {
        TaskTAttemptEvent castEvent = (TaskTAttemptEvent) event;
        if (task.getState() == TaskState.SUCCEEDED &&
            !castEvent.getTaskAttemptID().equals(task.successfulAttempt)) {
          // don't allow a different task attempt to override a previous
          // succeeded state
          return TaskState.SUCCEEDED;
        }
      }
please check whether this is a valid case and give your suggestion.",bikassaha,bikassaha,Major,Closed,Fixed,29/Aug/12 20:27,15/Feb/13 13:09
Bug,MAPREDUCE-4608,12605479,hadoop-mapreduce-client is missing some dependencies,"
commons-logging/commons-lang/commons-cli/commons-codec/guava should be defined with provided scope as they are used directly by mapreduce.

While Maven gets them transitively from hadoop-common (also provided), not being there makes IntelliJ to break (it seems intellij does not do a transitive closure with provided dependencies)",tucu00,tucu00,Major,Closed,Fixed,30/Aug/12 06:53,11/Oct/12 17:48
Bug,MAPREDUCE-4610,12605515,Support deprecated mapreduce.job.counters.limit property in MR2,"The property mapreduce.job.counters.limit was introduced in MAPREDUCE-1943, but the mechanism was changed in MAPREDUCE-901 where the property name was changed to mapreduce.job.counters.max without supporting the old name. We should deprecate but honour the old name to make it easier for folks to move from Hadoop 1 to Hadoop 2.",tomwhite,tomwhite,Major,Closed,Fixed,30/Aug/12 12:28,11/Oct/12 17:48
Bug,MAPREDUCE-4611,12605534,MR AM dies badly when Node is decomissioned,"The MR AM always thinks that it is being killed by the RM when it gets a kill signal and it has not finished processing yet.  In reality the RM kill signal is only sent when the client cannot communicate directly with the AM, which probably means that the AM is in a bad state already.  The much more common case is that the node is marked as unhealthy or decomissioned.

I propose that in the short term the AM will only clean up if 

 # The process has been asked by the client to exit (kill)
 # The process job has finished cleanly and is exiting already
 # This is that last retry of the AM retries.

The downside here is that the .staging directory will be leaked and the job will not show up in the history server on an kill from the RM in some cases.

At least until the full set of AM cleanup issues can be addressed, probably as part of MAPREDUCE-4428",revans2,revans2,Critical,Closed,Fixed,30/Aug/12 14:39,12/May/16 18:22
Bug,MAPREDUCE-4612,12605591,job summary file permissions not set when its created,"The job summary file permissions are not set when its written out by to the done intermediate directory. The conf and jhist files are both set properly but the summary file doesn't follow the same path.  

This can cause the summary file to not be copied to the done directory if the default umask is set to be restrictive (like 600) and the mapred user can't read it.",tgraves,tgraves,Critical,Closed,Fixed,30/Aug/12 21:22,12/May/16 18:22
Bug,MAPREDUCE-4629,12605837,Remove JobHistory.DEBUG_MODE,"Remove JobHistory.DEBUG_MODE for the following reasons:

1. No one seems to be using it - the config parameter corresponding to enabling it does not even exist in mapred-default.xml
2. The logging being done in DEBUG_MODE needs to move to LOG.debug() and LOG.trace()
3. Buggy handling of helper methods in DEBUG_MODE; e.g. directoryTime() and timestampDirectoryComponent().",kasha,kasha,Major,Closed,Fixed,01/Sep/12 02:35,03/Nov/14 18:34
Bug,MAPREDUCE-4633,12606163,history server doesn't set permissions on all subdirs ,"The job history server creates a bunch of subdirectories under the ""done"" directory.  They are like 2012/09/03/000000.  It only sets the permissions on the last one, ie 000000 to 770.    So the 2012/09/03 aren't explicitly set so if the umask is more restrictive, they won't be set as it expects.",oss.wakayama,tgraves,Critical,Closed,Fixed,04/Sep/12 21:33,12/May/16 18:22
Bug,MAPREDUCE-4635,12606190,MR side of YARN-83. Changing package of YarnClient,,bikassaha,bikassaha,Major,Closed,Fixed,05/Sep/12 01:43,02/May/13 02:29
Bug,MAPREDUCE-4637,12606262,Killing an unassigned task attempt causes the job to fail,Attempting to kill a task attempt that has been scheduled but is not running causes an invalid state transition and the AM to stop with an error. ,mayank_bansal,tomwhite,Major,Closed,Fixed,05/Sep/12 15:56,15/Feb/13 13:10
Bug,MAPREDUCE-4641,12606478,Exception in commitJob marks job as successful in job history,"If the job committer throws an {{IOException}} from {{commitJob}} then the job will be marked as FINISHED/FAILED on the RM apps page, but the history server will show the job as SUCCEEDED.",jlowe,jlowe,Major,Closed,Fixed,06/Sep/12 20:11,11/Oct/12 17:48
Bug,MAPREDUCE-4642,12606495,MiniMRClientClusterFactory should not use job.setJar(),"Currently, MiniMRClientClusterFactory does {{job.setJar(callerJar)}} so that the {{callerJar}} is added to the cache in MR2.  However, this makes the resulting configuration inconsistent between MR1 and MR2 as in MR1 the job jar is not set and in MR2 its set to the {{callerJar}}.  This difference can also cause some tests to fail in Oozie.  We should instead use the {{job.addCacheFile()}} method.  ",rkanter,rkanter,Major,Closed,Fixed,06/Sep/12 22:00,11/Oct/12 17:48
Bug,MAPREDUCE-4643,12606506,Make job-history cleanup-period configurable,"Job history cleanup should be made configurable. Currently, it is set to 1 month by default. The DEBUG_MODE (to be removed, see MAPREDUCE-4629) sets it to 20 minutes, but it should be configurable.",sandyr,kasha,Major,Closed,Fixed,06/Sep/12 23:41,03/Nov/14 18:33
Bug,MAPREDUCE-4646,12606685,client does not receive job diagnostics for failed jobs,"When a job fails the client is not showing any diagnostics.  For example, running a fail job results in this not-so-helpful message from the client:

{noformat}
2012-09-07 21:12:00,649 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1308)) - Job job_1347052207658_0001 failed with state FAILED due to:
{noformat}

...and nothing else to go with it indicating what went wrong.  The job diagnostics are apparently not making it back to the client.
",jlowe,jlowe,Major,Closed,Fixed,07/Sep/12 21:29,11/Oct/12 17:48
Bug,MAPREDUCE-4647,12606875,We should only unjar jobjar if there is a lib directory in it.,For backwards compatibility we recently added made is so we would unjar the job.jar and add anything to the classpath in the lib directory of that jar.  But this also slows job startup down a lot if the jar is large.  We should only unjar it if actually doing so would add something new to the classpath.,revans2,revans2,Major,Closed,Fixed,10/Sep/12 14:05,04/Sep/14 00:57
Bug,MAPREDUCE-4649,12606978,mr-jobhistory-daemon.sh needs to be updated post YARN-1,"Even today, JHS is assuming that YARN_HOME will be same as HADOOP_MAPRED_HOME besides other such assumptions. We need to fix it.",vinodkv,vinodkv,Major,Closed,Fixed,11/Sep/12 00:54,29/Aug/13 16:47
Bug,MAPREDUCE-4652,12607369,ValueAggregatorJob sets the wrong job jar,"Using branch-1 tarball, if the user tries to submit an example aggregatewordcount, the job fails with the following error:

{code}
ahmed@ubuntu:~/demo/deploy/hadoop-1.2.0-SNAPSHOT$ bin/hadoop jar hadoop-examples-1.2.0-SNAPSHOT.jar aggregatewordcount input examples-output/aggregatewordcount 2 textinputformat
12/09/12 17:09:46 INFO mapred.JobClient: originalJarPath: /home/ahmed/demo/deploy/hadoop-1.2.0-SNAPSHOT/hadoop-core-1.2.0-SNAPSHOT.jar
12/09/12 17:09:48 INFO mapred.JobClient: submitJarFile: hdfs://localhost:9000/tmp/hadoop-ahmed/mapred/staging/ahmed/.staging/job_201209121702_0008/job.jar
12/09/12 17:09:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12/09/12 17:09:48 WARN snappy.LoadSnappy: Snappy native library not loaded
12/09/12 17:09:48 INFO mapred.FileInputFormat: Total input paths to process : 21
12/09/12 17:09:49 INFO mapred.JobClient: Running job: job_201209121702_0008
12/09/12 17:09:50 INFO mapred.JobClient:  map 0% reduce 0%
12/09/12 17:09:58 INFO mapred.JobClient: Task Id : attempt_201209121702_0008_m_000000_0, Status : FAILED
java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
	... 9 more
Caused by: java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)
	... 14 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
	... 17 more
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.examples.AggregateWordCount$WordCountPlugInClass
	at org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.createInstance(UserDefinedValueAggregatorDescriptor.java:57)
	at org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.createAggregator(UserDefinedValueAggregatorDescriptor.java:64)
	at org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.<init>(UserDefinedValueAggregatorDescriptor.java:76)
	at org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.getValueAggregatorDescriptor(ValueAggregatorJobBase.java:54)
	at org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.getAggregatorDescriptors(ValueAggregatorJobBase.java:65)
	at org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.initializeMySpec(ValueAggregatorJobBase.java:74)
	at org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.configure(ValueAggregatorJobBase.java:42)
	... 22 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.examples.AggregateWordCount$WordCountPlugInClass
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:247)
	at org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.createInstance(UserDefinedValueAggregatorDescriptor.java:52)
	... 28 more
{code}",ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,13/Sep/12 00:12,15/May/13 05:15
Bug,MAPREDUCE-4654,12605304,TestDistCp is @ignored,"We should fix TestDistCp so that it actually runs, rather than being ignored.

{code}
@ignore
public class TestDistCp {
  private static final Log LOG = LogFactory.getLog(TestDistCp.class);
  private static List<Path> pathList = new ArrayList<Path>();
  ...
{code}",sandyr,cmccabe,Critical,Closed,Fixed,28/Aug/12 22:03,15/Feb/13 13:09
Bug,MAPREDUCE-4657,12607534,WindowsResourceCalculatorPlugin has NPE,"When Shell command execution is interrupted then WindowsResourceCalculatorPlugin has NPE.
code}
2012-08-31 13:01:00,140 ERROR [Thread-771] util.WindowsResourceCalculatorPlugin(69): java.io.IOException: java.lang.InterruptedException^M
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:424)^M
        at org.apache.hadoop.util.Shell.run(Shell.java:336)^M
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:540)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getSystemInfoInfoFromShell(WindowsResourceCalculatorPlugin.java:66)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.refreshIfNeeded(WindowsResourceCalculatorPlugin.java:81)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getAvailableVirtualMemorySize(WindowsResourceCalculatorPlugin.java:126)^M
        at org.apache.hadoop.mapred.TaskTracker.getAvailableVirtualMemoryOnTT(TaskTracker.java:1933)^M
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1834)^M
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1664)^M
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2516)^M
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)^M
        at java.lang.Thread.run(Thread.java:662)^M
^M
2012-08-31 13:01:00,140 ERROR [Thread-771] mapred.TaskTracker(1766): Caught exception: java.lang.NullPointerException^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.refreshIfNeeded(WindowsResourceCalculatorPlugin.java:83)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getAvailableVirtualMemorySize(WindowsResourceCalculatorPlugin.java:126)^M
        at org.apache.hadoop.mapred.TaskTracker.getAvailableVirtualMemoryOnTT(TaskTracker.java:1933)^M
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1834)^M
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1664)^M
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2516)^M
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)^M
        at java.lang.Thread.run(Thread.java:662)^M
{code}",bikassaha,bikassaha,Minor,Resolved,Fixed,13/Sep/12 22:30,04/Oct/12 02:48
Bug,MAPREDUCE-4662,12608085,JobHistoryFilesManager thread pool never expands,"The job history file manager creates a threadpool with core size 1 thread, max pool size 3.   It never goes beyond 1 thread though because its using a LinkedBlockingQueue which doesn't have a max size. 

    void start() {
      executor = new ThreadPoolExecutor(1, 3, 1,
          TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>());
    }

According to the ThreadPoolExecutor java doc page it only increases the number of threads when the queue is full. Since the queue we are using has no max size it never fills up and we never get more then 1 thread. ",kihwal,tgraves,Major,Closed,Fixed,18/Sep/12 16:04,16/Dec/14 20:15
Bug,MAPREDUCE-4669,12608294,MRAM web UI does not work with HTTPS,"With Kerberos enable, the MRAM runs as the user that submitted the job, thus the MRAM process cannot read the cluster keystore files to get the certificates to start its HttpServer using HTTPS.

We need to decouple the keystore used by RM/NM/NN/DN (which are cluster provided) from the keystore used by AMs (which ought to be user provided).
",rkanter,tucu00,Major,Resolved,Fixed,19/Sep/12 18:26,23/Oct/18 22:47
Bug,MAPREDUCE-4671,12608545,AM does not tell the RM about container requests that are no longer needed,"Say the AM wanted a container at hosts h1, h2, h3. After getting a container at h1 it should tell RM that it no longer needs containers at h2, h3. Otherwise on the RM h2, h3 remain valid allocation locations.
The AM RMContainerAllocator does remove these resource requests internally. When the resource request container count drops to 0 then it drops the resource request from its tables but forgets to send the 0 sized request to the RM.",bikassaha,bikassaha,Major,Closed,Fixed,20/Sep/12 22:59,27/Aug/13 22:22
Bug,MAPREDUCE-4673,12608674,make TestRawHistoryFile and TestJobHistoryServer more robust,"these unit tests fail if 2 different users run them on the same host as they are using /tmp/input path

following is the info from the test log

{code}
testcase classname=""org.apache.hadoop.mapred.TestJobHistoryServer"" name=""testHistoryServerStandalone"" time=""4.572"">
    <failure message=""/tmp/.input.crc (Permission denied)"" type=""junit.framework.AssertionFailedError"">junit.framework.AssertionFailedError: /tmp/.input.crc (Permission denied)
        at org.apache.hadoop.mapred.TestJobHistoryServer.testHistoryServerStandalone(TestJobHistoryServer.java:113)
</failure>
{code}",arpitgupta,arpitgupta,Major,Closed,Fixed,21/Sep/12 18:06,17/Oct/12 18:27
Bug,MAPREDUCE-4674,12608648,"Hadoop examples secondarysort has a typo ""secondarysrot"" in the usage","$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar secondarysort
Usage: secondarysrot <in> <out>
",rjustice,rjustice,Minor,Closed,Fixed,21/Sep/12 14:44,06/Feb/13 17:05
Bug,MAPREDUCE-4675,12608702,TestKillSubProcesses fails as the process is still alive after the job is done,"I ran this test in branch 1 and branch 1.1 and they both failed with the following

{code}
Testcase: testJobKillFailAndSucceed took 82.219 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.hadoop.mapred.TestKillSubProcesses.validateKillingSubprocesses(TestKillSubProcesses.java:245)
        at org.apache.hadoop.mapred.TestKillSubProcesses.runKillingJobAndValidate(TestKillSubProcesses.java:97)
        at org.apache.hadoop.mapred.TestKillSubProcesses.runTests(TestKillSubProcesses.java:336)
        at org.apache.hadoop.mapred.TestKillSubProcesses.testJobKillFailAndSucceed(TestKillSubProcesses.java:320)
{code}",bikassaha,arpitgupta,Major,Closed,Fixed,21/Sep/12 22:15,17/Oct/12 18:27
Bug,MAPREDUCE-4678,12608858,Running the Pentomino example with defaults throws java.lang.NegativeArraySizeException,"HADOOP_HOME/hadoop-examples.jar pentomino <output_dir> will fail with the following error message: 
{code}
INFO util.NativeCodeLoader: Loaded the native-hadoop library 
INFO mapred.FileInputFormat: Total input paths to process : 1 
INFO mapred.JobClient: Running job: job_xxxxx
INFO mapred.JobClient: map 0% reduce 0% 
INFO mapred.JobClient: Task Id : attempt_xxxx, Status : FAILED 
java.lang.NegativeArraySizeException 
at org.apache.hadoop.examples.dancing.DistributedPentomino$PentMap.map(Di 
stributedPentomino.java:95) 
at org.apache.hadoop.examples.dancing.DistributedPentomino$PentMap.map(Di 
stributedPentomino.java:51) 
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50) 
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391) 
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325) 
at org.apache.hadoop.mapred.Child$4.run(Child.java:270) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:396) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformat 
ion.java:1177) 
at org.apache.hadoop.mapred.Child.main(Child.java:264)
{code}",ctm,ctm,Minor,Closed,Fixed,24/Sep/12 12:26,02/May/13 02:29
Bug,MAPREDUCE-4680,12608939,Job history cleaner should only check timestamps of files in old enough directories,"Job history files are stored in yyyy/mm/dd folders.  Currently, the job history cleaner checks the modification date of each file in every one of these folders to see whether it's past the maximum age.  The load on HDFS could be reduced by only checking the ages of files in directories that are old enough, as determined by their name.",rkanter,sandyr,Major,Closed,Fixed,24/Sep/12 22:17,24/Feb/14 20:57
Bug,MAPREDUCE-4681,12609161,HDFS-3910 broke MR tests,HDFS-3910 changed signatures of DFSTestUtil functions and didn't change MR tests.,acmurthy,acmurthy,Major,Closed,Fixed,26/Sep/12 01:55,15/Feb/13 13:09
Bug,MAPREDUCE-4683,12609167,Create and distribute hadoop-mapreduce-client-core-tests.jar,"We need to fix our build to create/distribute hadoop-mapreduce-client-core-tests.jar, need this before MAPREDUCE-4253",aajisaka,acmurthy,Critical,Resolved,Fixed,26/Sep/12 02:37,08/Dec/16 02:40
Bug,MAPREDUCE-4685,12609185,DBCount should not use ACCESS ,"DBCount uses ACCESS as table name which is not supported for Oracle DBs since it is a keyword in Oracle as per http://docs.oracle.com/cd/B10501_01/appdev.920/a42525/apb.htm

Also, BIGINT isn't supported.

I will shortly post a patch to address this.",viji_r,viji_r,Major,Resolved,Fixed,26/Sep/12 07:12,12/May/16 18:22
Bug,MAPREDUCE-4689,12609319,JobClient.getMapTaskReports on failed job results in NPE,"When calling JobClient.getMapTaskReports for a job that has failed results in an NPE.  For example:

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.counters.AbstractCounters.<init>(AbstractCounters.java:107)
	at org.apache.hadoop.mapred.Counters.<init>(Counters.java:71)
	at org.apache.hadoop.mapred.Counters.downgrade(Counters.java:80)
	at org.apache.hadoop.mapred.TaskReport.downgrade(TaskReport.java:81)
	at org.apache.hadoop.mapred.TaskReport.downgradeArray(TaskReport.java:88)
	at org.apache.hadoop.mapred.JobClient.getTaskReports(JobClient.java:691)
	at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:681)
...
{noformat}
",jlowe,jlowe,Major,Closed,Fixed,26/Sep/12 23:06,04/Sep/14 00:57
Bug,MAPREDUCE-4691,12609466,"Historyserver can report ""Unknown job"" after RM says job has completed","Example traceback from the client:

{noformat}
2012-09-27 20:28:38,068 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
2012-09-27 20:28:38,530 [main] WARN  org.apache.hadoop.mapred.ClientServiceDelegate - Error from remote end: Unknown job job_1348097917603_3019
2012-09-27 20:28:38,530 [main] ERROR org.apache.hadoop.security.UserGroupInformation - PriviledgedActionException as:xxx (auth:KERBEROS) cause:org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1348097917603_3019
2012-09-27 20:28:38,531 [main] WARN  org.apache.pig.tools.pigstats.JobStats - Failed to get map task report
RemoteTrace: 
 at LocalTrace: 
        org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1348097917603_3019
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:156)
        at $Proxy11.getJobReport(Unknown Source)
        at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getJobReport(MRClientProtocolPBClientImpl.java:116)
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:298)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:383)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:482)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:184)
...
{noformat}
",revans2,jlowe,Critical,Closed,Fixed,27/Sep/12 22:45,04/Sep/14 00:57
Bug,MAPREDUCE-4693,12609606,Historyserver should provide counters for failed tasks,"Currently the historyserver is not providing counters for failed tasks, even though they are available via the AM as long as the job is still running.  Those counters are lost when the client needs to redirect to the historyserver after the job completes.",xgong,jlowe,Major,Closed,Fixed,28/Sep/12 22:14,27/Aug/13 22:22
Bug,MAPREDUCE-4695,12609762,Fix LocalRunner on trunk after MAPREDUCE-3223 broke it,"MAPREDUCE-3223 removed mapreduce.cluster.local.dir property from mapred-default.xml (since NM local dirs are now used) but failed to counter that LocalJobRunner, etc. still use it.

{code}
mr-3223.txt:-  <name>mapreduce.cluster.local.dir</name>
mr-3223.txt--  <value>${hadoop.tmp.dir}/mapred/local</value>
{code}

All local job tests have been failing since then.

This JIRA is to reintroduce it or provide an equivalent new config for fixing it.",qwertymaniac,qwertymaniac,Blocker,Resolved,Fixed,01/Oct/12 15:43,13/May/16 05:19
Bug,MAPREDUCE-4696,12609822,TestMRServerPorts throws NullReferenceException,"TestMRServerPorts throws 

{code}
java.lang.NullPointerException
    at org.apache.hadoop.mapred.TestMRServerPorts.canStartJobTracker(TestMRServerPorts.java:99)
    at org.apache.hadoop.mapred.TestMRServerPorts.testJobTrackerPorts(TestMRServerPorts.java:152)
{code}

Use the JobTracker.startTracker(string, string, boolean initialize) factory method to get a pre-initialized JobTracker for the test.
",gopalv,gopalv,Minor,Closed,Fixed,01/Oct/12 22:31,06/Mar/13 09:56
Bug,MAPREDUCE-4697,12609827,TestMapredHeartbeat fails assertion on HeartbeatInterval,"TestMapredHeartbeat fails test on heart beat interval

{code}
    FAILED
expected:<300> but was:<500>
junit.framework.AssertionFailedError: expected:<300> but was:<500>
    at org.apache.hadoop.mapred.TestMapredHeartbeat.testJobDirCleanup(TestMapredHeartbeat.java:68)
{code}

Replicate math for getNextHeartbeatInterval() in the test-case to ensure MRConstants changes do not break test-case.",gopalv,gopalv,Minor,Closed,Fixed,01/Oct/12 22:43,06/Mar/13 09:55
Bug,MAPREDUCE-4698,12609831,TestJobHistoryConfig throws Exception in testJobHistoryLogging,"TestJobHistoryConfig cannot find the LOG_DIR and throws 

{code}
Can not create a Path from a null string
java.lang.IllegalArgumentException: Can not create a Path from a null string
    at org.apache.hadoop.fs.Path.checkPathArg(Path.java:78)
    at org.apache.hadoop.fs.Path.<init>(Path.java:90)
    at org.apache.hadoop.mapred.JobHistory$JobInfo.getJobHistoryFileName(JobHistory.java:1337)
    at org.apache.hadoop.mapred.JobHistory$JobInfo.logSubmitted(JobHistory.java:1660)
    at org.apache.hadoop.mapred.JobHistory$JobInfo.logSubmitted(JobHistory.java:1641)
    at org.apache.hadoop.mapred.TestJobHistoryConfig.testJobHistoryLogging(TestJobHistoryConfig.java:123)
{code}",gopalv,gopalv,Minor,Closed,Fixed,01/Oct/12 22:57,17/Oct/12 18:27
Bug,MAPREDUCE-4699,12609833,TestFairScheduler & TestCapacityScheduler fails due to JobHistory exception,"TestFairScheduler fails due to exception from mapred.JobHistory

{code}
null
java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobHistory$JobInfo.logJobPriority(JobHistory.java:1975)
	at org.apache.hadoop.mapred.JobInProgress.setPriority(JobInProgress.java:895)
	at org.apache.hadoop.mapred.TestFairScheduler.testFifoPool(TestFairScheduler.java:2617)
{code}

TestCapacityScheduler fails due to

{code}
java.lang.NullPointerException
    at org.apache.hadoop.mapred.JobHistory$JobInfo.logJobPriority(JobHistory.java:1976)
    at org.apache.hadoop.mapred.JobInProgress.setPriority(JobInProgress.java:895)
    at org.apache.hadoop.mapred.TestCapacityScheduler$FakeTaskTrackerManager.setPriority(TestCapacityScheduler.java:653)
    at org.apache.hadoop.mapred.TestCapacityScheduler.testHighPriorityJobInitialization(TestCapacityScheduler.java:2666)
{code}

Update UtilsForTest::getJobTracker to call initialize()/initializeFileSystem() to match behaviour in pre-safe mode constructor.",gopalv,gopalv,Minor,Closed,Fixed,01/Oct/12 23:08,06/Mar/13 09:56
Bug,MAPREDUCE-4705,12610407,Historyserver links expire before the history data does,"The historyserver can serve up links to jobs that become useless well before the job history files are purged.  For example on a large, heavily used cluster we can end up rotating through the maximum number of jobs the historyserver can track fairly quickly.  If a user was investigating an issue with a job using a saved historyserver URL, that URL can become useless because the historyserver has forgotten about the job even though the history files are still sitting in HDFS.

We can tell the historyserver to keep track of more jobs by increasing {{mapreduce.jobhistory.joblist.cache.size}}, but this has a direct impact on the responsiveness of the main historyserver page since it serves up all the entries to the client at once.  It looks like Hadoop 1.x avoided this issue by encoding the history file location into the URLs served up by the historyserver, so it didn't have to track a mapping between job ID and history file location.",jlowe,jlowe,Critical,Closed,Fixed,04/Oct/12 21:32,06/Feb/13 17:05
Bug,MAPREDUCE-4706,12610415,FairScheduler#dump(): Computing of # running maps and reduces is commented out,"In FairScheduler#dump(), we conveniently comment the updating of number of running maps and reduces. It needs to be fixed for the dump to throw out meaningful information.",kasha,kasha,Critical,Closed,Fixed,04/Oct/12 21:56,03/Nov/14 18:33
Bug,MAPREDUCE-4712,12610651,mr-jobhistory-daemon.sh doesn't accept --config,"It says
{code}
$ $HADOOP_MAPRED_HOME/sbin/mr-jobhistory-daemon.sh --config /Users/vinodkv/tmp/conf/ start historyserver
Usage: mr-jobhistory-daemon.sh [--config <conf-dir>] (start|stop) <mapred-command>
{code}",vinodkv,vinodkv,Major,Closed,Fixed,05/Oct/12 21:54,15/Feb/13 13:10
Bug,MAPREDUCE-4716,12611022,TestHsWebServicesJobsQuery.testJobsQueryStateInvalid fails with jdk7,"Using jdk7 TestHsWebServicesJobsQuery.testJobsQueryStateInvalid  fails.

It looks like the string changed from ""const class"" to ""constant"" in jdk7.


Tests run: 25, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 9.713 sec <<< FAILURE!
testJobsQueryStateInvalid(org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery)  Time elapsed: 0.371 sec  <<< FAILURE!
java.lang.AssertionError: exception message doesn't match, got: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.InvalidState expected: No enum const class org.apache.hadoop.mapreduce.v2.api.records.JobState.InvalidState
        at org.junit.Assert.fail(Assert.java:91)        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.apache.hadoop.yarn.webapp.WebServicesTestUtils.checkStringMatch(WebServicesTestUtils.java:77)
        at org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery.testJobsQueryStateInvalid(TestHsWebServicesJobsQuery.java:286)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
",tgraves,tgraves,Major,Closed,Fixed,09/Oct/12 17:03,12/May/16 18:23
Bug,MAPREDUCE-4720,12611180,Browser thinks History Server main page JS is taking too long,"The main History Server page with the default settings of 20,000 jobs can cause browsers to think that the JS on the page is stuck and ask you if you want to kill it. This is a big usability problem.",raviprak,revans2,Major,Closed,Fixed,10/Oct/12 14:09,05/Sep/16 12:03
Bug,MAPREDUCE-4721,12611238,Task startup time in JHS is same as job startup time.,"As Bobby pointed out in https://issues.apache.org/jira/browse/MAPREDUCE-4711?focusedCommentId=13471696&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13471696

In the Map and Reduce tasks page, it should print the earliest task attempt launch time as TaskImpl:getLaunchTime() does.",raviprak,raviprak,Major,Closed,Fixed,10/Oct/12 20:33,12/May/16 18:22
Bug,MAPREDUCE-4724,12611847,job history web ui applications page should be sorted to display last app first,"The job history server jobs web page defaults the sort order to ascending, which has oldest jobs first (smallest job id).   I think its more useful to sort descending so that the newest jobs show first since those are more likely what people are going to look at.  YARN-159 changed this for RM apps page. ",tgraves,tgraves,Major,Closed,Fixed,15/Oct/12 15:40,03/Sep/14 23:17
Bug,MAPREDUCE-4729,12612253,job history UI not showing all job attempts,"We are seeing a case where a job runs but the AM is running out of memory in the first 3 attempts. The job eventually finishes on the 4th attempt.  When you go to the job history UI for that job, it only shows the last attempt.  This is bad since we want to see why the first 3 attempts failed.

The RM web ui shows all 4 attempts. 

Also I tested this locally by running ""kill"" on the app master and in that case the history server UI does show all attempts.",vinodkv,tgraves,Major,Closed,Fixed,17/Oct/12 16:21,06/Feb/13 17:05
Bug,MAPREDUCE-4730,12612335,AM crashes due to OOM while serving up map task completion events,We're seeing a repeatable OOM crash in the AM for a task with around 30000 maps and 3000 reducers.  Details to follow.,jlowe,jlowe,Blocker,Closed,Fixed,17/Oct/12 23:36,02/May/13 02:29
Bug,MAPREDUCE-4733,12612505,Reducer can fail to make progress during shuffle if too many reducers complete consecutively,"TaskAttemptListenerImpl implements getMapCompletionEvents by calling Job.getTaskAttemptCompletionEvents with the same fromEvent and maxEvents passed in from the reducer and then filtering the result for just map events. We can't filter the task completion event list and expect the caller's ""window"" into the list to match up.  As soon as a reducer event appears in the list it means we are redundantly sending map completion events that were already seen by the reducer.

Worst case the reducer will hang if all of the events in the requested window are reducer events.  In that case zero events will be reported back to the caller and it won't bump up fromEvent on the next call.  Reducer then never sees the final map completion events needed to complete the shuffle. This could happen in a case where all maps complete, more than MAX_EVENTS reducers complete consecutively, but some straggling reducers get fetch failures and cause a map to be restarted.",jlowe,jlowe,Major,Closed,Fixed,18/Oct/12 22:30,02/May/13 02:29
Bug,MAPREDUCE-4737,12610942, Hadoop does not close output file / does not call Mapper.cleanup if exception in map,"Find this in Pig unit test TestStore under Windows. There are dangling files because map does not close the file when exception happens in map(). In Windows, Hadoop will not remove a file if it is not closed. This happens in reduce() as well.",acmurthy,daijy,Major,Closed,Fixed,09/Oct/12 07:45,15/May/13 05:16
Bug,MAPREDUCE-4739,12612864,Some MapReduce tests fail to find winutils.,"All modules inherit a setting in the Surefire configuration for HADOOP_HOME via the hadoop-project pom.xml.  This setting is a relative path used in Shell.java to find winutils when running on Windows.  The MapReduce modules have a deeper directory structure, which makes the inherited value of HADOOP_HOME invalid and causes some tests to fail while calling winutils.",,cnauroth,Major,Resolved,Fixed,21/Oct/12 07:31,22/Oct/12 20:53
Bug,MAPREDUCE-4740,12612984,only .jars can be added to the Distributed Cache classpath,"Koji tracked down this one.


{noformat}
$ cat mycat.sh
#!/bin/sh
cat >& /dev/null
$JAVA_HOME/bin/jinfo $PPID | grep java.class.path
export | grep CLASSPATH
ls -l 

$ myfile=/user/me/myclasses.zip; yarn jar
hadoop-streaming.jar
-Dmapreduce.job.cache.archives=hdfs:///${myfile}
-Dmapreduce.job.classpath.archives=${myfile} -input in.txt -output out -reducer NONE -mapper mycat.sh  -file mycat.sh
{noformat}

So, cachearchive like class.zip or class.tar.gz were never set as part of the
classpath even though they were properly set by DistributedCache.addArchiveToClassPath.

It looks like we are parsing the classpath out of the configs, but then throwing that away.  It looks simple enough to add them in the correct place.",revans2,revans2,Blocker,Closed,Fixed,22/Oct/12 14:57,19/Aug/15 00:16
Bug,MAPREDUCE-4741,12612987,WARN and ERROR messages logged during normal AM shutdown,"The ApplicationMaster is logging WARN and ERROR messages during normal shutdown, and some users are misinterpreting these as serious problems.  For example:

{noformat}
2012-10-02 13:58:50,247 ERROR [ContainerLauncher Event Handler] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Returning, interrupted : java.lang.InterruptedException
[...]
2012-10-02 13:58:50,248 ERROR [Thread-47] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Returning, interrupted : java.lang.InterruptedException
2012-10-02 13:58:50,248 WARN [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Allocated thread interrupted. Returning.
[...]
2012-10-02 13:58:50,367 ERROR [TaskCleaner Event Handler] org.apache.hadoop.mapreduce.v2.app.taskclean.TaskCleanerImpl: Returning, interrupted : java.lang.InterruptedException
{noformat}

Warnings or errors should not be logged if everything is working as intended.",vinodkv,jlowe,Minor,Closed,Fixed,22/Oct/12 15:04,06/Feb/13 17:05
Bug,MAPREDUCE-4742,12613102,Fix typo in nnbench#displayUsage,,xieliang007,xieliang007,Trivial,Closed,Fixed,23/Oct/12 07:43,24/Apr/15 23:15
Bug,MAPREDUCE-4743,12613135,Job is marking as FAILED and also throwing the Transition exception instead of KILLED when issues a KILL command,"{code:xml}
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_KILL at SUCCEEDED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:605)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)
   at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:903)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:897)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:662)
{code}",devaraj,devaraj,Major,Resolved,Fixed,23/Oct/12 13:23,07/Jun/13 12:06
Bug,MAPREDUCE-4746,12613203,The MR Application Master does not have a config to set environment variables,"There is no mechanism for defining environment variables (i.e. LD_LIBRARY_PATH) for the MRAppMaster.

",robsparker,robsparker,Major,Closed,Fixed,23/Oct/12 21:47,03/Sep/14 23:17
Bug,MAPREDUCE-4748,12613375,Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED,"We saw this happen when running a large pig script.

{noformat}
2012-10-23 22:45:24,986 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Can't handle this event at current state for task_1350837501057_21978_m_040453
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Speculative execution was enabled, and that task did speculate so it looks like this is an error in the state machine either between the task attempts or just within that single task.",jlowe,revans2,Blocker,Closed,Fixed,24/Oct/12 21:53,06/Feb/13 17:05
Bug,MAPREDUCE-4749,12613414,Killing multiple attempts of a task taker longer as more attempts are killed,"The following was noticed on a mr job running on hadoop 1.1.0

1. Start an mr job with 1 mapper

2. Wait for a min

3. Kill the first attempt of the mapper and then subsequently kill the other 3 attempts in order to fail the job

The time taken to kill the task grew exponentially.

1st attempt was killed immediately.
2nd attempt took a little over a min
3rd attempt took approx. 20 mins
4th attempt took around 3 hrs.

The command used to kill the attempt was ""hadoop job -fail-task""

Note that the command returned immediately as soon as the fail attempt was accepted but the time the attempt was actually killed was as stated above.
",arpitgupta,arpitgupta,Major,Closed,Fixed,25/Oct/12 00:41,03/Dec/12 07:33
Bug,MAPREDUCE-4750,12613471,Enable NNBenchWithoutMR in MapredTestDriver,"Right now, we could run nnbench from MapredTestDriver only, there's no entry for NNBenchWithoutMR, it would be better enable it explicitly, such that we can do namenode benchmark with less influence factors",xieliang007,xieliang007,Major,Resolved,Fixed,25/Oct/12 11:07,12/May/16 18:24
Bug,MAPREDUCE-4751,12612304,AM stuck in KILL_WAIT for days,"We found some jobs were stuck in KILL_WAIT for days on end. The RM shows them as RUNNING. When you go to the AM, it shows it in the KILL_WAIT state, and a few maps running. All these maps were scheduled on nodes which are now in the RM's Lost nodes list. The running maps are in the FAIL_CONTAINER_CLEANUP state",vinodkv,raviprak,Major,Closed,Fixed,17/Oct/12 20:28,03/Sep/14 23:17
Bug,MAPREDUCE-4765,12614423,Restarting the JobTracker programmatically can cause DelegationTokenRenewal to throw an exception,"The DelegationTokenRenewal class has a global Timer; when you stop the JobTracker by calling {{stopTracker()}} on it (or {{stopJobTracker()}} in MiniMRCluster), the JobTracker will call {{close()}} on DelegationTokenRenewal, which cancels the Timer.  If you then start up the JobTracker again by calling {{startTracker()}} on it (or {{startJobTracker()}} in MiniMRCluster), the Timer won't necessarily be re-created; and DelegationTokenRenewal will later throw an exception when it tries to use the Timer again (because you can't reuse a canceled Timer).  

DelegationTokenRenewal doesn't seem to be used in trunk, so we only need this for branch-1",rkanter,rkanter,Minor,Closed,Fixed,01/Nov/12 19:43,15/May/13 05:15
Bug,MAPREDUCE-4771,12614836,KeyFieldBasedPartitioner not partitioning properly when configured,"Relative to Hadoop 0.20/1.x, KeyFieldBasedPartitioner is not distributing across partitions properly when configured.  This is related to the double-configure issue as described in HADOOP-7425.  KeyFieldBasedPartitioner is getting configured twice, and that ends up duplicating the keyspecs and causing the keys to be hashed twice.

KeyFieldBasedPartitioner should not duplicate keyspecs when configured twice.",jlowe,jlowe,Major,Closed,Fixed,05/Nov/12 19:13,03/Sep/14 23:17
Bug,MAPREDUCE-4772,12614849,Fetch failures can take way too long for a map to be restarted,"In one particular case we saw a NM go down at just the right time, that most of the reducers got the output of the map tasks, but not all of them.

The ones that failed to get the output reported to the AM rather quickly that they could not fetch from the NM, but because the other reducers were still running the AM would not relaunch the map task because there weren't more than 50% of the running reducers that had reported fetch failures.  Then because of the exponential back-off for fetches on the reducers it took until 1 hour 45 min for the reduce tasks to hit another 10 fetch failures and report in again. At that point the other reducers had finished and the job relaunched the map task.  If the reducers had still been running at 1:45 I have no idea how long it would have taken for each of the tasks to get to 30 fetch failures.

We need to trigger the map based off of percentage of reducers shuffling, not percentage of reducers running, we also need to have a maximum limit of the back off, so that we don't ever have the reducer waiting for days to try and fetch map output.  ",revans2,revans2,Critical,Closed,Fixed,05/Nov/12 20:19,03/Sep/14 23:17
Bug,MAPREDUCE-4774,12614937,JobImpl does not handle asynchronous task events in FAILED state,"The test org.apache.hadoop.mapred.TestClusterMRNotification.testMR frequently  fails in mapred build (e.g. see https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2988/testReport/junit/org.apache.hadoop.mapred/TestClusterMRNotification/testMR/ , or 
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2982//testReport/org.apache.hadoop.mapred/TestClusterMRNotification/testMR/).

The test aims to check Job status notifications received through HTTP Servlet. It runs 3 jobs: successfull, killed, and failed. 
The test expects the servlet to receive some expected notifications in some expected order. It also tries to test the retry-on-failure notification functionality, so on each 1st notification the servlet answers ""400 forcing error"", and on each 2nd notification attempt it answers ""ok"". 
In general, the test fails because the actual number and/or type of the notifications differs from the expected.

Investigation shows that actual root cause of the problem is an incorrect job state transition: the 3rd job mapred task fails (by intentionally thrown  RuntimeException, see UtilsForTests#runJobFail()), and the state of the task changes from RUNNING to FAILED.
At this point JobEventType.JOB_TASK_ATTEMPT_COMPLETED event is submitted (in  method org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handleTaskAttemptCompletion(TaskAttemptId, TaskAttemptCompletionEventStatus)), and this event gets processed in AsyncDispatcher, but this transition is impossible according to the event transition map (see org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl#stateMachineFactory). This causes the following exception to be thrown upon the event processing:
2012-11-06 12:22:02,335 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)
        at java.lang.Thread.run(Thread.java:662) 

So, the job gets into state ""INTERNAL_ERROR"", the job end notification like this is sent:
http://localhost:48656/notification/mapred?jobId=job_1352199715842_0002&amp;jobStatus=ERROR 
(here we can see ""ERROR"" status instead of ""FAILED"")
After that the notification servlet receives either only ""ERROR"" notification, or one more notification ""ERROR"" after ""FAILED"", which finally causes the test to fail. (Some variation in the test behavior caused by racing conditions because there are many asynchronous processings there, and the test is flaky, in fact).

In any way, it looks like the root cause of the problem is the possibility of the forbidden transition ""Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED"". 
Need an expert advice on how that should be fixed.",jlowe,iveselovsky,Major,Closed,Fixed,06/Nov/12 11:28,03/Sep/14 23:17
Bug,MAPREDUCE-4778,12615210,Fair scheduler event log is only written if directory exists on HDFS,"The fair scheduler event log is supposed to be written to the local filesystem, at {hadoop.log.dir}/fairscheduler.  The event log will not be written unless this directory exists on HDFS.",sandyr,sandyr,Major,Closed,Fixed,07/Nov/12 19:16,15/Feb/13 13:10
Bug,MAPREDUCE-4780,12615254,MapReduce distribution build fails on Windows,Distribution build relies on sh scripts that do not work on Windows.,cnauroth,cnauroth,Major,Resolved,Fixed,07/Nov/12 23:49,12/Nov/12 20:55
Bug,MAPREDUCE-4782,12615371,NLineInputFormat skips first line of last InputSplit,"NLineInputFormat creates FileSplits that are then used by LineRecordReader to generate Text values. To deal with an idiosyncrasy of LineRecordReader, the begin and length fields of the FileSplit are constructed differently for the first FileSplit vs. the rest.

After looping through all lines of a file, the final FileSplit is created, but the creation does not respect the difference of how the first vs. the rest of the FileSplits are created.

This results in the first line of the final InputSplit being skipped. I've created a patch to NLineInputFormat, and this fixes the problem.",mark.fuhs,mark.fuhs,Blocker,Closed,Fixed,08/Nov/12 19:54,10/Mar/15 04:30
Bug,MAPREDUCE-4784,12615621,TestRecovery occasionally fails,"TestRecovery is occasionally failing with this error:

{noformat}
testCrashed(org.apache.hadoop.mapreduce.v2.app.TestRecovery): TaskAttempt state is not correct (timedout) expected:<FAILED> but was:<STARTING>
{noformat}",haibochen,jlowe,Major,Resolved,Fixed,10/Nov/12 23:41,30/Aug/16 14:20
Bug,MAPREDUCE-4785,12615622,TestMRApp occasionally fails,"TestMRApp is failing occasionally with this error:

{noformat}
testUpdatedNodes(org.apache.hadoop.mapreduce.v2.app.TestMRApp): Expecting 2 more completion events for killed expected:<4> but was:<2>
{noformat}",haibochen,jlowe,Major,Closed,Fixed,10/Nov/12 23:58,06/Jan/17 01:00
Bug,MAPREDUCE-4786,12615680,Job End Notification retry interval is 5 milliseconds by default,"Courtesy [~stevenwillis] and [~qwertymaniac]
{quote}
From: Harsh J
I believe the configs of the latter of both of the above
classifications were meant to be added in as replacement names, but
the property names got added in wrong (as the former/older named ones)
in the XML.

the word ""seconds"" in the description of retries? The code in MR2's
JobEndNotifier seems to expect seconds but uses it directly in
Thread.sleep(…) without making it milliseconds, which may be a bug we
need to fix as well, perhaps in a same issue as the configs ones.

On Fri, Nov 9, 2012 at 11:21 PM, Steven Willis <swillis@compete.com> wrote:
> And I noticed that there are some duplicate properties with different values and different descriptions:
{quote}",raviprak,raviprak,Major,Closed,Fixed,12/Nov/12 06:25,12/May/16 18:23
Bug,MAPREDUCE-4787,12615735,TestJobMonitorAndPrint is broken,"Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.169 sec <<< FAILURE!
testJobMonitorAndPrint(org.apache.hadoop.mapreduce.TestJobMonitorAndPrint)  Time elapsed: 1105 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.TestJobMonitorAndPrint.testJobMonitorAndPrint(TestJobMonitorAndPrint.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
",robsparker,raviprak,Major,Closed,Fixed,12/Nov/12 15:04,12/May/16 18:23
Bug,MAPREDUCE-4790,12615793,MapReduce build script would be more readable using abspath,"This is a follow-up to MAPREDUCE-4780, which was resolved before addressing some feedback to use abspath instead of normpath for improved readability in the build script.",cnauroth,cnauroth,Major,Resolved,Fixed,12/Nov/12 20:53,13/Nov/12 22:20
Bug,MAPREDUCE-4792,12615939,Unit Test TestJobTrackerRestartWithLostTracker fails with ant-1.8.4,"Problem:
JUnit tag @Ignore is not recognized since the testcase is JUnit3 and not JUnit4:
Solution:
Migrate the testcase to JUnit4, including:
* Remove extends TestCase""
* Remove import junit.framework.TestCase;
* Add import org.junit.*; 
* Use appropriate annotations such as @After, @Before, @Test.",asanjar,asanjar,Major,Closed,Fixed,13/Nov/12 18:46,03/Dec/12 07:33
Bug,MAPREDUCE-4793,12615943,Problem with adding resources when using both -files and -file to hadoop streaming,"
It seems when
both -files and -file are present, it will trigger this IAE, and the error
message is just misleading. 

hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming.jar
-files
hdfs://host:port/user/foo/access.log#test
-input 'input' -output 'output' -mapper ""egrep '.*'"" -file tmp.file

Below is the error message
12/11/02 07:37:54 INFO mapreduce.JobSubmitter: Cleaning up the staging area
/user/haiyang/.staging/job_1351804437209_0575
Exception in thread ""main"" java.lang.IllegalArgumentException: Resource name
must be relative
        at
org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(MRApps.java:383)
        at
org.apache.hadoop.mapreduce.v2.util.MRApps.setupDistributedCache(MRApps.java:324)
        at
org.apache.hadoop.mapred.YARNRunner.createApplicationSubmissionContext(YARNRunner.java:419)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:288)",jlowe,tgraves,Major,Resolved,Fixed,13/Nov/12 19:14,22/Dec/12 13:21
Bug,MAPREDUCE-4794,12615994,DefaultSpeculator generates error messages on normal shutdown,"DefaultSpeculator can log the following error message on a normal shutdown of the ApplicationMaster:

{noformat}
2012-11-13 01:35:31,841 ERROR [DefaultSpeculator background processing] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: Background thread returning, interrupted : java.lang.InterruptedException
{noformat}

and in addition for some reason it logs the corresponding backtrace to stdout.

Like the errors fixed in MAPREDUCE-4741, this error message in the syslog and backtrace on stdout can be confusing to users as to whether the job really succeeded.",jlowe,jlowe,Major,Closed,Fixed,14/Nov/12 01:05,03/Sep/14 22:57
Bug,MAPREDUCE-4797,12616007,LocalContainerAllocator can loop forever trying to contact the RM,"If LocalContainerAllocator has trouble communicating with the RM it can end up retrying forever if the nature of the error is not a YarnException.

This can be particulary bad if the connection went down because the cluster was reset such that the RM and NM have lost track of the process and therefore nothing else will eventually kill the process.  In this scenario, the looping AM continues to pelt the RM with connection requests every second using a stale token, and the RM logs the SASL exceptions over and over.",jlowe,jlowe,Major,Closed,Fixed,14/Nov/12 02:15,03/Sep/14 23:17
Bug,MAPREDUCE-4798,12616011,TestJobHistoryServer fails some times with 'java.lang.AssertionError: Address already in use',"UT Failure in IHC 1.0.3: org.apache.hadoop.mapred.TestJobHistoryServer. This UT fails sometimes.

The error message is:
'Testcase: testHistoryServerStandalone took 5.376 sec
	Caused an ERROR
Address already in use
java.lang.AssertionError: Address already in use
	at org.apache.hadoop.mapred.TestJobHistoryServer.testHistoryServerStandalone(TestJobHistoryServer.java:113)'",sam liu,sam liu,Minor,Closed,Fixed,14/Nov/12 03:08,28/Jan/14 05:42
Bug,MAPREDUCE-4800,12616091,Cleanup o.a.h.mapred.MapTaskStatus - remove unused code,"o.a.h.mapred.MapTaskStatus gets and sets sortFinishTime which is not accessed anywhere.

Also setFinishTime should getter/setter instead of accessing fields directly.",kasha,kasha,Minor,Resolved,Fixed,14/Nov/12 18:00,03/Nov/14 18:33
Bug,MAPREDUCE-4801,12616298,ShuffleHandler can generate large logs due to prematurely closed channels,"We ran into an instance where many nodes on a cluster ran out of disk space because the nodemanager logs were huge.  Examining the logs showed many, many shuffle errors due to either ClosedChannelException or IOException from ""Connection reset by peer"" or ""Broken pipe"".
",jlowe,jlowe,Critical,Closed,Fixed,15/Nov/12 20:04,03/Sep/14 23:17
Bug,MAPREDUCE-4806,12616502,Cleanup: Some (5) private methods in JobTracker.RecoveryManager are not used anymore after MAPREDUCE-3837,"MAPREDUCE-3837 re-organized the job recovery code, moving out the code that was using the methods in RecoveryManager.

Now, the following methods in {{JobTracker.RecoveryManager}}seem to be unused:
# {{updateJob()}}
# {{updateTip()}}
# {{createTaskAttempt()}}
# {{addSuccessfulAttempt()}}
# {{addUnsuccessfulAttempt()}}",kasha,kasha,Major,Closed,Fixed,16/Nov/12 23:32,03/Nov/14 18:33
Bug,MAPREDUCE-4813,12617119,AM timing out during job commit,"The AM calls the output committer's {{commitJob}} method synchronously during JobImpl state transitions, which means the JobImpl write lock is held the entire time the job is being committed.  Holding the write lock prevents the RM allocator thread from heartbeating to the RM.  Therefore if committing the job takes too long (e.g.: the job has tons of files to commit and/or the namenode is bogged down) then the AM appears to be unresponsive to the RM and the RM kills the AM attempt.",jlowe,jlowe,Critical,Closed,Fixed,21/Nov/12 15:31,03/Sep/14 23:25
Bug,MAPREDUCE-4817,12617253,Hardcoded task ping timeout kills tasks localizing large amounts of data,"When a task is launched and spends more than 5 minutes localizing files, the AM will kill the task due to ping timeout.  The AM's TaskHeartbeatHandler currently tracks tasks via a progress timeout and a ping timeout.  The progress timeout can be controlled via mapreduce.task.timeout and even disabled by setting the property to 0.  The ping timeout, however, is hardcoded to 5 minutes and cannot be configured.  Therefore if the task takes too long localizing, it never gets running in order to ping back to the AM and the AM kills it due to ping timeout.",tgraves,jlowe,Critical,Closed,Fixed,22/Nov/12 01:55,03/Sep/14 23:25
Bug,MAPREDUCE-4819,12617668,AM can rerun job after reporting final job status to the client,"If the AM reports final job status to the client but then crashes before unregistering with the RM then the RM can run another AM attempt.  Currently AM re-attempts assume that the previous attempts did not reach a final job state, and that causes the job to rerun (from scratch, if the output format doesn't support recovery).

Re-running the job when we've already told the client the final status of the job is bad for a number of reasons.  If the job failed, it's confusing at best since the client was already told the job failed but the subsequent attempt could succeed.  If the job succeeded there could be data loss, as a subsequent job launched by the client tries to consume the job's output as input just as the re-attempt starts removing output files in preparation for the output commit.",bikassaha,jlowe,Blocker,Closed,Fixed,26/Nov/12 19:03,03/Sep/14 23:25
Bug,MAPREDUCE-4825,12617857,JobImpl.finished doesn't expect ERROR as a final job state,"TestMRApp.testJobError is causing AsyncDispatcher to exit with System.exit due to an exception being thrown.  From the console output from testJobError:

{noformat}
2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000
2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread
java.lang.IllegalArgumentException: Illegal job state: ERROR
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
	at java.lang.Thread.run(Thread.java:662)
2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye..
{noformat}
",jlowe,jlowe,Major,Closed,Fixed,27/Nov/12 19:01,03/Sep/14 23:25
Bug,MAPREDUCE-4832,12618217,MR AM can get in a split brain situation,"It is possible for a networking issue to happen where the RM thinks an AM has gone down and launches a replacement, but the previous AM is still up and running.  If the previous AM does not need any more resources from the RM it could try to commit either tasks or jobs.  This could cause lots of problems where the second AM finishes and tries to commit too.  This could result in data corruption.  ",jlowe,revans2,Critical,Closed,Fixed,29/Nov/12 20:15,15/Feb/13 13:09
Bug,MAPREDUCE-4833,12618227,Task can get stuck in FAIL_CONTAINER_CLEANUP,"If an NM goes down and the AM still tries to launch a container on it the ContainerLauncherImpl can get stuck in an RPC timeout.  At the same time the RM may notice that the NM has gone away and inform the AM of this, this triggers a TA_FAILMSG.  If the TA_FAILMSG arrives at the TaskAttemptImpl before the TA_CONTAINER_LAUNCH_FAILED message then the task attempt will try to kill the container, but the ContainerLauncherImpl will not send back a TA_CONTAINER_CLEANED event causing the attempt to be stuck.",robsparker,revans2,Critical,Closed,Fixed,29/Nov/12 20:53,15/Feb/13 13:10
Bug,MAPREDUCE-4836,12618343,Elapsed time for running tasks on AM web UI tasks page is 0,Yeah! The summary,raviprak,raviprak,Major,Closed,Fixed,30/Nov/12 17:16,12/May/16 18:22
Bug,MAPREDUCE-4842,12618634,Shuffle race can hang reducer,"Saw an instance where the shuffle caused multiple reducers in a job to hang.  It looked similar to the problem described in MAPREDUCE-3721, where the fetchers were all being told to WAIT by the MergeManager but no merge was taking place.",masokan,jlowe,Blocker,Closed,Fixed,03/Dec/12 21:09,08/Jun/16 20:21
Bug,MAPREDUCE-4843,12618731,"When using DefaultTaskController, JobLocalizer not thread safe","In our cluster, some times job will failed due to below exception:
2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)
	at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)
	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)
	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)

The root cause is JobLocalizer is not thread safe.
In DefaultTaskController.initializeJob method:
     JobLocalizer localizer = new JobLocalizer((JobConf)getConf(), user, jobid);
but in JobLocalizer, it just simply keep the reference of the conf.
When two TaskLauncher threads(mapLauncher and reduceLauncher) try to initializeJob at same time, it will have two JobLocalizer, but only one conf instance.
So some times ttConf.setStrings(JOB_LOCAL_CTXT, localDirs) will reset previous job's conf.
Then it will cause the previous job's job.xml stored at another user's dir.",kasha,zhaoyunjiong,Critical,Closed,Fixed,04/Dec/12 13:39,03/Nov/14 18:05
Bug,MAPREDUCE-4844,12618772,Counters / AbstractCounters have constant references not declared final,"Counters have a number of immutable fields that have not been declared 'final'.

For example, the field groups is not final. It is, however, accessed in a couple of methods that are declared 'synchronized'. While there is a happens-before relationship between these methods calls, there is none between the Counters object initialization and these synchronized methods.",brahmareddy,jira.shegalov,Major,Resolved,Fixed,04/Dec/12 19:23,30/Aug/16 01:20
Bug,MAPREDUCE-4848,12618824,TaskAttemptContext cast error during AM recovery,"Recently saw an AM that failed and tried to recover, but the subsequent attempt quickly exited with its own failure during recovery:

{noformat}
2012-12-05 02:33:36,752 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext
	at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)
	at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)
	at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:619)
2012-12-05 02:33:36,752 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
{noformat}

The RM then launched a third AM attempt which succeeded. The third attempt saw basically no progress after parsing the history file from the second attempt and ran the job again from scratch.",jerrychenhf,jlowe,Major,Closed,Fixed,05/Dec/12 04:01,15/Feb/13 13:10
Bug,MAPREDUCE-4850,12618900,Job recovery may fail if staging directory has been deleted,"The job staging directory is deleted in the job cleanup task, which happens before the job-info file is deleted from the system directory (by the JobInProgress garbageCollect() method). If the JT shuts down between these two operations, then when the JT restarts and tries to recover the job, it fails since the job.xml and splits are no longer available.",tomwhite,tomwhite,Major,Closed,Fixed,05/Dec/12 16:15,15/May/13 05:15
Bug,MAPREDUCE-4856,12622839,TestJobOutputCommitter uses same directory as TestJobCleanup,This can cause problems if one of the tests fails to delete.,sandyr,sandyr,Major,Closed,Fixed,07/Dec/12 00:50,15/Feb/13 13:10
Bug,MAPREDUCE-4858,12622870,TestWebUIAuthorization fails on branch-1,TestWebUIAuthorization fails on branch-1,acmurthy,acmurthy,Major,Closed,Fixed,07/Dec/12 07:14,06/Mar/13 09:55
Bug,MAPREDUCE-4859,12622945,TestRecoveryManager fails on branch-1,Looks like the tests are extremely flaky and just hang.,acmurthy,acmurthy,Major,Closed,Fixed,07/Dec/12 18:36,06/Mar/13 09:55
Bug,MAPREDUCE-4860,12622957,DelegationTokenRenewal attempts to renew token even after a job is removed,"mapreduce.security.token.DelegationTokenRenewal synchronizes on removeDelegationToken, but fails to synchronize on addToken, and renewing tokens in run().

This inconsistency is exposed by frequent failures of TestDelegationTokenRenewal:
{noformat}
Error Message

renew wasn't called as many times as expected expected:<4> but was:<5>
Stacktrace

junit.framework.AssertionFailedError: renew wasn't called as many times as expected expected:<4> but was:<5>
	at org.apache.hadoop.mapreduce.security.token.TestDelegationTokenRenewal.testDTRenewal(TestDelegationTokenRenewal.java:317)
	at org.apache.hadoop.mapreduce.security.token.TestDelegationTokenRenewal.testDTRenewalAfterClose(TestDelegationTokenRenewal.java:338)

{noformat}
",kasha,kasha,Major,Closed,Fixed,07/Dec/12 19:36,03/Nov/14 18:33
Bug,MAPREDUCE-4861,12622977,Cleanup: Remove unused mapreduce.security.token.DelegationTokenRenewal,"mapreduce.security.token.DelegationTokenRenewal doesn't seem to be used in branch-2 at all. grep on trunk yields no results, not even ReflectionUtils related suff.",kasha,kasha,Major,Closed,Fixed,07/Dec/12 21:55,03/Nov/14 18:33
Bug,MAPREDUCE-4869,12623337,TestMapReduceChildJVM fails in branch-trunk-win,"The YARN-233 patch for getting YARN working on Windows forgot to include a corresponding change in {{TestMapReduceChildJVM}}, so the test is failing now.",cnauroth,cnauroth,Major,Resolved,Fixed,11/Dec/12 06:44,28/Dec/12 04:31
Bug,MAPREDUCE-4870,12623409,TestMRJobsWithHistoryService causes infinite loop if it fails,"{{TestMRJobsWithHistoryService#testJobHistoryData}} has a periodic poll and sleep after job execution, checking for the application state to reach {{RMAppState#FINISHED}}.  If the job fails, then the application could be in a different terminal state, and this polling loop will never terminate.",cnauroth,cnauroth,Major,Resolved,Fixed,11/Dec/12 17:14,12/May/16 18:23
Bug,MAPREDUCE-4871,12623476,AM uses mapreduce.jobtracker.split.metainfo.maxsize but mapred-default has mapreduce.job.split.metainfo.maxsize,"When the user needs to configure a larger split metainfo file size, mapred-default.xml points to the mapreduce.job.split.metainfo.maxsize property.  However the ApplicationMaster actually uses the mapreduce.*jobtracker*.split.metainfo.maxsize property when determining the largest allowed size.  This leads to much confusion on the part of end-users trying to increase the allowed limit.",jlowe,jlowe,Major,Closed,Fixed,12/Dec/12 00:53,03/Sep/14 22:57
Bug,MAPREDUCE-4879,12623894,TeraOutputFormat may overwrite an existing output directory,"Unlike FileOutputFormat, TeraOutputFormat does not prevent TeraGen/Sort jobs from writing into an existing directory, and potentially overwriting previous runs.",jira.shegalov,jira.shegalov,Major,Closed,Fixed,14/Dec/12 06:32,10/Apr/15 20:19
Bug,MAPREDUCE-4884,12624237,"streaming tests fail to start MiniMRCluster due to ""Queue configuration missing child queue names for root""","Multiple tests in hadoop-streaming, such as {{TestFileArgs}}, fail to initialize {{MiniMRCluster}} due to a {{YarnException}} with reason ""Queue configuration missing child queue names for root"".",cnauroth,cnauroth,Major,Closed,Fixed,17/Dec/12 06:43,12/May/16 18:24
Bug,MAPREDUCE-4885,12618924,Streaming tests have multiple failures on Windows,"There are multiple test failures due to ""Queue configuration missing child queue names for root"".",cnauroth,cnauroth,Major,Closed,Fixed,05/Dec/12 18:37,12/May/16 18:22
Bug,MAPREDUCE-4888,12624491,NLineInputFormat drops data in 1.1 and beyond,"When trying to root cause why MAPREDUCE-4782 did not cause us issues on 1.0.2, I found out that HADOOP-7823 introduced essentially the exact same error into org.apache.hadoop.mapred.lib.NLineInputFormat.

In 1.X org.apache.hadoop.mapred.lib.NLineInputFormat and org.apache.hadoop.mapreduce.lib.input.NLineInputFormat are separate implementations.  The latter had an off by one error in it until MAPREDUCE-4782 fixed it. The former had no error in it until HADOOP-7823 introduced it in 1.1 and MAPREDUCE-375 combined the implementations together but picked the implementation with the off by one error in 0.21.

I will attach a patch that exposes the error.",vinodkv,revans2,Blocker,Closed,Fixed,18/Dec/12 16:50,06/Mar/13 09:55
Bug,MAPREDUCE-4890,12624566,Invalid TaskImpl state transitions when task fails while speculating,"There are a couple of issues when a task fails while speculating (i.e.: multiple attempts are active):

# The other active attempts are not killed.
# TaskImpl's FAILED state does not handle the T_ATTEMPT_* set of events which can be sent from the other active attempts.  These all need to be handled since they can be sent asynchronously from the other active task attempts.

Failure to handle this properly means jobs that are configured to normally tolerate failures via mapreduce.map.failures.maxpercent or mapreduce.reduce.failures.maxpercent and also speculate can easily end up failing due to invalid state transitions rather than complete successfully with a few explicitly allowed task failures.",jlowe,jlowe,Critical,Closed,Fixed,19/Dec/12 02:52,15/Feb/13 13:09
Bug,MAPREDUCE-4892,12624700,CombineFileInputFormat node input split can be skewed on small clusters,The CombineFileInputFormat split generation logic tries to group blocks by node in order to create splits. It iterates through the nodes and creates splits on them until there aren't enough blocks left on a node that can be grouped into a valid split. If the first few nodes have a lot of blocks on them then they can end up getting a disproportionately large share of the total number of splits created. This can result in poor locality of maps. This problem is likely to happen on small clusters where its easier to create a skew in the distribution of blocks on nodes.,bikassaha,bikassaha,Major,Closed,Fixed,19/Dec/12 19:57,27/Aug/13 22:21
Bug,MAPREDUCE-4893,12624704,MR AppMaster can do sub-optimal assignment of containers to map tasks leading to poor node locality,"Say the MR AppMaster asks the RM for 3 containers on nodes n1, n2 and n3. There are 10 node n1-n10 in the same rack. The RM can give it allocated containers in the list order n5, n2, n1. The way AM map->container assignment happens, the AM will try to assign node local maps to n5, failing which it will assign rack local maps to n5. These rack local maps could be node local on n2 and n1 and would have been assigned to containers on n1 and n2 if the AM had not made an early rack local match for them on n5. This can lead to poor locality.",bikassaha,bikassaha,Major,Closed,Fixed,19/Dec/12 20:02,13/Nov/13 15:40
Bug,MAPREDUCE-4894,12624781,Renewal / cancellation of JobHistory tokens,Equivalent of YARN-50 for JobHistory tokens.,sseth,sseth,Blocker,Closed,Fixed,20/Dec/12 08:27,03/Sep/14 23:25
Bug,MAPREDUCE-4895,12624805,Fix compilation failure of org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators,"Task https://issues.apache.org/jira/browse/YARN-223 breaks compilation of 'Apache Hadoop Gridmix' on branch-2.
There is an import of class org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.ProcResourceValues. 
Class ProcResourceValues were removed by YARN-223.

this patch removes this import.

applicable to branch-2

",dennisyv,dennisyv,Major,Closed,Fixed,20/Dec/12 11:24,15/Feb/13 13:10
Bug,MAPREDUCE-4896,12624878,"""mapred queue -info"" spits out ugly exception when queue does not exist",,sandyr,sandyr,Major,Closed,Fixed,20/Dec/12 19:41,27/Aug/13 22:21
Bug,MAPREDUCE-4898,12624927,FileOutputFormat.checkOutputSpecs and FileOutputFormat.setOutputPath incompatible with MR1,"In MR1, {{org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs}} throws {{org.apache.hadoop.mapred.FileAlreadyExistsException}} but now it throws {{org.apache.hadoop.fs.FileAlreadyExistsException}} instead, making them incompatible.  

In MR1, {{org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath}} doesn't throw any exceptions but now it throws an {{IOException}}, making them incompatible.  ",rkanter,rkanter,Major,Closed,Fixed,21/Dec/12 01:24,27/Aug/13 22:21
Bug,MAPREDUCE-4902,12625278,"Fix typo ""receievd"" should be ""received"" in log output","Noticed a typo in the log output, ""receievd"" should be ""received"" 

org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1356131733318_0002&reduce=0&map=attempt_1356131733318_0002_m_000001_0,attempt_1356131733318_0002_m_000003_0,attempt_1356131733318_0002_m_000000_0 sent hash and receievd reply

",chu11,chu11,Trivial,Closed,Fixed,26/Dec/12 18:21,12/May/16 18:24
Bug,MAPREDUCE-4904,12625399,TestMultipleLevelCaching failed in branch-1,"TestMultipleLevelCaching will failed:
{noformat}
Testcase: testMultiLevelCaching took 30.406 sec
        FAILED
Number of local maps expected:<0> but was:<1>
junit.framework.AssertionFailedError: Number of local maps expected:<0> but was:<1>
        at org.apache.hadoop.mapred.TestRackAwareTaskPlacement.launchJobAndTestCounters(TestRackAwareTaskPlacement.java:78)
        at org.apache.hadoop.mapred.TestMultipleLevelCaching.testCachingAtLevel(TestMultipleLevelCaching.java:113)
        at org.apache.hadoop.mapred.TestMultipleLevelCaching.testMultiLevelCaching(TestMultipleLevelCaching.java:69)

{noformat}",junping_du,mgong@vmware.com,Major,Closed,Fixed,28/Dec/12 07:28,15/May/13 05:16
Bug,KAFKA-233,12537090,"The producer's load balancing logic can send requests to dead brokers, when using the async producer option","The ZK producer, when used with the async producer option does the following 

1. Create a pool of async producers, one each for a broker registered under /broker/ids
2. On each send request, apply the Partitioner, to decide the broker and partition to send the data
3. Use the Async producer's send API to enqueue that data into the async producer's queue
4. When the data is dequeued by the ProducerSendThread, use the underlying sync producer to send it to the broker

The load balancing decision is taken in step 2, before entering the queue. This leaves a window of error, equal to the queue length, when a broker can go down. When this happens, potentially, a queue worth of data can fail to reach a broker, and will be dropped by the EventHandler. 

To correct this, the Producer, with the async option, needs to be refactored to allow only a single queue to hold all requests. And the application of the Partitioner should be moved to the end of the queue, in the EventHandler.",,nehanarkhede,Major,Closed,Fixed,03/Jan/12 19:08,19/Jun/14 05:16
Bug,KAFKA-241,12537450,ConsumerIterator throws a IllegalStateException after a ConsumerTimeout occurs,"Please find the test case attached.

After a timeout occurs (property consumer.timeout.ms > 0 ) the consumerIterator throws an IllegalStateException.

The work around seems to be to recreate the MessageStream an issue a new Iterator.",junrao,patricioe,Major,Closed,Fixed,06/Jan/12 00:55,19/Jun/14 05:16
Bug,KAFKA-247,12538348,max.message.size and fetch.size defaults should be consistent,"The default max.message.size for a producer is ~976kB. The default fetch.size for a consumer is 300kB. Having the default fetch.size less than the default max.message.size causes new users with messages larger than fetch.size to run into the InvalidMessageSizeException issue.

Making the default max.message.size less than or equal to the default fetch.size would eliminate that problem for most new setups.",pyritschard,bmatheny,Minor,Closed,Fixed,13/Jan/12 18:21,19/Jun/14 05:16
Bug,KAFKA-256,12539814,Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions,"There is a bug in the consumer rebalancing logic that makes a consumer not pull data from some partitions for a topic. It recovers only after the consumer group is restarted and doesn't hit this bug again.

Here is the observed behavior of the consumer when it hits the bug -

1. Consumer is consuming 2 topics with 1 partition each on 2 brokers
2. Broker 2 is bounced
3. Rebalancing operation triggers for topic_2, where the consumer decides to now consume data only from Broker 1 for topic_2
4. During the rebalancing operation, ZK has not yet deleted the /brokers/topics/topic_1/broker_2, so the consumer still decides to consumer from both brokers for topic_1
5. While restarting the fetchers, it tries to restart fetcher for broker 2 and throws a RuntimeException. Before this, it has successfully started fetcher for broker 1 and is consuming data from broker_1
6. This exception trickles all the way upto syncedRebalance API and the oldPartitionsPerTopicMap does not get updated to reflect that for topic_2, the consumer has now seen only broker_1. It still points to topic_2 -> broker_1, broker_2
7. Next rebalancing attempt gets triggered
8. By now, broker 2 is restarted and registered in zookeeper
9. For topic_2, the consumer tries to see if rebalancing needs to be done. Since it doesn't see a change in the cached topic partition map, it decides there is no need to rebalance.
10. It continues fetching only from broker_1
",nehanarkhede,nehanarkhede,Critical,Resolved,Fixed,25/Jan/12 19:32,03/Feb/12 01:13
Bug,KAFKA-259,12540545,Give better error message when trying to run shell scripts without having built/downloaded the jars yet,"Hi there, I've cloned from the kafka github repo and tried to run the start server script:

 ./bin/kafka-server-start.sh config/server.properties 

Which results in:

Exception in thread ""main"" java.lang.NoClassDefFoundError: kafka/Kafka
Caused by: java.lang.ClassNotFoundException: kafka.Kafka
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)

It seems that Im missing a build step? what have I forgotten to do?

Thanks in advance and I look forward to using kafka.

regards
rcdh",,rossc,Minor,Resolved,Fixed,31/Jan/12 12:40,30/May/13 03:27
Bug,KAFKA-261,12540838,Corrupted request shuts down the broker,"Currently, a corrupted produce request brings down the broker. Instead, we should just log it and let it go.",junrao,junrao,Major,Resolved,Fixed,01/Feb/12 23:40,02/Feb/12 17:34
Bug,KAFKA-262,12540955,Bug in the consumer rebalancing logic causes one consumer to release partitions that it does not own,"The consumer maintains a cache of topics and partitions it owns along with the fetcher queues corresponding to those. But while releasing partition ownership, this cache is not cleared. This leads the consumer to release a partition that it does not own any more. This can also lead the consumer to commit offsets for partitions that it no longer consumes from. 

The rebalance operation goes through following steps -

1. close fetchers
2. commit offsets
3. release partition ownership. 
4. rebalance, add topic, partition and fetcher queues to the topic registry, for all topics that the consumer process currently wants to own. 
5. If the consumer runs into conflict for one topic or partition, the rebalancing attempt fails, and it goes to step 1.

Say, there are 2 consumers in a group, c1 and c2. Both are consuming topic1 with partitions 0-0, 0-1 and 1-0. Say c1 owns 0-0 and 0-1 and c2 owns 1-0.

1. Broker 1 goes down. This triggers rebalancing attempt in c1 and c2.
2. c1's release partition ownership and during step 4 (above), fails to rebalance.
3. Meanwhile, c2 completes rebalancing successfully, and owns partition 0-1 and starts consuming data.
4. c1 starts next rebalancing attempt and during step 3 (above), it releases partition 0-1. During step 4, it owns partition 0-0 again, and starts consuming data.
5. Effectively, rebalancing has completed successfully, but there is no owner for partition 0-1 registered in Zookeeper.
",nehanarkhede,nehanarkhede,Major,Closed,Fixed,02/Feb/12 17:36,20/Oct/14 14:08
Bug,KAFKA-273,12542954,Occassional GZIP errors on the server while writing compressed data to disk,"Occasionally, we see the following errors on the Kafka server -

2012/02/08 14:58:21.832 ERROR [KafkaRequestHandlers] [kafka-processor-6] [kafka] Error processing MultiProducerRequest on NusImpressionSetEvent:0
java.io.EOFException: Unexpected end of ZLIB input stream
        at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:223)
        at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:141)
        at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:92)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at kafka.message.GZIPCompression.read(CompressionUtils.scala:52)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply$mcI$sp(CompressionUtils.scala:143)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)
        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)
        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)
        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)
        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)
        at scala.collection.immutable.Stream.foreach(Stream.scala:255)
        at kafka.message.CompressionUtils$.decompress(CompressionUtils.scala:143)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:119)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:132)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:81)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.message.MessageSet.foreach(MessageSet.scala:87)
        at kafka.log.Log.append(Log.scala:204)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:70)
        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)
        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)
        at kafka.server.KafkaRequestHandlers.handleMultiProducerRequest(KafkaRequestHandlers.scala:63)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)
        at kafka.network.Processor.handle(SocketServer.scala:297)
        at kafka.network.Processor.read(SocketServer.scala:320)
        at kafka.network.Processor.run(SocketServer.scala:215)
        at java.lang.Thread.run(Thread.java:619)
",nehanarkhede,nehanarkhede,Major,Resolved,Fixed,16/Feb/12 22:22,17/Aug/17 12:00
Bug,KAFKA-275,12542981,max.message.size is not enforced for compressed messages,"The max.message.size check is not performed for compressed messages, but only for each message that forms a compressed message. Due to this, even if the max.message.size is set to 1MB, the producer can technically send n 1MB messages as one compressed message. This can cause memory issues on the server as well as deserialization issues on the consumer. The consumer's fetch size has to be > max.message.size in order to be able to read data. If one message is larger than the fetch.size, the consumer will throw an exception and cannot proceed until the fetch.size is increased. 

Due to this bug, even if the fetch.size > max.message.size, the consumer can still get stuck on a message that is larger than max.message.size.",,nehanarkhede,Major,Resolved,Fixed,17/Feb/12 01:56,17/Aug/17 12:02
Bug,KAFKA-277,12543112,"Add a shallow iterator to the ByteBufferMessageSet, which is only used in SynchProducer.verifyMessageSize() function","Shallow iterator just traverse the first level messages of a ByteBufferMessageSet, compressed messages won't be decompressed and treated individually ",yeyangever,yeyangever,Major,Resolved,Fixed,17/Feb/12 22:41,23/Feb/12 22:59
Bug,KAFKA-278,12543113,Issues partitioning a new topic,"There are two cases where correct partitioning fails for a new topic.

Case 1: Topic exists on current Kafka cluster. A new broker is added to the cluster. The new broker will never host partitions for the existing topic.

To reproduce:
1) Create a cluster of brokers along with a ZooKeeper ensemble.
2) Send messages for a topic to the cluster.
3) Add a new broker to the cluster.
4) New broker will never see the existing topic.

Case 2: Topic does not exist on current Kafka cluster. Producer sends messages to a new topic that did not previously exist in the cluster. If, during the producer session, one or more partitions are not created on a broker, the broker will never host those partitions.

To reproduce:
1) Create a cluster of brokers along with a ZooKeeper ensemble.
2) Send messages to a new topic.
3) Shut down the producer before the topic is created on at least one broker.
4) The broker that did not allocate the topic will never host the topic.

My guess(!) here is that when a new producer is created, it gets a list of topics and partitions based on the current state of the brokers in the cluster. Since some brokers are missing the topic, the producer will never send messages to that broker and partitions will never be created.


Work around:
Manually create the topic/partition directories in the kafka logs directory and reboot kafka. It will register the topic/partitions in ZooKeeper.",,mabateman,Minor,Resolved,Fixed,17/Feb/12 22:57,11/Jul/13 22:43
Bug,KAFKA-279,12543270,kafka-console-producer does not take in customized values of --batch-size or --timeout,"1. While the default console-producer, console-consumer paradigm works great, when I try modiying the batch size

bin/kafka-console-producer.sh --batch-size 300   --zookeeper localhost:2181 --topic test1

it gives me a

Exception in thread ""main"" java.lang.NumberFormatException: null
    at java.lang.Integer.parseInt(Integer.java:443)
    at java.lang.Integer.parseInt(Integer.java:514)
    at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:207)
    at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
    at kafka.utils.Utils$.getIntInRange(Utils.scala:189)
    at kafka.utils.Utils$.getInt(Utils.scala:174)
    at kafka.producer.async.AsyncProducerConfigShared$class.$init$(AsyncProducerConfig.scala:45)
    at kafka.producer.ProducerConfig.<init>(ProducerConfig.scala:25)
    at kafka.producer.ConsoleProducer$.main(ConsoleProducer.scala:108)
    at kafka.producer.ConsoleProducer.main(ConsoleProducer.scala)

I have looked at the code and can't figure out what's wrong

2. When I do bin/kafka-console-producer.sh --timeout 30000   --zookeeper localhost:2181 --topic test1

I would think that console-producer would wait for 30s if the batch size (default 200) is not full. It doesn't. It takes the same time without the timeout parameter (default 1000) and dumps whatever the batch size.


Resolution from Jun

1. The code does the following to set batch size
     props.put(""batch.size"", batchSize)
Instead, it should do
     props.put(""batch.size"", batchSize.toString)

2. It sets the wrong property name for timeout. Instead of doing
   props.put(""queue.enqueueTimeout.ms"", sendTimeout.toString)
it should do
   props.put(""queue.time"", sendTimeout.toString)
",junrao,milindparikh,Minor,Resolved,Fixed,19/Feb/12 19:04,21/Mar/12 00:25
Bug,KAFKA-282,12543765,Currently iterated chunk is not cleared during consumer shutdown,"During consumer connector shutdown, fetch queues are cleared, but the currently iterated chunk is not cleared.",jjkoshy,jjkoshy,Major,Resolved,Fixed,22/Feb/12 23:30,25/Jun/15 07:05
Bug,KAFKA-284,12543957,C++ client does not compile,"clients/cpp/src/encoder.hpp has an unclosed comment on line 19.


/*
 * encoder.hpp
 *

Needs to be

/*
 * encoder.hpp
 */
",,nieksand,Trivial,Resolved,Fixed,24/Feb/12 06:14,01/Mar/12 22:24
Bug,KAFKA-286,12544143,consumer sometimes don't release partition ownership properly in ZK during rebalance,,junrao,junrao,Major,Resolved,Fixed,25/Feb/12 22:59,27/Feb/12 19:52
Bug,KAFKA-287,12544314,Running a consumer client using scala 2.8 fails,"Built the kafka library using the instructions found in the README. My client uses scala 2.9.1, sbt 0.11. My consumer client has this snippet of code: https://gist.github.com/a35006cc25e39ba386e2

The client compiles, but running it produces this stacktrace: https://gist.github.com/efeb85f50402b477d6e0

I think this may be because of a bug found in scala 2.9.0 (though I'm not sure if it was present in scala 2.8.0): https://issues.scala-lang.org/browse/SI-4575

To get around this, I built the kafka library using scala 2.9.1 (by changing build.properties).",,elben,Major,Resolved,Fixed,27/Feb/12 21:24,07/Feb/15 23:52
Bug,KAFKA-292,12545024,broker deletes all file segments when cleaning up an empty log segment,"Suppose that a log has only one log segment left and it's empty. If that segment expires and is deleted, we roll out a new segment of the same name. However, the deletion happens after the log is rolled. This will make the log directory empty, which should never happen.",junrao,junrao,Major,Resolved,Fixed,03/Mar/12 01:46,18/Mar/12 18:35
Bug,KAFKA-294,12545274,"""Path length must be > 0"" error during startup","When starting Kafka 0.7.0 using zkclient-0.1.jar, I get this error:

INFO 2012-03-06 02:39:04,072  main kafka.server.KafkaZooKeeper Registering broker /brokers/ids/1
FATAL 2012-03-06 02:39:04,111  main kafka.server.KafkaServer Fatal error during startup.
java.lang.IllegalArgumentException: Path length must be > 0
        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:48)
        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:35)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:620)
        at org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87)
        at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308)
        at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
        at org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304)
        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:213)
        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
        at kafka.utils.ZkUtils$.createParentPath(ZkUtils.scala:48)
        at kafka.utils.ZkUtils$.createEphemeralPath(ZkUtils.scala:60)
        at kafka.utils.ZkUtils$.createEphemeralPathExpectConflict(ZkUtils.scala:72)
        at kafka.server.KafkaZooKeeper.registerBrokerInZk(KafkaZooKeeper.scala:57)
        at kafka.log.LogManager.startup(LogManager.scala:124)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:80)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:47)
        at kafka.Kafka$.main(Kafka.scala:60)
        at kafka.Kafka.main(Kafka.scala)

The problem seems to be this code in ZkClient's createPersistent method:

String parentDir = path.substring(0, path.lastIndexOf('/'));
createPersistent(parentDir, createParents);
createPersistent(path, createParents);

which doesn't check for whether parentDir is an empty string, which it will become for /brokers/ids/1 after two recursions.
",,tomdz,Major,Resolved,Fixed,06/Mar/12 03:47,20/Jul/15 14:41
Bug,KAFKA-295,12545382,Bug in async producer DefaultEventHandler retry logic,"In the DefaultEventHandler's retry loop, the logic should not return after a successful retry.  Rather, it should set a boolean flag indicating that the retry was successful and exit or break the while loop.  In the end it should throw an exception only the flag is false.  Otherwise, it should continue the outer for loop and send remaining data to remaning brokers.",prashanth.menon,prashanth.menon,Major,Resolved,Fixed,06/Mar/12 18:00,02/Apr/12 01:50
Bug,KAFKA-299,12545763,Change broker request and response to use Seqs rather than Array,"The new Produce and Fetch request and response classes use primitive Arrays, but becaue they are case classes and Java's array hashCode/equals functionality is broken, the case class equality contract is broken as well.  We should change the models to use Seqs to resolve the issue along with gaining all the functional benefits that goes along with it.  This change will require appropriate Java versions to convert between Array's and Seqs for Java clients.",prashanth.menon,prashanth.menon,Major,Resolved,Fixed,09/Mar/12 02:56,07/Feb/15 23:51
Bug,KAFKA-305,12546581,SyncProducer does not correctly timeout,So it turns out that using the channel in SyncProducer like we are to perform blocking reads will not trigger socket timeouts (though we set it) and will block forever which is bad.  This bug identifies the issue: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802 and this article presents a potential work-around: http://stackoverflow.com/questions/2866557/timeout-for-socketchannel for workaround. The work-around is a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.,prashanth.menon,prashanth.menon,Critical,Resolved,Fixed,15/Mar/12 13:11,02/Apr/12 01:48
Bug,KAFKA-306,12546609,broker failure system test broken on replication branch,"The system test in system_test/broker_failure is broken on the replication branch. This test is a pretty useful failure injection test that exercises the consumer rebalancing feature, various replication features like leader election. It will be good to have this test fixed as well as run on every checkin to the replication branch",jfung,nehanarkhede,Major,Resolved,Fixed,15/Mar/12 16:33,10/Jul/12 18:06
Bug,KAFKA-307,12546701,Refactor server code to remove interdependencies between LogManager and KafkaZooKeeper,"Currently, LogManager wraps KafkaZooKeeper which is meant for all zookeeper interaction of a Kafka server. With replication, KafkaZookeeper will handle leader election, various state change listeners and then start replicas. Due to interdependency between LogManager and KafkaZookeeper, starting replicas is not possible until LogManager starts up completely. Due to this, we have to separate the broker startup procedures required for replication to get around this problem.

It will be good to refactor and clean up the server code, before diving deeper into replication.",nehanarkhede,nehanarkhede,Major,Resolved,Fixed,16/Mar/12 01:14,23/Mar/12 20:20
Bug,KAFKA-309,12546933,Bug in FileMessageSet's append API can corrupt on disk log,"In FileMessageSet's append API, we write a ByteBufferMessageSet to a log in the following manner -

    while(written < messages.sizeInBytes)
      written += messages.writeTo(channel, 0, messages.sizeInBytes)

In ByteBufferMessageSet, the writeTo API uses buffer.duplicate() to append to a channel -

  def writeTo(channel: GatheringByteChannel, offset: Long, size: Long): Long =
    channel.write(buffer.duplicate)

If the channel doesn't write the ByteBuffer in one call, then we call it again until sizeInBytes bytes are written. But the next call will use buffer.duplicate() to write to the FileChannel, which will write the entire ByteBufferMessageSet again to the file. 

Effectively, we have a corrupted set of messages on disk. 

Thinking about it, FileChannel is a blocking channel, so ideally, the entire ByteBuffer should be written to the FileChannel in one call. I wrote a test (attached here) and saw that it does. But I'm not aware if there are some corner cases when it doesn't do so. In those cases, Kafka will end up corrupting on disk log segment.
",nehanarkhede,nehanarkhede,Critical,Resolved,Fixed,18/Mar/12 01:14,22/Mar/12 16:59
Bug,KAFKA-310,12547095,Incomplete message set validation checks in kafka.log.Log's append API can corrupt on disk log,"The behavior of the ByteBufferMessageSet's iterator is to ignore and return false if some trailing bytes are found that cannot be de serialized into a Kafka message. The append API in Log, iterates through a ByteBufferMessageSet and validates the checksum of each message. Though, while appending data to the log, it just uses the underlying ByteBuffer that forms the ByteBufferMessageSet. Now, due to some bug, if the ByteBuffer has some trailing data, that will get appended to the on-disk log too. This can cause corruption of the log.",nehanarkhede,nehanarkhede,Critical,Resolved,Fixed,19/Mar/12 21:33,22/Mar/12 16:04
Bug,KAFKA-320,12548191,testZKSendWithDeadBroker fails intermittently due to ZKNodeExistsException,"The testZKSendWithDeadBroker inside ProducerTest fails intermittently with the following exception -

[error] Test Failed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)
java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.
        at kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:109)
        at kafka.server.KafkaZooKeeper.kafka$server$KafkaZooKeeper$$registerBrokerInZk(KafkaZooKeeper.scala:60)
        at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:52)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:84)
        at kafka.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:174)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:164)
        at junit.framework.TestCase.runBare(TestCase.java:130)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:120)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)

The test basically restarts a server and fails with this exception during the restart

This is unexpected, since server1, after shutting down, should trigger the deletion of its registration of the broker id from ZK. But, here is the Kafka bug causing this problem -

In the test during server1.shutdown(), we do close the zkClient associated with the broker and it successfully deletes the broker's registration info from Zookeeper. After this, server1 can be succesfully started. Then the test completes and in the teardown(), we call server1.shutdown(). During this, the server doesn't really shutdown, since it is protected with the isShuttingDown variable, which was never set to false in the startup() API. Now, this leads to an open zkclient connection for the current test run. 

If you try to re-run ProducerTest without exiting sbt, it will first bring up the zookeeper server. Then, since the kafka server during the previous run is still running, it can succesfully renew its session with zookeeper, and retain the /brokers/ids/0 ephemeral node. If it does this before server1.startup() is called in the test, the test will fail.

The fix is to set the shutdown related variables correctly in the startup API of KafkaServer. Also, during debugging this, I found that we don't close zkclient in the Producer as well. Due to this, unit tests throw a whole bunch of WARN that look like - 

[2012-03-26 14:14:27,703] INFO Opening socket connection to server nnarkhed-ld /127.0.0.1:2182 (org.apache.zookeeper.ClientCnxn:1061)
[2012-03-26 14:14:27,703] WARN Session 0x13650dbf8dd0005 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
",nehanarkhede,nehanarkhede,Critical,Resolved,Fixed,26/Mar/12 21:27,08/Apr/12 02:00
Bug,KAFKA-321,12548212,Remove dead brokers from ProducerPool,"Currently, the ProducerPool does not remove producers that are tied to dead brokers.  Keeping such producers around can adversely effect normal producer operation by handing them out and failling - one such scenario is when updating cached topic metadata.  It's best if we remove any producers that are tied to dead brokers to avoid such situations.",,prashanth.menon,Major,Resolved,Fixed,27/Mar/12 00:33,07/Feb/15 23:50
Bug,KAFKA-326,12549896,CallbackHandler.afterDequeuingExistingData is not called during event queue timeout,"CallbackHandler.afterDequeuingExistingData is only called when new events are coming and dequeued. It should be called when no new events are coming, but a queue timeout is reached.",junrao,junrao,Major,Resolved,Fixed,06/Apr/12 15:23,06/Apr/12 17:01
Bug,KAFKA-328,12549926,Write unit test for kafka server startup and shutdown API ,"Background discussion in KAFKA-320

People often try to embed KafkaServer in an application that ends up calling startup() and shutdown() repeatedly and sometimes in odd ways. To ensure this works correctly we have to be very careful about cleaning up resources. This is a good practice for making unit tests reliable anyway.

A good first step would be to add some unit tests on startup and shutdown to cover various cases:
1. A Kafka server can startup if it is not already starting up, if it is not currently being shutdown, or if it hasn't been already started
2. A Kafka server can shutdown if it is not already shutting down, if it is not currently starting up, or if it hasn't been already shutdown. ",balaji.seshadri@dish.com,nehanarkhede,Major,Resolved,Fixed,06/Apr/12 19:53,02/Dec/14 23:18
Bug,KAFKA-350,12558226,Enable message replication in the presence of controlled failures,KAFKA-46 introduced message replication feature in the absence of server failures. This JIRA will improve the log recovery logic and fix other bugs to enable message replication to happen in the presence of controlled server failures,nehanarkhede,nehanarkhede,Major,Resolved,Fixed,26/May/12 02:22,24/Jul/12 18:13
Bug,KAFKA-351,12558227,Refactor some new components introduced for replication ,Jay had some good refactoring suggestions as part of the review for KAFKA-46. I'd like to file this umbrella JIRA with individual sub tasks to cover those suggestions,junrao,nehanarkhede,Major,Resolved,Fixed,26/May/12 02:26,30/Oct/17 12:18
Bug,KAFKA-352,12558525,Throw exception to client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created,It will be good to inform the client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created. The exception should be something like UnknownTopicException that is descriptive.,nehanarkhede,nehanarkhede,Major,Resolved,Fixed,30/May/12 02:04,19/Jul/12 19:51
Bug,KAFKA-363,12559886,Replace numerical compression codes in config with something human readable,"Currently we have compression codes like 1 or 2, which is pretty unintuitive. What does 1 mean?

We should replace these with human-readable codes like ""snappy"", ""gzip"", or ""none"" and change the documentation. We can continue to support the existing integer codes in addition for backwards compatibility with existing configurations.",jkreps,jkreps,Minor,Closed,Fixed,08/Jun/12 19:13,03/Oct/12 19:51
Bug,KAFKA-367,12595050,StringEncoder/StringDecoder use platform default character set,"StringEncoder and StringDecoder take the platform default character set. This is bad since the messages they produce are sent off that machine. We should
-- add a new required argument to these that adds the character set and default to UTF-8 rather than the machine setting
-- add a commandline parameter for the console-* tools to let you specify the correct encoding.",initialcontext,jkreps,Major,Closed,Fixed,19/Jun/12 05:13,19/Jun/14 05:16
Bug,KAFKA-370,12595316,"Exception ""java.util.NoSuchElementException: None.get"" appears inconsistently in Mirror Maker log.","Exception in Mirror Maker log:
=========================
[2012-06-20 10:56:04,364] DEBUG Getting broker partition info for topic test01 (kafka.producer.BrokerPartitionInfo)
[2012-06-20 10:56:04,365] INFO Fetching metadata for topic test01 (kafka.producer.BrokerPartitionInfo)
[2012-06-20 10:56:04,366] ERROR Error in handling batch of 200 events (kafka.producer.async.ProducerSendThread)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:185)
        at scala.None$.get(Option.scala:183)
        at kafka.producer.ProducerPool.getAnyProducer(ProducerPool.scala:76)
        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:73)
        at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:45)
        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:129)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:95)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:94)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44)
        at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42)
        at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:94)
        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:49)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:96)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:82)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:60)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:59)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:37)

Steps to reproduce
=================
It cannot be reproduced consistently. However, running the following script 2 or 3 times (step 2) will show the error:

1. Apply kafka-306-v2.patch to 0.8 branch (revision 1352192 is used to reproduce this)

2. Under the directory <kafka home>/system_test/broker_failure, execute the following command:
=> $ bin/run-test.sh 5 0

3. Check the log under the directory <kafka home>/system_test/broker_failure:
=> $ grep Exception `ls kafka_mirror_maker*.log`
=>    kafka_mirror_maker2.log:java.util.NoSuchElementException: None.get

4. Also the kafka log sizes between source and target will not match:

[/tmp]  $ find kafka* -name *.kafka -ls
19400444 6104 -rw-r--r--   1 jfung    eng       6246655 Jun 20 10:56 kafka-source4-logs/test01-0/00000000000000000000.kafka
19400819 5356 -rw-r--r--   1 jfung    eng       5483627 Jun 20 10:56 kafka-target3-logs/test01-0/00000000000000000000.kafka

Notes about the patch kafka-306-v2.patch
===============================
This patch fix the broker_failure test suite to do the followings:

a. Start 4 kafka brokers as source cluster
b. Start 3 kafka brokers as target cluster
c. Start 3 mirror maker to enable mirroring
d. Send n messages to source cluster
e. No bouncing is performed in this test for simplicity
f. After the producer is stopped, validate the data count is matched between source & target
",junrao,jfung,Major,Closed,Fixed,20/Jun/12 18:28,22/Jun/12 20:16
Bug,KAFKA-371,12595368,Creating topic of empty string puts broker in a bad state,"Using the Java client library, I accidentally published a message where the topic name was the empty string. This put the broker in a bad state where publishing became impossible, and the following exception was logged 10-20 times per second:

2012-06-21 00:41:30,324 [kafka-processor-3] ERROR kafka.network.Processor  - Closing socket for /127.0.0.1 because of er
ror
kafka.common.InvalidTopicException: topic name can't be empty
        at kafka.log.LogManager.getOrCreateLog(LogManager.scala:165)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandle
rs.scala:75)
        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:58)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:43)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:43)
        at kafka.network.Processor.handle(SocketServer.scala:289)
        at kafka.network.Processor.read(SocketServer.scala:312)
        at kafka.network.Processor.run(SocketServer.scala:207)
        at java.lang.Thread.run(Thread.java:679)

Restarting Kafka did not help. I had to manually clear out the bad state in Zookeeper to resolve the problem.

The broker should not accept a message that would put it in such a bad state.",jkreps,martinkl,Major,Resolved,Fixed,21/Jun/12 01:09,14/Aug/12 17:23
Bug,KAFKA-372,12595466,Consumer doesn't receive all data if there are multiple segment files,"This issue happens inconsistently but could be reproduced by following the steps below (repeat step 4 a few times to reproduce it):

1. Check out 0.8 branch (currently reproducible with rev. 1352634)

2. Apply kafka-306-v4.patch

3. Please note that the log.file.size is set to 10000000 in system_test/broker_failure/config/server_*.properties (small enough to trigger multi segment files)

4. Under the directory <kafka home>/system_test/broker_failure, execute command:
$ bin/run-test.sh 20 0

5. After the test is completed, the result will probably look like the following:

========================================================
no. of messages published            : 14000
producer unique msg rec'd            : 14000
source consumer msg rec'd            : 7271
source consumer unique msg rec'd     : 7271
mirror consumer msg rec'd            : 6960
mirror consumer unique msg rec'd     : 6960
total source/mirror duplicate msg    : 0
source/mirror uniq msg count diff    : 311
========================================================

6. By checking the kafka log files, the sum of the sizes of the source cluster segments files are equal to those in the target cluster.

[/tmp] $  find kafka* -name *.kafka -ls

18620155 9860 -rw-r--r--   1 jfung    eng      10096535 Jun 21 11:09 kafka-source3-logs/test01-0/00000000000000000000.kafka
18620161 9772 -rw-r--r--   1 jfung    eng      10004418 Jun 21 11:11 kafka-source3-logs/test01-0/00000000000020105286.kafka
18620160 9776 -rw-r--r--   1 jfung    eng      10008751 Jun 21 11:10 kafka-source3-logs/test01-0/00000000000010096535.kafka
18620162 4708 -rw-r--r--   1 jfung    eng       4819067 Jun 21 11:11 kafka-source3-logs/test01-0/00000000000030109704.kafka
19406431 9920 -rw-r--r--   1 jfung    eng      10157685 Jun 21 11:10 kafka-target2-logs/test01-0/00000000000010335039.kafka
19406429 10096 -rw-r--r--   1 jfung    eng      10335039 Jun 21 11:09 kafka-target2-logs/test01-0/00000000000000000000.kafka
19406432 10300 -rw-r--r--   1 jfung    eng      10544850 Jun 21 11:11 kafka-target2-logs/test01-0/00000000000020492724.kafka
19406433 3800 -rw-r--r--   1 jfung    eng       3891197 Jun 21 11:12 kafka-target2-logs/test01-0/00000000000031037574.kafka

7. If the log.file.size in target cluster is configured to a very large value such that there is only 1 data file, the result would look like this:

========================================================
no. of messages published            : 14000
producer unique msg rec'd            : 14000
source consumer msg rec'd            : 7302
source consumer unique msg rec'd     : 7302
mirror consumer msg rec'd            : 13750
mirror consumer unique msg rec'd     : 13750
total source/mirror duplicate msg    : 0
source/mirror uniq msg count diff    : -6448
========================================================

8. The log files are like these:

[/tmp] $ find kafka* -name *.kafka -ls

18620160 9840 -rw-r--r--   1 jfung    eng      10075058 Jun 21 11:24 kafka-source2-logs/test01-0/00000000000010083679.kafka
18620155 9848 -rw-r--r--   1 jfung    eng      10083679 Jun 21 11:23 kafka-source2-logs/test01-0/00000000000000000000.kafka
18620162 4484 -rw-r--r--   1 jfung    eng       4589474 Jun 21 11:26 kafka-source2-logs/test01-0/00000000000030269045.kafka
18620161 9876 -rw-r--r--   1 jfung    eng      10110308 Jun 21 11:25 kafka-source2-logs/test01-0/00000000000020158737.kafka
19406429 34048 -rw-r--r--   1 jfung    eng      34858519 Jun 21 11:26 kafka-target3-logs/test01-0/00000000000000000000.kafka
",,jfung,Major,Resolved,Fixed,21/Jun/12 18:30,26/Jun/12 16:24
Bug,KAFKA-376,12596003,expose different data to fetch requests from the follower replicas and consumer clients,"Currently, the broker always uses highwatermark to calculate the available bytes to a fetch request, no matter where the request is from. Instead, we should use highwatermark for requests coming from real consumer clients and use logendoffset for requests coming from follower replicas.",prashanth.menon,junrao,Major,Closed,Fixed,26/Jun/12 23:40,10/Sep/12 15:29
Bug,KAFKA-377,12596209,some of the dist mirrors are giving a 403,"https://issues.apache.org/jira/browse/INFRA-4975

will make patch to change the links to by pass this",charmalloc,charmalloc,Major,Closed,Fixed,28/Jun/12 04:30,19/Sep/12 16:40
Bug,KAFKA-378,12596255,mirrors but must never be used for verification,"http://incubator.apache.org/guides/releasemanagement.html#understanding-distribution

Mirrored copies of checksums, KEYS and signature files (.asc and .md5 files) will be present on the mirrors but must never be used for verification. So, all links from the podling website to signatures, sums and KEYS need to refer to the original documents on www.apache.org

",,charmalloc,Major,Resolved,Fixed,28/Jun/12 12:49,28/Jun/12 16:14
Bug,KAFKA-379,12596325,TopicCount.constructTopicCount isn't thread-safe,"TopicCount uses scala.util.parsing.json.JSON, which isn't thread-safe https://issues.scala-lang.org/browse/SI-4929

If you have multiple consumers within the same JVM, and they all rebalance at the same time, you can get errors like the following:

[...] kafka.consumer.TopicCount$.constructTopicCount:39] ERROR: error parsing consumer json string [...]
java.lang.NullPointerException
        at scala.util.parsing.combinator.Parsers$NoSuccess.<init>(Parsers.scala:131)
        at scala.util.parsing.combinator.Parsers$Failure.<init>(Parsers.scala:158)
        at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:489)
        ...
        at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:742)
        at scala.util.parsing.json.JSON$.parseRaw(JSON.scala:71)
        at scala.util.parsing.json.JSON$.parseFull(JSON.scala:85)
        at kafka.consumer.TopicCount$.constructTopicCount(TopicCount.scala:32)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$getTopicCount(ZookeeperConsumerConnector.scala:422)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala:460)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:437)
        at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)
        at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:433)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.handleChildChange(ZookeeperConsumerConnector.scala:375)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

I ran into this on 0.7.0, but the code in trunk appears to be vulnerable to the same issue.",junrao,baroquebobcat,Major,Closed,Fixed,28/Jun/12 21:01,18/Sep/12 05:06
Bug,KAFKA-384,12596466,Fix intermittent test failures and remove unnecessary sleeps,"Seeing intermittent failures in 0.8 unit tests. Also, many sleeps can be removed (with producer acks in place) and I think MockTime isn't used in some places where it should.
",nehanarkhede,jjkoshy,Major,Resolved,Fixed,29/Jun/12 22:29,31/Jul/12 23:28
Bug,KAFKA-385,12596467,RequestPurgatory enhancements - expire/checkSatisfy issue; add jmx beans,"As discussed in KAFKA-353:
1 - There is potential for a client-side race condition in the implementations of expire and checkSatisfied. We can just synchronize on the DelayedItem.
2 - Would be good to add jmx beans to facilitate monitoring RequestPurgatory stats.
",jjkoshy,jjkoshy,Major,Closed,Fixed,29/Jun/12 22:40,16/Aug/12 23:35
Bug,KAFKA-386,12596468,Race condition in accessing ISR,"Also brought up in KAFKA-353 - Partition's inSyncReplicas is used by both KafkaApis and ReplicaManager; and is subject to concurrent writes. Should be able to just switch it to an AtomicReference, but need to look at the code more carefully to determine if that is sufficient.
",,jjkoshy,Major,Closed,Fixed,29/Jun/12 22:45,23/Aug/12 16:51
Bug,KAFKA-391,12596693,Producer request and response classes should use maps,"Producer response contains two arrays of error codes and offsets - the ordering in these arrays correspond to the flattened ordering of the request arrays.

It would be better to switch to maps in the request and response as this would make the code clearer and more efficient (right now, linear scans are used in handling producer acks).

We can probably do the same in the fetch request/response.",jjkoshy,jjkoshy,Blocker,Closed,Fixed,02/Jul/12 18:32,07/Nov/14 06:57
Bug,KAFKA-394,12597819,update site with steps and notes for doing a release under developer,steps in release process including updating the dist directory,charmalloc,charmalloc,Major,Resolved,Fixed,06/Jul/12 19:42,25/Nov/13 18:35
Bug,KAFKA-396,12598106,Mirroring system test fails on 0.8,Just making a note of this - will look into this later.,jjkoshy,jjkoshy,Major,Resolved,Fixed,09/Jul/12 22:58,14/Jun/13 03:59
Bug,KAFKA-403,12598739,0.7.1 release notes should point to origin like 0.7.0 does,,charmalloc,charmalloc,Major,Resolved,Fixed,13/Jul/12 18:18,16/Jul/12 18:05
Bug,KAFKA-405,12599038,Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk,KAFKA-46 introduced per partition leader high watermarks. But it stores those in one file per partition. A more performant solution would be to store all high watermarks in a single file on disk,nehanarkhede,nehanarkhede,Major,Resolved,Fixed,16/Jul/12 21:10,01/Aug/12 18:10
Bug,KAFKA-409,12599672,Refactor DefaultEventHandler ,"The code in DefaultEventHandler has evolved to be a big blob of complex procedural logic. This is difficult to understand and read. Particularly the partitionAndCollate() API returns a nested complex data structure Option[Map[Int, Map[(String, Int), Seq[ProducerData[K,Message]]]]]. This class would definitely benefit from a refactor",jkreps,nehanarkhede,Major,Resolved,Fixed,20/Jul/12 20:33,17/May/16 14:09
Bug,KAFKA-412,12600031,deal with empty TopicData list in producer and fetch request,"Both producer and fetch request can pass in an empty list of TopicData. Instead of handling those requests through RequestPurgatory, we should just send a response with an empty list immediately.",junrao,junrao,Major,Resolved,Fixed,24/Jul/12 15:52,27/Jul/12 00:17
Bug,KAFKA-413,12600044,single_host_multi_brokers system test fails on laptop," I got the following exception when running system_test/single_host_multi_brokers/bin/run-test.sh. This seems to only happen on laptop, not desktop.

2012-07-24 00:22:51 cleaning up kafka server log/data dir 
2012-07-24 00:22:53 starting zookeeper 
2012-07-24 00:22:55 starting cluster 
2012-07-24 00:22:55 starting kafka server 
2012-07-24 00:22:55 -> kafka_pids[1]: 75282 
2012-07-24 00:22:55 starting kafka server 
2012-07-24 00:22:55 -> kafka_pids[2]: 75286 
2012-07-24 00:22:55 starting kafka server 
2012-07-24 00:22:55 -> kafka_pids[3]: 75291 
2012-07-24 00:22:57 creating topic [mytest] on [localhost:2181] 
creation failed because of org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids 
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids 
at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) 
at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) 
at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) 
at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) 
at kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:363) 
at kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:80) 
at kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:86) 
at kafka.admin.CreateTopicCommand$.main(CreateTopicCommand.scala:73) 
at kafka.admin.CreateTopicCommand.main(CreateTopicCommand.scala) 
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids 
at org.apache.zookeeper.KeeperException.create(KeeperException.java:102) 
at org.apache.zookeeper.KeeperException.create(KeeperException.java:42) 
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249) 
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277) 
at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99) 
at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416) 
at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413) 
at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) 
... 7 more ",jfung,junrao,Major,Resolved,Fixed,24/Jul/12 18:01,26/Jul/12 16:59
Bug,KAFKA-415,12600197,Controller throws NoSuchElementException while marking a broker failed,"[2012-07-25 11:13:50,078] ERROR Error while removing broker by the controller (kafka.server.ControllerChannelManager:99)
java.util.NoSuchElementException: key not found: 0
        at scala.collection.MapLike$class.default(MapLike.scala:223)
        at scala.collection.mutable.HashMap.default(HashMap.scala:39)
        at scala.collection.MapLike$class.apply(MapLike.scala:134)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:39)
        at kafka.server.ControllerChannelManager.removeBroker(KafkaController.scala:138)
        at kafka.server.ControllerChannelManager$$anonfun$shutDown$3.apply(KafkaController.scala:111)
        at kafka.server.ControllerChannelManager$$anonfun$shutDown$3.apply(KafkaController.scala:110)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.server.ControllerChannelManager.shutDown(KafkaController.scala:110)
        at kafka.server.KafkaController.shutDown(KafkaController.scala:197)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:126)
        at kafka.server.LogRecoveryTest$$anonfun$testHWCheckpointWithFailuresMultipleLogSegments$5.apply(LogRecoveryTest.scala:237)
        at kafka.server.LogRecoveryTest$$anonfun$testHWCheckpointWithFailuresMultipleLogSegments$5.apply(LogRecoveryTest.scala:237)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.List.map(List.scala:45)
        at kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:237)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:164)
        at junit.framework.TestCase.runBare(TestCase.java:130)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:120)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
[info] Test Passed: testHWCheckpointWithFailuresMultipleLogSegments(kafka.server.LogRecoveryTest)
",yeyangever,nehanarkhede,Major,Closed,Fixed,25/Jul/12 18:15,23/Aug/12 18:36
Bug,KAFKA-416,12600198,Controller tests throw several zookeeper errors,"[info] == kafka.controller.ControllerBasicTest ==
[info] Test Starting: testControllerFailOver(kafka.controller.ControllerBasicTest)
[2012-07-25 11:16:06,911] WARN Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running (org.apache.zookeeper.server.NIOServerCnxn:639)
[info] Test Passed: testControllerFailOver(kafka.controller.ControllerBasicTest)
[info] Test Starting: testControllerCommandSend(kafka.controller.ControllerBasicTest)
[2012-07-25 11:16:13,802] WARN Session 0x138bf5a6e12000c for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:13,814] WARN Session 0x138bf5a6e12000a for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:13,822] WARN Session 0x138bf5a6e120008 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:13,866] WARN Session 0x138bf5a6e12000b for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:14,153] WARN Session 0x138bf5a6e120006 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,347] WARN Session 0x138bf5a6e12000c for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,712] WARN Session 0x138bf5a6e12000b for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,810] WARN Session 0x138bf5a6e12000a for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,848] WARN Session 0x138bf5a6e120008 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[info] Test Passed: testControllerCommandSend(kafka.controller.ControllerBasicTest)

These error messages suggest that the controller is not closing zookeeper sessions correctly. It could also mean that the unit test doesn't close zookeeper sessions correctly.",yeyangever,nehanarkhede,Major,Closed,Fixed,25/Jul/12 18:17,25/Aug/12 00:28
Bug,KAFKA-418,12600205,NullPointerException in ConsumerFetcherManager,"[info] Test Starting: testConsumerDecoder(kafka.consumer.ZookeeperConsumerConnectorTest)
[2012-07-25 11:50:23,324] ERROR ConsumerFetcherThread-group1_consumer1-0-1 error in fetching (kafka.consumer.ConsumerFetcherThread:99)
java.lang.NullPointerException
        at kafka.consumer.ConsumerFetcherManager.getPartitionTopicInfo(ConsumerFetcherManager.scala:124)
        at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:36)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5$$anonfun$apply$1.apply(AbstractFetcherThread.scala:97)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5$$anonfun$apply$1.apply(AbstractFetcherThread.scala:89)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5.apply(AbstractFetcherThread.scala:89)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5.apply(AbstractFetcherThread.scala:88)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.server.AbstractFetcherThread.run(AbstractFetcherThread.scala:88)
[2012-07-25 11:50:23,328] ERROR Closing socket for /127.0.0.1 because of error (kafka.network.Processor:99)
",junrao,nehanarkhede,Major,Closed,Fixed,25/Jul/12 18:52,27/Jul/12 00:46
Bug,KAFKA-420,12600349,maintain HW correctly with only 1 replica in ISR,"Currently, the HW maintenance logic is only triggered when handling fetch requests from the follower. As a result, if the ISR has only 1 replica, the HW won't be incremented since there is no request from the follower to trigger the maintenance logic.",junrao,junrao,Major,Closed,Fixed,26/Jul/12 17:12,25/Aug/12 00:47
Bug,KAFKA-421,12600384,LogRecoveryTest has a transient test failure," [0m[ [31merror [0m]  [0mTest Failed: testHWCheckpointWithFailuresMultipleLogSegments(kafka.server.LogRecoveryTest) [0m
java.lang.AssertionError: expected:<120> but was:<150>
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.failNotEquals(Assert.java:314)
        at org.junit.Assert.assertEquals(Assert.java:94)
        at org.junit.Assert.assertEquals(Assert.java:104)
        at kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:164)
        at junit.framework.TestCase.runBare(TestCase.java:130)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:120)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
 [0m[ [0minfo [0m]  [34m== core-kafka / kafka.server.LogRecoveryTest == [0m",yeyangever,nehanarkhede,Major,Closed,Fixed,26/Jul/12 20:59,29/Aug/12 00:00
Bug,KAFKA-424,12600439,Remove invalid mirroring arguments from kafka-server-start.sh,"Since r1310645, mirroring is in MirrorMaker, and kafka.Kafka.main()
only supports a single argument.
",jkreps,tommie,Trivial,Resolved,Fixed,27/Jul/12 08:44,14/Aug/12 19:59
Bug,KAFKA-425,12600586,Wrong class name in performance test scripts,perf tools were moved to perf by KAFKA-176 but scripts used to run those tools weren't updated accordingly.,jkreps,akitada,Major,Resolved,Fixed,29/Jul/12 07:00,14/Aug/12 19:57
Bug,KAFKA-428,12600874,need to update leaderAndISR path in ZK conditionally in ReplicaManager,"When the leader tries to update the leaderAndISR path in ZK, the path may have been updated by the controller. When this happens, the leader should abort the update and log it. The controller should send new requests to the leader later on.",yeyangever,junrao,Major,Closed,Fixed,31/Jul/12 15:44,25/Aug/12 00:07
Bug,KAFKA-431,12600935,LogCorruptionTest.testMessageSizeTooLarge fails occasionally,"It fails with the following exception:

[0m[[0minfo[0m] [0mTest Starting: testMessageSizeTooLarge(kafka.log.LogCorruptionTest)[0m
[2012-07-31 15:54:57,525] ERROR KafkaApi on Broker 0, Error while retrieving topic metadata (kafka.server.KafkaApis:99)
java.lang.NullPointerException
	at scala.util.parsing.combinator.Parsers$NoSuccess.<init>(Parsers.scala:131)
	at scala.util.parsing.combinator.Parsers$Failure.<init>(Parsers.scala:158)
	at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:489)
	at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:487)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:162)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:742)
	at scala.util.parsing.json.JSON$.parseRaw(JSON.scala:71)
	at scala.util.parsing.json.JSON$.parseFull(JSON.scala:85)
	at kafka.utils.ZkUtils$$anonfun$getPartitionAssignmentForTopics$1.apply(ZkUtils.scala:461)
	at kafka.utils.ZkUtils$$anonfun$getPartitionAssignmentForTopics$1.apply(ZkUtils.scala:456)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.LinearSeqLike$$anon$1.foreach(LinearSeqLike.scala:52)
	at kafka.utils.ZkUtils$.getPartitionAssignmentForTopics(ZkUtils.scala:456)
	at kafka.admin.AdminUtils$$anonfun$getTopicMetaDataFromZK$1.apply(AdminUtils.scala:93)
	at kafka.admin.AdminUtils$$anonfun$getTopicMetaDataFromZK$1.apply(AdminUtils.scala:91)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.immutable.List.map(List.scala:45)
	at kafka.admin.AdminUtils$.getTopicMetaDataFromZK(AdminUtils.scala:91)
	at kafka.server.KafkaApis$$anonfun$handleTopicMetadataRequest$1.apply(KafkaApis.scala:433)
	at kafka.server.KafkaApis$$anonfun$handleTopicMetadataRequest$1.apply(KafkaApis.scala:423)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.server.KafkaApis.handleTopicMetadataRequest(KafkaApis.scala:422)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:61)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:38)
	at java.lang.Thread.run(Thread.java:662)
",,junrao,Major,Closed,Fixed,31/Jul/12 23:57,23/Aug/12 17:17
Bug,KAFKA-432,12601089,allow consumer to read from followers,"For debugging purpose, it would be convenient if we allow a consumer to consume from a follower replica.",yeyangever,junrao,Blocker,Closed,Fixed,01/Aug/12 16:19,12/Oct/12 02:57
Bug,KAFKA-436,12601314,Multifetch response size overflow,"Fetchsize parameter is per partition in fetch requests. 
This makes the size of response unpredicatable and thus might overflow.

(original email : http://mail-archives.apache.org/mod_mbox/incubator-kafka-users/201208.mbox/%3C416A89FBBC95114F8B2D2E91781541E9638B201E%40SRVEX02.criteois.lan%3E   )

Three propositions : 
- fetch size is only a hint, broker will ensure that it won't overflow (maybe not sending too much data)
- broker respect fetchsize as a global limit and not by partition
- broker may not send data for all topics to avoid the overflow.

Changing size to int64 could also decrease probablity of such event.

",,kamaradclimber,Major,Resolved,Fixed,03/Aug/12 07:11,07/Feb/15 23:39
Bug,KAFKA-456,12602686,ProducerSendThread calls ListBuffer.size a whole bunch. That is a O(n) operation,"Hi all,

So there are various statements throughout the async code that call 'events.size', mostly for debugging purposes.
Problem is that this call is O(n), so it could add up if the batch size is high. (it's a ListBuffer)

I see this in at least ProducerSendThread (x4), likely more. Will factor this out myself soon when I start hacking on the project, just wanted to put this somewhere.
",mumrah,rathboma,Minor,Closed,Fixed,09/Aug/12 22:42,19/Jun/14 05:11
Bug,KAFKA-457,12603286,Leader Re-election is broken in rev. 1368092,"Steps to reproduce:

1. Check out rev. 1368092 and execute the command: $ ./sbt update package

2. Replace <kafka_home>/system_test/single_host_multi_brokers/bin/run-test.sh with the attached script because the logging message related to ""shut down completed"" and ""leader state transition"" has been modified in this revision.

3. Execute the command bin/run-test.sh

4. The test seems to be hung.

5. The reason is that there is no leader re-election happening after the first leader is terminated. By checking the logs, there may be only 1 ""completed the leader state transition"" log message found in the server logs:

$ grep ""completed the leader state transition"" *
kafka_server_3.log:[2012-08-13 10:07:58,085] INFO Replica Manager on Broker 3, completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)

6. The correct behavior as in rev 1367821, re-election log messages are found after leader termination as follows:
$ grep ""completed the leader state transition"" *
kafka_server_1.log:[2012-08-13 09:40:23,542] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)
kafka_server_1.log:[2012-08-13 09:42:17,881] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)
kafka_server_2.log:[2012-08-13 09:40:29,082] INFO Broker 2 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)
kafka_server_3.log:[2012-08-13 09:44:06,695] INFO Broker 3 completed the leader state transition for topic mytest part

",yeyangever,jfung,Blocker,Resolved,Fixed,13/Aug/12 17:26,15/Aug/12 23:26
Bug,KAFKA-459,12603479,KafkaController.RequestSendThread can throw exception on broker socket,It can hit NullPointerException at line 74 in KafkaController,yeyangever,junrao,Major,Closed,Fixed,14/Aug/12 16:51,15/Aug/12 15:08
Bug,KAFKA-460,12603483,ControllerChannelManager needs synchronization btw shutdown and add/remove broker,,yeyangever,junrao,Major,Closed,Fixed,14/Aug/12 17:11,15/Aug/12 15:09
Bug,KAFKA-461,12603484,remove support for format for magic byte 0 in 0.8,"Since 0.8 is a non-backward compatible release, should we remove the support for magic byte 0 in Message and support only magic byte 1?",swapnilghike,junrao,Major,Closed,Fixed,14/Aug/12 17:18,21/Aug/12 04:56
Bug,KAFKA-463,12603598,log.truncateTo needs to handle targetOffset smaller than the lowest offset in the log,"When this happens, we should truncate all existing log segments.",swapnilghike,junrao,Blocker,Closed,Fixed,15/Aug/12 15:15,17/Sep/12 14:55
Bug,KAFKA-464,12603613,KafkaController NPE in SessionExpireListener,"Sometime see the following in LogRecoverTest.

[2012-08-15 09:06:01,845] ERROR Error handling event ZkEvent[New session event sent to kafka.server.KafkaController$SessionExpireListener@e8ae59a] (org.I0Itec.zkclient.ZkEventThread)
java.lang.NullPointerException
	at kafka.server.KafkaController$SessionExpireListener.handleNewSession(KafkaController.scala:284)
	at org.I0Itec.zkclient.ZkClient$4.run(ZkClient.java:472)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
",yeyangever,junrao,Major,Closed,Fixed,15/Aug/12 16:53,17/Aug/12 23:10
Bug,KAFKA-465,12603614,Performance test scripts - refactoring leftovers from tools to perf package,"The performance test shell scripts seem like they weren't updated to the new package hierarchy, and still reference kafka.tools.ProducerPerformance for example. Patch attached.",,queinnec,Major,Resolved,Fixed,15/Aug/12 17:02,11/Jul/13 22:22
Bug,KAFKA-467,12603879,Controller based leader election failed ERROR messages in LazyInitProducerTest,"[info] Test Starting: testMultiProduce(kafka.integration.LazyInitProducerTest)
[2012-08-17 08:46:45,165] ERROR Timing out after 500 ms since leader is not elected for topic test1 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:45,668] ERROR Timing out after 500 ms since leader is not elected for topic test2 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:46,171] ERROR Timing out after 500 ms since leader is not elected for topic test3 partition 0 (kafka.utils.TestUtils$:93)
[info] Test Passed: testMultiProduce(kafka.integration.LazyInitProducerTest)
[info] Test Starting: testMultiProduceResend(kafka.integration.LazyInitProducerTest)
[2012-08-17 08:46:49,028] ERROR Timing out after 1500 ms since leader is not elected for topic test1 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:50,531] ERROR Timing out after 1500 ms since leader is not elected for topic test2 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:52,034] ERROR Timing out after 1500 ms since leader is not elected for topic test3 partition 0 (kafka.utils.TestUtils$:93)
[info] Test Passed: testMultiProduceResend(kafka.integration.LazyInitProducerTest)

Leader election should not time out",yeyangever,nehanarkhede,Major,Closed,Fixed,17/Aug/12 15:49,18/Aug/12 05:30
Bug,KAFKA-468,12603911,String#getBytes is platform dependent,"Just noticed while looking at the source that some calls to java.lang.String#getBytes do not include the encoding. They should probably specify ""UTF-8"" for platform-independence.
",,mumrah,Trivial,Resolved,Fixed,17/Aug/12 20:42,22/Nov/12 22:04
Bug,KAFKA-469,12603932,Message size is not checked at the server,"Message size is checked currently only in SyncProducer and not at the server. Therefore, non-java clients can push bigger messages to the server. Need a message size check at the server as well.",swapnilghike,swapnilghike,Major,Closed,Fixed,18/Aug/12 00:45,29/Aug/12 14:54
Bug,KAFKA-470,12603944,transient unit test failure in RequestPurgatoryTest,"[error] Test Failed: testRequestExpirey(kafka.server.RequestPurgatoryTest)
junit.framework.AssertionFailedError: Time for expiration was about 20ms
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at kafka.server.RequestPurgatoryTest.testRequestExpirey(RequestPurgatoryTest.scala:75)
",junrao,junrao,Major,Closed,Fixed,18/Aug/12 05:11,07/Sep/12 03:55
Bug,KAFKA-471,12603970,Transient failure in ProducerTest,"[0m[[31merror[0m] [0mTest Failed: testSendToNewTopic(kafka.producer.ProducerTest)[0m
java.lang.AssertionError: Message set should not have any more messages
	at org.junit.Assert.fail(Assert.java:69)
	at org.junit.Assert.assertTrue(Assert.java:32)
	at org.junit.Assert.assertFalse(Assert.java:51)
	at kafka.producer.ProducerTest.testSendToNewTopic(ProducerTest.scala:183)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)",junrao,nehanarkhede,Major,Closed,Fixed,19/Aug/12 04:07,07/Sep/12 03:59
Bug,KAFKA-472,12604068,update metadata in batches in Producer,"Currently, the producer obtains the metadata of topics being produced one at a time. This means that tools like mirror maker will make many getMetadata requests to the broker. Ideally, we should make BrokerPartition.getBrokerPartitionInfo() a batch api that takes a list of topics.",,junrao,Major,Resolved,Fixed,20/Aug/12 15:40,03/Apr/14 14:26
Bug,KAFKA-473,12604077,Use getMetadata Api in ZookeeperConsumerConnector,"Currently, ZookeeperConsumerConnector gets topic metadata from ZK directly. It's better to use the getMetadata Api since it's batched. This is especially helpful if the consumer client is in a different data center.",yeyangever,junrao,Blocker,Closed,Fixed,20/Aug/12 16:05,27/Sep/12 01:20
Bug,KAFKA-474,12604078,support changing host/port of a broker,"Currently, the consumer client caches the host/port of a broker and never refreshes the cache. This means that if a broker changes to a different host, the consumer client won't be able to connect to the new host without a restart. One possibility is to change AbstractFetcherManager to maintain a map of <Broker, fetcher>, instead of <broker id, fetcher>.",mumrah,junrao,Major,Closed,Fixed,20/Aug/12 16:14,09/Oct/12 17:09
Bug,KAFKA-476,12604252,change Pool to use Option ,It would be good if we change the get API in Pool to use Option.,,junrao,Major,Resolved,Fixed,21/Aug/12 17:20,07/Feb/15 23:38
Bug,KAFKA-481,12604688,Require values in Utils.getTopic* methods to be positive,"KafkaConfig can currently accept negative values for topic specific properties, need to prevent this. ",swapnilghike,swapnilghike,Major,Closed,Fixed,23/Aug/12 21:57,25/Aug/12 05:30
Bug,KAFKA-489,12605259,Add metrics collection and graphs to the system test framework,"We have a new system test framework that allows defining a test cluster, starting kafka processes in the cluster, running tests and collecting logs. In addition to this, it will be great to have the ability to do the following for each test case run -

1. collect metrics as exposed by mbeans
2. collect various system metrics exposed by sar/vmstat/jvm
3. graph the metrics

The expected output of this work should be the ability to output a link to all the graphs for each test case.",nehanarkhede,nehanarkhede,Major,Resolved,Fixed,28/Aug/12 17:32,06/Sep/12 22:28
Bug,KAFKA-490,12605302,Check max message size on server instead of producer,"Message size is checked currently only in SyncProducer and not at the server. Therefore, non-java clients can push bigger messages to the server. Need a message size check at the server. Can remove the check from producer side since server can send acks in 0.8.",swapnilghike,swapnilghike,Blocker,Closed,Fixed,28/Aug/12 21:44,12/Sep/12 01:31
Bug,KAFKA-491,12605322,KafkaRequestHandler needs to handle exceptions,"Currently, if apis.handle() throws an exception (e.g., if the broker receives an invalid request), KafkaRequestHandler will die. We need to handle exceptions properly.",,junrao,Major,Closed,Fixed,29/Aug/12 01:02,20/Nov/12 00:05
Bug,KAFKA-495,12605833,"Handle topic names with ""/"" on Kafka server","If a producer publishes data to topic ""foo/foo"", the Kafka server ends up creating an invalid directory structure on the server. This corrupts the zookeeper data structure for the topic - /brokers/topics/foo/foo. This leads to rebalancing failures on the consumer as well as errors on the zookeeper based producer. 

We need to harden the invalid topic handling on the Kafka server side to avoid this.",swapnilghike,nehanarkhede,Major,Closed,Fixed,01/Sep/12 01:25,01/Oct/12 15:51
Bug,KAFKA-496,12606425,high level producer send should return a response,"Currently, Producer.send() doesn't return any value. In 0.8, since each produce request will be acked, we should pass the response back. What we can do is that if the producer is in sync mode, we can return a map of (topic,partitionId) -> (errorcode, offset). If the producer is in async mode, we can just return a null.",jkreps,junrao,Blocker,Resolved,Fixed,06/Sep/12 16:10,11/Apr/16 23:18
Bug,KAFKA-497,12606434,recover consumer during unclean leadership change,"When we do an unclean leadership change (i.e., no live broker in ISR), offsets already exposed to consumers may no longer be valid. As a result, a consumer could either receive an OffsetOutOfRange exception or hit an InvalidMessageSizeException while iterating a fetched message set. In either case, we need to be able to recover the consumption (probably from the current latest offset from the leader). We need to recover both regular and follower consumers.",,junrao,Blocker,Closed,Fixed,06/Sep/12 16:51,09/Oct/12 15:48
Bug,KAFKA-498,12606436,Controller code has race conditions and synchronization bugs,The controller maintains some internal data structures that are updated by state changes triggered by zookeeper listeners. There are race conditions in the controller channel manager and the controller state machine.,nehanarkhede,nehanarkhede,Blocker,Closed,Fixed,06/Sep/12 16:53,12/Sep/12 04:14
Bug,KAFKA-499,12606438,Refactor controller state machine ,"Currently, the controller logic is very procedural and is similar to KafkaZookeeper. Controller should have a well defined state machine with states and transitions. This will make it easier to understand and maintain the controller code. ",nehanarkhede,nehanarkhede,Blocker,Closed,Fixed,06/Sep/12 16:57,18/Sep/12 20:35
Bug,KAFKA-500,12606799,javaapi support for getTopoicMetaData,TopicMetaRequest and TopicMetaResponse use scala Seq and Option. We need a version so that java applications can use more easily.,swapnilghike,junrao,Blocker,Closed,Fixed,09/Sep/12 21:41,25/Sep/12 00:44
Bug,KAFKA-501,12606803,getOfffset Api needs to return different latest offset to regular and follower consumers,"For follower consumers, getOffset should return logEndOffset as the latest offset. For regular consumers, getOffset should return highWatermark as the latest offset. ",jjkoshy,junrao,Blocker,Closed,Fixed,10/Sep/12 00:09,01/Oct/12 04:08
Bug,KAFKA-504,12606971,UnsatisfiedLinkError causes snappy unit tests to fail on hudson server,"I am not sure why this happens. It may be that some of the hudson slaves that Apache uses aren't running Linux or x86 or it may be some bug in the library packaging. In any case, we can't assume that native libraries will always load, so I propose just making the test pass if the library is not loadable.",jkreps,jkreps,Major,Resolved,Fixed,10/Sep/12 23:56,08/Oct/12 19:19
Bug,KAFKA-506,12607147,Store logical offset in log,"Currently we only support retention by dropping entire segment files. A more nuanced retention policy would allow dropping individual messages from a segment file by recopying it. This is not currently possible because the lookup structure we use to locate messages is based on the file offset directly.

To fix this we should move to a sequential, logical offset (0,1,2,3,...) which would allow deleting individual messages (e.g. 2) without deleting the entire segment.

It is desirable to make this change in the 0.8 timeframe since we are already doing data format changes.

As part of this we would explicitly store the key field given by the producer for partitioning (right now there is no way for the consumer to find the value used for partitioning).

This combination of features would allow a key-based retention policy that would clean obsolete values either by a user defined key.

The specific use case I am targeting is a commit log for local state maintained by a process doing some kind of near-real-time processing. The process could log out its local state changes and be able to restore from this log in the event of a failure. However I think this is a broadly useful feature.

The following changes would be part of this:
1. The log format would now be
      8 byte offset
      4 byte message_size
      N byte message
2. The offsets would be changed to a sequential, logical number rather than the byte offset (e.g. 0,1,2,3,...)
3. A local memory-mapped lookup structure will be kept for each log segment that contains the mapping from logical to physical offset.

I propose to break this into two patches. The first makes the log format changes, but retains the physical offset. The second adds the lookup structure and moves to logical offset.

Here are a few issues to be considered for the first patch:
1. Currently a MessageSet implements Iterable[MessageAndOffset]. One surprising thing is that the offset is actually the offset of the next message. I think there are actually several uses for the current offset. I would propose making this hold the current message offset since with logical offsets the next offset is always just current_offset+1. Note that since we no longer require messages to be dense, it is not true that if the next offset is N the current offset is N-1 (because N-1 may have been deleted). Thoughts or objections?
2. Currently during iteration over a ByteBufferMessageSet we throw an exception if there are zero messages in the set. This is used to detect fetches that are smaller than a single message size. I think this behavior is misplaced and should be moved up into the consumer.
3. In addition to adding a key in Message, I made two other changes: (1) I moved the CRC to the first field and made it cover the entire message contents (previously it only covered the payload), (2) I dropped support for Magic=0, effectively making the attributes field required, which simplifies the code (since we are breaking compatibility anyway).

",jkreps,jkreps,Major,Resolved,Fixed,11/Sep/12 23:23,16/Nov/17 10:04
Bug,KAFKA-508,12607271,split out partiondata from fetchresponse and producerrequest,,charmalloc,charmalloc,Blocker,Closed,Fixed,12/Sep/12 17:01,05/Oct/12 15:30
Bug,KAFKA-509,12607366,server should shut down on encountering invalid highwatermark file,"1. Somehow I managed to produce the following .highwatermark file (most probably while playing with kafka-create-topic.sh) - 
0
7
abra.kabra 0 0
0 0 0
abrakabra 0 0
\0 0 0
king. 0 0
abra..kabra 0 0
... 0 0

Perhaps the first two lines are not valid. But I am not able to reproduce this issue today. 

2. Since the topic names can contain a whitespace, perhaps the delimiter should change from a space char to / which is not allowed anymore in topic names.

3. With this .highwatermark file, the kafka server produces following - 
[2012-09-12 14:54:49,456] INFO Replica Manager on Broker 0: Becoming Leader for topic [abra.kabra] partition [0] (kafka.server.ReplicaManager)
[2012-09-12 14:54:49,456] INFO [ReplicaFetcherManager on broker 0, ], removing fetcher on topic abra.kabra, partition 0 (kafka.server.ReplicaFetcherManager)
[2012-09-12 14:54:49,457] ERROR Replica Manager on Broker 0: Error processing leaderAndISR request LeaderAndIsrRequest(1,,true,1000,Map((...,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (\0,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (abrakabra,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (0,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (abra..kabra,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (king.,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (abra.kabra,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" })) (kafka.server.ReplicaManager)
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.substring(String.java:1937)
	at kafka.server.HighwaterMarkCheckpoint$$anonfun$1.apply(HighwaterMarkCheckpoint.scala:103)
	at kafka.server.HighwaterMarkCheckpoint$$anonfun$1.apply(HighwaterMarkCheckpoint.scala:96)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.immutable.Range$ByOne$class.foreach(Range.scala:282)
	at scala.collection.immutable.Range$$anon$2.foreach(Range.scala:265)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.immutable.Range.map(Range.scala:39)
	at kafka.server.HighwaterMarkCheckpoint.read(HighwaterMarkCheckpoint.scala:96)
        ... (more junk) ..

And then it goes in a while(1) loop to print the following - 
[2012-09-12 14:54:50,458] ERROR Replica Manager on Broker 0: Highwatermark for topic abra.kabra partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic ... partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic \0 partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic 0 partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic king. partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,460] ERROR Replica Manager on Broker 0: Highwatermark for topic abra..kabra partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,460] ERROR Replica Manager on Broker 0: Highwatermark for topic abrakabra partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)

The server should shut down on encountering the error.",yeyangever,swapnilghike,Blocker,Closed,Fixed,12/Sep/12 23:58,02/Oct/12 15:48
Bug,KAFKA-510,12607396,broker needs to know the replication factor per partition,"A broker needs to know the replication factor to report under replicated partitions.
",yeyangever,junrao,Blocker,Closed,Fixed,13/Sep/12 04:30,11/Oct/12 23:12
Bug,KAFKA-511,12607486,offset returned in Producer response may not be correct,The problems is that we append messages to the log and then get the offset. Another produce request could have sneaked in between the 2 steps.,junrao,junrao,Blocker,Closed,Fixed,13/Sep/12 17:23,09/Oct/12 16:44
Bug,KAFKA-512,12607671,Remove checksum from ByteBufferMessageSet.iterator,"Messages are explicitly checksumed in Log.append. But there is also a checksum computed and checked automatically in ByteBufferMessageSet.iterator as we iterate. This iterator is used quite a lot and as a result we compute this checksum 39 times on a single message produce. It turns out the default crc32 implementation in java is quite expensive so this is not good.

The proposed fix is to remove the automatic checksum from the iterator and add explicit isValid() checks in the consumer as well as retaining the existing check in Log.append().

If folks are in agreement I will probably include this in the KAFKA-506 patch as that already contains a lot of ByteBufferMessageSet changes.",jkreps,jkreps,Blocker,Closed,Fixed,14/Sep/12 16:24,09/Oct/12 15:44
Bug,KAFKA-515,12607716,Log cleanup can close a file channel opnened by Log.read before the transfer completes,"If consumers are lagging behind, then log cleanup activities can close a file channel opened by Log.read 
1. before the transfer the starts (broker will probably throw an exception in this case) OR
2. during the transfer (possibility of half baked corrupted data being sent to consumer?)

We probably haven't hit this race condition in practice because the consumers consume data well before the logs are cleaned up.

To avoid this issue, we could avoid cleaning up the file until the transfer is complete. Reference counting?",jkreps,swapnilghike,Major,Resolved,Fixed,14/Sep/12 21:02,13/Jan/13 03:56
Bug,KAFKA-516,12607717,Consider catching all exceptions in ShutdownableThread,I don't think there is any case where we want an uncaught exception to kill the thread. In fact this can be a bit hard to debug if an important background thread disappears. We should consider catching everything.,,jkreps,Major,Closed,Fixed,14/Sep/12 21:12,09/Oct/12 15:40
Bug,KAFKA-522,12608493,OutOfMemoryError in System Test,"A. This is only reproducible in a distributed environment:

1. Modify system_test/cluster_config.json to have all broker entities running in 3 different remote hosts
2. (Optional) Update system_test/system_test_runner.py to log messages at DEBUG level by uncommenting the following line: 
# namedLogger.setLevel(logging.DEBUG)

3. In <kafka_home>/system_test, run “python -B system_test_runner.py”


B. In this specific test session, the error occurred in Broker-3:

[2012-09-20 16:26:25,777] INFO Closing socket connection to /x.x.x.x. (kafka.network.Processor)

[2012-09-20 16:26:25,778] INFO 3 successfully elected as leader (kafka.server.ZookeeperLeaderElector)

[2012-09-20 16:26:25,779] INFO [Controller 3], Broker 3 starting become controller state transition (kafka.controller.KafkaController)

[2012-09-20 16:26:25,950] INFO [Controller-3-to-broker-2-send-thread], Starting  (kafka.controller.RequestSendThread)

[2012-09-20 16:26:25,950] ERROR Error handling event ZkEvent[Data of /controller changed sent to kafka.server.ZookeeperLeaderElector$LeaderChangeListener@75088a1b] (org.I0Itec.zkclient.ZkEventThread)

java.lang.OutOfMemoryError: unable to create new native thread

        at java.lang.Thread.start0(Native Method)

        at java.lang.Thread.start(Thread.java:597)

        at kafka.controller.ControllerChannelManager.kafka$controller$ControllerChannelManager$$startRequestSendThread(ControllerChannelManager.scala:97)

        at kafka.controller.ControllerChannelManager$$anonfun$startup$1.apply(ControllerChannelManager.scala:40)

        at kafka.controller.ControllerChannelManager$$anonfun$startup$1.apply(ControllerChannelManager.scala:40)

        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)

        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)

        at scala.collection.Iterator$class.foreach(Iterator.scala:631)

        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)

        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)

        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)

        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)

        at kafka.controller.ControllerChannelManager.startup(ControllerChannelManager.scala:40)

        at kafka.controller.KafkaController.startChannelManager(KafkaController.scala:230)

        at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:223)

        at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:72)

        at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:47)

        at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)

        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)

        at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)

        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

[2012-09-20 16:26:31,767] INFO [BrokerChangeListener on Controller 3]: Broker change listener fired for path /brokers/ids with children 3,2,1 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:26:31,775] INFO [BrokerChangeListener on Controller 3]: Newly added brokers: 1, deleted brokers: , all brokers: 3,2,1 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:26:31,777] ERROR [BrokerChangeListener on Controller 3]: Error while handling broker changes (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

java.lang.OutOfMemoryError: unable to create new native thread

        at java.lang.Thread.start0(Native Method)

        at java.lang.Thread.start(Thread.java:597)

        at kafka.controller.ControllerChannelManager.kafka$controller$ControllerChannelManager$$startRequestSendThread(ControllerChannelManager.scala:97)

        at kafka.controller.ControllerChannelManager.addBroker(ControllerChannelManager.scala:61)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$liftedTree1$1$7.apply(ReplicaStateMachine.scala:212)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$liftedTree1$1$7.apply(ReplicaStateMachine.scala:212)

        at scala.collection.immutable.Set$Set1.foreach(Set.scala:81)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.liftedTree1$1(ReplicaStateMachine.scala:212)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply$mcV$sp(ReplicaStateMachine.scala:203)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply(ReplicaStateMachine.scala:199)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply(ReplicaStateMachine.scala:199)

        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener.handleChildChange(ReplicaStateMachine.scala:199)

        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)

        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

[2012-09-20 16:27:22,821] INFO [BrokerChangeListener on Controller 3]: Broker change listener fired for path /brokers/ids with children 3,2 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:27:22,823] INFO [BrokerChangeListener on Controller 3]: Newly added brokers: , deleted brokers: 1, all brokers: 3,2 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:27:22,825] INFO [Controller-3-to-broker-1-send-thread], Shutting down (kafka.controller.RequestSendThread)

[2012-09-20 16:27:23,279] INFO Unable to read additional data from server sessionid 0x139e47e2eb00004, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)

[2012-09-20 16:27:23,379] INFO zookeeper state changed (Disconnected) (org.I0Itec.zkclient.ZkClient)



C. At around the same time, the following are the main test output:

2012-09-20 16:26:25,251 - INFO - ======================================================

2012-09-20 16:26:25,252 - INFO - Iteration 2 of 2

2012-09-20 16:26:25,252 - INFO - ======================================================

2012-09-20 16:26:25,252 - INFO - looking up leader... (kafka_system_test_utils)

2012-09-20 16:26:25,252 - DEBUG - executing command [ssh host2 ""grep -i -h 'Completed the leader state transition'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-1/kafka_server_9091.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:25,344 - DEBUG - found the log line : [2012-09-20 16:25:45,195] INFO Replica Manager on Broker 1: Completed the leader state transition for topic test_1 partition 0 (kafka.server.ReplicaManager) (kafka_system_test_utils)

2012-09-20 16:26:25,344 - DEBUG - brokerid: [1] entity_id: [1] (kafka_system_test_utils)

2012-09-20 16:26:25,345 - DEBUG - executing command [ssh host3 ""grep -i -h 'Completed the leader state transition'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-2/kafka_server_9092.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:25,455 - DEBUG - executing command [ssh host3 ""grep -i -h 'Completed the leader state transition'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-3/kafka_server_9093.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:25,540 - INFO - ======================================================

2012-09-20 16:26:25,540 - INFO - validating leader election

2012-09-20 16:26:25,540 - INFO - ======================================================

2012-09-20 16:26:25,540 - INFO - found leader in entity [1] with brokerid [1] for partition [0] (kafka_system_test_utils)

2012-09-20 16:26:25,541 - INFO - ======================================================

2012-09-20 16:26:25,541 - INFO - bounce_leader flag : true

2012-09-20 16:26:25,541 - INFO - ======================================================

2012-09-20 16:26:25,541 - INFO - stopping leader in entity 1 with pid 32679 (kafka_system_test_utils)

2012-09-20 16:26:25,541 - DEBUG - executing command [ssh host2 'pid=32679; prev_pid=""""; echo $pid; while [[ ""x$pid"" != ""x"" ]]; do prev_pid=$pid;   for child in $(ps -o pid,ppid ax | awk ""{ if ( \$2 == $pid ) { print \$1 }}"");     do echo $child; pid=$child;   done;   if [ $prev_pid == $pid ]; then     break;   fi; done' 2> /dev/null (system_test_utils)

2012-09-20 16:26:25,669 - DEBUG - terminating process id: 32679 in host: host2 (kafka_system_test_utils)

2012-09-20 16:26:25,670 - DEBUG - executing command [ssh host2 'kill -15 32681'] (system_test_utils)

2012-09-20 16:26:25,673 - DEBUG - executing command [ssh host2 'kill -15 32679'] (system_test_utils)

2012-09-20 16:26:25,675 - INFO - sleeping for 5s for leader re-election to complete (kafka_system_test_utils)

2012-09-20 16:26:30,681 - INFO - looking up broker shutdown... (kafka_system_test_utils)

2012-09-20 16:26:30,681 - DEBUG - executing command [ssh host2 ""grep -i -h 'shut down completed'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-1/kafka_server_9091.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - found the log line : [2012-09-20 16:26:25,776] INFO [Kafka Server 1], shut down completed (kafka.server.KafkaServer) (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - brokerid: [1] entity_id: [1] (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - unix timestamp of shut down completed: 1348158385.776000 (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - looking up new leader (kafka_system_test_utils)
",,jfung,Major,Resolved,Fixed,20/Sep/12 16:58,20/Sep/12 18:28
Bug,KAFKA-523,12608661,OffsetRequest handler does not handle errors,"There is not error handling in the KafkaRequestHandlers#handleOffsetRequest, as a result invalid requests get no data back since they raise an Exception in the server.

",,mumrah,Minor,Resolved,Fixed,21/Sep/12 16:18,21/Apr/17 22:55
Bug,KAFKA-527,12608923,Compression support does numerous byte copies,"The data path for compressing or decompressing messages is extremely inefficient. We do something like 7 (?) complete copies of the data, often for simple things like adding a 4 byte size to the front. I am not sure how this went by unnoticed.

This is likely the root cause of the performance issues we saw in doing bulk recompression of data in mirror maker.

The mismatch between the InputStream and OutputStream interfaces and the Message/MessageSet interfaces which are based on byte buffers is the cause of many of these.

",yasuhiro.matsuda,jkreps,Critical,Resolved,Fixed,24/Sep/12 20:38,29/Mar/15 22:53
Bug,KAFKA-528,12608934,IndexOutOfBoundsException thrown by kafka.consumer.ConsumerFetcherThread,"1. Attached file system_test_1348521165.tar.gz contains all the associated log files for this test session.

2. The system test output log can be found at: system_test_1348521165/system_test_output.log

3. The following log message can be found at: system_test_1348521165/logs/console_consumer-5/console_consumer.log

[2012-09-24 14:15:12,016] ERROR [ConsumerFetcherThread-console-consumer-16186_jfung-1348521311426-2c83ced7-0-1], Error due to  (kafka.consumer.ConsumerFetcherThread)

java.lang.IndexOutOfBoundsException

        at java.nio.Buffer.checkIndex(Buffer.java:512)

        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)

        at kafka.message.Message.magic(Message.scala:119)

        at kafka.message.Message.checksum(Message.scala:132)

        at kafka.message.Message.isValid(Message.scala:144)

        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:118)

        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:149)

        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:89)

        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:61)

        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:53)

        at kafka.message.ByteBufferMessageSet.shallowValidBytes(ByteBufferMessageSet.scala:54)

        at kafka.message.ByteBufferMessageSet.validBytes(ByteBufferMessageSet.scala:49)

        at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:54)

        at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:42)

        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:103)

        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:96)

        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)

        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:96)

        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",junrao,jfung,Blocker,Closed,Fixed,24/Sep/12 21:49,29/Sep/12 18:19
Bug,KAFKA-529,12608935,kafka.server.ReplicaFetcherThread: java.lang.IndexOutOfBoundsException,"1. Attached file system_test_1348521165.tar.gz contains all the associated log files for this test session.

2. The system test output log can be found at: system_test_1348521165/system_test_output.log 

3. The following log message can be found at: system_test_1348521165/logs/broker-2/kafka_server_9092.log

[2012-09-24 14:14:19,601] WARN No previously checkpointed highwatermark value found for topic test_1 partition 1. Returning 0 as the highwatermark (ka
fka.server.HighwaterMarkCheckpoint)
[2012-09-24 14:14:19,604] INFO [Kafka Log on Broker 2], Truncated log segment /tmp/kafka_server_2_logs/test_1-1/00000000000000000000.kafka to target o
ffset 0 (kafka.log.Log)
[2012-09-24 14:14:19,611] INFO [ReplicaFetcherThread-1-0-on-broker-2-], Starting  (kafka.server.ReplicaFetcherThread)
[2012-09-24 14:14:19,611] INFO [ReplicaFetcherManager on broker 2, ], adding fetcher on topic test_1, partion 1, initOffset 0 to broker 1 with fetcherId 0 (kafka.server.ReplicaFetcherManager)
[2012-09-24 14:14:19,973] ERROR [ReplicaFetcherThread-1-0-on-broker-2-], Error due to  (kafka.server.ReplicaFetcherThread)
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Buffer.java:512)
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at kafka.message.Message.magic(Message.scala:119)
        at kafka.message.Message.checksum(Message.scala:132)
        at kafka.message.Message.isValid(Message.scala:144)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:118)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:149)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:89)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:61)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:53)
        at kafka.message.ByteBufferMessageSet.verifyMessageSize(ByteBufferMessageSet.scala:79)
        at kafka.log.Log.append(Log.scala:250)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:42)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:103)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:96)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:96)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
[2012-09-24 14:14:19,975] INFO [ReplicaFetcherThread-1-0-on-broker-2-], Stopped  (kafka.server.ReplicaFetcherThread)
",,jfung,Blocker,Resolved,Fixed,24/Sep/12 22:04,01/Oct/12 22:47
Bug,KAFKA-532,12609099,Multiple controllers can co-exist during soft failures,"If the current controller experiences an intermittent soft failure (GC pause) in the middle of leader election or partition reassignment, a new controller might get elected and start communicating new state change decisions to the brokers. After recovering from the soft failure, the old controller might continue sending some stale state change decisions to the brokers, resulting in unexpected failures. We need to introduce a controller generation id that increments with controller election. The brokers should reject any state change requests by a controller with an older generation id.",nehanarkhede,nehanarkhede,Blocker,Closed,Fixed,25/Sep/12 18:34,04/Dec/12 23:44
Bug,KAFKA-535,12609464,"Significant difference in time taken to produce messages between 1, -1 for request-num-acks","There is a significant difference in time taken for ProducerPerformance to produce messages between 1 & -1 for request-num-acks.

The following are the log4j messages from ProducerPerformance with consequent calls from the system test script.

** Please note the time elapsed in consequent timestamps of calling ProducerPerformance.

The overall test scenarios:
1. This test is set up to have 1 zookeeper, 1 broker cluster of 6 nodes (distributed systems, non-local), replica factor 6, 1 topic, 1 partition
2. The script will wait for ProducerPerformance to complete sending all messages (500 each call in this case) before calling the producer again. 


1. request-num-acks = -1. The rate is about 10 messages per second. The timestamp indicates that it takes 60+ seconds for ProducerPerformance to completely sending 500 messages and exit by itself.

2012-09-26 21:20:56,102 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [0] (kafka_system_test_utils)
2012-09-26 21:20:56,102 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 0 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks -1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)
. . .
2012-09-26 21:22:00,162 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [500] (kafka_system_test_utils)
2012-09-26 21:22:00,162 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 500 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks -1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)


2. request-num-acks = 1. The rate is about 150 ~ 200 messages per second. The timestamp indicates that it takes < 3 seconds for ProducerPerformance to completely sending 500 messages.

2012-09-26 21:29:23,698 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [500] (kafka_system_test_utils)
2012-09-26 21:29:23,698 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 500 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks 1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)
. . .
2012-09-26 21:29:26,576 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [1000] (kafka_system_test_utils)
2012-09-26 21:29:26,577 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 1000 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks 1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)
",,jfung,Blocker,Resolved,Fixed,27/Sep/12 22:31,20/Mar/14 21:44
Bug,KAFKA-537,12609555,expose clientId and correlationId in ConsumerConfig,"We need to expose clientId and correlationId in ConsumerConfig and use it properly in AbstractFetcherThread. For follower fetchers, we should set the clientId to a special string, something list ""follower"".",yeyangever,junrao,Major,Closed,Fixed,28/Sep/12 15:59,16/Oct/12 17:31
Bug,KAFKA-539,12609711,Replica.hw should be initialized to the smaller of checkedpointed HW and log end offset,"Currently, replica.hw is always initialized to checkedpointed HW. However, on unclean shutdown, log end offset could be less than checkedpointed HW.",,junrao,Blocker,Closed,Fixed,01/Oct/12 04:05,09/Oct/12 05:18
Bug,KAFKA-540,12610152,log.append() should halt on IOException,"See the following entry in the broker log in a system test run. We interrupted the ReplicaFetcherThread during shutdown. However, log.append halts the system when we hit the interrupted exception. The fix is not to halt the system in log.append and just pass on the exception. The caller can decide what to do.

[2012-10-03 15:08:53,124] FATAL [Kafka Log on Broker 2], Halting due to unrecoverable I/O error while handling producer request (kafka.log.Log)
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:203)
        at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:128)
        at kafka.log.FileMessageSet.append(FileMessageSet.scala:155)
        at kafka.log.LogSegment.append(LogSegment.scala:60)
        at kafka.log.Log.liftedTree1$1(Log.scala:282)
        at kafka.log.Log.append(Log.scala:270)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:42)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:105)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:98)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:98)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
(",junrao,junrao,Blocker,Closed,Fixed,03/Oct/12 22:34,04/Oct/12 03:47
Bug,KAFKA-541,12610167,Use metrics CSV reporter instead of jmx tool for system tests,"The existing system test framework spawns off a bunch of jmxtool processes to collect metrics. This is rather heavy-weight and also, requires advance knowledge of all the beans (many of which are dynamically registered). E.g., per-topic stats pop-up only after the topics are produced to.

Since we are using metrics-core, we can just turn on the CSV reporter to collect these stats. I had originally thought version 2.1.3 had various bugs that rendered it unusable for CSV reporter, but I gave it another try and it seems to be fine. Will post some output.",yeyangever,jjkoshy,Major,Closed,Fixed,04/Oct/12 00:09,25/Oct/12 01:21
Bug,KAFKA-542,12610173,Upgrade to metrics 3.x,"metrics 3.0.0 fixes some issues with the csv reporter that affect 0.8

E.g., with metrics 2.1.3 we would just have NumDelayedRequests.csv but with 3.0.0 we would have kafka.server.ProducerRequestPurgatory.NumDelayedRequests.csv and kafka.server.FetchRequestPurgatory.NumDelayedRequests.csv
",jjkoshy,jjkoshy,Major,Closed,Fixed,04/Oct/12 00:52,09/Oct/12 21:29
Bug,KAFKA-543,12610179,Metadata request from DefaultEventHandler.handle repeats same topic over and over,"It looks like we are calling BrokerPartitionInfo.updateInfo() with a list of the same topic repeated many times:

Here is the line:
Utils.swallowError(brokerPartitionInfo.updateInfo(outstandingProduceRequests.map(_.getTopic)))

The outstandingProduceRequests can (and generally would) have many entries for the same topic.

For example if I use the producer performance test with the default batch size on a topic ""test"" my metadata request will have the topic ""test"" repeated 200 times. On the server side we do several zk reads for each of these repetitions.

This is causing the metadata api to timeout in my perf test periodically.

I think the fix is simply to de-duplicate prior to the call (and perhaps again on the server in case of a misbehaving client).",jkreps,jkreps,Blocker,Closed,Fixed,04/Oct/12 03:16,16/Oct/12 04:29
Bug,KAFKA-544,12610377,Retain key in producer and expose it in the consumer,"KAFKA-506 added support for retaining a key in the messages, however this field is not yet set by the producer.

The proposal for doing this is to change the producer api to change ProducerData to allow only a single key/value pair so it has a one-to-one mapping to Message. That is change from
  ProducerData(topic: String, key: K, data: Seq[V])
to
  ProducerData(topic: String, key: K, data: V)

The key itself needs to be encoded. There are several ways this could be handled. A few of the options:
1. Change the Encoder and Decoder to be MessageEncoder and MessageDecoder and have them take both a key and value.
2. Another option is to change the type of the encoder/decoder to not refer to Message so it could be used for both the key and value.

I favor the second option but am open to feedback.

One concern with our current approach to serialization as well as both of these proposals is that they are inefficient. We go from Object=>byte[]=>Message=>MessageSet with a copy at each step. In the case of compression there are a bunch of intermediate steps. We could theoretically clean this up by instead having an interface for the encoder that was something like
   Encoder.writeTo(buffer: ByteBuffer, object: AnyRef)
and
   Decoder.readFrom(buffer:ByteBuffer): AnyRef
However there are two problems with this. The first is that we don't actually know the size of the data until  it is serialized so we can't really allocate the bytebuffer properly and might need to resize it. The second is that in the case of compression there is a whole other path to consider. Originally I thought maybe it would be good to try to fix this, but now I think it should be out-of-scope and we should revisit the efficiency issue in a future release in conjunction with our internal handling of compression.



",jkreps,jkreps,Blocker,Closed,Fixed,04/Oct/12 19:23,04/Dec/12 23:43
Bug,KAFKA-549,12610638,ConsumerOffsetChecker does not deal with hostnames published in zookeeper,"When configuring the broker using the 'hostname=' property in server.properties to publish hostnames in zookeeper, the ConsumerOffsetChecker can't parse the hostname from the zknode

The attached patch changes the host regexp to match hostnames as well as ip addresses.",,bob.cotton@gmail.com,Minor,Closed,Fixed,05/Oct/12 21:17,08/Oct/12 23:19
Bug,KAFKA-550,12610676,Wildcarded consumption is single-threaded,"It's surprising that we haven't noticed this before, but I was looking at a CPU usage profile on yourkit and It turns out that only one mirror maker thread is actually doing anything. Basically I suspect some bug in fetcher -> queue mapping. Only one queue seems to have any data. I'll look into this probably next week.",jjkoshy,jjkoshy,Major,Closed,Fixed,06/Oct/12 01:38,10/Oct/12 16:30
Bug,KAFKA-551,12610678,Log.truncateTo() may need to trucate immutable log segment,"In makeFollower, we need to first truncate the local log to high watermark. It's possible that we need to truncate into segments before the last one. The problem is that all segments except the last one are immutable. So the truncation will fail which prevents the replica fetcher from being started.

One solution is to reopen the segment as mutable during truncation, if it's not mutable already.",jkreps,junrao,Blocker,Closed,Fixed,06/Oct/12 01:53,09/Oct/12 16:42
Bug,KAFKA-553,12610981,confusing reference to zk.connect in config/producer.properties,"https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8/config/producer.properties
still has comments relative to zookeeper:
    # need to set either broker.list or zk.connect
    # zk.connect=
    …
But https://issues.apache.org/jira/browse/KAFKA-369 has removed the dependency to
zookeeper.
Those old comments are confusing since they imply that you can still use zk.connect with producers.",,yazgoo,Trivial,Closed,Fixed,09/Oct/12 12:53,10/Oct/12 15:47
Bug,KAFKA-556,12611017,Change MessageSet.sizeInBytes to Int,"With kafka-506, there are various places where we assume that each log segment is less than 2GB.",swapnilghike,junrao,Major,Closed,Fixed,09/Oct/12 16:23,24/Oct/12 16:20
Bug,KAFKA-557,12611020,Replica fetch thread doesn't need to recompute message id,"With kafka-506, the leader broker computes the logical id for each message produced. This could involve decompressing and recompressing messages, which are expensive. When data is replicated from the leader to the follower, we could avoid recomputing the logical message id since it's the same.",jkreps,junrao,Blocker,Closed,Fixed,09/Oct/12 16:34,16/Oct/12 04:28
Bug,KAFKA-562,12611078,Non-failure System Test Log Segment File Checksums mismatched,"To reproduce this issue
1. Download 0.8 branch (reproduced in r1396343)
2. Apply the patch attached
3. Build Kafka under <kafka_home> by running ""./sbt update package""
4. In the directory <kafka_home>/system_test, run ""python -B system_test_runner.py"" and it will run the case ""testcase_0002"" which will reproduce this issue.
5. The log segment files will be located in /tmp",,jfung,Major,Resolved,Fixed,09/Oct/12 22:09,22/Nov/12 21:56
Bug,KAFKA-563,12611083,KafkaScheduler shutdown in ZookeeperConsumerConnector should check for config.autocommit,Kafkascheduler starts up only if ConsumerConfig.autocommit is true. Its shutdown should check for this condition too.,,swapnilghike,Major,Closed,Fixed,09/Oct/12 22:46,10/Oct/12 16:34
Bug,KAFKA-567,12611389,Replication Data Loss in Mirror Maker Bouncing testcase,"* Test Description: 
1. Start a 3-broker cluster as source
2. Start a 3-broker cluster as target
3. Start 1 instance of Mirror Maker to replicate data from source to target
4. While producer is sending data into source cluster, stop Mirror Maker with ""kill -15"". Start Mirror Maker again after 1 second.
5. Start a consumer to consume data from target cluster.
6. Compare the MessageID in the data between producer log and consumer log.

* To reproduce this issue, please do the followings:

1. Download the latest 0.8 branch
2. Apply the patch attached to this JIRA
3. Build kafka by running ""./sbt update package""
4. Execute the test in directory ""system_test"" : ""python -B system_test_runner.py""

* The test result may look like the following:

_test_case_name  :  testcase_5002
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  true
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     Log segment checksum matching across all replicas  :  FAILED
     Unique messages from consumer on [test_1]  :  355
     Unique messages from producer on [test_1]  :  400
     Validate for data matched on topic [test_1]  :  FAILED

* Attached a tar file for the system test output log, the brokers' log4j files and data log segment files.

* There are no unusual Exception / Error found in the logs. However, there are consistently data loss in this Mirror Maker bouncing test case. Not sure if this is related to KAFKA-552.",junrao,jfung,Blocker,Closed,Fixed,11/Oct/12 17:08,12/Oct/12 20:35
Bug,KAFKA-569,12611471,Cleanup kafka.utils.Utils,kafka.utils.Utils is a real mess. It is full of odd little pieces of business logic dropped there. We should clean this up.,jkreps,jkreps,Major,Closed,Fixed,12/Oct/12 00:29,13/Oct/12 16:36
Bug,KAFKA-570,12611625,Kafka should not need snappy jar at runtime,"CompressionFactory imports snappy jar in a pattern match. The purpose of importing it this way seems to be avoiding the import unless snappy compression is actually required. However, kafka throws a ClassNotFoundException if snappy jar is removed at runtime from lib_managed. 

This exception can be easily seen by producing some data with the console producer.",,swapnilghike,Major,Resolved,Fixed,12/Oct/12 20:06,07/Jun/16 02:45
Bug,KAFKA-572,12611665,Merged log segment checksums mismatched in Leader failure System Test case,,,jfung,Major,Resolved,Fixed,13/Oct/12 05:12,13/Oct/12 22:22
Bug,KAFKA-573,12611772,System Test : Leader Failure Log Segment Checksum Mismatched When request-num-acks is 1,"• Test Description:

1. Start a 3-broker cluster as source
2. Send messages to source cluster
3. Find leader and terminate it (kill -15)
4. Start the broker again
5. Start a consumer to consume data
6. Compare the MessageID in the data between producer log and consumer log.

• Issue: There will be data loss if request-num-acks is set to 1. 

• To reproduce this issue, please do the followings:

1. Download the latest 0.8 branch
2. Apply the patch attached to this JIRA
3. Build kafka by running ""./sbt update package""
4. Execute the test in directory ""system_test"" : ""python -B system_test_runner.py""
5. This test will execute testcase_2 with the following settings:
    Replica factor : 3
    No. of partitions : 1
    No. of bouncing : 1",junrao,jfung,Major,Closed,Fixed,14/Oct/12 22:43,19/Oct/12 00:10
Bug,KAFKA-574,12612061,KafkaController unnecessarily reads leaderAndIsr info from ZK,"KafkaController calls updateLeaderAndIsrCache() in onBrokerFailure(). This is unnecessary since in onBrokerFailure(), we will make leader and isr change anyway so there is no need to first read that information from ZK. Latency is critical in onBrokerFailure() since it determines how quickly a leader can be made online.

Similarly, updateLeaderAndIsrCache() is called in onBrokerStartup() unnecessarily. In this case, the controller does not change the leader or the isr. It just needs to send the current leader and the isr info to the newly started broker. We already cache leader in the controller. Isr in theory could change any time by the leader. So, reading from ZK doesn't guarantee that we can get the latest isr anyway. Instead, we just need to get the isr last selected by the controller (which can be cached together with the leader in the controller). If the leader epoc in a broker is at or larger than the epoc in the leaderAndIsr request, the broker can just ignore it. Otherwise, the leader and the isr selected by the controller should be used. ",prashanth.menon,junrao,Blocker,Closed,Fixed,16/Oct/12 16:09,15/Nov/12 00:29
Bug,KAFKA-575,12612062,Partition.makeFollower() reads broker info from ZK,"To follow a new leader, Partition.makeFollower() has to obtain the broker info of the new leader. Currently, it reads that info from ZK for every affected partition. This increases the time for a leader to truly available. 
",swapnilghike,junrao,Blocker,Closed,Fixed,16/Oct/12 16:18,30/Oct/12 01:29
Bug,KAFKA-576,12612074,SimpleConsumer throws UnsupportedOperationException: empty.head ,"* In this case, there are 15 log segment files in broker-1 data dir:

ls -l /tmp/kafka_server_1_logs/test_1-0/
total 240
-rw-r--r-- 1 jfung eng    16 Oct 16 10:41 00000000000000000000.index
-rw-r--r-- 1 jfung eng 10440 Oct 16 10:40 00000000000000000000.log
-rw-r--r-- 1 jfung eng     8 Oct 16 10:41 00000000000000000020.index
-rw-r--r-- 1 jfung eng 10440 Oct 16 10:40 00000000000000000020.log
. . .
-rw-r--r-- 1 jfung eng     8 Oct 16 10:41 00000000000000000280.index
-rw-r--r-- 1 jfung eng 10440 Oct 16 10:41 00000000000000000280.log


* The following are the dump log segment of the first log segment file

bin/kafka-run-class.sh kafka.tools.DumpLogSegments /tmp/kafka_server_1_logs/test_1-0/00000000000000000000.log 
Dumping /tmp/kafka_server_1_logs/test_1-0/00000000000000000000.log
Starting offset: 0
offset: 0 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1663889063
offset: 1 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 2803454828
offset: 2 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 683347625
. . .
offset: 18 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1892511043
offset: 19 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 601297044

* Output of SimpleConsumerShell:
. . .
next offset = 16
Topic:test_1:ThreadID:2:MessageID:0000000043:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 17
Topic:test_1:ThreadID:3:MessageID:0000000063:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 18
Topic:test_1:ThreadID:4:MessageID:0000000083:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
. . .

* It appears that SimpleConsumerShell doesn't advance to the next log segment file

* It should probably block inside the while loop to prevent infinite looping",yeyangever,jfung,Major,Closed,Fixed,16/Oct/12 17:54,24/Oct/12 14:21
Bug,KAFKA-577,12612446,extend DumpLogSegments to verify consistency btw data and index,"It would be good to extend DumpLogSegments to do the following verification:
1. The offsets stored in the index match those in the log data.
2. The offsets in the data log is consecutive.",,junrao,Major,Closed,Fixed,18/Oct/12 16:18,05/Nov/12 23:39
Bug,KAFKA-578,12612447,Leader finder thread in ConsumerFetcherManager needs to handle exceptions,"Saw the leader finder thread due to the following exception. We need to add a try/catch clause to handle the exceptions.

[2012-10-11 17:17:14,192] ERROR [mm_regtest_grp_jrao-ld-1350000983383-47e0e557-leader-finder-thread], Error due to  (kafka.consumer.ConsumerFetcherManager$$anon$1)
kafka.common.KafkaException: fetching topic metadata for topics [Set(test_1)] from broker [ArrayBuffer()] failed
        at kafka.utils.Utils$.getTopicMetadata(Utils.scala:704)
        at kafka.consumer.ConsumerFetcherManager$$anon$1.doWork(ConsumerFetcherManager.scala:55)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",,junrao,Blocker,Closed,Fixed,18/Oct/12 16:24,23/Oct/12 22:15
Bug,KAFKA-579,12612811,remove connection timeout in SyncProducer,"Currently, SyncProducer has a few parameters that control how long the client should wait to establish a socket connection and how frequent the connection should be re-established. Those parameters seem to be needed primarily for vip on a load balancer. In 0.8, SyncProducer doesn't deal with VIP any more. So, we probably should get rid of those parameters. One of the issues that I have seen is that when a broker is down, SyncProducer will still wait connectionTimeout time to try to establish a connection. This is unnecessary since the high level producer already has the retry logic and is delaying requests like getMetadataRequest.
",swapnilghike,junrao,Blocker,Closed,Fixed,20/Oct/12 01:52,24/Oct/12 17:21
Bug,KAFKA-580,12613157,system test testcase_0122 under replication fails due to large # of data loss,"testcase_0122 fails sometimes because a large # of messages is lost with ack = 1. In this case, we expect only a small number of messages to be lost when there are broker failures.",junrao,junrao,Blocker,Closed,Fixed,23/Oct/12 15:21,23/Oct/12 17:26
Bug,KAFKA-584,12613415,produce/fetch remote time metric not set correctly when num.acks = 1,"When num.acks = 1, the produce/fetch remote time is set to a very high value (several hours). This is due to a race condition on the apiLocalTime, which is initialized to -1, that makes the (responseTime - apiLocalTime) a very large value.
",junrao,nehanarkhede,Major,Closed,Fixed,25/Oct/12 00:43,25/Oct/12 18:16
Bug,KAFKA-585,12613421,Remove custom metrics jar and replace with latest from metrics HEAD,"This is for at least until metrics 3.x is mavenized.

Also:

The KafkaCSVMetricsReporter object may be better named as KafkaMetricsReporter since startCSVMetricsReporter
potentially starts up other (non-CSV) reporters (if any) as well - in which case KafkaMetricsReporter.scala would be a
better place for it. Or, you can just filter out non-CSV reporters.

Also, the top-level/config/server.properties need not enable the csv reporter. I thought the system test replication
suite's server.properties would need to be patched, but it isn't. Should look into whether the test suite picks up the top-level
config as a template.",,jjkoshy,Major,Closed,Fixed,25/Oct/12 01:25,14/Nov/12 18:58
Bug,KAFKA-586,12613532,system test configs are broken,"system test suite has a set of default config values that are picked up from the testsuite/config directory. One can override the value of a config in the testcase_properties.json file. This is great, but the assumption is that the config property that is being overridden should also present in the testsuite/config/*.properties file. 

Currently, there are a number of properties in KafkaConfig that are not in the testsuite/config/*.properties file. So the tests might intend to override some properties, but that will be ignored. 

Let's either add all the configs in the testsuite/config/*.properties file or remove this depedency and override the property specified in testcase_properties.json.",jfung,nehanarkhede,Critical,Closed,Fixed,25/Oct/12 18:22,31/Oct/12 15:26
Bug,KAFKA-588,12613674,Index truncation doesn't seem to remove the last entry properly,"[2012-10-26 08:04:13,333] INFO [Kafka Log on Broker 3], Truncated log segment /tmp/kafka_server_3_logs/test_1-0/00000000000000130500.log to target offset 429050 (kafka.log.
Log)
[2012-10-26 08:04:13,333] INFO [ReplicaFetcherManager on broker 3] adding fetcher on topic test_1, partion 0, initOffset 429050 to broker 2 with fetcherId 0 (kafka.server.R
eplicaFetcherManager)
[2012-10-26 08:04:13,335] INFO Replica Manager on Broker 3: Handling leader and isr request LeaderAndIsrRequest(1,,1000,Map((test_1,1) -> PartitionStateInfo({ ""ISR"":""2,3"",""leader"":""2"",""leaderEpoch"":""2"" },3), (test_1,0) -> PartitionStateInfo({ ""ISR"":""2,3"",""leader"":""2"",""leaderEpoch"":""2"" },3))) (kafka.server.ReplicaManager)
[2012-10-26 08:04:13,335] INFO Replica Manager on Broker 3: Starting the follower state transition to follow leader 2 for topic test_1 partition 1 (kafka.server.ReplicaManager)
[2012-10-26 08:04:13,335] INFO Partition [test_1, 1] on broker 3: Current leader epoch [2] is larger or equal to the requested leader epoch [2], discard the become follower request (kafka.cluster.Partition)
[2012-10-26 08:04:13,336] INFO Replica Manager on Broker 3: Starting the follower state transition to follow leader 2 for topic test_1 partition 0 (kafka.server.ReplicaManager)
[2012-10-26 08:04:13,336] INFO Partition [test_1, 0] on broker 3: Current leader epoch [2] is larger or equal to the requested leader epoch [2], discard the become follower request (kafka.cluster.Partition)
[2012-10-26 08:04:13,588] ERROR [ReplicaFetcherThread-2-0-on-broker-3], Error due to  (kafka.server.ReplicaFetcherThread)
java.lang.IllegalArgumentException: Attempt to append an offset (429050) no larger than the last offset appended (429050).
        at kafka.log.OffsetIndex.append(OffsetIndex.scala:180)
        at kafka.log.LogSegment.append(LogSegment.scala:56)
        at kafka.log.Log.append(Log.scala:273)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:116)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:99)
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:99)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",jkreps,junrao,Blocker,Closed,Fixed,26/Oct/12 16:00,04/Dec/12 23:45
Bug,KAFKA-589,12613686,Clean shutdown after startup connection failure,"Hi,

I'm embedding the kafka server (0.7.2) in an application container.   I've noticed that if I try to start the server without zookeeper being available, by default it gets a zk connection timeout after 6 seconds, and then throws an Exception out of KafkaServer.startup()....E.g., I see this stack trace:

Exception in thread ""main"" org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 6000
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:44)
	at kafka.log.LogManager.<init>(LogManager.scala:93)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:58)
        ....
        ....

So that's ok, I can catch the exception, and then shut everything down gracefully, in this case.  However, when I do this, it seems there is a daemon thread still around, which doesn't quit, and so the server never actually exits the jvm.  Specifically, this thread seems to hang around:

""kafka-logcleaner-0"" prio=5 tid=7fd9b48b1000 nid=0x112c08000 waiting on condition [112c07000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7f40d4be8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
	at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:609)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:602)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:680)

Looking at the code in kafka.log.LogManager(), it does seem like it starts up the scheduler to clean logs, before then trying to connect to zk (and in this case fail):

  /* Schedule the cleanup task to delete old logs */
  if(scheduler != null) {
    info(""starting log cleaner every "" + logCleanupIntervalMs + "" ms"")    
    scheduler.scheduleWithRate(cleanupLogs, 60 * 1000, logCleanupIntervalMs)
  }

So this scheduler does not appear to be stopped if startup fails.  However, if I catch the above RuntimeException, and then call KafkaServer.shutdown(), then it will stop the scheduler, and all is good.

However, it seems odd that if I get an exception when calling KafkaServer.startup(), that I should still have to do a KafkaServer.shutdown().  Rather, wouldn't it be better to have it internally cleanup after itself if startup() gets an exception?  I'm not sure I can reliably call shutdown() after a failed startup()....",ewencp,jbrosenberg,Minor,Resolved,Fixed,26/Oct/12 17:24,26/Sep/14 04:24
Bug,KAFKA-590,12613785,System Test - 4 cases failed due to insufficient no. of retry in ProducerPerformance,"1. Functional Test Area : Replication with Leader Hard Failure (1 Topic, 3 Partitions)

2. Testcases failed : 

0151 (Sync Producer, Acks = -1, No Compression)
0152 (Async Producer, Acks = -1, No Compression)
0155 (Sync Producer, Acks = -1, Compressed)
0156 (Async Producer, Acks = -1, Compressed)

3. Sample test results :

2012-10-25 18:22:20,206 - INFO - ======================================================
2012-10-25 18:22:20,206 - INFO - validating data matched
2012-10-25 18:22:20,206 - INFO - ======================================================
2012-10-25 18:22:20,206 - DEBUG - request-num-acks [-1] (kafka_system_test_utils)
2012-10-25 18:22:20,228 - INFO - no. of unique messages on topic [test_1] sent from publisher  : 900 (kafka_system_test_utils)
2012-10-25 18:22:20,235 - INFO - no. of unique messages on topic [test_1] at simple_consumer_1.log : 853 (kafka_system_test_utils)
2012-10-25 18:22:20,242 - INFO - no. of unique messages on topic [test_1] at simple_consumer_2.log : 853 (kafka_system_test_utils)
2012-10-25 18:22:20,247 - INFO - no. of unique messages on topic [test_1] at simple_consumer_3.log : 853 (kafka_system_test_utils)

4. Investigations :

a. Merge log segment files per partition:
Under test_1351181987/testcase_0151/logs/broker-1/kafka_server_1_logs:
cat test_1-0/00000000000000000000.log >> merged_test_1_0/00000000000000000000.log
cat test_1-0/00000000000000000197.log >> merged_test_1_0/00000000000000000000.log
. . .

b. Retrieve all CRC from merged data log segment:
bin/kafka-run-class.sh kafka.tools.DumpLogSegments merged_test_1_0/00000000000000000000.log | grep crc | sed 's/.* crc: //' | sort -u > test_1_0_crc.log
. . .

c. Merge the CRC files together:
cat test_1_0_crc.log >> all_crc.log
cat test_1_1_crc.log >> all_crc.log
cat test_1_2_crc.log >> all_crc.log

d. Sort the merged CRC file:
cat all_crc.log | sort -u > all_crc_sorted.log

e. Get the no. of 'failed to send' CRC in producer_performance.log (70 in this case):
grep 'failed to send' producer_performance.log | sed 's/.* crc = //' | sed 's/, key = null.*//' | sort -u | wc -l
70

f. Match those 'failed to send' CRC from producer_performance.log to see how many messages eventually got retried to send successfully:

$ for i in `grep 'failed to send' ../../producer_performance-4/producer_performance.log | sed 's/.* crc = //' | sed 's/, key = null.*//' | sort -u`; do echo -n ""$i => ""; grep $i all_crc_sorted.log || echo ""n/a""; done;
. . .
1302684126 => n/a
1456125554 => 1456125554
15299643 => n/a
1653550869 => 1653550869
1741661084 => n/a
1764395211 => 1764395211
. . .
(23 msgs are sent successfully in retry)

g. As a result, (70 messages 'failed to send' in producer_performance.log - 23 messages successfully sent in retry) = 47 messages are lost (which matches the data loss count in the test result)

Therefore, if the no. of retry is increased to a higher value, all the messages could be sent successfully.
",,jfung,Major,Closed,Fixed,27/Oct/12 23:31,31/Oct/12 21:50
Bug,KAFKA-591,12613953,Add test cases to test log size retention and more,"Add test cases to test the followings:

1. Log Size Retention

2. Replica Factor < no. of brokers in a cluster

3. Multiple instances of Migration Tool

4. Multiple instances of Mirror Maker

5. Set ""log.index.interval.bytes"" to be slightly smaller than message size to force the indexing to be performed for each message",jfung,jfung,Major,Closed,Fixed,29/Oct/12 21:05,09/Nov/12 22:58
Bug,KAFKA-592,12613969,Register metrics beans at kafka server startup ,"jmx beans are not registered until the corresponding part of the code executes. To set alerts on some of the server side beans, they need to be registered at server startup.",swapnilghike,swapnilghike,Blocker,Closed,Fixed,29/Oct/12 23:23,30/Oct/12 01:37
Bug,KAFKA-593,12613975,Empty log index file created when it shouldn't be empty,"We have met empty index file during system test when it shouldn't be empty. In this case, there're around 100 messages in each segment, each of size around 100 bytes, given the ""logIndexIntervalBytes"" 4096, there should be at least 2 log index entries, but we see empty index file. The kafka and zookeeper logs are attached



[yye@yye-ld kafka_server_3_logs]$ cd test_1-2/
[yye@yye-ld test_1-2]$ ls -l
total 84
-rw-r--r-- 1 yye eng        8 Oct 29 15:22 00000000000000000000.index
-rw-r--r-- 1 yye eng    10248 Oct 29 15:22 00000000000000000000.log
-rw-r--r-- 1 yye eng        8 Oct 29 15:22 00000000000000000100.index
-rw-r--r-- 1 yye eng    10296 Oct 29 15:22 00000000000000000100.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000200.index
-rw-r--r-- 1 yye eng    10293 Oct 29 15:23 00000000000000000200.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000300.index
-rw-r--r-- 1 yye eng    10274 Oct 29 15:23 00000000000000000300.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000399.index
-rw-r--r-- 1 yye eng    10276 Oct 29 15:23 00000000000000000399.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000498.index
-rw-r--r-- 1 yye eng    10256 Oct 29 15:23 00000000000000000498.log
-rw-r--r-- 1 yye eng 10485760 Oct 29 15:23 00000000000000000596.index
-rw-r--r-- 1 yye eng     3564 Oct 29 15:23 00000000000000000596.log
",,yeyangever,Major,Resolved,Fixed,30/Oct/12 00:07,06/Nov/12 03:44
Bug,KAFKA-596,12614203,LogSegment.firstAppendTime not reset after truncate to,"Currently, we don't reset LogSegment.firstAppendTime after the segment is truncated. What can happen is that we truncate the segment to size 0 and on next append, a new log segment with the same starting offset is rolled because the time-based rolling is triggered.",swapnilghike,junrao,Major,Closed,Fixed,31/Oct/12 16:48,05/Nov/12 23:11
Bug,KAFKA-597,12614258,Refactor KafkaScheduler,"It would be nice to cleanup KafkaScheduler. Here is what I am thinking

Extract the following interface:

trait Scheduler {
  def startup()
  def schedule(fun: () => Unit, name: String, delayMs: Long = 0, periodMs: Long): Scheduled
  def shutdown(interrupt: Boolean = false)
}

class Scheduled {
  def lastExecution: Long
  def cancel()
}

We would have two implementations, KafkaScheduler and  MockScheduler. KafkaScheduler would be a wrapper for ScheduledThreadPoolExecutor. MockScheduler would only allow manual time advancement rather than using the system clock, we would switch unit tests over to this.

This change would be different from the existing scheduler in a the following ways:
1. Would not return a ScheduledFuture (since this is useless)
2. shutdown() would be a blocking call. The current shutdown calls, don't really do what people want.
3. We would remove the daemon thread flag, as I don't think it works.
4. It returns an object which let's you cancel the job or get the last execution time.

",jkreps,jkreps,Minor,Resolved,Fixed,31/Oct/12 18:46,10/Dec/12 19:31
Bug,KAFKA-600,12614599,kafka should respond gracefully rather than crash when unable to write due to ENOSPC,"problem:
user starts kafka with log.dir value set to a small partition and begins writing data to the mq.  when the disk partition is full, kafka crashes.  given that this product is used for both reading and writing operations, crashing seems rather drastic even if the error message is helpful.   something more robust would be appreciated.  perhaps, logging an error and rejecting additional write requests while accepting additional read requests?  perhaps, sending an email alert to Operations?  at least shutdown gracefully so the user is aware that received messages were saved with a helpful message providing some details of the last message received.  when tens or hundreds of thousands of messages can be processed in a second, it isn't helpful to merely log a timestamp and crash.

steps to reproduce:
1) download and install kafka
2) modify server.properties
    # vi /opt/kafka-0.7.2-incubating-src/config/server.properties
    set log.dir=""/var/log/kafka""
3) modify log4j
    # vi /opt/kafka-0.7.2-incubating-src/config/log4j.properties
    set fileAppender.File=/var/log/kafka/kafka-request.log
4) start kafka service
    $ sudo bash
    # ulimit -c unlimited
    # /opt/kafka-0.7.2-incubating-src/bin/kafka-server-start.sh /opt/kafka-0.7.2-incubating-src/config/server.properties &
6) begin writing data to hostname:9092
7) review /var/log/kafka-request.log

results:
$ grep log.dir /opt/kafka-0.7.2-incubating-src/config/server.properties
log.dir=/var/log/kafka
$ df -h /var/log/kafka
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       4.0G  4.0G     0 100% /
$ tail /var/log/kafka/kafka-request.log
17627442 [ZkClient-EventThread-14-10.0.20.242:2181] INFO  kafka.server.KafkaZooKeeper  - Begin registering broker topic /brokers/topics/raw/0 with 1 partitions
17627444 [ZkClient-EventThread-14-10.0.20.242:2181] INFO  kafka.server.KafkaZooKeeper  - End registering broker topic /brokers/topics/raw/0
17627445 [ZkClient-EventThread-14-10.0.20.242:2181] INFO  kafka.server.KafkaZooKeeper  - done re-registering broker
18337676 [kafka-processor-3] ERROR kafka.network.Processor  - Closing socket for /10.0.20.138 because of error
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218)
        at sun.nio.ch.IOUtil.read(IOUtil.java:191)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
        at kafka.utils.Utils$.read(Utils.scala:538)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Processor.read(SocketServer.scala:311)
        at kafka.network.Processor.run(SocketServer.scala:214)
        at java.lang.Thread.run(Thread.java:722)
18391974 [kafka-processor-4] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18422004 [kafka-processor-5] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18434563 [kafka-processor-6] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18485005 [kafka-processor-7] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18497083 [kafka-processor-0] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18525720 [kafka-processor-1] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18543843 [kafka-processor-2] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18563230 [kafka-processor-4] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18575613 [kafka-processor-5] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18677568 [kafka-processor-6] ERROR kafka.network.Processor  - Closing socket for /10.0.20.138 because of error
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218)
        at sun.nio.ch.IOUtil.read(IOUtil.java:191)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
        at kafka.utils.Utils$.read(Utils.scala:538)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Processor.read(SocketServer.scala:311)
        at kafka.network.Processor.run(SocketServer.scala:214)
        at java.lang.Thread.run(Thread.java:722)
18828016 [kafka-processor-7] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
18844274 [kafka-processor-0] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
18849691 [kafka-processor-1] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
18896883 [kafka-processor-2] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
22383195 [kafka-processor-2] FATAL kafka.log.Log  - Halting due to unrecoverable I/O error while handling producer request
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:59)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:89)
        at sun.nio.ch.IOUtil.write(IOUtil.java:60)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:195)
        at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:76)
        at kafka.message.FileMessageSet.append(FileMessageSet.scala:159)
        at kafka.log.LogSegment.append(Log.scala:105)
        at kafka.log.Log.liftedTree1$1(Log.scala:246)
        at kafka.log.Log.append(Log.scala:242)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:69)
        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:53)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)
        at kafka.network.Processor.handle(SocketServer.scala:296)
        at kafka.network.Processor.read(SocketServer.scala:319)
        at kafka.network.Processor.run(SocketServer.scala:214)
        at java.lang.Thread.run(Thread.java:722)
",,polarbearcold,Major,Resolved,Fixed,02/Nov/12 22:54,14/Dec/12 14:56
Bug,KAFKA-603,12615279,System Test Data Validation Failure - Replication Factor less than No. of Broker ,,,jfung,Critical,Closed,Fixed,08/Nov/12 03:25,09/Nov/12 18:43
Bug,KAFKA-604,12615350,Add missing metrics in 0.8,"It would be good if we add the following metrics:

Producer: droppedMessageRate per topic

ReplicaManager: partition count on the broker

FileMessageSet: logFlushTimer per log (i.e., partition). Also, logFlushTime should probably be moved to LogSegment since the flush now includes index flush time.",swapnilghike,junrao,Blocker,Closed,Fixed,08/Nov/12 16:59,26/Feb/13 20:46
Bug,KAFKA-606,12615661,System Test Transient Failure (case 0302 GC Pause) - Log segments mismatched across replicas,,,jfung,Major,Closed,Fixed,11/Nov/12 21:54,03/Jul/13 21:43
Bug,KAFKA-607,12615664,System Test Transient Failure (case 4011 Log Retention) - ConsoleConsumer receives less data,,,jfung,Major,Closed,Fixed,11/Nov/12 22:32,03/Jul/13 21:43
Bug,KAFKA-608,12615925,getTopicMetadata does not respect producer config settings,"ProducerPool.scala contains the following code:
object ProducerPool{
  def createSyncProducer(configOpt: Option[ProducerConfig], broker: Broker): SyncProducer = {
    val props = new Properties()
    props.put(""host"", broker.host)
    props.put(""port"", broker.port.toString)
    if(configOpt.isDefined)
      props.putAll(configOpt.get.props.props)
    new SyncProducer(new SyncProducerConfig(props))
  }
}

Note also, that ClientUtils.getTopicMetadata() does the following:
   ProducerPool.createSyncProducer(None, brokers(i))

As a result there is no way to control the socket settings for the get metadata request.

My recommendation is that we require the config to be specified in the 

Note that this creates a new sync producer without using ANY of the settings the user had given for the producer. In particular the socket timeout is defaulted to 500ms. 

This causes unit tests to fail a lot since a newly started test may easily timeout on a 500ms request.",nehanarkhede,jkreps,Blocker,Closed,Fixed,13/Nov/12 17:58,05/Dec/12 18:32
Bug,KAFKA-609,12615928,"System Test Transient Failure 9001 (Migration tool) - ConsoleConsumer terminates after ""can't rebalance after 4 retries""",,,jfung,Major,Closed,Fixed,13/Nov/12 18:02,03/Jul/13 21:41
Bug,KAFKA-612,12616127,move shutting down of fetcher thread out of critical path,"Shutting down a fetch thread seems to take more than 200ms since we need to interrupt the thread. Currently, we shutdown fetcher threads while processing a leaderAndIsr request. This can delay some of the partitions to become a leader.",junrao,junrao,Major,Closed,Fixed,14/Nov/12 21:35,19/Nov/12 05:52
Bug,KAFKA-613,12616249,MigrationTool should disable shallow iteration in the 0.7 consumer,"If shallow iteration is enabled, we should override it and log a warning.",,junrao,Major,Closed,Fixed,15/Nov/12 15:09,17/Nov/12 01:34
Bug,KAFKA-614,12616271,DumpLogSegment offset verification is incorrect for compressed messages,"During verification, DumpLogSegment tries to make sure that offsets are consecutive. However, this won't be true for compressed messages since FileMessageSet only does shallow iteration. The simplest fix is to skip the verification for compressed messages.",,junrao,Major,Closed,Fixed,15/Nov/12 18:00,29/Nov/12 00:10
Bug,KAFKA-616,12616322,Implement acks=0,"For completeness it would be nice to handle the case where acks=0 in the produce request. The meaning of this would be that the broker immediately responds without blocking even on the local write. The advantage of this is that it would often isolate the producer from any latency in the local write (which we have occasionally seen).

Since we don't block on the append the response would contain a placeholder for all the fields--e.g. offset=-1 and no error.

This should be pretty easy to implement, just an if statement in KafkaApis.handleProduceRequest to send the response immediately in this case (and again to avoid sending a second response later).",nehanarkhede,jkreps,Major,Closed,Fixed,15/Nov/12 22:11,19/Jun/14 05:11
Bug,KAFKA-618,12616351,Deadlock between leader-finder-thread and consumer-fetcher-thread during broker failure,"This causes the test failure reported in KAFKA-607. This affects high-level consumers - if they hit the deadlock then they would get wedged (or at least until the consumer timeout).

Here is the threaddump output that shows the issue:

Found one Java-level deadlock:
=============================
""ConsumerFetcherThread-console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-0-1"":
  waiting for ownable synchronizer 0x00007f2283ad0000, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by ""console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-leader-finder-thread""
""console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-leader-finder-thread"":
  waiting to lock monitor 0x00007f2288297190 (object 0x00007f2283ab01d0, a java.lang.Object),
  which is held by ""ConsumerFetcherThread-console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-0-1""

Java stack information for the threads listed above:
===================================================
""ConsumerFetcherThread-console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-0-1"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f2283ad0000> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at kafka.consumer.ConsumerFetcherManager.getPartitionTopicInfo(ConsumerFetcherManager.scala:131)
        at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:43)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:116)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:99)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:99)
        - locked <0x00007f2283ab01d0> (a java.lang.Object)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
""console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-leader-finder-thread"":
        at kafka.server.AbstractFetcherThread.addPartition(AbstractFetcherThread.scala:142)
        - waiting to lock <0x00007f2283ab01d0> (a java.lang.Object)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:49)
        - locked <0x00007f2283ab0338> (a java.lang.Object)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$5.apply(ConsumerFetcherManager.scala:81)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$5.apply(ConsumerFetcherManager.scala:76)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.consumer.ConsumerFetcherManager$$anon$1.doWork(ConsumerFetcherManager.scala:76)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)

Found 1 deadlock.
",,jjkoshy,Blocker,Closed,Fixed,16/Nov/12 01:20,17/Nov/12 01:39
Bug,KAFKA-619,12616430,Regression : System Test 900x (Migration Tool) - java.lang.ClassCastException: kafka.message.Message cannot be cast to [B,"This error is happening in : testcase_900x (migration tool test group). The issue is that no data is received by ConsoleConsumer. All Migration Tool log4j messages are showing the following error. 

. . .
[2012-11-16 08:28:55,361] INFO FetchRunnable-1 start fetching topic: test_1 part: 3 offset: 0 from 127.0.0.1:9093 (kafka.consumer.FetcherRunnable)
[2012-11-16 08:28:55,361] INFO FetchRunnable-0 start fetching topic: test_1 part: 3 offset: 0 from 127.0.0.1:9092 (kafka.consumer.FetcherRunnable)
[2012-11-16 08:28:55,361] INFO FetchRunnable-2 start fetching topic: test_1 part: 0 offset: 0 from 127.0.0.1:9091 (kafka.consumer.FetcherRunnable)
Migration thread failure due to java.lang.ClassCastException: kafka.message.Message cannot be cast to [B
java.lang.ClassCastException: kafka.message.Message cannot be cast to [B
        at kafka.serializer.DefaultEncoder.toBytes(Encoder.scala:36)
        at kafka.producer.async.DefaultEventHandler$$anonfun$serialize$1.apply(DefaultEventHandler.scala:111)
        at kafka.producer.async.DefaultEventHandler$$anonfun$serialize$1.apply(DefaultEventHandler.scala:106)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32)
        at kafka.producer.async.DefaultEventHandler.serialize(DefaultEventHandler.scala:106)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:47)
        at kafka.producer.Producer.send(Producer.scala:75)
        at kafka.javaapi.producer.Producer.send(Producer.scala:32)
        at kafka.tools.KafkaMigrationTool$MigrationThread.run(KafkaMigrationTool.java:287)
[2012-11-16 08:30:04,854] INFO test-consumer-group_jfung-ld-1353083318174-2b62271b begin rebalancing consumer test-consumer-group_jfung-ld-1353083318174-2b62271b try #0 (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-16 08:30:04,858] INFO Constructing topic count for test-consumer-group_jfung-ld-1353083318174-2b62271b from *2*.* using \*(\p{Digit}+)\*(.*) as pattern. (kafka.consumer.TopicCount$)
. . .",,jfung,Major,Closed,Fixed,16/Nov/12 16:35,03/Jul/13 21:40
Bug,KAFKA-621,12616447,System Test 9051 : ConsoleConsumer doesn't receives any data for 20 topics but works for 10,"* This issue may be related to KAFKA-618

* To reproduce the issue:
    1. Download the latest 0.8 branch and apply the attached patch
    2. In <kafka_home>, execute ""./sbt update package""
    3. In <kafka_home>/system_test, execute ""python -B system_test_runner.py"" and it will execute testcase_9051

* The validation output would be as follows:
validation_status  : 
     Unique messages from consumer on [t001]  :  0
     Unique messages from consumer on [t002]  :  0
     . . .
     Unique messages from consumer on [t019]  :  0
     Unique messages from consumer on [t020]  :  0
     Unique messages from producer on [t001]  :  1000
     Unique messages from producer on [t002]  :  1000
     . . .
     Unique messages from producer on [t018]  :  1000
     Unique messages from producer on [t019]  :  1000
     Unique messages from producer on [t020]  :  1000
     Validate for data matched on topic [t001]  :  FAILED
     Validate for data matched on topic [t002]  :  FAILED
     . . .
     Validate for data matched on topic [t019]  :  FAILED
     Validate for data matched on topic [t020]  :  FAILED
     Validate for merged log segment checksum in cluster [source]  :  PASSED

* However, it will work fine if there are only 10 topics
     In system_test/replication_testsuite/testcase_9051/testcase_9051_properties.json, update the following line to 10 topics:

      ""topic"": ""t001,t002,t003,t004,t005,t006,t007,t008,t009,t010,t011,t012,t013,t014,t015,t016,t017,t018,t019,t020"",",,jfung,Major,Closed,Fixed,16/Nov/12 17:39,03/Jul/13 21:38
Bug,KAFKA-622,12616485,Create mbeans per client ,"Currently we create one mbean of each type for a given mbean server, regardless of the number of clients. We should create MBeans per client for both producer and consumer. To do that we need to introduce clientId in mbean names.",swapnilghike,swapnilghike,Blocker,Closed,Fixed,16/Nov/12 21:51,04/Dec/12 20:06
Bug,KAFKA-626,12616759,Produce requests dropped due to socket timeouts on get metadata requests,"The setup of the test includes 2 servers with the following properties overridden -

num.partitions=10
default.replication.factor=2

Ran producer performance to send 1000 messages to 8 topics in async mode. Each of the topics are auto created on the broker and default to 10 partitions. No broker was bounced during this test. 

The producer log has the following errors -

[2012-11-18 17:44:04,622] WARN fetching topic metadata for topics [Set(test1114, test1117, test1115, test1116, test1118)] from broker [id:0,creatorId:localhost-1353289442325,host:localhost,port:9091] failed (kafka.client.ClientUtils$)
java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:201)
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86)
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:221)
        at kafka.utils.Utils$.read(Utils.scala:393)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56)
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:76)
        at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:74)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:101)
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:25)
        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:75)
        at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:62)
        at kafka.utils.Utils$.swallow(Utils.scala:185)
        at kafka.utils.Logging$class.swallowError(Logging.scala:105)
        at kafka.utils.Utils$.swallowError(Utils.scala:44)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:62)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:103)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:86)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:66)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:65)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:43)
[2012-11-18 17:44:04,624] INFO Fetching metadata for topic Set(test1114, test1117, test1115, test1116, test1118) (kafka.client.ClientUtils$)
[2012-11-18 17:44:04,624] INFO Connected to localhost:9092 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,805] INFO Disconnecting from localhost:9092 (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,806] INFO Disconnecting from 127.0.0.1:9091 (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,806] INFO Disconnecting from 127.0.0.1:9092 (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,815] INFO Connected to 127.0.0.1:9092 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,910] INFO Connected to 127.0.0.1:9091 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,048] INFO Fetching metadata for topic Set(test1115, test1118) (kafka.client.ClientUtils$)
[2012-11-18 17:44:05,049] INFO Connected to localhost:9091 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,111] INFO Disconnecting from localhost:9091 (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,112] INFO Disconnecting from 127.0.0.1:9091 (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,112] INFO Disconnecting from 127.0.0.1:9092 (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,114] ERROR Failed to send the following requests: ArrayBuffer(KeyedMessage(test1115,1,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1115,11,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1115,21,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1118,5,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1118,15,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100]))) (kafka.producer.async.DefaultEventHandler)
[2012-11-18 17:44:05,122] ERROR Error in handling batch of 200 events (kafka.producer.async.ProducerSendThread)
kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:70)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:103)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:86)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:66)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:65)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:43)


These errors don't happen when I run producer performance on fewer topics. 

Also, the consumer receives 8995 messages, the expected messages is 8000 (1000/topic). Since the producer failed to send a request, most of these messages could be duplicates from previous requests.",nehanarkhede,nehanarkhede,Blocker,Closed,Fixed,19/Nov/12 16:50,13/Dec/12 00:23
Bug,KAFKA-627,12616769,Make UnknownTopicOrPartitionException a WARN in broker,"Currently, when sending messages to a topic that doesn't yet exist, the broker spews out these ""errors"" as it tries to auto-create new topics. I spoke with Neha, and she said that this should be a warning, not an error.

Could you please change it to something less scary, if, in fact, it's not scary.

2012/11/14 22:38:53.238 INFO [LogManager] [kafka-request-handler-6] [kafka] []  [Log Manager on Broker 464] Created log for 'firehoseReads'-5
2012/11/14 22:38:53.241 WARN [HighwaterMarkCheckpoint] [kafka-request-handler-6] [kafka] []  No previously checkpointed highwatermark value found for topic firehoseReads partition 5. Returning 0 as the highwatermark
2012/11/14 22:38:53.242 INFO [Log] [kafka-request-handler-6] [kafka] []  [Kafka Log on Broker 464], Truncated log segment /export/content/kafka/i001_caches/firehoseReads-5/00000000000000000000.log to target offset 0
2012/11/14 22:38:53.242 INFO [ReplicaFetcherManager] [kafka-request-handler-6] [kafka] []  [ReplicaFetcherManager on broker 464] adding fetcher on topic firehoseReads, partion 5, initOffset 0 to broker 466 with fetcherId 0
2012/11/14 22:38:53.248 ERROR [ReplicaFetcherThread] [ReplicaFetcherThread-466-0-on-broker-464] [kafka] []  [ReplicaFetcherThread-466-0-on-broker-464], error for firehoseReads 5 to broker 466
kafka.common.UnknownTopicOrPartitionException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at java.lang.Class.newInstance0(Class.java:355)
        at java.lang.Class.newInstance(Class.java:308)
        at kafka.common.ErrorMapping$.exceptionFor(ErrorMapping.scala:68)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5$$anonfun$apply$3.apply(AbstractFetcherThread.scala:124)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5$$anonfun$apply$3.apply(AbstractFetcherThread.scala:124)
        at kafka.utils.Logging$class.error(Logging.scala:102)
        at kafka.utils.ShutdownableThread.error(ShutdownableThread.scala:23)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:123)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:99)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:99)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)",,criccomini,Major,Resolved,Fixed,19/Nov/12 17:34,12/Jul/19 22:15
Bug,KAFKA-628,12616852,System Test Failure Case 5005 (Mirror Maker bouncing) - Data Loss in ConsoleConsumer,,,jfung,Major,Closed,Fixed,19/Nov/12 22:44,14/Dec/12 18:18
Bug,KAFKA-633,12617740,AdminTest.testShutdownBroker fails,"0m[ [31merror [0m]  [0mTest Failed: testShutdownBroker(kafka.admin.AdminTest) [0m
junit.framework.AssertionFailedError: expected:<2> but was:<3>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:277)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:195)
	at junit.framework.Assert.assertEquals(Assert.java:201)
	at kafka.admin.AdminTest.testShutdownBroker(AdminTest.scala:381)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)",jjkoshy,junrao,Blocker,Closed,Fixed,27/Nov/12 05:43,07/Dec/12 01:10
Bug,KAFKA-634,12617847,ConsoleProducer compresses messages and ignores the --compress flag,"I am using the kafka-producer-shell.sh script without the --compress option

however my messages seem to be gzipped

the docs say compression is off by default:
http://incubator.apache.org/kafka/configuration.html

The only producer.properties file I can find is at:
/home/ubuntu/kafka-0.7.2-incubating-src/config/producer.properties

In there is:
compression.codec=0

My process looks like:

root      1748  1746  0 Nov19 ?        00:02:37 java -Xmx512M -server -Dlog4j.configuration=file:/usr/local/bin/kafka/../config/log4j.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -cp :/usr/local/bin/kafka/../project/boot/scala-2.8.0/lib/scala-compiler.jar:/usr/local/bin/kafka/../project/boot/scala-2.8.0/lib/scala-library.jar:/usr/local/bin/kafka/../core/target/scala_2.8.0/kafka-0.7.2.jar:/usr/local/bin/kafka/../core/lib/*.jar:/usr/local/bin/kafka/../perf/target/scala_2.8.0/kafka-perf-0.7.2.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/jopt-simple-3.2.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/log4j-1.2.15.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/snappy-java-1.0.4.1.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/zkclient-0.1.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/zookeeper-3.3.4.jar kafka.producer.ConsoleProducer --topic logtail --zookeeper x.x.x.x:2181

But the messages come out gobbledegook unless I use a client that understands compressed messages, and in that client it identifies the bit as set to 1, gzip compression.


Jun Rao junrao@gmail.com via incubator.apache.org 
Nov 26 (1 day ago)
to kafka-users 

This seems to be a bug in ConsoleProducer. It also compresses messages and
ignores the --compress flag. Could you file a jira?

Thanks,
Jun",,anentropic,Major,Resolved,Fixed,27/Nov/12 18:11,09/Jan/13 12:19
Bug,KAFKA-636,12617922,Make log segment delete asynchronous,"We have a few corner-case bugs around delete of segment files:
1. It is possible for delete and truncate to kind of cross streams and end up with a case where you have no segments.
2. Reads on the log have no locking (which is good) but as a result deleting a segment that is being read will result in some kind of I/O exception.
3. We can't easily fix the synchronization problems without deleting files inside the log's write lock. This can be a problem as deleting a 2GB segment can take a couple of seconds even on an unloaded system.

The proposed fix for these problems is to make file removal asynchronous using the following scheme as the new delete scheme:
1. Immediately remove the file from segment map and rename the~ file from X to X.deleted (e.g. 0000000.log to 000000.log.deleted. We think renaming a file will not impact reads since the file is already open and hence the name is irrelevant. This will always be O(1) and can be done inside the write lock.
2. Schedule a future operation to delete the file. The time to wait would be configurable but we would just default it to 60 seconds and probably no one would ever change it.
3. On startup we would delete any files with the .deleted suffix as they would have been pending deletes that didn't take place.

I plan to do this soon working against the refactored log (KAFKA-521). We can opt to back port the patch for 0.8 if we are feeling daring.",jkreps,jkreps,Major,Resolved,Fixed,28/Nov/12 03:51,11/Dec/12 19:47
Bug,KAFKA-640,12618184,System Test Failures : kafka.common.InvalidClientIdException in broker log4j messages,"* To reproduce the issue, download and build the latest Kafka 0.8 branch and execute this command: ""<kafka_home>/system_test $ python -B system_test_runner.py""

* The following exception is found in the broker log4j messages in most System Test cases: 

[2012-11-29 09:06:21,322] WARN No previously checkpointed highwatermark value found for topic test_1 partition 1. Returning 0 as the highwatermark (kafka.server.HighwaterMarkCheckpoint)
[2012-11-29 09:06:21,326] INFO [Kafka Log on Broker 1], Truncated log segment /tmp/kafka_server_1_logs/test_1-1/00000000000000000000.log to target offset 0 (kafka.log.Log)
[2012-11-29 09:06:21,333] ERROR Replica Manager on Broker 1: Error processing leaderAndISR request LeaderAndIsrRequest(1,,1000,Map((test_1,1) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3,1"",""leader"":""2"",""leaderEpoch"":""0"" },1),3), (test_1,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""1,2,3"",""leader"":""1"",""leaderEpoch"":""0"" },1),3)),Set(id:2,creatorId:127.0.0.1-1354208764997,host:127.0.0.1,port:9092, id:1,creatorId:127.0.0.1-1354208760105,host:127.0.0.1,port:9091),1) (kafka.server.ReplicaManager)
kafka.common.InvalidClientIdException: ClientId replica-fetcher-host_127.0.0.1-port_9092 is illegal, contains a character other than ASCII alphanumerics, _ and -
        at kafka.utils.ClientId$.validate(ClientIdAndTopic.scala:36)
        at kafka.consumer.SimpleConsumer.<init>(SimpleConsumer.scala:81)
        at kafka.server.AbstractFetcherThread.<init>(AbstractFetcherThread.scala:44)
        at kafka.server.ReplicaFetcherThread.<init>(ReplicaFetcherThread.scala:26)
        at kafka.server.ReplicaFetcherManager.createFetcherThread(ReplicaFetcherManager.scala:26)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:44)
        at kafka.cluster.Partition.makeFollower(Partition.scala:190)
        at kafka.server.ReplicaManager.kafka$server$ReplicaManager$$makeFollower(ReplicaManager.scala:236)
        at kafka.server.ReplicaManager$$anonfun$becomeLeaderOrFollower$3.apply(ReplicaManager.scala:201)
        at kafka.server.ReplicaManager$$anonfun$becomeLeaderOrFollower$3.apply(ReplicaManager.scala:191)
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)
        at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:191)
        at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:129)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:60)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:662)
",swapnilghike,jfung,Major,Closed,Fixed,29/Nov/12 17:22,30/Nov/12 17:50
Bug,KAFKA-642,12618254,Protocol tweaks for 0.8,"There are a couple of things in the protocol that are not idea. It would be good to tweak these for 0.8 so we start clean.

Here is a set of problems and proposals:

Problems:
1. Correlation id is not used across all the requests. I don't think it can work as intended because of this.
2. On reflection I am not sure that we need a correlation id field. I think that since we need to guarantee that processing is sequential on any particular socket we can correlate with a simple queue. (e.g. as the client sends messages it adds them to a queue and as it receives responses it just correlates to whatever is at the head of the queue).
3. The metadata response seems to have a number of problems. Among them is that it weirdly repeats all the broker information many times. The response includes the ISR, leader (maybe), and the replicas. Each of these repeat all the broker information. This is super weird. I think what we should be doing here is including all broker information for all brokers and then just having the appropriate ids for the isr, leader, and replicas.
4. For topic discovery I think we need to support the case where no topics are specified in the metadata request and for this return information about all topics. I don't think we do this now.
5. I don't understand what the creator id is.
6. The offset request and response is not fully thought through and should be generalized.

Proposals:
1, 2. Correlation id. This is not strictly speaking needed, but it is maybe useful for debugging to be able to trace a particular request from client to server. So we will extend this across all the requests.
3. For metadata response I will try to fix this up by normalizing out the broker list and having the isr, replicas, and leader field just have the node id.
4. This should be uncontroversial and easy to add.
5. Let's remove creator id, it isn't used.
6. Let's generalize offset request. My proposal is below:

Rename TopicMetadata API to ClusterMetadata, as this will contain all the data that is known cluster-wide. Then let's generalize the offset request to be PartitionMetadata--namely stuff about a particular partition on a particular server.

The format of PartitionMetdata would be the following:

PartitionMetadataRequest => [TopicName [PartitionId MinSegmentTime MaxSegmentInfos]]
  TopicName => string
  PartitionId => uint32
  MinSegmentTime => uint64
  MaxSegmentInfos => int32

PartitionMetadataResponse => [TopicName [PartitionMetadata]]
  TopicName => string
  PartitionMetadata => PartitionId LogSize NumberOfSegments LogEndOffset HighwaterMark [SegmentData]
  SegmentData => StartOffset LastModifiedTime
  LogSize => uint64
  NumberOfSegments => int32
  LogEndOffset => int64
  HighwaterMark => int64

This would be general enough that we could continue to add to it for any new pieces of data we need.",jkreps,jkreps,Blocker,Closed,Fixed,30/Nov/12 00:49,15/May/13 23:56
Bug,KAFKA-646,12618775,Provide aggregate stats at the high level Producer and ZookeeperConsumerConnector level,"WIth KAFKA-622, we measure ProducerRequestStats and FetchRequestAndResponseStats at the SyncProducer and SimpleConsumer level respectively. We could also aggregate them in the high level Producer and ZookeeperConsumerConnector level to provide an overall sense of request/response rate/size at the client level. Currently, I am not completely clear about the math that might be necessary for such  aggregation or if metrics already provides an API for aggregating stats of the same type.

We should also address the comments by Jun at KAFKA-622, I am copy pasting them here:

60. What happens if have 2 instances of Consumers with the same clientid in the same jvm? Does one of them fail because it fails to register metrics? Ditto for Producers.
61. ConsumerTopicStats: What if a topic is named AllTopics? We use to handle this by adding a - in topic specific stats.
62. ZookeeperConsumerConnector: Do we need to validate groupid?
63. ClientId: Does the clientid length need to be different from topic length?
64. AbstractFetcherThread: When building a fetch request, do we need to pass in brokerInfo as part of the client id? BrokerInfo contains the source broker info and the fetch requests are always made to the source broker.",swapnilghike,swapnilghike,Blocker,Closed,Fixed,04/Dec/12 19:44,17/Dec/12 22:07
Bug,KAFKA-648,12618781,Use uniform convention for naming properties keys ,"Currently, the convention that we seem to use to get a property value in *Config is as follows:

val configVal = property.getType(""config.val"", ...) // dot is used to separate two words in the key and the first letter of second word is capitalized in configVal.

We should use similar convention for groupId, consumerId, clientId, correlationId.

This change will probably be backward non-compatible.",sriramsub,swapnilghike,Blocker,Closed,Fixed,04/Dec/12 20:04,14/Jan/13 05:52
Bug,KAFKA-654,12618991,Irrecoverable error while trying to roll a segment that already exists,"I tried setting up a 5 broker 0.8 cluster and sending messages to 100s of topics on it. For a couple of topic partitions, the produce requests never succeed since they fail on the leader with the following error - 

[2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000
0.log already exists; deleting it first (kafka.log.Log)
[2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000
0.index already exists; deleting it first (kafka.log.Log)
[2012-12-05 22:54:05,715] ERROR [ReplicaFetcherThread-1-0-on-broker-2], Error due to  (kafka.server.R
eplicaFetcherThread)
kafka.common.KafkaException: Trying to roll a new log segment for topic partition NusWriteEvent-4 with start offset 0 while it already exsits
        at kafka.log.Log.rollToOffset(Log.scala:456)
        at kafka.log.Log.roll(Log.scala:434)
        at kafka.log.Log.maybeRoll(Log.scala:423)
        at kafka.log.Log.append(Log.scala:257)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)

",nehanarkhede,nehanarkhede,Blocker,Closed,Fixed,06/Dec/12 06:19,19/Dec/17 23:43
Bug,KAFKA-660,12622732,"Change (topic, partition) tuples to TopicAndPartition","For any use cases like the following :
 private val allPartitions = new Pool[(String, Int), Partition]

we should convert (topic, partition) tuples to TopicAndPartition objects.",,swapnilghike,Major,Resolved,Fixed,06/Dec/12 22:40,07/Feb/15 23:36
Bug,KAFKA-661,12622841,Prevent a shutting down broker from re-entering the ISR,"There is a timing issue in controlled shutdown that affects low-volume topics. The leader that is being shut down receives a leaderAndIsrRequest informing it is no longer the leader and thus starts up a follower which starts issuing fetch requests to the new leader. We then shrink the ISR and send a StopReplicaRequest to the shutting down broker. However, the new leader upon receiving the fetch request expands the ISR again.

This does not really have critical impact in the sense that it can cause producers to that topic to timeout. However, there are probably very few or no produce requests coming in as it primarily affects low-volume topics. The shutdown logic itself seems to be working correctly in that the leader has been successfully moved.

One possible approach would be to use the callback feature in the ControllerBrokerRequestBatch and wait until the StopReplicaRequest has been processed by the shutting down broker before shrinking the ISR; and there are probably other ways as well.",,jjkoshy,Major,Resolved,Fixed,07/Dec/12 01:14,30/Dec/19 22:54
Bug,KAFKA-664,12622938,Kafka server threads die due to OOME during long running test,"I set up a Kafka cluster with 5 brokers (JVM memory 512M) and set up a long running producer process that sends data to 100s of partitions continuously for ~15 hours. After ~4 hours of operation, few server threads (acceptor and processor) exited due to OOME -

[2012-12-07 08:24:44,355] ERROR OOME with size 1700161893 (kafka.network.BoundedByteBufferReceive)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:24:44,356] ERROR Uncaught exception in thread 'kafka-acceptor': (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:24:44,356] ERROR Uncaught exception in thread 'kafka-processor-9092-1': (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:24:46,344] INFO Unable to reconnect to ZooKeeper service, session 0x13afd0753870103 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:24:46,344] INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)
[2012-12-07 08:24:46,344] INFO Initiating client connection, connectString=eat1-app309.corp:12913,eat1-app310.corp:12913,eat1-app311.corp:12913,eat1-app312.corp:12913,eat1-app313.corp:12913 sessionTimeout=15000 watcher=org.I0Itec.zkclient.ZkClient@19202d69 (org.apache.zookeeper.ZooKeeper)
[2012-12-07 08:24:55,702] ERROR OOME with size 2001040997 (kafka.network.BoundedByteBufferReceive)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:25:01,192] ERROR Uncaught exception in thread 'kafka-request-handler-0': (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:25:08,739] INFO Opening socket connection to server eat1-app311.corp/172.20.72.75:12913 (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:25:14,221] INFO Socket connection established to eat1-app311.corp/172.20.72.75:12913, initiating session (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:25:17,943] INFO Client session timed out, have not heard from server in 3722ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:25:19,805] ERROR error in loggedRunnable (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:25:23,528] ERROR OOME with size 1853095936 (kafka.network.BoundedByteBufferReceive)
java.lang.OutOfMemoryError: Java heap space


It seems like it runs out of memory while trying to read the producer request, but its unclear so far. ",jkreps,nehanarkhede,Blocker,Closed,Fixed,07/Dec/12 18:07,06/Jan/16 19:49
Bug,KAFKA-665,12622980,Outgoing responses delayed on a busy Kafka broker ,"In a long running test, I observed that after a few hours of operation, few requests start timing out, mainly because they spent very long time sitting in the response queue -

[2012-12-07 22:05:56,670] TRACE Completed request with correlation id 3965966 and client : TopicMetadataRequest:4009, queueTime:1, localTime:28, remoteTime:0, sendTime:3980 (kafka.network.RequestChannel$)
[2012-12-07 22:04:12,046] TRACE Completed request with correlation id 3962561 and client : TopicMetadataRequest:3449, queueTime:0, localTime:29, remoteTime:0, sendTime:3420 (kafka.network.RequestChannel$)
[2012-12-07 22:05:56,670] TRACE Completed request with correlation id 3965966 and client : TopicMetadataRequest:4009, queueTime:1, localTime:28, remoteTime:0, sendTime:3980 (kafka.network.RequestChannel$)

We might have a problem in the way we process outgoing responses. Basically, if the processor thread blocks on enqueuing requests in the request queue, it doesn't come around to processing its responses which are ready to go out. ",,nehanarkhede,Critical,Closed,Fixed,07/Dec/12 22:24,14/May/16 12:42
Bug,KAFKA-668,12623441,Controlled shutdown admin tool should not require controller JMX url/port to be supplied,The controlled shutdown admin command takes a zookeeper string and also requires the user to supply the controller's jmx url/port. This is a bit annoying since the purpose of the zookeeper string is to discover the controller. The tool should require exactly one of these options. If zookeeper is supplied then discover the controller and its jmx port (which means we will need to add the jmx port information to zk).,,jjkoshy,Major,Closed,Fixed,11/Dec/12 19:37,02/Jan/13 22:09
Bug,KAFKA-670,12623596,Cleanup spurious .index files if present in the log directory on initialization,"It is possible that an index file could somehow be left on the filesystem with no corresponding log file. This is not currently handled well. If the .index file happens to fall on the same offset as a new log segment, then when that segment is created terrible things will happen.

We should check this condition on initialization and add some unit tests against it.",jkreps,jkreps,Blocker,Resolved,Fixed,12/Dec/12 17:50,13/Dec/12 21:22
Bug,KAFKA-671,12623612,DelayedProduce requests should not hold full producer request data,"Per summary, this leads to unnecessary memory usage.",sriramsub,jjkoshy,Blocker,Closed,Fixed,12/Dec/12 19:50,27/Feb/13 01:36
Bug,KAFKA-672,12623643,Merge 0.8 changes to trunk,,jkreps,jkreps,Major,Resolved,Fixed,12/Dec/12 23:50,18/Dec/12 17:51
Bug,KAFKA-673,12623824,Broker recovery check logic is reversed,We are currently running recovery when there IS a clean shutdown and not recovering when there isn't.,jkreps,jkreps,Critical,Resolved,Fixed,13/Dec/12 20:50,16/Dec/12 19:12
Bug,KAFKA-678,12625036,We should default to NullEncoder for producer,"Currently we default to using whatever serializer you set for your value to also work for your key. This works if the serializer is of a generic sort (Avro, Java serialization, etc) and both key and value map into this. However if you have a custom serializer this is not the right thing to do.

I think it would be better to default this to NullEncoder which defaults to maintain the pre-0.8 behavior of not retaining the key.",jkreps,jkreps,Major,Resolved,Fixed,21/Dec/12 18:14,09/Feb/14 23:59
Bug,HIVE-2690,12537112,a bug in 'alter table concatenate' that causes filenames getting double url encoded,,he yongqiang,he yongqiang,Major,Closed,Fixed,03/Jan/12 23:12,09/Jan/13 10:23
Bug,HIVE-2702,12537866,Enhance listPartitionsByFilter to add support for integral types both for equality and non-equality,"listPartitionsByFilter supports only non-string partitions. This is because its explicitly specified in generateJDOFilterOverPartitions in ExpressionTree.java. 

//Can only support partitions whose types are string
      if( ! table.getPartitionKeys().get(partitionColumnIndex).
          getType().equals(org.apache.hadoop.hive.serde.Constants.STRING_TYPE_NAME) ) {
        throw new MetaException
        (""Filtering is supported only on partition keys of type string"");
      }",sershe,aniket486,Major,Closed,Fixed,10/Jan/12 07:18,26/Feb/15 02:42
Bug,HIVE-2705,12537966,SemanticAnalyzer twice swallows an exception it shouldn't,"Twice SemanticAnalyzer catches an exception and drops it, just passing on the original message's in a new SemanticException. This means that those that see the message in the output cannot tell what generated the original exception.  These original exceptions should be wrapped, as they are in other parts of the code.",jghoman,jghoman,Major,Closed,Fixed,10/Jan/12 21:08,09/Jan/13 10:24
Bug,HIVE-2706,12537973,StackOverflowError when using custom UDF after adding archive after adding jars,"When a custom UDF is used in a query after add an archive, such as a zip file, after adding jars, the XMLEncoder enters an infinite loop when serializing the map reduce task, as part of sending it to be executed. This results in a stack overflow error.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,10/Jan/12 21:46,10/Apr/13 03:43
Bug,HIVE-2711,12538222,Make the header of RCFile unique,"The RCFile implementation was copied from Hadoop's SequenceFile and copied the 'magic' string in the header. This means that you can't use the header to distinguish between RCFiles and SequenceFiles.

I'd propose that we create a new header for RCFiles (RCF?) to replace the current SEQ. To maintain compatibility, we'll need to continue to accept the current 'SEQ\06' and just make new files contain the new header.",omalley,omalley,Major,Closed,Fixed,12/Jan/12 16:30,10/Jan/13 19:53
Bug,HIVE-2714,12538249,Lots of special characters are not handled in LIKE,"Currently LIKE converts the string to a regular expression.  It tries to handle special characters but many are not correctly handled, e.g. +, ?, |, etc.  That code should just use Pattern.quote.",jonchang,jonchang,Major,Closed,Fixed,12/Jan/12 19:20,09/Jan/13 10:24
Bug,HIVE-2715,12538256,Upgrade Thrift dependency to 0.9.0,"I work on HCatalog (0.2). Recently, we ran into HCat_server running out of memory every few days, and it boiled down to a bug in thrift, (THRIFT-1468, recently fixed).

HCat-0.2-branch depends on Hive-0.8, which in turn depends on thrift-0.5.0. (The bug also exists on 0.7.0.)

May I please enquire if Hive can't depend on a more current version of thrift? (Does it break the metastore?) I'm afraid I'm not privy to the reasoning behind Hive's dependency on a slightly dated thrift-lib. 
",ashutoshc,mithun,Major,Closed,Fixed,12/Jan/12 19:41,10/Jan/13 19:53
Bug,HIVE-2718,12538290,NPE in union followed by join,,he yongqiang,he yongqiang,Major,Closed,Fixed,13/Jan/12 08:03,09/Jan/13 10:24
Bug,HIVE-2721,12538589,ability to select a view qualified by the database / schema name,"HIVE-1517 added support for selecting tables from different databases (aka schemas) by qualifying the tables with the database name. The feature work did not however extend this support to views. Note that this point came up in the earlier JIRA, but was not addressed. See the following two comments:

https://issues.apache.org/jira/browse/HIVE-1517?focusedCommentId=12996641&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12996641

https://issues.apache.org/jira/browse/HIVE-1517?focusedCommentId=12996679&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12996679",martint,rmorton,Blocker,Closed,Fixed,17/Jan/12 01:31,10/Jan/13 19:53
Bug,HIVE-2723,12538597,"should throw  ""Ambiguous column reference key""  Exception in particular join condition",,navis,caofangkun,Minor,Closed,Fixed,17/Jan/12 03:14,16/May/13 21:10
Bug,HIVE-2724,12538720,Remove unused lib/log4j-1.2.15.jar,"There's still a file, lib/log4j-1.2.15.jar, even though log4j is now pulled in via Ivy. As a result, this older log4j gets pulled into the Hive release tarball, and may end up in classpaths as well. It should be removed.",abayer,abayer,Major,Closed,Fixed,18/Jan/12 00:51,09/Jan/13 10:25
Bug,HIVE-2725,12538870,Fix flaky testing infrastructure ,To begin with  org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert2_overwrite_partitions and org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl5 are failing on trunk for a while now.,ashutoshc,ashutoshc,Major,Closed,Fixed,18/Jan/12 19:38,30/Apr/12 21:11
Bug,HIVE-2732,12539233,Reduce Sink deduplication fails if the child reduce sink is followed by a join,"set hive.optimize.reducededuplication=true;
set hive.auto.convert.join=true;
explain select * from (select * from src distribute by key sort by key) a join src b on a.key = b.key;

fails with the following exception

java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.SelectOperator cannot be cast to org.apache.hadoop.hive.ql.exec.ReduceSinkOperator
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertMapJoin(MapJoinProcessor.java:313)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(MapJoinProcessor.java:226)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver$CommonJoinTaskDispatcher.processCurrentTask(CommonJoinResolver.java:174)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver$CommonJoinTaskDispatcher.dispatch(CommonJoinResolver.java:287)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(CommonJoinResolver.java:68)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:72)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapRedTasks(SemanticAnalyzer.java:7019)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7312)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:889)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

If hive.auto.convert.join is set to false, it produces an incorrect plan where the two halves of the join are processed in two separate map reduce tasks, and the reducers of these two tasks both contain the join operator resulting in an exception.",navis,kevinwilfong,Major,Closed,Fixed,21/Jan/12 00:44,10/Jan/13 19:53
Bug,HIVE-2734,12539250,Fix some nondeterministic test output,"Many Hive query tests lack an ORDER BY clause, and consequently the ordering
of the rows in the result set is nondeterministic:

groupby1_limit
input11_limit
input1_limit
input_lazyserde
join18_multi_distinct
join_1to1
join_casesensitive
join_filters
join_nulls
merge3
rcfile_columnar
rcfile_lazydecompress
rcfile_union
sample10
udf_sentences
union24
columnarserde_create_shortcut
combine1
global_limit",zhenxiao,zhenxiao,Major,Closed,Fixed,21/Jan/12 03:09,09/Jan/13 10:25
Bug,HIVE-2735,12539356,PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table,"As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.",sushanth,ashutoshc,Major,Closed,Fixed,22/Jan/12 22:10,09/Jan/13 10:25
Bug,HIVE-2736,12539474,Hive UDFs cannot emit binary constants,"I recently wrote a UDF which emits BINARY values (as implemented in [HIVE-2380|https://issues.apache.org/jira/browse/HIVE-2380]). When testing this, I encountered the following exception (because I was evaluating f(g(constant string))) and g() was emitting a BytesWritable type.

FAILED: Hive Internal Error: java.lang.RuntimeException(Internal error: Cannot find ConstantObjectInspector for BINARY)
java.lang.RuntimeException: Internal error: Cannot find ConstantObjectInspector for BINARY
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(PrimitiveObjectInspectorFactory.java:196)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getConstantObjectInspector(ObjectInspectorUtils.java:899)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:128)
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:214)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:684)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:805)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:161)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:7708)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:2301)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:2103)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:6126)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:6097)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6723)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7484)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:889)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

It looks like a pretty simple fix - add a case for BINARY in PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector() and implement a WritableConstantByteArrayObjectInspector class (almost identical to the others). I'm happy to do this, although this is my first foray into the world of contributing to FOSS so I might end up asking a few stupid questions.",philip.tromans,philip.tromans,Minor,Closed,Fixed,23/Jan/12 14:22,10/Jan/13 19:53
Bug,HIVE-2746,12539686,Metastore client doesn't log properly in case of connection failure to server,LOG.error(e.getStackTrace()) in current code prints memory location of StackTraceElement[] instead of message.,ashutoshc,ashutoshc,Major,Closed,Fixed,24/Jan/12 22:49,02/May/13 02:29
Bug,HIVE-2749,12539838,CONV returns incorrect results sometimes,...because it fails to reset state.,jonchang,jonchang,Major,Closed,Fixed,25/Jan/12 22:44,09/Jan/13 10:24
Bug,HIVE-2750,12539853,Hive multi group by single reducer optimization causes invalid column reference error,"After the optimization, if two query blocks have the same distinct clause and the same group by keys, but the first query block does not reference all the rows the second query block does, an invalid column reference error is raised for the columns unreferenced in the first query block.

E.g.
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT src.key) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT src.key), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1);

This results in an invalid column reference error on src.value",kevinwilfong,kevinwilfong,Major,Closed,Fixed,26/Jan/12 02:09,18/Oct/16 22:42
Bug,HIVE-2752,12539890,Index names are case sensitive,"The following script:

DROP TABLE IF EXISTS TestTable;

CREATE TABLE TestTable (a INT);

DROP INDEX IF EXISTS TestTableA_IDX ON TestTable;

CREATE INDEX TestTableA_IDX ON TABLE TestTable (a) AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' WITH DEFERRED REBUILD;

ALTER INDEX TestTableA_IDX ON TestTable REBUILD;

results in the following exception:

MetaException(message:index testtablea_idx doesn't exist)
	at org.apache.hadoop.hive.metastore.ObjectStore.alterIndex(ObjectStore.java:1880)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$30.run(HiveMetaStore.java:1930)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$30.run(HiveMetaStore.java:1927)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.executeWithRetry(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_index(HiveMetaStore.java:1927)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_index(HiveMetaStoreClient.java:868)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterIndex(Hive.java:398)
	at org.apache.hadoop.hive.ql.exec.DDLTask.alterIndex(DDLTask.java:902)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:236)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:338)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:436)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:642)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

When you execute: ""SHOW INDEXES ON TestTable;"", you get:

TestTableA_IDX      	testtable           	a                   	default__testtable_testtablea_idx__	compact

so it looks like things don't get lower cased when they go into the metastore, but they do when the rebuild op is trying to execute.",navis,philip.tromans,Minor,Resolved,Fixed,26/Jan/12 11:29,29/Mar/14 06:10
Bug,HIVE-2753,12539932,Remove empty java files,"When looking at the 0.8.1 rc1, I discovered there were a set of empty Java files that were likely left over from using 'patch' without the -E.

{quote}
jdbc/src/java/org/apache/hadoop/hive/jdbc/JdbcSessionState.java
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeIndexEvaluator.java
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinObject.java
ql/src/java/org/apache/hadoop/hive/ql/exec/PathUtil.java
ql/src/java/org/apache/hadoop/hive/ql/exec/TypedBytesRecordReader.java
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterPartitionProtectModeDesc.java
ql/src/java/org/apache/hadoop/hive/ql/plan/TouchDesc.java
ql/src/test/org/apache/hadoop/hive/ql/plan/TestAddPartition.java
serde/src/gen-java/org/apache/hadoop/hive/serde/test/Constants.java
shims/src/0.20/java/org/apache/hadoop/fs/ProxyFileSystem.java
shims/src/0.20/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
{quote}",omalley,omalley,Major,Closed,Fixed,26/Jan/12 17:24,09/Jan/13 10:25
Bug,HIVE-2754,12539954,NPE in union with lateral view,,he yongqiang,he yongqiang,Major,Closed,Fixed,26/Jan/12 19:20,09/Jan/13 10:25
Bug,HIVE-2755,12539976,union follwowed by union_subq does not work if the subquery union has reducers,,he yongqiang,he yongqiang,Major,Closed,Fixed,26/Jan/12 23:30,09/Jan/13 10:25
Bug,HIVE-2757,12539988,hive can't find hadoop executor scripts without HADOOP_HOME set,"The trouble is that in Hadoop 0.23 HADOOP_HOME has been deprecated. I think it would be really nice if bin/hive can be modified to capture the which hadoop
and pass that as a property into the JVM.",rvs,rvs,Major,Closed,Fixed,27/Jan/12 02:21,10/Jan/13 19:53
Bug,HIVE-2758,12540176,Metastore is caching too aggressively,"The metastore is caching values, like table names and locations too aggressively, leading to inconsistencies across Hive clients and metastore Thrift servers.

For example, open two Hive clients, in each call
DESCRIBE FORMATTED table_foo;

Then in one of those clients, execute
ALTER TABLE table_foo RENAME TO table_bar;

Then in both clients call
DESCRIBE FORMATTED table_bar;

In the client that executed the alter command, the location is correct, however, in the other Hive client, it will still show the original location of table_foo.

A similar experiment can be done using metastore Thrift servers, substituting get_table for DESCRIBE FORMATTED and alter_table for ALTER TABLE ... RENAME TO.

On the Thrift server you can see that the one which did not execute the alter command, not only returns the wrong location, despite calling get_table('table_bar') it will return a table that still has the name table_foo.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,27/Jan/12 18:43,31/Oct/14 12:28
Bug,HIVE-2759,12540183,Change global_limit.q into linux format file,"global_limit.q is in dos format, and has ""\ No newline at end of file"".
Needs to be changed into linux format, removing all ""^M"" at the end of each line.",zhenxiao,zhenxiao,Major,Closed,Fixed,27/Jan/12 19:49,09/Jan/13 10:25
Bug,HIVE-2761,12540304,Remove lib/javaewah-0.3.jar,"After HIVE-2391 it is retrieved from maven repo via ivy, we can get rid of it from our lib/",appodictic,ashutoshc,Major,Closed,Fixed,29/Jan/12 21:16,09/Jan/13 10:25
Bug,HIVE-2762,12540455,Alter Table Partition Concatenate Fails On Certain Characters,"Alter table partition concatenate creates a Java URI object for the location of a partition.  If the partition name contains certain characters, such as } or space ' ', the object constructor fails, causing the query to fail. ",kevinwilfong,kevinwilfong,Major,Closed,Fixed,30/Jan/12 23:14,09/Jan/13 10:25
Bug,HIVE-2769,12540647,union with a multi-table insert is not working,,namit,namit,Major,Closed,Fixed,31/Jan/12 23:23,09/Jan/13 10:23
Bug,HIVE-2772,12540739,make union31.q deterministic,,namit,namit,Major,Closed,Fixed,01/Feb/12 16:55,09/Jan/13 10:23
Bug,HIVE-2778,12541359,Fail on table sampling ,"Trying table sampling on any non-empty table throws NPE. This does not occur by test on mini-MR.
{noformat}
select count(*) from emp tablesample (0.1 percent);     
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(CombineHiveInputFormat.java:450)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:403)
	at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:971)
	at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:963)
	at org.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:880)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:833)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:807)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:432)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:136)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Job Submission failed with exception 'java.lang.NullPointerException(null)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask

{noformat}",navis,navis,Major,Closed,Fixed,06/Feb/12 02:36,02/May/13 02:30
Bug,HIVE-2782,12541720,New BINARY type produces unexpected results with supported UDFS when using MapReduce2,"When using MapReduce2 for Hive

ba_table_udfs is failing with unexpected output:

[junit] Begin query: ba_table_udfs.q
[junit] 12/01/23 13:32:28 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 13:32:28 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] diff -a -I file: -I pfile: -I hdfs: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I lastAccessTime -I [Oo]wner -I CreateTime -I LastAccessTime -I Location -I LOCATION ' -I transient_lastDdlTime -I last_modified_ -I java.lang.RuntimeException -I at org -I at sun -I at java -I at junit -I Caused by: -I LOCK_QUERYID: -I LOCK_TIME: -I grantTime -I [.][.][.] [0-9]* more -I job_[0-9]*_[0-9]* -I USING 'java -cp /home/cloudera/Code/hive/build/ql/test/logs/clientpositive/ba_table_udfs.q.out /home/cloudera/Code/hive/ql/src/test/results/clientpositive/ba_table_udfs.q.out
[junit] 20,26c20,26
[junit] < 2	10val_10	1
[junit] < 3	164val_164	1
[junit] < 3	150val_150	1
[junit] < 2	18val_18	1
[junit] < 3	177val_177	1
[junit] < 2	12val_12	1
[junit] < 2	11val_11	1
[junit] —
[junit] > 3	120val_120	1
[junit] > 3	192val_192	1
[junit] > 3	119val_119	1
[junit] > 3	187val_187	1
[junit] > 3	176val_176	1
[junit] > 3	199val_199	1
[junit] > 3	118val_118	1
[junit] Exception: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] junit.framework.AssertionFailedError: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] at junit.framework.Assert.fail(Assert.java:50)
[junit] at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_udfs(TestCliDriver.java:129)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] at java.lang.reflect.Method.invoke(Method.java:616)
[junit] at junit.framework.TestCase.runTest(TestCase.java:168)
[junit] at junit.framework.TestCase.runBare(TestCase.java:134)
[junit] at junit.framework.TestResult$1.protect(TestResult.java:110)
[junit] at junit.framework.TestResult.runProtected(TestResult.java:128)
[junit] at junit.framework.TestResult.run(TestResult.java:113)
[junit] at junit.framework.TestCase.run(TestCase.java:124)
[junit] at junit.framework.TestSuite.runTest(TestSuite.java:243)
[junit] at junit.framework.TestSuite.run(TestSuite.java:238)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.)
[junit] Cleaning up TestCliDriver
[junit] Tests run: 2, Failures: 1, Errors: 0, Time elapsed: 10.751 sec
[junit] Test org.apache.hadoop.hive.cli.TestCliDriver FAILED
[for] /home/cloudera/Code/hive/ql/build.xml: The following error occurred while executing this line:
[for] /home/cloudera/Code/hive/build.xml:328: The following error occurred while executing this line:
[for] /home/cloudera/Code/hive/build-common.xml:453: Tests failed!",cwsteinbach,zhenxiao,Major,Closed,Fixed,08/Feb/12 00:25,09/Jan/13 10:23
Bug,HIVE-2788,12541740,"When integrating into MapReduce2, Hive is unable to handle corrupt rcfile archive","archive_corrupt.q is failing due to file format exception when loading archive_corrupt.rc:

[junit] Running org.apache.hadoop.hive.cli.TestCliDriver
[junit] Begin query: archive_corrupt.q
[junit] Copying file: file:/home/cloudera/Code/hive/data/files/archive_corrupt.rc
[junit] Exception: Client Execution failed with error code = 9
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] junit.framework.AssertionFailedError: Client Execution failed with error code = 9
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] at junit.framework.Assert.fail(Assert.java:50)
[junit] at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_corrupt(TestCliDriver.java:109)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] at java.lang.reflect.Method.invoke(Method.java:616)
[junit] at junit.framework.TestCase.runTest(TestCase.java:168)
[junit] at junit.framework.TestCase.runBare(TestCase.java:134)
[junit] at junit.framework.TestResult$1.protect(TestResult.java:110)
[junit] at junit.framework.TestResult.runProtected(TestResult.java:128)
[junit] at junit.framework.TestResult.run(TestResult.java:113)
[junit] at junit.framework.TestCase.run(TestCase.java:124)
[junit] at junit.framework.TestSuite.runTest(TestSuite.java:243)
[junit] at junit.framework.TestSuite.run(TestSuite.java:238)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.)
[junit] Cleaning up TestCliDriver
[junit] Tests run: 2, Failures: 1, Errors: 0, Time elapsed: 6.778 sec
[junit] Test org.apache.hadoop.hive.cli.TestCliDriver FAILED
[for] /home/cloudera/Code/hive/ql/build.xml: The following error occurred while executing this line:
[for] /home/cloudera/Code/hive/build.xml:328: The following error occurred while executing this line:
[for] /home/cloudera/Code/hive/build-common.xml:453: Tests failed!

And, in /build/ql/test/logs/clientpositive/archive_corrupt.q.out:

PREHOOK: query: drop table tstsrcpart
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table tstsrcpart
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table tstsrcpart like srcpart
PREHOOK: type: CREATETABLE
POSTHOOK: query: create table tstsrcpart like srcpart
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@tstsrcpart
PREHOOK: query: load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')
PREHOOK: type: LOAD
PREHOOK: Output: default@tstsrcpart
Failed with exception Wrong file format. Please check the file's format.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask


When running the following:
load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')

Get exception:
Failed with exception Wrong file format. Please check the file's format.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask

",zhenxiao,zhenxiao,Major,Closed,Fixed,08/Feb/12 02:59,10/Jan/13 19:53
Bug,HIVE-2789,12541742,query_properties.q contains non-deterministic queries,"query_properties.q test failure:

[junit] Begin query: query_properties.q
[junit] 12/01/23 16:59:13 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:13 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:18 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:18 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:22 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:22 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:27 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:27 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:32 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:32 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:36 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:36 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:41 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:41 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:46 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:46 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:50 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:50 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:55 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:55 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 16:59:59 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 16:59:59 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:04 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:04 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:08 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:08 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:13 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:13 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:18 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:18 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:22 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:22 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:27 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:27 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] 12/01/23 17:00:31 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 17:00:31 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] diff -a -I file: -I pfile: -I hdfs: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I lastAccessTime -I [Oo]wner -I CreateTime -I LastAccessTime -I Location -I LOCATION ' -I transient_lastDdlTime -I last_modified_ -I java.lang.RuntimeException -I at org -I at sun -I at java -I at junit -I Caused by: -I LOCK_QUERYID: -I LOCK_TIME: -I grantTime -I [.][.][.] [0-9]* more -I job_[0-9]*_[0-9]* -I USING 'java -cp /home/cloudera/Code/hive/build/ql/test/logs/clientpositive/query_properties.q.out /home/cloudera/Code/hive/ql/src/test/results/clientpositive/query_properties.q.out
[junit] 91c91
[junit] < 97	val_97
[junit] —
[junit] > 238	val_238
[junit] junit.framework.AssertionFailedError: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] at junit.framework.Assert.fail(Assert.java:50)
[junit] at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_properties(TestCliDriver.java:227)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] at java.lang.reflect.Method.invoke(Method.java:616)
[junit] at junit.framework.TestCase.runTest(TestCase.java:168)
[junit] at junit.framework.TestCase.runBare(TestCase.java:134)
[junit] at junit.framework.TestResult$1.protect(TestResult.java:110)
[junit] at junit.framework.TestResult.runProtected(TestResult.java:128)
[junit] at junit.framework.TestResult.run(TestResult.java:113)
[junit] at junit.framework.TestCase.run(TestCase.java:124)
[junit] at junit.framework.TestSuite.runTest(TestSuite.java:243)
[junit] at junit.framework.TestSuite.run(TestSuite.java:238)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] Exception: Client execution results failed with error code = 1
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.)
[junit] Cleaning up TestCliDriver

The query that produces the diff is:

SELECT * FROM SRC DISTRIBUTE BY src.key LIMIT 1

The query produces nondeterministic results without and ORDER BY clause.",zhenxiao,zhenxiao,Major,Closed,Fixed,08/Feb/12 03:08,10/Jan/13 19:53
Bug,HIVE-2791,12541746,filter is still removed due to regression of HIVE-1538 althougth HIVE-2344,,binlijin,binlijin,Major,Closed,Fixed,08/Feb/12 04:46,28/Jan/16 00:25
Bug,HIVE-2792,12541916,SUBSTR(CAST(<string> AS BINARY)) produces unexpected results,,navis,cwsteinbach,Major,Closed,Fixed,09/Feb/12 02:25,09/Jan/13 10:23
Bug,HIVE-2793,12541920,Disable loadpart_err.q on 0.23,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,09/Feb/12 03:40,09/Jan/13 10:23
Bug,HIVE-2794,12541930,Aggregations without grouping should return NULL when applied to partitioning column of a partitionless table,,zhenxiao,cwsteinbach,Major,Closed,Fixed,09/Feb/12 06:02,16/May/13 21:10
Bug,HIVE-2800,12542211,"NPE in ""create index"" without comment clause in external metastore","This happens only when using external metastore (with --hiveconf hive.metastore.uris=thrift://localhost:8088 --hiveconf hive.metastore.local=false). Also if I gave a comment in the statement, this exception go away.

Here is the statement:
create index test111 on table hcat_test(name) as 'compact' with deferred rebuild;

Here is the stack:
2012-02-10 17:07:42,612 ERROR exec.Task (SessionState.java:printError(380)) - FAILED: Error in metadata: java.lang.NullPointerException
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.createIndex(Hive.java:725)
        at org.apache.hadoop.hive.ql.exec.DDLTask.createIndex(DDLTask.java:822)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1291)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1082)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:933)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:185)
        at org.apache.hadoop.hive.metastore.api.Index.write(Index.java:1032)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_index_args.write(ThriftHiveMetastore.java:47518)
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_add_index(ThriftHiveMetastore.java:1675)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.add_index(ThriftHiveMetastore.java:1666)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createIndex(HiveMetaStoreClient.java:853)
        at org.apache.hadoop.hive.ql.metadata.Hive.createIndex(Hive.java:722)
        ... 17 more",sushanth,daijy,Minor,Closed,Fixed,11/Feb/12 01:17,10/Jan/13 19:54
Bug,HIVE-2803,12542283,utc_from_timestamp and utc_to_timestamp returns incorrect results.,"How to reproduce:

{noformat}
$ echo ""2011-12-25 09:00:00.123456"" > /tmp/data5.txt
hive> create table ts1(t1 timestamp);
hive> load data local inpath '/tmp/data5.txt' overwrite into table ts1;
hive> select t1, from_utc_timestamp(t1, 'JST'), from_utc_timestamp(t1, 'JST') from ts1 limit 1;
{noformat}

The following result is expected:
{noformat}
 2011-12-25 09:00:00.123456      2011-12-25 18:00:00.123456      2011-12-25 18:00:00.123456
{noformat}
However, the above query return incorrect result like this:
{noformat}
 2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456
{noformat}

This is because GenericUDFFromUtcTimestamp.applyOffset() does setTime() improperly.
On evaluating query, timestamp argument always returns the same instance.
GenericUDFFromUtcTimestamp.applyOffset() does setTime() on the instance.
That means it adds all offsets in the query.
",tamtam180,tamtam180,Major,Closed,Fixed,12/Feb/12 11:50,10/Jan/13 19:53
Bug,HIVE-2804,12542461,Task log retrieval fails on Hadoop 0.23,,zhenxiao,cwsteinbach,Major,Closed,Fixed,13/Feb/12 23:44,02/May/13 02:30
Bug,HIVE-2811,12542982,Export LANG=en_US.UTF-8 to environment while running tests,"Most OS has this value by default, but in some cases (on apache build machines) it has a different value making inputddl5.q test to fail. This variable should be exported while running the tests.",appodictic,ashutoshc,Major,Closed,Fixed,17/Feb/12 01:59,09/Jan/13 10:24
Bug,HIVE-2817,12543928,Drop any table even without privilege,"You can drop any table if you use fully qualified name 'database.table' even you don't have any previlige.

{code}
hive> set hive.security.authorization.enabled=true;
hive> revoke all on default from user test_user;
hive> drop table abc;
hive> drop table abc;
Authorization failed:No privilege 'Drop' found for outputs { database:default, table:abc}. Use show grant to get more details.
hive> drop table default.abc;
OK
Time taken: 0.13 seconds
{code}

The table and the file in {{/usr/hive/warehouse}} or external file will be deleted. If you don't have hadoop access permission on {{/usr/hive/warehouse}} or external files, you will see a hadoop access error

{code}
12/02/23 15:35:35 ERROR hive.log: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=test_user, access=WRITE, inode=""/user/myetl"":myetl:etl:drwxr-xr-x
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
{code}
",chenchun,bewang.tech,Major,Resolved,Fixed,23/Feb/12 23:38,14/Apr/14 22:15
Bug,HIVE-2818,12543930,"Create table should check privilege of target database, not default database","Hive seems check the current database to determine the privilege of a statement when you use fully qualified name like 'database.table'
 
{code}
hive> set hive.security.authorization.enabled=true;
hive> create database test_db;
hive> grant all on database test_db to user test_user;
hive> revoke all on database default from test_user;
hive> use default;
hive> create table test_db.new_table (id int);
Authorization failed:No privilege 'Create' found for outputs { database:default}. Use show grant to get more details.
hive> use test_db;
hive> create table test_db.new_table (id int);
{code}",navis,bewang.tech,Major,Resolved,Fixed,23/Feb/12 23:55,31/Mar/14 22:13
Bug,HIVE-2820,12543937,Invalid tag is used for MapJoinProcessor,"Testing HIVE-2810, I've found tag and alias are used in very confusing manner. For example, query below fails..

{code}
hive> set hive.auto.convert.join=true;                                                                                     
hive> select /*+ STREAMTABLE(a) */ * from myinput1 a join myinput1 b on a.key=b.key join myinput1 c on a.key=c.key;        
Total MapReduce jobs = 4
Ended Job = 1667415037, job is filtered out (removed at runtime).
Ended Job = 1739566906, job is filtered out (removed at runtime).
Ended Job = 1113337780, job is filtered out (removed at runtime).
12/02/24 10:27:14 WARN conf.HiveConf: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /home/navis/hive/conf/hive-default.xml
Execution log at: /tmp/navis/navis_20120224102727_cafe0d8d-9b21-441d-bd4e-b83303b31cdc.log
2012-02-24 10:27:14	Starting to launch local task to process map join;	maximum memory = 932118528
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.processOp(HashTableSinkOperator.java:312)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.startForward(MapredLocalTask.java:325)
	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:272)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:685)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Execution failed with exit status: 2
Obtaining error information
{code}

Failed task has a plan which doesn't make sense.
{noformat}
  Stage: Stage-8
    Map Reduce Local Work
      Alias -> Map Local Tables:
        b 
          Fetch Operator
            limit: -1
        c 
          Fetch Operator
            limit: -1
      Alias -> Map Local Operator Tree:
        b 
          TableScan
            alias: b
            HashTable Sink Operator
              condition expressions:
                0 {key} {value}
                1 {key} {value}
                2 {key} {value}
              handleSkewJoin: false
              keys:
                0 [Column[key]]
                1 [Column[key]]
                2 [Column[key]]
              Position of Big Table: 0
        c 
          TableScan
            alias: c
            Map Join Operator
              condition map:
                   Inner Join 0 to 1
                   Inner Join 0 to 2
              condition expressions:
                0 {key} {value}
                1 {key} {value}
                2 {key} {value}
              handleSkewJoin: false
              keys:
                0 [Column[key]]
                1 [Column[key]]
                2 [Column[key]]
              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
              Position of Big Table: 0
              Select Operator
                expressions:
                      expr: _col0
                      type: int
                      expr: _col1
                      type: int
                      expr: _col4
                      type: int
                      expr: _col5
                      type: int
                      expr: _col8
                      type: int
                      expr: _col9
                      type: int
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-4
    Map Reduce
      Alias -> Map Operator Tree:
        a 
          TableScan
            alias: a
            HashTable Sink Operator
              condition expressions:
                0 {key} {value}
                1 {key} {value}
                2 {key} {value}
              handleSkewJoin: false
              keys:
                0 [Column[key]]
                1 [Column[key]]
                2 [Column[key]]
              Position of Big Table: 0
      Local Work:
        Map Reduce Local Work
{noformat}",navis,navis,Major,Closed,Fixed,24/Feb/12 01:40,16/May/13 21:10
Bug,HIVE-2824,12544093,typo in configuration parameter,"hive.files.umask.vlaue

should be:

hive.files.umask.value",sho.shimauchi,sho.shimauchi,Major,Closed,Fixed,25/Feb/12 02:16,09/Jan/13 10:24
Bug,HIVE-2831,12544534,TestContribCliDriver.dboutput and TestCliDriver.input45 fail on 0.23,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,28/Feb/12 23:47,09/Jan/13 10:24
Bug,HIVE-2833,12544871,Fix test failures caused by HIVE-2716,,kevinwilfong,cwsteinbach,Major,Closed,Fixed,02/Mar/12 00:55,09/Jan/13 10:24
Bug,HIVE-2837,12545027,insert into external tables should not be allowed,"This is a very risky thing to allow. 
Since, the external tables can point to any user location, which can potentially corrupt some other tables.",namit,namit,Major,Closed,Fixed,03/Mar/12 01:54,09/Jan/13 10:24
Bug,HIVE-2838,12545099,cleanup readentity/writeentity,"Ideally, there should be one common entity instead of readentity/writeentity.

Unfortunately, that would be a backward incompatible change since users os hive might have written
there own hooks, where they are using readentity/writeentity.
We should atleast create a common class, and then we can deprecate read/write entity later, for a new release.

For now, I propose to make a backward compatible change.",namit,namit,Major,Closed,Fixed,04/Mar/12 17:20,09/Jan/13 10:24
Bug,HIVE-2839,12545125,Filters on outer join with mapjoin hint is not applied correctly,"Testing HIVE-2820, I've found some queries with mapjoin hint makes exceptions.
{code}
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND true limit 10;

FAILED: Hive Internal Error: java.lang.ClassCastException(org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc cannot be cast to org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)
java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc cannot be cast to org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertMapJoin(MapJoinProcessor.java:363)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.generateMapJoinOperator(MapJoinProcessor.java:483)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.transform(MapJoinProcessor.java:689)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:87)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7519)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:431)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:891)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
{code}
and 
{code}
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND b.key * 10 < '1000' limit 10;

java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:198)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:212)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1321)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1325)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1325)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:495)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)
	... 8 more
{code}",navis,navis,Minor,Closed,Fixed,05/Mar/12 02:44,16/May/13 21:10
Bug,HIVE-2840,12545276,INPUT__FILE__NAME virtual column returns unqualified paths on Hadoop 0.23,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,06/Mar/12 04:13,09/Jan/13 10:24
Bug,HIVE-2856,12545739,Fix TestCliDriver escape1.q failure on MR2,"Additional '^' in escape test:

[junit] Begin query: escape1.q
[junit] Copying file: file:/home/cloudera/Code/hive/data/files/escapetest.txt
[junit] 12/01/23 15:22:15 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 15:22:15 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] diff -a -I file: -I pfile: -I hdfs: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I lastAccessTime -I [Oo]wner -I CreateTime -I LastAccessTime -I Location -I LOCATION ' -I transient_lastDdlTime -I last_modified_ -I java.lang.RuntimeException -I at org -I at sun -I at java -I at junit -I Caused by: -I LOCK_QUERYID: -I LOCK_TIME: -I grantTime -I [.][.][.] [0-9]* more -I job_[0-9]*_[0-9]* -I USING 'java -cp /home/cloudera/Code/hive/build/ql/test/logs/clientpositive/escape1.q.out /home/cloudera/Code/hive/ql/src/test/results/clientpositive/escape1.q.out
[junit] 893d892
[junit] < 1	1	^
[junit] junit.framework.AssertionFailedError: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] at junit.framework.Assert.fail(Assert.java:50)
[junit] at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1(TestCliDriver.java:131)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] at java.lang.reflect.Method.invoke(Method.java:616)
[junit] at junit.framework.TestCase.runTest(TestCase.java:168)
[junit] at junit.framework.TestCase.runBare(TestCase.java:134)
[junit] at junit.framework.TestResult$1.protect(TestResult.java:110)
[junit] at junit.framework.TestResult.runProtected(TestResult.java:128)
[junit] at junit.framework.TestResult.run(TestResult.java:113)
[junit] at junit.framework.TestCase.run(TestCase.java:124)
[junit] at junit.framework.TestSuite.runTest(TestSuite.java:243)
[junit] at junit.framework.TestSuite.run(TestSuite.java:238)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
[junit] Exception: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
[junit] See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.)",zhenxiao,zhenxiao,Major,Closed,Fixed,09/Mar/12 00:19,09/Jan/13 10:25
Bug,HIVE-2857,12545766,QTestUtil.cleanUp() fails with FileNotException on 0.23,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,09/Mar/12 03:33,09/Jan/13 10:25
Bug,HIVE-2860,12545941,TestNegativeCliDriver autolocal1.q fails on 0.23,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,10/Mar/12 04:26,10/Jan/13 19:53
Bug,HIVE-2863,12546110,Ambiguous table name or column reference message displays when table and column names are the same,"Given the following table:

CREATE TABLE `Y` (`y` DOUBLE) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' STORED AS TEXTFILE;

The following query fails:

SELECT  `Y`.`y`  FROM `Y`  WHERE  ( `y` = 1 )
ERROR: java.sql.SQLException: Query returned non-zero code: 10, cause: FAILED: Error in
       semantic analysis: Line 1:36 Ambiguous table alias or column reference '`y`'
ERROR: Unable to execute Hadoop query.
ERROR: Prepare error. SQL statement: SELECT  `Y`.`y`  FROM `Y`  WHERE  ( `y` = 1 ).

The problem goes away if the table and column names do not match.",navis,mcazzari,Major,Closed,Fixed,12/Mar/12 15:24,09/Jan/13 10:25
Bug,HIVE-2874,12546670,Renaming external partition changes location,"Renaming an external partition will change the location of that partition to the default location of a managed partition with the same name.

E.g. If ex_table is external and has partition part=1 with location /.../managed_table/part=1

Calling ALTER TABLE ex_table PARTITION (part = '1') RENAME TO PARTITION (part = '2');

Will change the location of the partition to /.../ex_table/part=2

",zhenxiao,kevinwilfong,Major,Closed,Fixed,15/Mar/12 21:56,10/Jan/13 19:53
Bug,HIVE-2875,12546673,Renaming partition changes partition location prefix,"Renaming a partition changes the location of the partition to the default location of the table, followed by the partition specification.  It should just change the partition specification of the path.

If the path does not end with the old partition specification, we should probably throw an exception because renaming a partition should not change the path so dramatically, and not changing the path to reflect the new partition name could leave the partition in a very confusing state.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,15/Mar/12 22:02,09/Jan/13 10:24
Bug,HIVE-2882,12547178,Problem with Hive using JDBC,"I am trying to implement a task in Hive (Similar to Stored Procedure in SQL (Block of queries)).
In SQL, when we write cursor, first we execute select query and then fetching the records we perform some actions.

Likely I have fired a select query in Hive as:

String driverName = ""org.apache.hadoop.hive.jdbc.HiveDriver"";
Class.forName(driverName);
Connection con = DriverManager.getConnection(""jdbc:hive://localhost:10000/default"", """", """");
String sql=null;
Statement stmt = con.createStatement();
Statement stmt1 = con.createStatement();
ResultSet res=null;
ResultSet rs1=null;

sql=""select a,c,b from tbl_name"";
res=stmt.executeQuery();    -----------> CONTAINS 30 RECORDS
while(res.next())
{
     sql=""select d,e,f, from t1"";
     rs1=stmt1.executeQuery();
     like wise many queries are there.....

.
.
.
..
}
But the problem is that while loop executes only once instead of 30 times when the inner query (inside while) gets execute.

And If I create two different connection for both the queries then all works fine.
Like:
String driverName = ""org.apache.hadoop.hive.jdbc.HiveDriver"";
Class.forName(driverName);
Connection con = DriverManager.getConnection(""jdbc:hive://localhost:10000/default"", """", """");
Connection con1 = DriverManager.getConnection(""jdbc:hive://localhost:10000/default"", """", """");
String sql=null;
Statement stmt = con.createStatement();
Statement stmt1 = con1.createStatement();
ResultSet res=null;
ResultSet rs1=null;

To sum up, when I iterate through a result set do I need to use a different connection(and statement object) to
execute other queries????",,bhavesh25shah,Critical,Resolved,Fixed,20/Mar/12 11:22,09/Jan/13 10:36
Bug,HIVE-2883,12547247,Metastore client doesnt close connection properly,"While closing connection, it always fail with following trace. Seemingly, it doesnt have any harmful effects.
{code}
12/03/20 10:55:02 ERROR hive.metastore: Unable to shutdown local metastore client
org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142)
	at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:163)
	at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:91)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
	at com.facebook.fb303.FacebookService$Client.send_shutdown(FacebookService.java:421)
	at com.facebook.fb303.FacebookService$Client.shutdown(FacebookService.java:415)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:310)
{code}",ashutoshc,ashutoshc,Major,Closed,Fixed,20/Mar/12 17:59,24/Apr/13 01:39
Bug,HIVE-2888,12547408,Creating an external table using the 'LIKE' clause actually creates a managed table.,"When creating a new table with both the EXTERNAL and LIKE clauses, the new table does not behave as an EXTERNAL table thus resulting in potential data loss.

Example:
create external table test1 (VAL string) location '/user/craig/test1';
create external table test2 like test1 location '/user/craig/test2';
drop table test1;

/user/craig/test1 - still exists

drop table test2;

/user/craig/test2 - is deleted (bad)

If I do an extended describe on both tables, test1 shows up as being external while test2 is a managed table.",,fistan684,Major,Resolved,Fixed,21/Mar/12 16:42,12/Apr/12 05:29
Bug,HIVE-2893,12547555,Hive MetaStore is not changing from Derby to MySQL,"I am trying to change Metastore configuration from Derby to MySQL. All the Docs which is available in Hive says that I have to change ConnectionURL, UserName, Password of hive/conf/hive-site.xml file. But problem there is no hive-site.xml file in Hive 0.8.1 stable release download. Rather it has file called hive-default.xml.template. Which also says,

<!-- WARNING!!! This file is provided for documentation purposes ONLY!     -->
<!-- WARNING!!! Any changes you make to this file will be ignored by Hive. -->
<!-- WARNING!!! You must make your changes in hive-site.xml instead.       -->   

But no hive-site.xml file in the conf directory of release. Please some one update a clear documentation how to change derby db to MySQL.",,srini1985,Blocker,Resolved,Fixed,22/Mar/12 06:57,22/Mar/12 09:02
Bug,HIVE-2901,12547934,Hive union with NULL constant and string in same column returns all null,"select x from (select value as x from src union all select NULL as x from src)a;

This query produces all nulls, where value is a string column.

Notably, 
select x from (select key as x from src union all select NULL as x from src)a;
where key is a string, but can be cast to a double, the query returns correct results.",navis,kevinwilfong,Critical,Closed,Fixed,24/Mar/12 01:53,14/May/13 08:07
Bug,HIVE-2904,12547991,ant gen-test failed,"When I ran the commands introduced in Getting Started page, ant gen-test failed with the following error.

{quote}
$ ant gen-test                                                                                                                                       
Buildfile: /Users/sho/src/apache/hive/ql/build.xml

test-conditions:
     [echo] Project: ql

test-init:
     [echo] Project: ql
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/data
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/logs/clientpositive
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/logs/clientnegative
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/logs/positive
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/logs/negative
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/data/warehouse
    [mkdir] Created dir: /Users/sho/src/apache/hive/build/ql/test/data/metadb

gen-test:
     [echo] ql
 [qtestgen] Template Path:/Users/sho/src/apache/hive/ql/src/test/templates
 [qtestgen] 2012/03/25 15:27:10 org.apache.velocity.runtime.log.JdkLogChute log
 [qtestgen] ???: FileResourceLoader : adding path '/Users/sho/src/apache/hive/ql/src/test/templates'
 [qtestgen] Generated /Users/sho/src/apache/hive/build/ql/test/src/org/apache/hadoop/hive/ql/parse/TestParse.java from template TestParse.vm
 [qtestgen] Template Path:/Users/sho/src/apache/hive/ql/src/test/templates
 [qtestgen] 2012/03/25 15:27:10 org.apache.velocity.runtime.log.JdkLogChute log
 [qtestgen] ???: FileResourceLoader : adding path '/Users/sho/src/apache/hive/ql/src/test/templates'
 [qtestgen] Generated /Users/sho/src/apache/hive/build/ql/test/src/org/apache/hadoop/hive/ql/parse/TestParseNegative.java from template TestParseNegative.vm
 [qtestgen] Template Path:/Users/sho/src/apache/hive/ql/src/test/templates
 [qtestgen] 2012/03/25 15:27:10 org.apache.velocity.runtime.log.JdkLogChute log
 [qtestgen] ???: FileResourceLoader : adding path '/Users/sho/src/apache/hive/ql/src/test/templates'
 [qtestgen] Generated /Users/sho/src/apache/hive/build/ql/test/src/org/apache/hadoop/hive/cli/TestCliDriver.java from template TestCliDriver.vm

BUILD FAILED
/Users/sho/src/apache/hive/ql/build.xml:116: Problem: failed to create task or type if
Cause: The name is undefined.
Action: Check the spelling.
Action: Check that any custom tasks/types have been declared.
Action: Check that any <presetdef>/<macrodef> declarations have taken place.
{quote}

Getting Started: https://cwiki.apache.org/confluence/display/Hive/GettingStarted+EclipseSetup",tamtam180,sho.shimauchi,Major,Closed,Fixed,25/Mar/12 06:33,10/Jan/13 19:54
Bug,HIVE-2905,12548057,Desc table can't show non-ascii comments,"When desc a table with command line or hive jdbc way, the table's comment can't be read.
1. I have updated javax.jdo.option.ConnectionURL parameter in hive-site.xml file.
   jdbc:mysql://*.*.*.*:3306/hive?characterEncoding=UTF-8
2. In mysql database, the comment field of COLUMNS table can be read normally.",navis,zhousheng29,Major,Closed,Fixed,26/Mar/12 04:35,29/Oct/13 13:34
Bug,HIVE-2907,12548158,Hive error when dropping a table with large number of partitions,"Running into an ""Out Of Memory"" error when trying to drop a table with 128K partitions.

The methods dropTable in metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java 
and dropTable in ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java encounter out of memory errors 
when dropping tables with lots of partitions because they try to load the metadata for every partition into memory.",mousom,mousom,Minor,Closed,Fixed,26/Mar/12 18:18,03/Oct/15 07:56
Bug,HIVE-2913,12548801,BlockMergeTask Doesn't Honor Job Configuration Properties when used directly,"BlockMergeTask has a main() and when used directly (instead of say partition concatenate feature), the -jobconf arguments are not honored. This is not something most people directly use.

Usage:
BlockMergeTask -input <colon seperated input paths>  -outputDir outputDir [-jobconf k1=v1 [-jobconf k2=v2] ...] 

To reproduce:
Run BlockMergeTask with say -jobconf mapred.job.name=test and launched job will have a different name.",thiruvel,thiruvel,Minor,Closed,Fixed,30/Mar/12 09:38,09/Jan/13 10:24
Bug,HIVE-2916,12548932,Hive Type Information for Table Columns,"Hi
I am using Hive-0.8.1.  I am trying to get type information for table columns from MetaStore.  The Developer documentation says the components of ql/typeinfo is responsible for this.
i.e, under the query processor section in https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-QueryProcessor

  However the directory ""typeinfo"" is missing in hadoop/hive/ql.I understand from typeinfo has java files that is responsible for getting all struct fieldnames, categories, primitive types. etc.
Indrani 

",,indrani,Blocker,Resolved,Fixed,31/Mar/12 04:25,31/Mar/12 05:20
Bug,HIVE-2918,12549144,Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI,"Dynamic Partition insert showing an error with the number of partitions created even after the default value of 'hive.exec.max.dynamic.partitions' is bumped high to 2000.

Error Message:
""Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413.""

These are the following properties set on hive CLI

hive> set hive.exec.dynamic.partition=true;
hive> set hive.exec.dynamic.partition.mode=nonstrict;
hive> set hive.exec.max.dynamic.partitions=2000;
hive> set hive.exec.max.dynamic.partitions.pernode=2000;

This is the query with console error log

hive> 
    > INSERT OVERWRITE TABLE partn_dyn Partition (pobox)
    > SELECT country,state,pobox FROM non_partn_dyn;
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201204021529_0002, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201204021529_0002
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_201204021529_0002
2012-04-02 16:05:28,619 Stage-1 map = 0%,  reduce = 0%
2012-04-02 16:05:39,701 Stage-1 map = 100%,  reduce = 0%
2012-04-02 16:05:50,800 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201204021529_0002
Ended Job = 248865587, job is filtered out (removed at runtime).
Moving data to: hdfs://0.0.0.0/tmp/hive-cloudera/hive_2012-04-02_16-05-24_919_5976014408587784412/-ext-10000
Loading data to table default.partn_dyn partition (pobox=null)
Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask

I checked the job.xml of the first map only job, there the value hive.exec.max.dynamic.partitions=2000 is reflected but the move task is taking the default value from hive-site.xml . If I change the value in hive-site.xml then the job completes successfully. Bottom line,the property 'hive.exec.max.dynamic.partitions'set on CLI is not being considered by move task




",cwsteinbach,bejoyks,Major,Closed,Fixed,02/Apr/12 11:08,25/Mar/16 04:54
Bug,HIVE-2920,12549212,TestStatsPublisherEnhanced throws NPE on JDBC connection failure,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,02/Apr/12 19:34,09/Jan/13 10:24
Bug,HIVE-2923,12549264,testAclPositive in TestZooKeeperTokenStore failing in clean checkout when run on Mac,"When running testAclPositive in TestZooKeeperTokenStore in a clean checkout, it fails with the error:

Failed to validate token path. 

org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to validate token path.
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:207)
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.setConf(ZooKeeperTokenStore.java:225)
at org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.testAclPositive(TestZooKeeperTokenStore.java:170)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at junit.framework.TestCase.runTest(TestCase.java:168)
at junit.framework.TestCase.runBare(TestCase.java:134)
at junit.framework.TestResult$1.protect(TestResult.java:110)
at junit.framework.TestResult.runProtected(TestResult.java:128)
at junit.framework.TestResult.run(TestResult.java:113)
at junit.framework.TestCase.run(TestCase.java:124)
at junit.framework.TestSuite.runTest(TestSuite.java:232)
at junit.framework.TestSuite.run(TestSuite.java:227)
at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /zktokenstore-testAcl
at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:778)
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ensurePath(ZooKeeperTokenStore.java:119)
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:204)
... 17 more


This message is also printed to standard out:
Unable to load realm mapping info from SCDynamicStore

The test seems to run fine in Linux, but more than one developer has reported this on a Mac.",thw,kevinwilfong,Blocker,Closed,Fixed,03/Apr/12 00:40,09/Jan/13 10:24
Bug,HIVE-2929,12549844,race condition in DAG execute tasks for hive,"select ...
(
SubQuery involving MapReduce
union all
SubQuery involving MapReduce
);

or 

select ...
(SubQuery involving MapReduce)
join
(SubQuery involving MapReduce)
;

If both the subQueries finish at nearly the same time, there is a race 
condition in which the results of the subQuery finishing last will be completely missed.
",namit,namit,Major,Closed,Fixed,06/Apr/12 06:10,10/Jan/13 19:54
Bug,HIVE-2933,12550181,analyze command throw NPE when table doesn't exists,analyze command throw NPE when table doesn't exists,ransom,gemini5201314,Minor,Closed,Fixed,09/Apr/12 01:07,10/Jan/13 19:54
Bug,HIVE-2941,12550432,Hive should expand nested structs when setting the table schema from thrift structs,"When setting a table serde, the deserializer is queried for its schema, which is used to set the metastore table schema. The current implementation uses the class name stored in the field as the field type.

By storing the class name as the field type, users cannot see the contents of a struct with ""describe tblname"". Applications that query HiveMetaStore for the table schema (specifically HCatalog in this case) see an unknown field type, rather than a struct containing known field types.

Hive should store the expanded schema in the metastore so users browsing the schema see expanded fields, and applications querying metastore see familiar types.

DETAILS

Set the table serde to something like this. This serde uses the built-in {{ThriftStructObjectInspector}}.

{code}
alter table foo_test
  set serde ""com.twitter.elephantbird.hive.serde.ThriftSerDe""
  with serdeproperties (""serialization.class""=""com.foo.Foo"");
{code}


This causes a call to {{MetaStoreUtils.getFieldsFromDeserializer}} which returns a list of fields and their schemas. However, currently it does not handle nested structs, and if {{com.foo.Foo}} above contains a field {{com.foo.Bar}}, the class name {{com.foo.Bar}} would appear as the field type. Instead, nested structs should be expanded.",traviscrawford,traviscrawford,Major,Closed,Fixed,10/Apr/12 19:41,10/Jan/13 19:53
Bug,HIVE-2942,12550437,substr on string containing UTF-8 characters produces StringIndexOutOfBoundsException,"After HIVE-2792, the substr function produces a StringIndexOutOfBoundsException when called on a string containing UTF-8 characters without the length argument being present.

E.g.
select substr(str, 1) from table1;

now fails with that exception if str contains a UTF-8 character for any row in the table.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,10/Apr/12 20:26,10/Jan/13 19:53
Bug,HIVE-2948,12550614,HiveFileFormatUtils should use Path.SEPARATOR instead of File.Separator,Because its munging hdfs paths and not OS paths.,ashutoshc,ashutoshc,Major,Closed,Fixed,12/Apr/12 01:09,09/Jan/13 10:24
Bug,HIVE-2955,12551169,Queries consists of metadata-only-query returns always empty value,"For partitioned table, simple query on partition column returns always null or empty value, for example,
{code}
create table emppart(empno int, ename string) partitioned by (deptno int);
.. load partitions..

select distinct deptno from emppart; // empty
select min(deptno), max(deptno) from emppart;  // NULL and NULL
{code}
",navis,navis,Minor,Closed,Fixed,17/Apr/12 05:31,05/Jan/14 13:01
Bug,HIVE-2957,12551228,Hive JDBC doesn't support TIMESTAMP column,"Steps to replicate:
1. Create a table with at least one column of type TIMESTAMP
2. Do a DatabaseMetaData.getColumns () such that this TIMESTAMP column is part of the resultset.
3. When you iterate over the TIMESTAMP column it would fail, throwing the below exception:

Exception in thread ""main"" java.sql.SQLException: Unrecognized column type: timestamp
	at org.apache.hadoop.hive.jdbc.Utils.hiveTypeToSqlType(Utils.java:56)
	at org.apache.hadoop.hive.jdbc.JdbcColumn.getSqlType(JdbcColumn.java:62)
	at org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData$2.next(HiveDatabaseMetaData.java:244)
",rding,bharathganesh,Minor,Closed,Fixed,17/Apr/12 14:25,06/Sep/15 15:22
Bug,HIVE-2958,12551238,GROUP BY causing ClassCastException [LazyDioInteger cannot be cast LazyInteger],"This relates to https://issues.apache.org/jira/browse/HIVE-1634.

The following work fine:

{code}
CREATE EXTERNAL TABLE tim_hbase_occurrence ( 
  id int,
  scientific_name string,
  data_resource_id int
) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (
  ""hbase.columns.mapping"" = "":key#b,v:scientific_name#s,v:data_resource_id#b""
) TBLPROPERTIES(
  ""hbase.table.name"" = ""mini_occurrences"", 
  ""hbase.table.default.storage.type"" = ""binary""
);
SELECT * FROM tim_hbase_occurrence LIMIT 3;
SELECT * FROM tim_hbase_occurrence WHERE data_resource_id=1081 LIMIT 3;
{code}

However, the following fails:
{code}
SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id;
{code}

The error given:
{code}
0 TS
2012-04-17 16:58:45,693 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Initialization Done 7 MAP
2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Processing alias tim_hbase_occurrence for file hdfs://c1n2.gbif.org/user/hive/warehouse/tim_hbase_occurrence
2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.MapOperator: 7 forwarding 1 rows
2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.TableScanOperator: 0 forwarding 1 rows
2012-04-17 16:58:45,716 INFO org.apache.hadoop.hive.ql.exec.SelectOperator: 1 forwarding 1 rows
2012-04-17 16:58:45,723 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""id"":1444,""scientific_name"":null,""data_resource_id"":1081}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)
	... 9 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger
	at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)
	at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:150)
	at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:142)
	at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory.java:119)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)
	... 18 more
{code}

",navis,timrobertson100,Blocker,Closed,Fixed,17/Apr/12 15:02,09/Jan/13 10:24
Bug,HIVE-2970,12551963,several jars in hive tar generated are not required,"The hive 0.9 build is currently producing tar which contains 31 additional jars when compared with 0.8 release, and most of them are not needed.",thejas,thejas,Major,Closed,Fixed,21/Apr/12 02:12,09/Jan/13 10:25
Bug,HIVE-2971,12552178,GET_JSON_OBJECT fails on some valid JSON keys,"hive> SELECT GET_JSON_OBJECT(""{\""Form Name\"": 12345}"", ""$.Form\ Name"") FROM akramer_one_row;
Mapred Local Task Succeeded . Convert the Join into MapJoin
OK
NULL

...this also returns null for ""$.Form Name"" and ""$.Form\\ Name"". It should return the relevant key.

Removing the space works fine, however, spaces are allowed as JSON keys (see spec at http://www.json.org/ ). As such, this is a bug.

Claiming that this is org.json's problem, or something similar, does not solve this bug. It's Hive that claims this gets a JSON object, so it needs to provide the JSON object.",gangtimliu,akramer,Minor,Closed,Fixed,23/Apr/12 18:44,10/Jan/13 19:53
Bug,HIVE-2975,12552228,Filter parsing does not recognize '!=' as operator and silently ignores invalid tokens,"Should support operator as alternative to '<>' and should not produce false positives on lexer errors.
 ",thw,thw,Major,Closed,Fixed,24/Apr/12 01:48,02/May/13 02:29
Bug,HIVE-2976,12552233,Fix maven-build Ant target,,thw,cwsteinbach,Major,Closed,Fixed,24/Apr/12 02:58,10/Jan/13 19:54
Bug,HIVE-2990,12553429,Remove hadoop-source Ivy resolvers and Ant targets,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,30/Apr/12 22:13,10/Jan/13 19:54
Bug,HIVE-2992,12553481,JOIN + LATERAL VIEW + MAPJOIN fails to return result (seems to stop halfway through and no longer do the final reduce part),"Okay first off; I know JOIN + LATERAL VIEW together isn't working so I moved my JOIN into a subquery and that makes the query work properly

However when I added a MAPJOIN hint for the JOIN in the subquery it will also stop doing the reducer for the main query!
This only happens when there's a LATERAL VIEW in there though, if I remove the LATERAL VIEW then the main query still get's a reducer to do grouping


Here's a gist: https://gist.github.com/2499436 Containing the queries and a PHP script which you can run to execute the test case I'm using, which does;
  - setup a database called hive_mapjoin
  - setup tables
  - load some test data
  - do the selects
You'll need the https://github.com/rcongiu/Hive-JSON-Serde/downloads json-serde-1.1-jar-with-dependencies.jar with it though and change the path ;)
I guess looking at the queries you guys can probally figure out a better testcase, but maybe it's helpful :-)


Not sure if this is a bug or me doing something that just isn't supposed to be working, but I can't seem to find any pointers that this wouldn't be supported...


Here's another gist with the plan.xml: https://gist.github.com/2499658",,ruben.devries,Major,Resolved,Fixed,01/May/12 12:52,12/Jul/12 11:23
Bug,HIVE-2999,12553684,Offline build is not working,"It's fine without -Doffline=true option. But with offline option (ant -Doffline=true clean package), it's failing with error message like this.

{noformat}
ivy-retrieve:
     [echo] Project: common
[ivy:retrieve] :: loading settings :: file = /home/navis/apache/oss-hive/ivy/ivysettings.xml
[ivy:retrieve] 
[ivy:retrieve] :: problems summary ::
[ivy:retrieve] :::: WARNINGS
[ivy:retrieve] 		module not found: org.apache.hadoop#hadoop-common;0.20.2
[ivy:retrieve] 	==== local: tried
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-common/0.20.2/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-common;0.20.2!hadoop-common.jar:
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-common/0.20.2/jars/hadoop-common.jar
[ivy:retrieve] 	==== apache-snapshot: tried
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.20.2/hadoop-common-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-common;0.20.2!hadoop-common.jar:
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.20.2/hadoop-common-0.20.2.jar
[ivy:retrieve] 	==== maven2: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.20.2/hadoop-common-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-common;0.20.2!hadoop-common.jar:
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.20.2/hadoop-common-0.20.2.jar
[ivy:retrieve] 	==== datanucleus-repo: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-common;0.20.2!hadoop-common.jar:
[ivy:retrieve] 	  http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-common/0.20.2/hadoop-common-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-common;0.20.2!hadoop-common.jar:
[ivy:retrieve] 	  http://mirror.facebook.net/facebook/hive-deps/hadoop/core/hadoop-common-0.20.2/hadoop-common-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source2: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-common;0.20.2!hadoop-common.jar:
[ivy:retrieve] 	  http://archive.cloudera.com/hive-deps/hadoop/core/hadoop-common-0.20.2/hadoop-common-0.20.2.jar
[ivy:retrieve] 		module not found: org.apache.hadoop#hadoop-auth;0.20.2
[ivy:retrieve] 	==== local: tried
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-auth/0.20.2/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-auth;0.20.2!hadoop-auth.jar:
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-auth/0.20.2/jars/hadoop-auth.jar
[ivy:retrieve] 	==== apache-snapshot: tried
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-auth/0.20.2/hadoop-auth-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-auth;0.20.2!hadoop-auth.jar:
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-auth/0.20.2/hadoop-auth-0.20.2.jar
[ivy:retrieve] 	==== maven2: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/0.20.2/hadoop-auth-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-auth;0.20.2!hadoop-auth.jar:
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/0.20.2/hadoop-auth-0.20.2.jar
[ivy:retrieve] 	==== datanucleus-repo: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-auth;0.20.2!hadoop-auth.jar:
[ivy:retrieve] 	  http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-auth/0.20.2/hadoop-auth-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-auth;0.20.2!hadoop-auth.jar:
[ivy:retrieve] 	  http://mirror.facebook.net/facebook/hive-deps/hadoop/core/hadoop-auth-0.20.2/hadoop-auth-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source2: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-auth;0.20.2!hadoop-auth.jar:
[ivy:retrieve] 	  http://archive.cloudera.com/hive-deps/hadoop/core/hadoop-auth-0.20.2/hadoop-auth-0.20.2.jar
[ivy:retrieve] 		module not found: org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2
[ivy:retrieve] 	==== local: tried
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-mapreduce-client-core/0.20.2/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2!hadoop-mapreduce-client-core.jar:
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-mapreduce-client-core/0.20.2/jars/hadoop-mapreduce-client-core.jar
[ivy:retrieve] 	==== apache-snapshot: tried
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-mapreduce-client-core/0.20.2/hadoop-mapreduce-client-core-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2!hadoop-mapreduce-client-core.jar:
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-mapreduce-client-core/0.20.2/hadoop-mapreduce-client-core-0.20.2.jar
[ivy:retrieve] 	==== maven2: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/0.20.2/hadoop-mapreduce-client-core-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2!hadoop-mapreduce-client-core.jar:
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/0.20.2/hadoop-mapreduce-client-core-0.20.2.jar
[ivy:retrieve] 	==== datanucleus-repo: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2!hadoop-mapreduce-client-core.jar:
[ivy:retrieve] 	  http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/0.20.2/hadoop-mapreduce-client-core-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2!hadoop-mapreduce-client-core.jar:
[ivy:retrieve] 	  http://mirror.facebook.net/facebook/hive-deps/hadoop/core/hadoop-mapreduce-client-core-0.20.2/hadoop-mapreduce-client-core-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source2: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-mapreduce-client-core;0.20.2!hadoop-mapreduce-client-core.jar:
[ivy:retrieve] 	  http://archive.cloudera.com/hive-deps/hadoop/core/hadoop-mapreduce-client-core-0.20.2/hadoop-mapreduce-client-core-0.20.2.jar
[ivy:retrieve] 		module not found: org.apache.hadoop#hadoop-archives;0.20.2
[ivy:retrieve] 	==== local: tried
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-archives/0.20.2/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-archives;0.20.2!hadoop-archives.jar:
[ivy:retrieve] 	  /home/navis/.ivy2/local/org.apache.hadoop/hadoop-archives/0.20.2/jars/hadoop-archives.jar
[ivy:retrieve] 	==== apache-snapshot: tried
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-archives/0.20.2/hadoop-archives-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-archives;0.20.2!hadoop-archives.jar:
[ivy:retrieve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-archives/0.20.2/hadoop-archives-0.20.2.jar
[ivy:retrieve] 	==== maven2: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-archives/0.20.2/hadoop-archives-0.20.2.pom
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-archives;0.20.2!hadoop-archives.jar:
[ivy:retrieve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-archives/0.20.2/hadoop-archives-0.20.2.jar
[ivy:retrieve] 	==== datanucleus-repo: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-archives;0.20.2!hadoop-archives.jar:
[ivy:retrieve] 	  http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-archives/0.20.2/hadoop-archives-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-archives;0.20.2!hadoop-archives.jar:
[ivy:retrieve] 	  http://mirror.facebook.net/facebook/hive-deps/hadoop/core/hadoop-archives-0.20.2/hadoop-archives-0.20.2.jar
[ivy:retrieve] 	==== hadoop-source2: tried
[ivy:retrieve] 	  -- artifact org.apache.hadoop#hadoop-archives;0.20.2!hadoop-archives.jar:
[ivy:retrieve] 	  http://archive.cloudera.com/hive-deps/hadoop/core/hadoop-archives-0.20.2/hadoop-archives-0.20.2.jar
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 		:: org.apache.hadoop#hadoop-common;0.20.2: not found
{noformat}

At first I thought it's my problem(ivy cache or something). But log above shows that ivy is searching for non-existing artifacts in hadoop-0.20.2, which seemed to be a bug.",navis,navis,Major,Closed,Fixed,03/May/12 00:06,10/Jan/13 19:53
Bug,HIVE-3000,12553687,Potential infinite loop / log spew in ZookeeperHiveLockManager,"See ZookeeperHiveLockManger.lock()

If Zookeeper is in a bad state, it's possible to get an exception (e.g. org.apache.zookeeper.KeeperException$SessionExpiredException) when we call lockPrimitive(). There is a bug in the exception handler where the loop does not exit because the break in the switch statement gets out the switch, not the do..while loop. Because tryNum was not incremented due to the exception, lockPrimitive() will be called in an infinite loop, as fast as possible. Since the exception is printed for each call, Hive will produce significant log spew.
",namit,pauly,Major,Closed,Fixed,03/May/12 00:53,10/Jan/13 19:53
Bug,HIVE-3008,12554269,Memory leak in TUGIContainingTransport,Identical bug as in THRIFT-1468,ashutoshc,ashutoshc,Major,Closed,Fixed,08/May/12 02:04,06/Sep/13 16:12
Bug,HIVE-3013,12554543,TestCliDriver cannot be debugged with eclipse since hadoop_home is set incorrectly,,cwsteinbach,namit,Major,Closed,Fixed,09/May/12 19:17,10/Jan/13 19:53
Bug,HIVE-3014,12554850,Fix metastore test failures caused by HIVE-2757,,zhenxiao,cwsteinbach,Major,Closed,Fixed,11/May/12 00:46,10/Jan/13 19:53
Bug,HIVE-3019,12555051,Add JUnit to list of test dependencies managed by Ivy,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,13/May/12 04:03,10/Jan/13 19:53
Bug,HIVE-3028,12555931,Fix javadoc again,"HIVE-2646 broke the javadoc, because javadoc needs the Hadoop jars on the classpath.

{quote}
  [javadoc] Building index for all classes...
  [javadoc] Generating /Users/owen/work/eclipse/hive/build/dist/docs/api/stylesheet.css...
  [javadoc] 3 errors
  [javadoc] 168 warnings
{quote}",omalley,omalley,Major,Closed,Fixed,15/May/12 23:45,10/Jan/13 19:53
Bug,HIVE-3029,12555949,Update ShimLoader to work with Hadoop 2.x,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,16/May/12 03:22,10/Jan/13 19:53
Bug,HIVE-3030,12555957,escape more chars for script operator,"Only new line was being escaped.
The same behavior needs to be done for carriage returns, and tabs",namit,namit,Major,Closed,Fixed,16/May/12 05:44,10/Jan/13 19:53
Bug,HIVE-3031,12556237,hive docs target does not work,"Running ant docs does not work for two reasons:

a) ant, when called from the docs target, doesn't know what to do with ivy, presumably because the ivy-init-antlib target hasn't been called.
b) The velocity jar is not pulled in by ivy, since there's no dependency added to it in ivy.",sushanth,sushanth,Major,Closed,Fixed,17/May/12 13:42,10/Jan/13 19:53
Bug,HIVE-3049,12557105,setup classpath for templates correctly for eclipse,,sding,namit,Major,Closed,Fixed,23/May/12 23:53,10/Jan/13 19:53
Bug,HIVE-3052,12558065,TestHadoop20SAuthBridge always uses the same port,"Similar to https://issues.apache.org/jira/browse/HIVE-2959

TestHadoop20SAuthBridge uses fixed port 10000 (and 10010) for testing which is default port of hive server, making test fail if someone is testing it(hive server).",navis,navis,Trivial,Closed,Fixed,25/May/12 09:34,10/Jan/13 19:53
Bug,HIVE-3057,12558217,metastore.HiveMetaStore$HMSHandler should set the thread local raw store to null in shutdown(),"The shutdown() function of metastore.HiveMetaStore$HMSHandler does not set the thread local RawStore variable (in threadLocalMS) to null. Subsequent getMS() calls may get the wrong RawStore object.
",traviscrawford,pengfeng,Major,Closed,Fixed,26/May/12 00:17,10/Jan/13 19:53
Bug,HIVE-3058,12558229,hive.transform.escape.input breaks tab delimited data,"With the hive.transform.escape.input set, all tabs going into a script, including those used to separate columns are escaped.",kevinwilfong,kevinwilfong,Major,Resolved,Fixed,26/May/12 04:36,20/Jan/13 07:23
Bug,HIVE-3059,12558294,revert HIVE-2703,,namit,namit,Major,Closed,Fixed,28/May/12 02:04,18/Jul/13 23:23
Bug,HIVE-3062,12558394,Insert into table overwrites existing table if table name contains uppercase character,"""Insert into table <table-name> ~~"" is expected to append query result into the table. But when the table name contains uppercase character, it overwrite existing table.",navis,navis,Critical,Closed,Fixed,29/May/12 08:15,25/Jan/13 07:04
Bug,HIVE-3063,12558518,drop partition for non-string columns is failing,,namit,namit,Major,Closed,Fixed,30/May/12 00:45,10/Jan/13 19:53
Bug,HIVE-3070,12558744,Filter on outer join condition removed while merging join tree,"should the result of query A: 

select s.aa, s.bb, c.key keyc from (select a.key aa, b.key bb from src a left outer join src b on a.key=b.key) s left outer join src c on s.bb=c.key and s.bb<10 where s.aa<20;

be the same as query B:

select a.key keya, b.key keyb, c.key keyc from src a left outer join src b on a.key=b.key left outer join src c on b.key=c.key and b.key<10 where a.key<20;

?

Currently, the result is different, query B gets wrong result!

In SemanticAnalyzer.java, mergeJoins():

ArrayList<ArrayList<ASTNode>> filters = target.getFilters();
for (int i = 0; i < nodeRightAliases.length; i++) {
  filters.add(node.getFilters().get(i + 1));
}

filters in node.getFilters().get(0) are lost.
",navis,yuzone,Major,Closed,Fixed,31/May/12 09:45,10/Jan/13 19:53
Bug,HIVE-3076,12558896,drop partition does not work for non-partition columns,There is still a problem in case there is a mixture of string and non-string partition columns.,,namit,Major,Closed,Fixed,01/Jun/12 01:23,10/Jan/13 19:53
Bug,HIVE-3079,12559003,Revert HIVE-2989,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,01/Jun/12 20:28,10/Jan/13 19:53
Bug,HIVE-3081,12559113,ROFL Moment. Numberator and denaminator typos,,appodictic,appodictic,Major,Closed,Fixed,04/Jun/12 03:09,10/Jan/13 19:53
Bug,HIVE-3082,12559130,Oracle Metastore schema script doesn't include DDL for DN internal tables,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,04/Jun/12 07:17,10/Jan/13 19:53
Bug,HIVE-3084,12559223,Hive CI failing due to script_broken_pipe1.q,"Hive's [CI job|https://builds.apache.org/job/Hive-trunk-h0.21/] regularly fails due to this test:

{code}
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_script_broken_pipe1
{code}

Running just that test in a loop I'm not able to reproduce the issue locally. This may be related to state from a previous test causing this one to fail?

{code}
ant clean package
for x in $(seq 1 10); 
  do ant test -Dtestcase=TestNegativeCliDriver -Dqfile=script_broken_pipe1.q;
done
{code}",hagleitn,traviscrawford,Major,Closed,Fixed,04/Jun/12 17:07,16/May/13 21:10
Bug,HIVE-3085,12559291,make parallel tests work,"https://cwiki.apache.org/Hive/unit-test-parallel-execution.html

I was trying to run the tests using the instructions above.
I was able to run them using a single machine (parallelism of 4 in ~2 hours).

The conf. file is as follows: .hive_ptest.conf

{
  ""qfile_hosts"": [
    [""root@<MC>"", 4]
  ],

  ""other_hosts"": [
  [""root@<MC>"", 1]
  ],

  ""master_base_path"": ""/data/users/tmp"",
  ""host_base_path"": ""/data/users/hivetests"",
  ""java_home"": ""/usr/local/jdk-6u24-64""

}",sding,namit,Major,Closed,Fixed,04/Jun/12 23:13,10/Jan/13 19:53
Bug,HIVE-3090,12559481,Timestamp type values not having nano-second part breaks row,"Timestamp values are reading additional one byte if nano-sec part is zero, breaking following columns.  
{noformat}
>create table timestamp_1 (t timestamp, key string, value string);
>insert overwrite table timestamp_1 select cast('2011-01-01 01:01:01' as timestamp), key, value from src limit 5;

>select t,key,value from timestamp_1;
2011-01-01 01:01:01		238
2011-01-01 01:01:01		86
2011-01-01 01:01:01		311
2011-01-01 01:01:01		27
2011-01-01 01:01:01		165

>select t,key,value from timestamp_1 distribute by t;
2011-01-01 01:01:01		
2011-01-01 01:01:01		
2011-01-01 01:01:01		
2011-01-01 01:01:01		
2011-01-01 01:01:01		
{noformat}
",navis,navis,Critical,Closed,Fixed,06/Jun/12 02:56,10/Jan/13 19:53
Bug,HIVE-3092,12559511,"Hive tests should load Hive classes from build directory, not Ivy cache","As discussed in HIVE-895, currently the tests pull in jars for other components rather from Ivy rather than using the built classes and jars in the build directory (bit.ly/LzndQU).  This means that absent a very-clean, one is testing against a previous version of the code and cross-component tests are invalid.",kevinwilfong,jghoman,Major,Closed,Fixed,06/Jun/12 10:38,10/Jan/13 19:53
Bug,HIVE-3098,12559587,Memory leak from large number of FileSystem instances in FileSystem.CACHE,"The problem manifested from stress-testing HCatalog 0.4.1 (as part of testing the Oracle backend).

The HCatalog server ran out of memory (-Xmx2048m) when pounded by 60-threads, in under 24 hours. The heap-dump indicates that hadoop::FileSystem.CACHE had 1000000 instances of FileSystem, whose combined retained-mem consumed the entire heap.

It boiled down to hadoop::UserGroupInformation::equals() being implemented such that the ""Subject"" member is compared for equality (""==""), and not equivalence ("".equals()""). This causes equivalent UGI instances to compare as unequal, and causes a new FileSystem instance to be created and cached.

The UGI.equals() is so implemented, incidentally, as a fix for yet another problem (HADOOP-6670); so it is unlikely that that implementation can be modified.

The solution for this is to check for UGI equivalence in HCatalog (i.e. in the Hive metastore), using an cache for UGI instances in the shims.

I have a patch to fix this. I'll upload it shortly. I just ran an overnight test to confirm that the memory-leak has been arrested.",mithun,mithun,Major,Closed,Fixed,06/Jun/12 18:34,03/Sep/20 05:13
Bug,HIVE-3100,12559659,Add HiveCLI that runs over JDBC,,prasadm,cwsteinbach,Major,Closed,Fixed,07/Jun/12 09:32,10/Jan/13 19:53
Bug,HIVE-3101,12559669,dropTable will all ways excute hook.rollbackDropTable whether drop table success or faild.,"see  the codes:
 boolean success = false;
    try {
      client.drop_table(dbname, name, deleteData);
      if (hook != null) {
        hook.commitDropTable(tbl, deleteData);
      }
    } catch (NoSuchObjectException e) {
      if (!ignoreUknownTab) {
        throw e;
      }
    } finally {
      if (!success && (hook != null)) {
        hook.rollbackDropTable(tbl);
      }
    }

success  will always false, whether  the drop was success or faild.
so it's a bug. 

",ransom,ransom,Major,Closed,Fixed,07/Jun/12 11:12,10/Jan/13 19:53
Bug,HIVE-3112,12560251,clear hive.metastore.partition.inherit.table.properties till HIVE-3109 is fixed,,namit,namit,Major,Closed,Fixed,11/Jun/12 21:59,10/Jan/13 19:53
Bug,HIVE-3120,12560377,make copyLocal work for parallel tests,"It would be very useful if I can test a local patch using the
parallel test framework.",sding,namit,Major,Closed,Fixed,12/Jun/12 16:42,10/Jan/13 19:53
Bug,HIVE-3123,12560422,Hadoop20Shim. CombineFileRecordReader does not report progress within files,"When using CombineHiveInputFormat the progress of the task only changes on each of the processed parts of the split, but not within them. This patch fixes the issue and the progress of the task will be reported continuously.",,dms,Trivial,Closed,Fixed,13/Jun/12 01:58,10/Jan/13 19:53
Bug,HIVE-3124,12560430,Error in Removing ProtectMode from a Table,"hive> alter table table_name disable NO_DROP CASCADE;
Failed with exception null
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",bmandhani,bmandhani,Minor,Closed,Fixed,13/Jun/12 03:15,10/Jan/13 19:53
Bug,HIVE-3125,12560478,sort_array doesn't work with LazyPrimitive,"The sort_array function doesn't work against data that's actually come out of a table. The test suite only covers constants given in the query.

If you try and use sort_array on an array from a table, then you get a ClassCastException that you can't convert LazyX to Comparable.",philip.tromans,philip.tromans,Major,Closed,Fixed,13/Jun/12 11:06,10/Jan/13 19:52
Bug,HIVE-3126,12560537,Generate & build the velocity based Hive tests on windows by fixing the path issues,"1)Escape the backward slash in Canonical Path if unit test runs on windows.
2)Diff comparison – 
     a.	Ignore the extra spacing on windows
     b.	Ignore the different line endings on windows & Unix
     c.	Convert the file paths to windows specific. (Handle spaces etc..)
3)Set the right file scheme & class path separators while invoking the junit task from 
",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,13/Jun/12 17:41,10/Jan/13 19:53
Bug,HIVE-3127,12560539,Pass hconf values as XML instead of command line arguments to child JVM,"The maximum length of the DOS command string is 8191 characters (in Windows latest versions http://support.microsoft.com/kb/830473). This limit will be exceeded easily when it appends individual –hconf values to the command string. To work around this problem, Write all changed hconf values to a temp file and pass the temp file path to the child jvm to read and initialize the -hconf parameters from file.",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,13/Jun/12 17:52,11/Aug/14 07:44
Bug,HIVE-3128,12560540,use commons-compress instead of forking tar process,TAR tool doesn’t exist by default on windows systems so use the CAB files on windows,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,13/Jun/12 17:55,10/Jan/13 19:53
Bug,HIVE-3134,12560582,Drop table/index/database can result in orphaned locations,"Today when a managed table has a partition with a location which is not a subdirectory of the table's location, when the table is dropped the partition's data is not deleted from HDFS, resulting in an orphaned directory (the data exists but nothing points to it).

The same applies to dropping a database with cascade and a table has a location outside the database.

I think it is safe to assume managed tables/partitions own the directories they point to, so we should clean these up.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,13/Jun/12 23:36,10/Jan/13 19:53
Bug,HIVE-3135,12560587,add an option in ptest to run on a single machine,There is no need for any sudo in that case,namit,namit,Major,Closed,Fixed,14/Jun/12 01:35,10/Jan/13 19:54
Bug,HIVE-3140,12560695,"Comment indenting is broken for ""describe"" in CLI","Just go into the CLI and type ""describe [TABLE_NAME]"". If a comment has multiple lines, it is completely unreadable due to poor comment indenting. For example:
birthdayParam         string             1 = comment1
2 = comment2
3 = comment3

But it supposed to display as:
birthdayParam         string             1 = comment1
                                         2 = comment2
                                         3 = comment3

Comments should be indented the same amount on each line, i.e., if the comment starts at row k for the first line of the comment, it should be indented by k on line 2.

",zhenxiao,xhou,Major,Closed,Fixed,14/Jun/12 22:58,12/May/13 18:28
Bug,HIVE-3142,12560716,Bug in parallel test for singlehost flag,,sding,sding,Major,Closed,Fixed,15/Jun/12 05:59,10/Jan/13 19:53
Bug,HIVE-3149,12560852,Dynamically generated paritions deleted by Block level merge,"When creating partitions in a table using dynamic partitions and a Block level merge is executed at the end of the query, some partitions may be lost.  Specifically if the values of two or more dynamic partition keys end in the same sequence of numbers, all but the largest will be dropped.

I was not able to confirm it, but I suspect that if a map reduce job is speculated as part of the merge, the duplicate data will not be deleted either.

E.g.
insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 2 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';

In this query, if a Block level merge is executed at the end, only one of the partitions ds=2008-04-08/hr=a1 and ds=2008-04-08/hr=b1 will appear in the final table.",kevinwilfong,kevinwilfong,Blocker,Closed,Fixed,16/Jun/12 01:32,10/Jan/13 19:53
Bug,HIVE-3155,12595087,Memory leak in Hive,"Start the hive in server mode (Hive 0.9 with hadoop 0.23)
Run the sample application containing all queries 
After running the application for 20 hours ,it is found the each time new DFS client object is getting created .Due to this there are so many objects getting added into ArrayList maintained by LeaseRenewer.This internally leads to Memory leak.

",mithun,kristamswathi,Major,Resolved,Fixed,19/Jun/12 10:56,16/Dec/14 23:59
Bug,HIVE-3161,12595154,drop the temporary function at end of autogen_colalias.q,This should not be needed once HIVE-3160 if fixed,namit,namit,Major,Closed,Fixed,19/Jun/12 19:25,02/May/13 02:30
Bug,HIVE-3164,12595199,Fix non-deterministic testcases failures when running Hive0.9.0 on MapReduce2,"The following testcases are failing when running Hive0.9.0 on MapReduce2 due to non-deterministic testcase result:

groupby7_noskew_multi_single_reducer.q
groupby_complex_types_multi_single_reducer.q
groupby_multi_single_reducer.q
leftsemijoin.q",zhenxiao,zhenxiao,Major,Closed,Fixed,20/Jun/12 03:23,10/Jan/13 19:54
Bug,HIVE-3165,12595211,Hive thrift code doesnt generate quality hashCode(),,gangtimliu,gangtimliu,Minor,Closed,Fixed,20/Jun/12 05:53,10/Jan/13 19:52
Bug,HIVE-3168,12595376,LazyBinaryObjectInspector.getPrimitiveJavaObject copies beyond length of underlying BytesWritable,"LazyBinaryObjectInspector.getPrimitiveJavaObject copies the full capacity of the LazyBinary's underlying BytesWritable object, which can be greater than the size of the actual contents. 

This leads to additional characters at the end of the ByteArrayRef returned. When the LazyBinary object gets re-used, there can be remnants of the later portion of previous entry. 
This was not seen while reading through hive queries, which I think is because a copy elsewhere seems to create LazyBinary with length == capacity. (probably LazyBinary copy constructor). This was seen when MR or pig used Hcatalog to read the data.",thejas,thejas,Major,Closed,Fixed,21/Jun/12 02:53,10/Jan/13 19:53
Bug,HIVE-3171,12595463,Bucketed sort merge join doesn't work when multiple files exist for small alias,"Executing a query with the MAPJOIN hint and the bucketed sort merge join optimizations enabled:
{noformat}
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
{noformat}

works fine with partitioned tables if there is only one partition in the table. However, if you add a second partition, Hive attempts to do a regular map-side join which can fail because the tables are too large. Hive ought to be able to still do the bucketed sort merge join with partitions.",navis,fwiffo,Major,Closed,Fixed,21/Jun/12 18:09,10/Jan/13 19:53
Bug,HIVE-3173,12595518,implement getTypeInfo database metadata method ,"The JDBC driver does not implement the database metadata method getTypeInfo. Hence, an application cannot dynamically determine the available type information and associated properties. ",xguo27,the6campbells,Major,Resolved,Fixed,22/Jun/12 02:23,21/Jul/17 18:46
Bug,HIVE-3178,12595530,retry not honored in RetryingRawMetastore,"The retrymetastore catches JDOException, but they are always wrapped by reflection.",namit,namit,Major,Closed,Fixed,22/Jun/12 05:57,10/Jan/13 19:53
Bug,HIVE-3179,12595575,HBase Handler doesn't handle NULLs properly,"We found a quite severe issue in the HBase Handler which actually means that Hive potentially returns incorrect data if a column has NULL values in HBase (which means the cell doesn't even exist)

In HBase Shell:

{noformat}
create 'hive_hbase_test', 'test'
put 'hive_hbase_test', '1', 'test:c1', 'c1-1'
put 'hive_hbase_test', '1', 'test:c2', 'c2-1'
put 'hive_hbase_test', '1', 'test:c3', 'c3-1'
put 'hive_hbase_test', '2', 'test:c1', 'c1-2'
{noformat}

In Hive:

{noformat}
DROP TABLE IF EXISTS hive_hbase_test;
CREATE EXTERNAL TABLE hive_hbase_test (
  id int,
  c1 string,
  c2 string,
  c3 string
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (""hbase.columns.mapping"" =
"":key#s,test:c1#s,test:c2#s,test:c3#s"")
TBLPROPERTIES(""hbase.table.name"" = ""hive_hbase_test"");

hive> select * from hive_hbase_test;
OK
1	c1-1	c2-1	c3-1
2	c1-2	NULL	NULL

hive> select c1 from hive_hbase_test;
c1-1
c1-2

hive> select c1, c2 from hive_hbase_test;
c1-1	c2-1
c1-2	NULL
{noformat}

So far everything is correct but now:

{noformat}
hive> select c1, c2, c2 from hive_hbase_test;
c1-1	c2-1	c2-1
c1-2	NULL	c2-1
{noformat}

Selecting c2 twice works the first time but the second time we
actually get the value from the previous row.

{noformat}
hive> select c1, c3, c2, c2, c3, c3, c1 from hive_hbase_test;
c1-1	c3-1	c2-1	c2-1	c3-1	c3-1	c1-1
c1-2	NULL	NULL	c2-1	c3-1	c3-1	c1-2
{noformat}

We've narrowed this down to an early initialization of {{fieldsInited\[fieldID] = true}} in {{LazyHBaseRow#uncheckedGetField}} and we'll try to provide a patch which surely needs review.",larsfrancke,larsfrancke,Critical,Closed,Fixed,22/Jun/12 12:25,16/May/13 21:10
Bug,HIVE-3180,12595660,Fix Eclipse classpath template broken in HIVE-3128,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,23/Jun/12 02:09,10/Jan/13 19:53
Bug,HIVE-3189,12595719,cast ( <string type> as bigint) returning null values,"select rnum, c1, cast(c1 as bigint) from cert.tsdchar tsdchar where rnum in (0,1,2)

create table if not exists CERT.TSDCHAR ( RNUM int , C1 string)
row format sequencefile

rnum	c1	_c2
0	-1                         	<null>
1	0                          	<null>
2	10                         	<null>
",xguo27,the6campbells,Major,Closed,Fixed,24/Jun/12 15:21,15/Oct/13 23:30
Bug,HIVE-3191,12595721,timestamp - timestamp causes null pointer exception,"select tts.rnum, tts.cts - tts.cts from cert.tts tts

Error: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)
SQLState:  42000
ErrorCode: 12

create table if not exists CERT.TTS ( RNUM int , CTS timestamp) 
stored as sequencefile;",jdere,the6campbells,Major,Closed,Fixed,24/Jun/12 15:33,15/Oct/13 23:31
Bug,HIVE-3197,12595820,Hive compile errors under Java 7 (JDBC 4.1),"Hi, I've been trying to compile Hive trunk from source and getting failures:

{code}
    [javac] hive-svn/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveCallableStatement.java:48: error: HiveCallableStatement is not abstract and does not override abstract method <T>getObject(String,Class<T>) in CallableStatement
    [javac] public class HiveCallableStatement implements java.sql.CallableStatement {
    [javac]        ^
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in method <T>getObject(String,Class<T>)
{code}

I think this is because JDBC 4.1 is part of Java 7, and is not source-compatible with older JDBC versions. Any chance you guys could add JDBC 4.1 support?",,brianbloniarz,Major,Closed,Fixed,25/Jun/12 18:19,16/May/13 21:10
Bug,HIVE-3205,12596197,Bucketed mapjoin on partitioned table which has no partition throws NPE,"{code}
create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;
{code}

throws NPE
{noformat}
2012-06-28 08:59:13,459 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer$BucketMapjoinOptProc.process(BucketMapJoinOptimizer.java:269)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.transform(BucketMapJoinOptimizer.java:100)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:87)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7564)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:50)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
{noformat}",navis,navis,Minor,Closed,Fixed,28/Jun/12 00:46,10/Jan/13 19:53
Bug,HIVE-3206,12596205,FileUtils.tar assumes wrong directory in some cases,"Bucket mapjoin throws exception archiving stored hashtables. 
{noformat}
hive> set hive.optimize.bucketmapjoin = true;
hive> select /*+mapjoin(a)*/ a.key, a.value, b.value 
    > from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b 
    > on a.key=b.key;
Total MapReduce jobs = 1
12/06/28 12:36:18 WARN conf.HiveConf: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /home/navis/hive/conf/hive-default.xml
Execution log at: /tmp/navis/navis_20120628123636_5298a863-605c-4b98-bbb3-0a132c85c5a3.log
2012-06-28 12:36:18	Starting to launch local task to process map join;	maximum memory = 932118528
2012-06-28 12:36:18	Processing rows:	153	Hashtable size:	153	Memory usage:	1771376	rate:	0.002
2012-06-28 12:36:18	Dump the hashtable into file: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket22.txt.hashtable
2012-06-28 12:36:18	Upload 1 File to: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket22.txt.hashtable File size: 9644
2012-06-28 12:36:19	Processing rows:	309	Hashtable size:	156	Memory usage:	1844568	rate:	0.002
2012-06-28 12:36:19	Dump the hashtable into file: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket23.txt.hashtable
2012-06-28 12:36:19	Upload 1 File to: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket23.txt.hashtable File size: 10023
2012-06-28 12:36:19	End of local task; Time Taken: 0.773 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
java.io.IOException: This archives contains unclosed entries.
	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.finish(TarArchiveOutputStream.java:214)
	at org.apache.hadoop.hive.common.FileUtils.tar(FileUtils.java:276)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:391)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1324)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:944)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Job Submission failed with exception 'java.io.IOException(This archives contains unclosed entries.)'
java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
	at org.apache.hadoop.fs.Path.<init>(Path.java:90)
	at org.apache.hadoop.hive.ql.exec.Utilities.getHiveJobID(Utilities.java:380)
	at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:193)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:460)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1324)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:944)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask
{noformat}

Seemed to be regression from HIVE-3128.",navis,navis,Major,Closed,Fixed,28/Jun/12 03:40,10/Jan/13 19:53
Bug,HIVE-3215,12596335,JobDebugger should use RunningJob.getTrackingURL ,"When a MR job fails, the JobDebugger tries to construct the job tracker URL by connecting to the job tracker, but that is better done by using RunningJob#getTrackingURL.

Also, it tries to construct URLs to the tasks, which is not reliable, because the job could have been retired and the URL would not work.",bmandhani,rvadali,Minor,Closed,Fixed,28/Jun/12 22:28,10/Jan/13 19:53
Bug,HIVE-3218,12596480,Stream table of SMBJoin/BucketMapJoin with two or more partitions is not handled properly,"{noformat}

drop table hive_test_smb_bucket1;
drop table hive_test_smb_bucket2;

create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.enforce.bucketing = true;
set hive.enforce.sorting = true;

insert overwrite table hive_test_smb_bucket1 partition (ds='2010-10-14') select key, value from src;
insert overwrite table hive_test_smb_bucket1 partition (ds='2010-10-15') select key, value from src;
insert overwrite table hive_test_smb_bucket2 partition (ds='2010-10-15') select key, value from src;


set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SELECT /* + MAPJOIN(b) */ * FROM hive_test_smb_bucket1 a JOIN hive_test_smb_bucket2 b ON a.key = b.key;
{noformat}
which make bucket join context..
{noformat}
Alias Bucket Output File Name Mapping:
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-14/000000_0 0
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-14/000001_0 1
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-15/000000_0 0
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-15/000001_0 1
{noformat}
fails with exception
{noformat}
java.lang.RuntimeException: Hive Runtime Error while closing operators
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:226)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://localhost:9000/tmp/hive-navis/hive_2012-06-29_22-17-49_574_6018646381714861925/_task_tmp.-ext-10001/_tmp.000001_0 to: hdfs://localhost:9000/tmp/hive-navis/hive_2012-06-29_22-17-49_574_6018646381714861925/_tmp.-ext-10001/000001_0
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:198)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$300(FileSinkOperator.java:100)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:717)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:557)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	... 8 more
{noformat}",navis,navis,Critical,Closed,Fixed,30/Jun/12 00:33,10/Jan/13 19:53
Bug,HIVE-3221,12596532,HiveConf.getPositionFromInternalName does not support more than sinle digit column numbers,"For positions above 9, HiveConf.getPositionFromInternalName only looks at the last digit, and thus, causes collisions.
",sushanth,sushanth,Major,Closed,Fixed,01/Jul/12 12:51,10/Jan/13 19:53
Bug,HIVE-3225,12596860,NPE on a join query with authorization enabled,"when performing a join query which filters by a non-existent partition in the where clause (ie):

select t1.a as a1, t2.a as a2 from t1 join t2 on t1.a=t2.a where t2.part=""non-existent"";

It returns an NPE. It seems that the partition since non-existent is not part of the list of inputs (or maybe optimized out?). But the TableScanOperator still has a reference to it which causes an NPE after tableUsePartLevelAuth.get() returns null.


FAILED: Hive Internal Error: java.lang.NullPointerException(null)java.lang.NullPointerException        at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:617)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:486)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:909)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:689)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:557)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
",toffer,toffer,Major,Closed,Fixed,03/Jul/12 02:29,10/Jan/13 19:54
Bug,HIVE-3226,12597516,ColumnPruner is not working on LateralView,"Column pruning is not applied to LVJ and SEL operator, which makes exceptions at various stages. For example,
{noformat}
drop table array_valued_src;
create table array_valued_src (key string, value array<string>);
insert overwrite table array_valued_src select key, array(value) from src;

select sum(val) from (select a.key as key, b.value as array_val from src a join array_valued_src b on a.key=b.key) i lateral view explode (array_val) c as val;

... 9 more
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:157)
	... 14 more
Caused by: java.lang.RuntimeException: cannot find field _col0 from [0:_col5]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:345)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:143)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:57)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:896)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:922)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
	at org.apache.hadoop.hive.ql.exec.JoinOperator.initializeOp(JoinOperator.java:62)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:150)
{noformat}",navis,navis,Major,Closed,Fixed,05/Jul/12 02:08,10/Jan/13 19:53
Bug,HIVE-3230,12597613,Make logging of plan progress in HadoopJobExecHelper configurable,"Currently, by default, every second a job is run a massive JSON string containing the query plan, the tasks, and some counters is logged to the hive_job_log.  For large, long running jobs that can easily reach gigabytes of data. This logging should be configurable as average user doesn't need this logging.",kevinwilfong,kevinwilfong,Minor,Closed,Fixed,05/Jul/12 17:33,10/Jan/13 19:54
Bug,HIVE-3232,12597638,Resource Leak: Fix the File handle leak in EximUtil.java,"1) Not closing the file handle EximUtil after reading the metadata from the file.
2) Nit: Get the path from URI to handle the Windows paths.
",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,05/Jul/12 19:45,10/Jan/13 19:53
Bug,HIVE-3240,12597829,Fix non-deterministic results in newline.q and timestamp_lazy.q,"newline.q and timestamp_lazy.q have non-deterministic results, which make them fail on MapReduce2",zhenxiao,zhenxiao,Major,Closed,Fixed,06/Jul/12 20:46,10/Jan/13 19:53
Bug,HIVE-3242,12597876,Fix cascade_dbdrop.q when building hive on hadoop0.23,"cascade_dbdrop.q is failing when building hive on hadoop0.23:

Failing with the following diff:
-Command failed with exit code = -1
+Command failed with exit code = 1",zhenxiao,zhenxiao,Major,Closed,Fixed,07/Jul/12 00:57,10/Jan/13 19:53
Bug,HIVE-3243,12597973,ignore white space between entries of hive/hbase table mapping,"In hive/hbase integration, when creating a hive/hbase table, white space is not ignored in hbase.columns.mapping. 
e.g. ""cf:foo, cf:bar"" will create two column families ""cf"" and "" cf"" in the underlying hbase table, which is certainly not what the user want and make them confused.",shane_huang,weidong.bian,Trivial,Closed,Fixed,09/Jul/12 03:24,10/Jan/13 19:53
Bug,HIVE-3246,12598095,java primitive type for binary datatype should be byte[],"PrimitiveObjectInspector.getPrimitiveJavaObject is supposed to return a java object. But in case of binary datatype, it returns ByteArrayRef (not java standard type). The suitable java object for it would be byte[]. 
",thejas,thejas,Major,Closed,Fixed,09/Jul/12 21:30,10/Jan/13 19:53
Bug,HIVE-3247,12598104,Sorted by order of table not respected,"When a table a sorted by a column or columns, and data is inserted with hive.enforce.sorting=true, regardless of whether the metadata says the table is sorted in ascending or descending order, the data will be sorted in ascending order.

e.g.
create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;

create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS;

insert overwrite table table_desc select key, value from src;

insert overwrite table table_asc select key, value from src;

select * from table_desc;
...
96	val_96
97	val_97
97	val_97
98	val_98
98	val_98

select * from table_asc;
...
96	val_96
97	val_97
97	val_97
98	val_98
98	val_98",navis,kevinwilfong,Major,Closed,Fixed,09/Jul/12 22:08,10/Jan/13 19:53
Bug,HIVE-3248,12598174,lack of semi-colon in .q file leads to missing the next statement,"set hive.check.par=1

select count(1) from src;
select count(1) from src;



If the above .q file is executed, the first statement is lost.
Found this while reviewing https://issues.apache.org/jira/browse/HIVE-2848",appodictic,namit,Major,Closed,Fixed,10/Jul/12 12:24,10/Jan/13 19:54
Bug,HIVE-3249,12598278,Upgrade guava to 11.0.2,"Hadoop has upgraded to this new version of Guava. We should, too, so we don't have compatibility issues running on Hadoop 2.0+

currently, hive is using guava-r09.jar
we should update it to guava-11.0.2.jar",zhenxiao,zhenxiao,Major,Closed,Fixed,10/Jul/12 21:14,10/Jan/13 19:53
Bug,HIVE-3251,12598409,Hive doesn't remove scrach directories while killing running MR job,"While killing running MR job, hive doesn't clean up scratch directory (mapred.cache.files). So that, afterwards, scratch directory is left there in hdfs. HDFS name node doesn't know it and try to do lease recovery. while such instances happen more, it will eventually crash namenode.

The fix is to leverage hdfs clean up functionality. While creating scratch dirs, hive registers it to hdfs cleanup hook. While killing happens, hdfs will clean them up.

",gangtimliu,gangtimliu,Major,Closed,Fixed,11/Jul/12 17:43,10/Jan/13 19:53
Bug,HIVE-3253,12598549,ArrayIndexOutOfBounds exception for deeply nested structs,"It was observed that while creating table with deeply nested structs might throw this exception:

{code}
java.lang.ArrayIndexOutOfBoundsException: 9
        at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:281)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyStructInspector(LazyFactory.java:354)
{code}

The reason being that currently the separators array has been hardcoded to be of size 8 in the LazySimpleSerde.

{code}
// Read the separators: We use 8 levels of separators by default, but we
// should change this when we allow users to specify more than 10 levels
// of separators through DDL.
serdeParams.separators = new byte[8];
{code}

If possible, we should increase this size or at least make it configurable to properly handle deeply nested structs.",thejas,swarnim,Major,Closed,Fixed,12/Jul/12 13:26,28/Jan/15 18:11
Bug,HIVE-3256,12598658,Update asm version in Hive,"Hive trunk are currently using asm version 3.1, Hadoop trunk are on 3.2. Any
objections to bumping the Hive version to 3.2 to be inline with Hadoop",ashutoshc,zhenxiao,Major,Closed,Fixed,13/Jul/12 03:51,15/Oct/13 23:30
Bug,HIVE-3257,12598782,Fix avro_joins.q testcase failure when building hive on hadoop0.23,"avro_joins.q is failing when building hive on hadoop0.23 for both MR1 and MR2. It has an execution exception:

This query fails when execution:

SELECT e.title, e.air_date, d.first_name, d.last_name, d.extra_field, e.air_date
FROM doctors4 d JOIN episodes e ON (d.number=e.doctor)
ORDER BY d.last_name, e.title


Execution failed with exit status: 2
Obtaining error information
Task failed!
Task ID:
Stage-1
Logs:
/home/cloudera/Code/hive/build/ql/tmp//hive.log
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask

",zhenxiao,zhenxiao,Major,Closed,Fixed,14/Jul/12 00:32,02/May/13 02:30
Bug,HIVE-3261,12598937,alter the number of buckets for a non-empty partitioned table should not be allowed,This is dangerous since the code uses the table metadata everywhere to get the number of buckets,namit,namit,Major,Closed,Fixed,16/Jul/12 09:43,10/Jan/13 19:54
Bug,HIVE-3262,12598947,bucketed mapjoin silently ignores mapjoin hint,"If the bucketed mapjoin is not performed, it is silently ignored.

Atleast under strict mode, it should lead to an error.
Would wait for HIVE-3210 before working on this.",namit,namit,Major,Closed,Fixed,16/Jul/12 11:16,10/Jan/13 19:53
Bug,HIVE-3264,12599153,Add support for binary dataype to AvroSerde,"When the AvroSerde was written, Hive didn't have a binary type, so Avro's byte array type is converted an array of small ints.  Now that HIVE-2380 is in, this step isn't necessary and we can convert both Avro's bytes type and probably fixed type to Hive's binary type.",initialcontext,jghoman,Major,Closed,Fixed,17/Jul/12 18:45,08/Sep/17 23:05
Bug,HIVE-3265,12599193,HiveHistory.printRowCount() throws NPE,,shreepadma,cwsteinbach,Major,Closed,Fixed,17/Jul/12 23:22,10/Jan/13 19:53
Bug,HIVE-3267,12599219,escaped columns in cluster/distribute/order/sort by are not working,"The following query:

select `key`, value from src cluster by `key`, value;


fails",namit,namit,Major,Closed,Fixed,18/Jul/12 05:24,10/Jan/13 19:54
Bug,HIVE-3268,12599228,expressions in cluster by are not working,"The following query fails:

select key+key, value from src cluster by key+key, value;
",namit,namit,Major,Closed,Fixed,18/Jul/12 08:11,22/Aug/13 20:40
Bug,HIVE-3273,12599373,Add avro jars into hive execution classpath,avro*.jar should be added to hive execution classpath,zhenxiao,zhenxiao,Major,Closed,Fixed,18/Jul/12 23:07,10/Jan/13 19:54
Bug,HIVE-3275,12599389,Fix autolocal1.q testcase failure when building hive on hadoop0.23 MR2,"autolocal1.q is failing only on hadoop0.23 MR2, due to cluster initialization problem:

Begin query: autolocal1.q
diff -a /var/lib/jenkins/workspace/zhenxiao-CDH4-Hive-0.9.0/build/ql/test/logs/clientnegative/autolocal1.q.out /var/lib/jenkins/workspace/zhenxiao-CDH4-Hive-0.9.0/ql/src/test/results/clientnegative/autolocal1.q.out
5c5
< Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
—
> Job Submission failed with exception 'java.lang.IllegalArgumentException(Does not contain a valid host:port authority: abracadabra)'
Exception: Client execution results failed with error code = 1
See build/ql/tmp/hive.log, or try ""ant test ... -Dtest.silent=false"" to get more logs.
Failed query: autolocal1.q
",zhenxiao,zhenxiao,Major,Closed,Fixed,19/Jul/12 01:34,10/Jan/13 19:53
Bug,HIVE-3276,12599401,optimize union sub-queries,"It might be a good idea to optimize simple union queries containing map-reduce jobs in at least one of the sub-qeuries.

For eg:

a query like:



insert overwrite table T1 partition P1
select * from 
(
  subq1
    union all
  subq2
) u;


today creates 3 map-reduce jobs, one for subq1, another for subq2 and 
the final one for the union. 

It might be a good idea to optimize this. Instead of creating the union 
task, it might be simpler to create a move task (or something like a move
task), where the outputs of the two sub-queries will be moved to the final 
directory. This can easily extend to more than 2 sub-queries in the union.

This is very useful if there is a select * followed by filesink after the
union. This can be independently useful, and also be used to optimize the
skewed joins -- 
https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization.

If there is a select, filter between the union and the filesink, the select
and the filter can be moved before the union, and the follow-up job can
still be removed.",namit,namit,Major,Closed,Fixed,19/Jul/12 04:42,15/Sep/14 03:03
Bug,HIVE-3279,12599536,Table schema not being copied to Partitions with no columns,"Hive has a feature where {{Partition}}'s without any defined columns use the {{Table}} schema. This happens in {{[Partition.initialize|https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java#L167]}}

{code}
// set default if columns are not set
if (tPartition.getSd().getCols() == null) {
  if (table.getCols() != null) {
    tPartition.getSd().setCols(table.getCols());
  }
}
{code}

There's an issue though, because {{[Table.getEmptyTable|https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java#L121]}} initializes cols to an empty array, which of course is not null, causing the above feature to not work as expected.

I'm not sure of the fix - is there a case where cols can indeed be null? I think the best thing to do here is:

{code}
-        if (tPartition.getSd().getCols() == null) {
+        if (tPartition.getSd().getCols() == null || tPartition.getSd().getCols().size() == 0) {
{code}

Thoughts?",traviscrawford,traviscrawford,Major,Closed,Fixed,19/Jul/12 21:33,10/Jan/13 19:52
Bug,HIVE-3282,12599551,Convert runtime exceptions to semantic exceptions for missing partitions/tables in show/describe statements,"The SHOW PARTITIONS command in Hive does not check for valid table and partition names during query compilation. Calling this command with non-existent table causes a run-time exception.

The DESC command also does not check for this in semantic analysis.

hive> desc xxxyyy;
OK
Table xxxyyy does not exist
Time taken: 1.403 seconds

hive> show partitions xxxyyy;
Table xxxyyy does not exist
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
",sambavi,sambavi,Minor,Closed,Fixed,19/Jul/12 23:07,06/Feb/13 09:23
Bug,HIVE-3283,12599574,bucket information should be used from the partition instead of the table,"Currently Hive uses the number of buckets from the table object.
Ideally, the number of buckets from the partition should be used",kevinwilfong,namit,Major,Closed,Fixed,20/Jul/12 04:15,10/Jan/13 19:53
Bug,HIVE-3289,12599816,sort merge join may not work silently,"The user does not know, if the sort-merge join is working or not.


create table table_asc(key int, value string) CLUSTERED BY (key) SORTED BY (key asc) 
INTO 1 BUCKETS STORED AS RCFILE; 
create table table_desc(key int, value string) CLUSTERED BY (key) SORTED BY (key desc) 
INTO 1 BUCKETS STORED AS RCFILE; 

set hive.enforce.sorting = true;

insert overwrite table table_asc select key, value from src;    
insert overwrite table table_desc select key, value from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain 
select /*+mapjoin(a)*/ * from table_asc a join table_desc b on a.key = b.key;
select /*+mapjoin(a)*/ * from table_asc a join table_desc b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from table_asc a join table_desc b on a.key = b.key;
select /*+mapjoin(b)*/ * from table_asc a join table_desc b on a.key = b.key;



In the above test, the sort-merge join is not obeyed as expected.
If you user explicitly asked for sort-merge join, and it is not being
obeyed, the operation should fail.",namit,namit,Major,Closed,Fixed,23/Jul/12 08:01,02/May/13 02:30
Bug,HIVE-3291,12599897,fix fs resolvers ,"shims module fails to compile when compiling hive against 1.0 using the fs resolvers as the force=true flag forces it to use the available version of hadoop.

In a scenario where you want to build hadoop-1.0 and shims would still want to build against 20.2 and if you happen to use fs resolver ie -Dresolvers=true , fs resolvers would just use 1.0 of hadoop for shims and shims compilation will fail.",ashishujjain,gkesavan,Major,Closed,Fixed,23/Jul/12 20:20,10/Jan/13 19:53
Bug,HIVE-3293,12599969,Load file into a table does not update table statistics,"create table smb_bucket_1(key int, value string);

load data local inpath '../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;


does not update the stats for smb_bucket_1",namit,namit,Major,Closed,Fixed,24/Jul/12 09:52,10/Jan/13 19:53
Bug,HIVE-3295,12600090,HIVE-3128 introduced bug causing dynamic partitioning to fail,"HIVE-3128 introduced a new commons-compress jar and imports classes from it in FileUtils.java  The FileUtils class is accessed by dynamic partitioning in the map reduce cluster where the jar is not available, causing the query to fail.",kevinwilfong,kevinwilfong,Blocker,Closed,Fixed,24/Jul/12 23:24,10/Jan/13 19:54
Bug,HIVE-3297,12600120,change hive.auto.convert.join's default value to true,"For unit tests also, this parameter should be set to true.
",ashutoshc,namit,Major,Closed,Fixed,25/Jul/12 05:31,20/May/14 07:24
Bug,HIVE-3300,12600184,LOAD DATA INPATH fails if a hdfs file with same name is added to table,"If we are loading data from local fs to hive tables using 'LOAD DATA LOCAL INPATH' and if a file with the same name exists in the table's location then the new file will be suffixed by *_copy_1.

But if we do the 'LOAD DATA INPATH'  for a file in hdfs then there is no rename happening but just a move task is getting triggered. Since a file with same name exists in same hdfs location, hadoop fs move operation throws an error.


hive> LOAD DATA INPATH '/userdata/bejoy/site.txt' INTO TABLE test.site;
Loading data to table test.site
Failed with exception null
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
hive> 

",navis,bejoyks,Major,Closed,Fixed,25/Jul/12 16:02,16/May/13 21:10
Bug,HIVE-3301,12600246,Fix quote printing bug in mapreduce_stack_trace.q testcase failure when running hive on hadoop23,"When running hive on hadoop0.23, mapreduce_stack_trace.q is failing due to quote printing bug:

quote is printed as: '&quot;', instead of ""

Seems not able to state the bug clearly in html:

quote is printed as 'address sign' + 'quot' + semicolon
not the expected 'quote sign'",zhenxiao,zhenxiao,Major,Closed,Fixed,26/Jul/12 00:45,10/Jan/13 19:54
Bug,HIVE-3302,12600247,Race condition in query plan for merging at the end of a query,"In the query plan that's used to merge files at the end of a query, the dependency tree looks something like:
                   MoveTask(1)
                  /           \
...ConditionalTask             MoveTask(2)...
                  \           /
                   MergeTask

Here MoveTask(1) moves the partition data to a temporary location, and MoveTask(2) moves it to the final location.

However if there are dynamic partitions generated and some of these partitions are merged and others are moved, the dependency tree is changed at runtime to:
...ConditionalTask           MoveTask(2)...
                  \         /
                   MergeTask
                            \
                             MoveTask(1)

This produces a race condition between the two MoveTasks where if MoveTask(2) runs before MoveTask(1) the partitions moved by MoveTask(1) will get moved to an intermediate location and never moved to the final location.  In this case those partitions are quietly lost.",kevinwilfong,kevinwilfong,Critical,Closed,Fixed,26/Jul/12 00:47,10/Jan/13 19:53
Bug,HIVE-3303,12600252,Fix error code inconsistency bug in mapreduce_stack_trace.q and mapreduce_stack_trace_turnoff.q when running hive on hadoop23,"when running hive on hadoop23, mapreduce_stack_trace.q and mapreduce_stack_trace_turnoff.q are having inconsistent error code diffs:

[junit] diff -a /home/cloudera/Code/hive/build/ql/test/logs/clientnegative/mapreduce_stack_trace.q.out /home/cloudera/Code/hive/ql/src/test/results/clientnegative/mapreduce_stack_trace.q.out
[junit] < FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
[junit] > FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.MapRedTask. Unable to initialize custom script.


[junit] diff -a /home/cloudera/Code/hive/build/ql/test/logs/clientnegative/mapreduce_stack_trace_turnoff.q.out /home/cloudera/Code/hive/ql/src/test/results/clientnegative/mapreduce_stack_trace_turnoff.q.out
[junit] 5c5
[junit] < FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
[junit] —
[junit] > FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.MapRedTask. Unable to initialize custom script

The error code 20000(which indicates unable to initialize custom script) could not be retrieved. 

",zhenxiao,zhenxiao,Major,Closed,Fixed,26/Jul/12 01:14,10/Jan/13 19:53
Bug,HIVE-3306,12600430,SMBJoin/BucketMapJoin should be allowed only when join key expression is exactly matches with sort/cluster key,"CREATE TABLE bucket_small (key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '/home/navis/apache/oss-hive/data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small;
load data local inpath '/home/navis/apache/oss-hive/data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small;

CREATE TABLE bucket_big (key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '/home/navis/apache/oss-hive/data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big;
load data local inpath '/home/navis/apache/oss-hive/data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big;
load data local inpath '/home/navis/apache/oss-hive/data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big;
load data local inpath '/home/navis/apache/oss-hive/data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big;

select count(*) FROM bucket_small a JOIN bucket_big b ON a.key + a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key + a.key = b.key;
returns 116 (same) 

But with BucketMapJoin or SMBJoin, it returns 61. But this should not be allowed cause hash(a.key) != hash(a.key + a.key). 
Bucket context should be utilized only with exact matching join expression with sort/cluster key.",navis,navis,Minor,Closed,Fixed,27/Jul/12 06:20,10/Jan/13 19:53
Bug,HIVE-3308,12600465,Mixing avro and snappy gives null values,"On default hive uses LazySimpleSerDe for output.
When I now enable compression and ""select count(*) from avrotable"" the output is a file with the .avro extension but this then will display null values since the file is in reality not an avro file but a file created by LazySimpleSerDe using compression so should be a .snappy file.
This causes any job (exception select * from avrotable is that not truly a job) to show null values.
If you use any serde other then avro you can temporarily fix this by setting ""set hive.output.file.extension=.snappy"" and it will correctly work again but this won't work on avro since it overwrites the hive.output.file.extension during initializing.

When you dump the query result into a table with ""create table bla as"" you can rename the .avro file into .snappy and the ""select from bla"" will also magiacally work again.

Input and Ouput serdes don't always match so when I use avro as an input format it should not set the hive.output.file.extension.
Onces it's set all queries will use it and fail making the connection useless to reuse.",bennies,bennies,Major,Closed,Fixed,27/Jul/12 13:52,16/May/13 21:10
Bug,HIVE-3310,12600501,[Regression] TestMTQueries test is failing on trunk,"Hudson reported https://builds.apache.org/job/Hive-trunk-h0.21/1571/ this as a regression. Previous build was clean https://builds.apache.org/job/Hive-trunk-h0.21/1570/ 
",navis,ashutoshc,Major,Closed,Fixed,27/Jul/12 19:06,10/Jan/13 19:54
Bug,HIVE-3311,12600521,Convert runtime exceptions to semantic exceptions for validation of alter table commands,"validateAlterTableType in DDLTask.java does a bunch of checks to ensure that the alter table/view commands are correct (operations match table type, command macthes table type).
This JIRA tracks moving these to semantic exceptions.",sambavi,sambavi,Minor,Closed,Fixed,27/Jul/12 22:09,01/Apr/17 21:17
Bug,HIVE-3325,12600882,serde-reported partition cols should not be persisted in metastore,"In HIVE-3279 an issue was fixed where serde-reported columns were not added to partitions. However, the fix in that issue caused serde-reported columns to be stored in the partition storage descriptor.

Serde-reported columns should be dynamic and only reported at runtime (not stored in the partition storage descriptor).",traviscrawford,traviscrawford,Major,Resolved,Fixed,31/Jul/12 16:29,27/Jul/13 19:33
Bug,HIVE-3338,12601602,Archives broken for hadoop 1.0,Hadoop archiving through hive is broken for 1.0. The hadoop archive command line and paths have changed for 1.0 and were not addressed in the shim layer.,vikram.dixit,vikram.dixit,Major,Closed,Fixed,06/Aug/12 18:52,03/Aug/14 18:50
Bug,HIVE-3339,12601671,Change the rules in SemanticAnalyzer to use Operator.getName() instead of hardcoded names,"This should be done for code cleanup.

Instead of the rule being:

SEL%

It should say SelectOperator.getName()%

It would make the rules more readable.",zhenxiao,namit,Minor,Closed,Fixed,07/Aug/12 06:30,10/Jan/13 19:54
Bug,HIVE-3340,12602066,shims unit test failures fails further test progress,enable failonerror flag so that unit test's can continue on even when shims unit test fails.,gkesavan,gkesavan,Major,Closed,Fixed,07/Aug/12 18:21,10/Jan/13 19:54
Bug,HIVE-3341,12602076,Making hive tests run against different MR versions,"After we build hive, we want to have the ability to run unit tests against specific hadoop versions. Currently, the classpath constructed has multiple hadoop jars, which makes compiling okay, but running non-deterministic.

An example is HIVE-3156, where running against 0.23 shows issues with a couple of tests (which should either be shimmed out, or separated into directories the way it's done in the shims/ directory) - It would also be nice to find these issues out at test-compile time itself, rather than having them fail at test runtime.

",sushanth,sushanth,Major,Closed,Fixed,07/Aug/12 19:19,10/Jan/13 19:53
Bug,HIVE-3343,12602103,Hive: Query misaligned result for Group by followed by Join with filter and skip a group-by result,"This simple Hive query would generate wrong result:

select a.key, b.k2, b.k3
from src a
join (
  select key, 
         min(key) as k,
         min(key)+1 as k1,
         min(key)+2 as k2,
         min(key)+3 as k3
  from src
  group by key
) b
on a.key=b.key and b.k1 < 5;


0       3.0     1.0
0       3.0     1.0
0       3.0     1.0
2       5.0     3.0

The right result is:
0	2.0	3.0
0	2.0	3.0
0	2.0	3.0
2	4.0	5.0",gangtimliu,gangtimliu,Major,Closed,Fixed,08/Aug/12 00:02,10/Jan/13 19:53
Bug,HIVE-3345,12602308,Add junit exclude utility to disable testcases,"There is no test.junit.exclude utility in current trunk
we could disable testcases using this utility",zhenxiao,zhenxiao,Major,Closed,Fixed,08/Aug/12 22:31,10/Jan/13 19:53
Bug,HIVE-3348,12602330,semi-colon in comments in .q file does not work,"-- comment ;
-- comment

select count(1) from src;



The above test file fails",nickcollins,namit,Major,Closed,Fixed,09/Aug/12 06:03,16/May/13 21:10
Bug,HIVE-3358,12602434,Warning printed at error level,"In org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(), the message ""WARNING: Comparing a bigint and a string may result in a loss of precision."" is emitted at Log.ERROR level. This should probably be done at Log.WARN level.",philip.tromans,philip.tromans,Trivial,Resolved,Fixed,09/Aug/12 09:23,02/Oct/12 03:22
Bug,HIVE-3365,12602666,Upgrade Hive's Avro dependency to version 1.7,,shreepadma,cwsteinbach,Major,Closed,Fixed,09/Aug/12 20:51,10/Jan/13 19:53
Bug,HIVE-3375,12603260,bucketed map join should check that the number of files match the number of buckets,"Currently, we get NPE if that is not the case",namit,namit,Major,Closed,Fixed,13/Aug/12 15:47,10/Jan/13 19:53
Bug,HIVE-3379,12603386,stats are not being collected correctly for analyze table with dynamic partitions,"analyze table T partition (ds, hr);

does not collect stats correctly",namit,namit,Major,Closed,Fixed,14/Aug/12 05:53,10/Jan/13 19:53
Bug,HIVE-3381,12603394,Result of outer join is not valid,"Outer joins, especially full outer joins or outer join with filter on 'ON clause' is not showing proper results. For example, query in test join_1to1.q
{code}
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66 ORDER BY a.key1 ASC, a.key2 ASC, a.value ASC, b.key1 ASC, b.key2 ASC, b.value ASC;
{code}

results
{code}
NULL	NULL	NULL	NULL	NULL	66
NULL	NULL	NULL	NULL	10050	66
NULL	NULL	NULL	10	10010	66
NULL	NULL	NULL	30	10030	88
NULL	NULL	NULL	35	10035	88
NULL	NULL	NULL	40	10040	88
NULL	NULL	NULL	40	10040	88
NULL	NULL	NULL	50	10050	88
NULL	NULL	NULL	50	10050	88
NULL	NULL	NULL	50	10050	88
NULL	NULL	NULL	70	10040	88
NULL	NULL	NULL	70	10040	88
NULL	NULL	NULL	70	10040	88
NULL	NULL	NULL	70	10040	88
NULL	NULL	66	NULL	NULL	NULL
NULL	10050	66	NULL	NULL	NULL
5	10005	66	5	10005	66
15	10015	66	NULL	NULL	NULL
20	10020	66	20	10020	66
25	10025	88	NULL	NULL	NULL
30	10030	66	NULL	NULL	NULL
35	10035	88	NULL	NULL	NULL
40	10040	66	NULL	NULL	NULL
40	10040	66	40	10040	66
40	10040	88	NULL	NULL	NULL
40	10040	88	NULL	NULL	NULL
50	10050	66	NULL	NULL	NULL
50	10050	66	50	10050	66
50	10050	66	50	10050	66
50	10050	88	NULL	NULL	NULL
50	10050	88	NULL	NULL	NULL
50	10050	88	NULL	NULL	NULL
50	10050	88	NULL	NULL	NULL
50	10050	88	NULL	NULL	NULL
50	10050	88	NULL	NULL	NULL
60	10040	66	60	10040	66
60	10040	66	60	10040	66
60	10040	66	60	10040	66
60	10040	66	60	10040	66
70	10040	66	NULL	NULL	NULL
70	10040	66	NULL	NULL	NULL
70	10040	66	NULL	NULL	NULL
70	10040	66	NULL	NULL	NULL
80	10040	88	NULL	NULL	NULL
80	10040	88	NULL	NULL	NULL
80	10040	88	NULL	NULL	NULL
80	10040	88	NULL	NULL	NULL
{code} 

but it seemed not right. This should be 
{code}
NULL	NULL	NULL	NULL	NULL	66
NULL	NULL	NULL	NULL	10050	66
NULL	NULL	NULL	10	10010	66
NULL	NULL	NULL	25	10025	66
NULL	NULL	NULL	30	10030	88
NULL	NULL	NULL	35	10035	88
NULL	NULL	NULL	40	10040	88
NULL	NULL	NULL	50	10050	88
NULL	NULL	NULL	70	10040	88
NULL	NULL	NULL	70	10040	88
NULL	NULL	NULL	80	10040	66
NULL	NULL	NULL	80	10040	66
NULL	NULL	66	NULL	NULL	NULL
NULL	10050	66	NULL	NULL	NULL
5	10005	66	5	10005	66
15	10015	66	NULL	NULL	NULL
20	10020	66	20	10020	66
25	10025	88	NULL	NULL	NULL
30	10030	66	NULL	NULL	NULL
35	10035	88	NULL	NULL	NULL
40	10040	66	40	10040	66
40	10040	88	NULL	NULL	NULL
50	10050	66	50	10050	66
50	10050	66	50	10050	66
50	10050	88	NULL	NULL	NULL
50	10050	88	NULL	NULL	NULL
60	10040	66	60	10040	66
60	10040	66	60	10040	66
60	10040	66	60	10040	66
60	10040	66	60	10040	66
70	10040	66	NULL	NULL	NULL
70	10040	66	NULL	NULL	NULL
80	10040	88	NULL	NULL	NULL
80	10040	88	NULL	NULL	NULL
{code}",navis,navis,Critical,Closed,Fixed,14/Aug/12 07:10,16/May/13 21:09
Bug,HIVE-3383,12603402,MinimrCliDriver test hangs when run on 20S (aka 1.0) hadoop profile,"I ran 
{code} 
ant clean package test -Dtestcase=TestMinimrCliDriver -Dhadoop.mr.rev=20S
{code}
This hangs after printing following on console:
{code}
test:
    [junit] WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
    [junit] Starting DataNode 0 with dfs.data.dir: build/test/data/dfs/data/data1,build/test/data/dfs/data/data2
    [junit] Starting DataNode 1 with dfs.data.dir: build/test/data/dfs/data/data3,build/test/data/dfs/data/data4
    [junit] Starting DataNode 2 with dfs.data.dir: build/test/data/dfs/data/data5,build/test/data/dfs/data/data6
    [junit] Starting DataNode 3 with dfs.data.dir: build/test/data/dfs/data/data7,build/test/data/dfs/data/data8
    [junit] Generating rack names for tasktrackers
    [junit] Generating host names for tasktrackers
{code}",,ashutoshc,Major,Resolved,Fixed,14/Aug/12 08:14,24/Dec/14 01:32
Bug,HIVE-3384,12603418,HIVE JDBC module won't compile under JDK1.7 as new methods added in JDBC specification,"jdbc module couldn't be compiled with jdk7 as it adds some abstract method in the JDBC specification 

some error info:
 error: HiveCallableStatement is not abstract and does not override abstract
method <T>getObject(String,Class<T>) in CallableStatement
.
.
.

",cdrome,weidong.bian.intel,Minor,Closed,Fixed,14/Aug/12 08:54,19/Dec/13 11:16
Bug,HIVE-3385,12603495,fixing 0.23 test build,"Follow up jira after HIVE-3341, we need to make hive tests work on 0.23.

For starters, we need to add in a jar into build/ivy/lib/hadoop0.23.shim/ that includes MiniMRCluster. With 0.23, MiniMRCluster has moved to hadoop-mapreduce-client-jobclient-{$version}-tests.jar and that needs to be included in as an ivy dependence.
",sushanth,sushanth,Major,Closed,Fixed,14/Aug/12 19:35,10/Jan/13 19:53
Bug,HIVE-3387,12603611,meta data file size exceeds limit,"The cause is certainly that we use an array list instead of a set structure in the split locations API. Looks like a bug in Hive's CombineFileInputFormat.

Reproduce:
Set mapreduce.jobtracker.split.metainfo.maxsize=100000000 when submitting the Hive query. Run a big hive query that write data into a partitioned table. Due to the large number of splits, you encounter an exception on the job submitted to Hadoop and the exception said:

meta data size exceeds 100000000.
",navis,alo.alt,Major,Closed,Fixed,15/Aug/12 16:10,02/May/13 02:30
Bug,HIVE-3389,12603685,running tests for hadoop 23,,sushanth,namit,Major,Resolved,Fixed,16/Aug/12 05:20,24/Mar/14 20:53
Bug,HIVE-3392,12603792,Hive unnecessarily validates table SerDes when dropping a table,"natty@hadoop1:~$ hive
hive> add jar /home/natty/source/sample-code/custom-serdes/target/custom-serdes-1.0-SNAPSHOT.jar;
Added /home/natty/source/sample-code/custom-serdes/target/custom-serdes-1.0-SNAPSHOT.jar to class path
Added resource: /home/natty/source/sample-code/custom-serdes/target/custom-serdes-1.0-SNAPSHOT.jar
hive> create table test (a int) row format serde 'hive.serde.JSONSerDe';            
OK
Time taken: 2.399 seconds


natty@hadoop1:~$ hive
hive> drop table test;                                                               
FAILED: Hive Internal Error: java.lang.RuntimeException(MetaException(message:org.apache.hadoop.hive.serde2.SerDeException SerDe hive.serde.JSONSerDe does not exist))
java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException SerDe hive.serde.JSONSerDe does not exist)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:262)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:253)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:490)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:162)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:943)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDropTable(DDLSemanticAnalyzer.java:700)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:210)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:889)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException SerDe com.cloudera.hive.serde.JSONSerDe does not exist)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:211)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:260)
	... 20 more

hive> add jar /home/natty/source/sample-code/custom-serdes/target/custom-serdes-1.0-SNAPSHOT.jar;
Added /home/natty/source/sample-code/custom-serdes/target/custom-serdes-1.0-SNAPSHOT.jar to class path
Added resource: /home/natty/source/sample-code/custom-serdes/target/custom-serdes-1.0-SNAPSHOT.jar
hive> drop table test;
OK
Time taken: 0.658 seconds
hive> 
",navis,natty,Major,Closed,Fixed,16/Aug/12 22:22,13/Nov/14 19:40
Bug,HIVE-3396,12603922,hive.stats.reliable config causes FileSinkOperator to fail when writing empty file,"With the configs hive.stats.reliable and hive.stats.autogather are set to true, and using either the HBase or JDBC Stats Publishers, if a FileSinkOperator does not receive any rows, and hence collects no stats, it will throw an exception.

Related, if hive.stats.reliable is set to false it will still log a warning which seems unnecessary.

Repro:

create table tmptable(key string, value string) partitioned by (part string);

set hive.stats.autogather=true;
set hive.stats.reliable=true;

insert overwrite table tmptable partition (part = '1') select * from src where key = 'no_such_value';",kevinwilfong,kevinwilfong,Major,Closed,Fixed,17/Aug/12 23:00,10/Jan/13 19:53
Bug,HIVE-3403,12604358,user should not specify mapjoin to perform sort-merge bucketed join,"Currently, in order to perform a sort merge bucketed join, the user needs
to set hive.optimize.bucketmapjoin.sortedmerge to true, and also specify the 
mapjoin hint.

The user should not specify any hints.",namit,namit,Major,Closed,Fixed,22/Aug/12 10:12,04/Dec/16 05:48
Bug,HIVE-3407,12604988,Update Hive CLI xdoc (sync with CLI wikidoc),"CLI documentation for Hive exists in two places (wikidocs and xdocs) and both of the versions are out of date, but the xdocs version was worse: 

* [http://hive.apache.org/docs/r0.9.0/language_manual/cli.html]
* [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli]

A revised CLI wikidoc is available and will soon be exchanged for the old wikidoc. Although there's some resistance to moving more of the wikidocs into xdocs, for now let's have current information in both places instead of removing the xdocs version.",leftyl,leftyl,Major,Resolved,Fixed,26/Aug/12 02:30,08/May/13 16:31
Bug,HIVE-3409,12605052,Increase test.junit.timeout value,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,27/Aug/12 09:55,10/Jan/13 19:53
Bug,HIVE-3411,12605168,Filter predicates on outer join overlapped on single alias is not handled properly,"Currently, join predicates on outer join are evaluated in join operator (or HashSink for MapJoin) and the result value is tagged to end of each values(as a boolean), which is used for joining values. But when predicates are overlapped on single alias, all the predicates are evaluated with AND conjunction, which makes invalid result. 

For example with table a with values,
{noformat}
100 40
100 50
100 60
{noformat}

Query below has overlapped predicates on alias b, which is making all the values on b are tagged with true(filtered)
{noformat}
select * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60);

NULL	NULL	100	40	NULL	NULL
NULL	NULL	100	50	NULL	NULL
NULL	NULL	100	60	NULL	NULL

-- Join predicate
Join Operator
  condition map:
       Right Outer Join0 to 1
       Left Outer Join1 to 2
  condition expressions:
    0 {VALUE._col0} {VALUE._col1}
    1 {VALUE._col0} {VALUE._col1}
    2 {VALUE._col0} {VALUE._col1}
  filter predicates:
    0 
    1 {(VALUE._col1 = 50)} {(VALUE._col1 = 60)}
    2 
{noformat}

but this should be 

{noformat}
NULL	NULL	100	40	NULL	NULL
100	50	100	50	NULL	NULL
NULL	NULL	100	60	100	60
{noformat}",navis,navis,Minor,Closed,Fixed,28/Aug/12 02:59,02/May/13 02:30
Bug,HIVE-3412,12605317,"Fix TestCliDriver.repair on Hadoop 0.23.3, 3.0.0, and 2.2.0-alpha","TestCliDriver.repair fails on the following Hadoop versions:
0.23.3, 3.0.0, 2.2.0-alpha

repair.q fails with ""dfs -mkdir"":
[junit] mkdir: `../build/ql/test/data/warehouse/repairtable/p1=a/p2=a': No such file or directory

The problem is, after fixing HADOOP-8551, which changes the hdfs Shell syntax for mkdir:
https://issues.apache.org/jira/browse/HADOOP-8551

all ""dfs -mkdir"" commands should provide ""-p"" in order to execute without error.

This is an intentional change in HDFS. And HADOOP-8551 will be included in 0.23.3, 3.0.0, 2.2.0-alpha versions.
",zhenxiao,zhenxiao,Major,Closed,Fixed,29/Aug/12 00:28,10/Jan/13 19:54
Bug,HIVE-3413,12605327,Fix pdk.PluginTest on hadoop23,"When running Hive test on Hadoop0.23, pdk.PluginTest is failing:

test:
    [junit] Running org.apache.hive.pdk.PluginTest
    [junit] Hive history file=/tmp/cloudera/hive_job_log_cloudera_201208281845_172375530.txt
    [junit] Total MapReduce jobs = 1
    [junit] Launching Job 1 out of 1
    [junit] Number of reduce tasks determined at compile time: 1
    [junit] In order to change the average load for a reducer (in bytes):
    [junit]   set hive.exec.reducers.bytes.per.reducer=<number>
    [junit] In order to limit the maximum number of reducers:
    [junit]   set hive.exec.reducers.max=<number>
    [junit] In order to set a constant number of reducers:
    [junit]   set mapred.reduce.tasks=<number>
    [junit] WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
    [junit] Execution log at: /tmp/cloudera/cloudera_20120828184545_6deeb166-7dd4-40d3-9ff7-c5d5277aee39.log
    [junit] java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.
    [junit]     at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:123)
    [junit]     at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:85)
    [junit]     at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:78)
    [junit]     at org.apache.hadoop.mapred.JobClient.init(JobClient.java:487)
    [junit]     at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:466)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:424)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:688)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:616)
    [junit]     at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
    [junit] Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
    [junit] Execution failed with exit status: 1
    [junit] Obtaining error information
    [junit]
    [junit] Task failed!
    [junit] Task ID:
    [junit]   Stage-1
    [junit]
    [junit] Logs:
    [junit]
    [junit] /tmp/cloudera/hive.log
    [junit] FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask]>)
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 33.9 sec

With details in ./build/builtins/TEST-org.apache.hive.pdk.PluginTest.txt:


Testsuite: org.apache.hive.pdk.PluginTest
Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 33.9 sec
------------- Standard Error -----------------
GLOBAL SETUP:  Copying file: file:/home/cloudera/Code/hive2/builtins/test/onerow.txt
Deleted /home/cloudera/Code/hive2/build/builtins/warehouse/onerow
Copying file: file:/home/cloudera/Code/hive2/builtins/test/iris.txt
Deleted /home/cloudera/Code/hive2/build/builtins/warehouse/iris
org.apache.hive.builtins.UDAFUnionMap TEARDOWN:
Hive history file=/tmp/cloudera/hive_job_log_cloudera_201208281845_840355011.txt
GLOBAL TEARDOWN:
Hive history file=/tmp/cloudera/hive_job_log_cloudera_201208281845_252250000.txt
OK
Time taken: 6.874 seconds
OK
Time taken: 0.512 seconds
------------- ---------------- ---------------

Testcase: SELECT size(UNION_MAP(MAP(sepal_width, sepal_length))) FROM iris took 4.428 sec
    FAILED
expected:<[23]> but was:<[
Hive history file=/tmp/cloudera/hive_job_log_cloudera_201208281845_172375530.txt
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
Execution log at: /tmp/cloudera/cloudera_20120828184545_6deeb166-7dd4-40d3-9ff7-c5d5277aee39.log
java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.
    at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:123)
    at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:85)
    at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:78)
    at org.apache.hadoop.mapred.JobClient.init(JobClient.java:487)
    at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:466)
    at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:424)
    at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:688)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:616)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
Execution failed with exit status: 1
Obtaining error information

Task failed!
Task ID:
  Stage-1

Logs:

/tmp/cloudera/hive.log
",zhenxiao,zhenxiao,Major,Closed,Fixed,29/Aug/12 02:11,10/Jan/13 19:52
Bug,HIVE-3416,12605436,Fix TestAvroSerdeUtils.determineSchemaCanReadSchemaFromHDFS when running Hive on hadoop23,"TestAvroSerdeUtils determinSchemaCanReadSchemaFromHDFS is failing when running hive on hadoop23:

$ant very-clean package -Dhadoop.version=0.23.1 -Dhadoop-0.23.version=0.23.1 -Dhadoop.mr.rev=23

$ant test -Dhadoop.version=0.23.1 -Dhadoop-0.23.version=0.23.1 -Dhadoop.mr.rev=23 -Dtestcase=TestAvroSerdeUtils


 <testcase classname=""org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils"" name=""determineSchemaCanReadSchemaFromHDFS"" time=""0.21"">
    <error message=""org/apache/hadoop/net/StaticMapping"" type=""java.lang.NoClassDefFoundError"">java.lang.NoClassDefFoundError: org/apache/hadoop/net/StaticMapping
    at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:534)
    at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:489)
    at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:360)
    at org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.determineSchemaCanReadSchemaFromHDFS(TestAvroSerdeUtils.java:187)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:616)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.net.StaticMapping
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
    ... 25 more
</error>
  </testcase>
",zhenxiao,zhenxiao,Major,Closed,Fixed,29/Aug/12 21:16,10/Jan/13 19:53
Bug,HIVE-3424,12605961,Error by upgrading a Hive 0.7.0 database to 0.8.0 (008-HIVE-2246.mysql.sql) ,"{code}
+----------------------------------------+ 
| Inserted table columns into COLUMNS_V2 | 
+----------------------------------------+ 
| Inserted table columns into COLUMNS_V2 | 
+----------------------------------------+ 
+--------------------------+ 
| Completed migrating idxs | 
+--------------------------+ 
| Completed migrating idxs | 
+--------------------------+ 
ERROR 1025 (HY000) at line 250 in file: '008-HIVE-2246.mysql.sql': Error on rename of './metastore/COLUMNS_OLD' to './metastore/#sql2-14e6-6eb' (errno: 152)
{code}",alo.alt,alo.alt,Blocker,Closed,Fixed,03/Sep/12 06:45,10/Jan/13 19:52
Bug,HIVE-3428,12606175,Fix log4j configuration errors when running hive on hadoop23,"There are log4j configuration errors when running hive on hadoop23, some of them may fail testcases, since the following log4j error message could printed to console, or to output file, which diffs from the expected output:

[junit] < log4j:ERROR Could not find value for key log4j.appender.NullAppender
[junit] < log4j:ERROR Could not instantiate appender named ""NullAppender"".
[junit] < 12/09/04 11:34:42 WARN conf.HiveConf: hive-site.xml not found on CLASSPATH
",hagleitn,zhenxiao,Major,Closed,Fixed,04/Sep/12 22:55,16/May/13 21:10
Bug,HIVE-3429,12606191,Bucket map join involving table with more than 1 partition column causes FileNotFoundException,"Running a bucket map join exception on a table with more than one partition results in an exception is below.  This is because the partition spec is added to the file name, which unintentionally, produces a new subdirectory.   

 [junit] java.io.FileNotFoundException: /Users/kevinwilfong/Documents/hive_driver_start/build/ql/scratchdir/local/hive_2012-09-04_18-35-38_679_3765928822897237252/-local-10002/HashTable-Stage-1/MapJoin-b-21-(ds=2008-04-08 (No such file or directory)
    [junit] 	at java.io.FileInputStream.open(Native Method)
    [junit] 	at java.io.FileInputStream.<init>(FileInputStream.java:120)
    [junit] 	at org.apache.hadoop.hive.common.CompressionUtils.tar(CompressionUtils.java:59)
    [junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:398)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137)
    [junit] 	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:135)
    [junit] 	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
    [junit] 	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1326)
    [junit] 	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1112)
    [junit] 	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:945)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347)
    [junit] 	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:712)
    [junit] 	at org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7(TestMinimrCliDriver.java:288)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at junit.framework.TestCase.runTest(TestCase.java:168)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:134)
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:110)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:128)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:113)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:124)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:232)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:227)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] java.lang.IllegalArgumentException: Can not create a Path from an empty string
    [junit] 	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
    [junit] 	at org.apache.hadoop.fs.Path.<init>(Path.java:90)
    [junit] 	at org.apache.hadoop.hive.ql.exec.Utilities.getHiveJobID(Utilities.java:381)
    [junit] 	at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:194)
    [junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:472)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137)
    [junit] 	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:135)
    [junit] 	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
    [junit] 	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1326)
    [junit] 	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1112)
    [junit] 	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:945)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
    [junit] 	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347)
    [junit] 	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:712)
    [junit] 	at org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7(TestMinimrCliDriver.java:288)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at junit.framework.TestCase.runTest(TestCase.java:168)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:134)
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:110)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:128)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:113)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:124)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:232)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:227)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] junit.framework.AssertionFailedError: Client Execution failed with error code = 1",kevinwilfong,kevinwilfong,Major,Closed,Fixed,05/Sep/12 01:53,10/Jan/13 19:53
Bug,HIVE-3435,12606303,Get pdk pluginTest passed when triggered from both builtin tests and pdk tests on hadoop23 ,"Hive pdk pluginTest is running twice in unit testing, one is triggered from running builtin tests, another is triggered from running pdk tests.

HIVE-3413 fixed pdk pluginTest on hadoop23 when triggered from running builtin tests. While, when triggered from running pdk tests directly on hadoop23, it is failing:

Testcase: SELECT tp_rot13('Mixed Up!') FROM onerow; took 6.426 sec
FAILED
expected:<[]Zvkrq Hc!> but was:<[2012-09-04 18:13:01,668 WARN [main] conf.HiveConf (HiveConf.java:<clinit>(73)) - hive-site.xml not found on CLASSPATH
]Zvkrq Hc!>
junit.framework.ComparisonFailure: expected:<[]Zvkrq Hc!> but was:<[2012-09-04 18:13:01,668 WARN [main] conf.HiveConf (HiveConf.java:<clinit>(73)) - hive-site.xml not found on CLASSPATH
]Zvkrq Hc!>",zhenxiao,zhenxiao,Major,Closed,Fixed,05/Sep/12 21:38,10/Jan/13 19:53
Bug,HIVE-3436,12606306, Difference in exception string from native method causes script_pipe.q to fail on windows,,thejas,thejas,Major,Closed,Fixed,05/Sep/12 22:00,10/Jan/13 19:53
Bug,HIVE-3437,12606341,0.23 compatibility: fix unit tests when building against 0.23,Many unit tests fail as a result of building the code against hadoop 0.23. Initial focus will be to fix 0.9.,cdrome,cdrome,Major,Closed,Fixed,06/Sep/12 01:46,02/May/13 02:29
Bug,HIVE-3440,12606485,Fix pdk PluginTest failing on trunk-h0.21,"Get the failure when running on hadoop21, triggered directly from pdk(when triggered from builtin, pdk test is passed).

Here is the execution log:

2012-09-06 13:46:05,646 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(256)) - job_local_0001
java.lang.RuntimeException: Error in configuring object
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:354)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:616)
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
    ... 5 more
Caused by: java.lang.RuntimeException: Error in configuring object
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)
    ... 10 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:616)
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
    ... 13 more
Caused by: java.lang.RuntimeException: Map operator initialization failed
    at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
    ... 18 more
Caused by: java.lang.NoClassDefFoundError: org/codehaus/jackson/map/ObjectMapper
    at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.<clinit>(GenericUDTFJSONTuple.java:54)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:113)
    at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDTF(FunctionRegistry.java:545)
    at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDTF(FunctionRegistry.java:539)
    at org.apache.hadoop.hive.ql.exec.FunctionRegistry.<clinit>(FunctionRegistry.java:472)
    at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59)
    at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:154)
    at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:98)
    at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:137)
    at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:898)
    at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:924)
    at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
    at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:358)
    at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:434)
    at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:390)
    at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:166)
    at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:358)
    at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:441)
    at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:358)
    at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98)
    ... 18 more
Caused by: java.lang.ClassNotFoundException: org.codehaus.jackson.map.ObjectMapper
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
",zhenxiao,zhenxiao,Major,Closed,Fixed,06/Sep/12 21:00,10/Jan/13 19:53
Bug,HIVE-3441,12606498,"testcases escape1,escape2 fail on windows",,thejas,thejas,Major,Closed,Fixed,06/Sep/12 22:48,10/Jan/13 19:53
Bug,HIVE-3443,12606515,Hive Metatool should take serde_param_key from the user to allow for changes to avro serde's schema url key,"Hive Metatool should take serde_param_key from the user to allow for chanes to avro serde's schema url key. In the past ""avro.schema.url"" key used to be called ""schema.url"". ",shreepadma,shreepadma,Critical,Closed,Fixed,07/Sep/12 01:01,13/Apr/15 22:18
Bug,HIVE-3446,12606558,PrimitiveObjectInspector doesn't handle timestamps properly,Getting java.sql.Timestamp from a TimestampWritable is broken due to an incorrect mapping in PrimitiveObjectInspectorUtils.,samt,samt,Major,Closed,Fixed,07/Sep/12 11:08,16/May/13 21:11
Bug,HIVE-3448,12606674,GenMRSkewJoinProcessor uses File.Separator instead of Path.Separator ,This causes testcase skewjoin.q to fail on windows.,thejas,thejas,Major,Closed,Fixed,07/Sep/12 20:54,10/Jan/13 19:54
Bug,HIVE-3451,12607176,map-reduce jobs does not work for a partition containing sub-directories,"Consider the following test:




-- The test verifies that sub-directories are supported for versions of hadoop
-- where MAPREDUCE-1501 is fixed. So, enable this test only for hadoop 23.
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

CREATE TABLE fact_daily(x int) PARTITIONED BY (ds STRING);
CREATE TABLE fact_tz(x int) PARTITIONED BY (ds STRING, hr STRING) 
LOCATION 'pfile:${system:test.tmp.dir}/fact_tz';

INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='1')
SELECT key+11 FROM src WHERE key=484;

ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE');
ALTER TABLE fact_daily ADD PARTITION (ds='1')
LOCATION 'pfile:${system:test.tmp.dir}/fact_tz/ds=1';

set mapred.input.dir.recursive=true;
SELECT * FROM fact_daily WHERE ds='1';

SELECT count(1) FROM fact_daily WHERE ds='1';



Say, the above file was named: recursive_dir.q

and we ran the test for hadoop 23:

by executing:

ant test -Dhadoop.mr.rev=23 -Dtest.print.classpath=true -Dhadoop.version=2.0.0-alpha -Dhadoop.security.version=2.0.0-alpha -Dtestcase=TestCliDriver -Dqfile=recursive_dir.q

The select * from the table works fine, but the last command does not work
since it requires a map-reduce job.

This will prevent other features which are creating sub-directories to add
any tests which requires a map-reduce job. The work-around is to issue
queries which do not require map-reduce jobs.",gangtimliu,namit,Major,Closed,Fixed,12/Sep/12 05:38,31/Mar/13 01:25
Bug,HIVE-3452,12607340,Missing column causes null pointer exception,"select * from src where src = 'alkdfaj';
FAILED: SemanticException null",jeanxu,jeanxu,Minor,Closed,Fixed,12/Sep/12 21:16,10/Jan/13 19:53
Bug,HIVE-3454,12607355,Problem with CAST(BIGINT as TIMESTAMP),"Ran into an issue while working with timestamp conversion.
CAST(unix_timestamp() as TIMESTAMP) should create a timestamp for the current time from the BIGINT returned by unix_timestamp()

Instead, however, a 1970-01-16 timestamp is returned.",aihuaxu,rharris,Major,Closed,Fixed,12/Sep/12 22:40,23/Nov/15 19:26
Bug,HIVE-3455,12607356,"ANSI CORR(X,Y) is incorrect","A simple test with 2 collinear vectors returns a wrong result.
The problem is the merge of variances, file:

http://svn.apache.org/viewvc/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java?revision=1157222&view=markup

lines:
347: myagg.xvar += xvarB + (xavgA - xavgB) * (xavgA - xavgB) * myagg.count;
348: myagg.yvar += yvarB + (yavgA - yavgB) * (yavgA - yavgB) * myagg.count;

the correct merge should be like this:
347: myagg.xvar += xvarB + (xavgA - xavgB) * (xavgA - xavgB) / myagg.count * nA * nB;
348: myagg.yvar += yvarB + (yavgA - yavgB) * (yavgA - yavgB) / myagg.count * nA * nB;

",maximbo,maximbo,Major,Resolved,Fixed,12/Sep/12 22:49,06/Dec/13 00:55
Bug,HIVE-3458,12607377,Parallel test script doesnt run all tests,Parallel test script when run on a cluster of machines in multi-threaded mode doesnt report back on all tests in the suite. ,ivangorbachev,sambavi,Major,Closed,Fixed,13/Sep/12 01:13,10/Jan/13 19:53
Bug,HIVE-3459,12607380,Dynamic partition queries producing no partitions fail with hive.stats.reliable=true,Dynamic partition inserts which result in no partitions (either because the input is empty or all input rows are filtered out) will fail because stats cannot be collected if hive.stats.reliable=true.,kevinwilfong,kevinwilfong,Major,Closed,Fixed,13/Sep/12 01:51,10/Jan/13 19:53
Bug,HIVE-3461,12607551,hive unit tests fail to get lock using zookeeper on windows,"Following exception is seen from test cases when lock is attempted -
{code}
2012-08-26 10:33:33,597 ERROR ZooKeeperHiveLockManager (ZooKeeperHiveLockManager.java:lock(317)) - Serious Zookeeper exception:
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hive_zookeeper_namespace/default
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:778)
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.createChild(ZooKeeperHiveLockManager.java:285)
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lockPrimitive(ZooKeeperHiveLockManager.java:353)
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lock(ZooKeeperHiveLockManager.java:303)
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lock(ZooKeeperHiveLockManager.java:220)
        at org.apache.hadoop.hive.ql.Driver.acquireReadWriteLocks(Driver.java:828)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:944)
        at org.apache.hadoop.hive.ql.QTestUtil.runLoadCmd(QTestUtil.java:458)
        at org.apache.hadoop.hive.ql.QTestUtil.createSources(QTestUtil.java:505)
        at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:55)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:374)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)
{code}",thejas,thejas,Major,Closed,Fixed,14/Sep/12 00:39,10/Jan/13 19:54
Bug,HIVE-3464,12607571,Merging join tree may reorder joins which could be invalid,"Currently, hive merges join tree from right to left regardless of join types, which may introduce join reordering. For example,

select * from a join a b on a.key=b.key join a c on b.key=c.key join a d on a.key=d.key; 

Hive tries to merge join tree in a-d=b-d, a-d=a-b, b-c=a-b order and a-d=a-b and b-c=a-b will be merged. Final join tree is ""a-(bdc)"".

With this, ab-d join will be executed prior to ab-c. But if join type of -c and -d is different, this is not valid.",navis,navis,Major,Closed,Fixed,14/Sep/12 05:42,16/May/13 21:10
Bug,HIVE-3465,12607654,insert into statement overwrites if target table is prefixed with database name,"insert into statement works as expected when the target table is not prefixed with database name.  It does append correctly.

However, if I specify a database name prefixing the target table, the statement just overwrites instead of appends:

insert into table my_database.target select * from source;
",navis,kaufman,Major,Closed,Fixed,14/Sep/12 15:07,10/Jan/13 19:53
Bug,HIVE-3475,12607998,INLINE UDTF doesn't convert types properly,"I suppose the issue is in line:
this.forwardObj [ i ] = res.convertIfNecessary(rowList.get( i ), f.getFieldObjectInspector());

there is never reason for conversion, it should just be:
this.forwardObj [ i ] = rowList.get( i )

Caused by: java.lang.ClassCastException: org.apache.hadoop.io.LongWritable cannot be cast to java.lang.Long
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.get(JavaLongObjectInspector.java:39)
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:203)
	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:427)
	at org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.serialize(ColumnarSerDe.java:169)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:569)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.processOp(LateralViewJoinOperator.java:133)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.forwardUDTFOutput(UDTFOperator.java:112)
	at org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.collect(UDTFCollector.java:44)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.forward(GenericUDTF.java:81)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.process(GenericUDTFInline.java:63)
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.processOp(UDTFOperator.java:98)",navis,ikabiljo,Minor,Closed,Fixed,18/Sep/12 03:24,15/Oct/13 23:29
Bug,HIVE-3477,12608046,Duplicate data possible with speculative execution for dynamic partitions,"Consider a query like:

insert overwrite T partition (ds)
select * from
(mapreduce-subq1
  union all
mapreduce-subq2)x;

Once, mapreduce-subq1 and mapreduce-subq2 are done, the task for the union
is invoked. At the end of the union task, jobClose is invoked.

Note that there are 2 tablescan operators. The tree is something like:


TABLESCAN1  --
              \
               UNION -- SELECT -- FILESINK
              /
TABLESCAN2  --


In the current setup, jobClose will be invoked twice for FileSink.
In case of speculative execution, it is possible that data is still is
being written to tmp Dir. after jobClose is finished once. 

The correct fix would be to make sure that jobClose is only invoked once.",namit,namit,Major,Closed,Fixed,18/Sep/12 11:23,10/Jan/13 19:54
Bug,HIVE-3478,12608074,Remove the specialized logic to handle the file schemas in windows vs unix from build.xml,"After more testing, I realized that this special check can be removed by changing the “\\” with “\\\” to work on both platforms",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,18/Sep/12 14:49,10/Jan/13 19:53
Bug,HIVE-3479,12608075,Bug fix: Return the child JVM exit code to the parent process to handle the error conditions,It is a bug in the script and noticed it while fixing some of the Negative CLI test failures,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,18/Sep/12 14:50,10/Jan/13 19:52
Bug,HIVE-3480,12608077,<Resource leak>: Fix the file handle leaks in Symbolic & Symlink related input formats.,Noticed these file handle leaks while fixing the Symlink related unit test failures on Windows.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,18/Sep/12 15:02,26/Feb/16 09:13
Bug,HIVE-3481,12608078,<Resource leak>: Hiveserver is not closing the existing driver handle before executing the next command. It results in to file handle leaks.,Close the driver object if it exists before creating another driver object. Bunch of HiveServer & JDBC related unit tests are failing because of these file handle leaks.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,18/Sep/12 15:04,10/Jan/13 19:54
Bug,HIVE-3483,12608126,joins using partitioned table give incorrect results on windows ,"Hive CombineFilter.getSplits returns single split instead of multiple splits on windows. The comparison in CombineFilter.accept() was using paths converted to strings instead of using Paths themselves, and one of the strings had ""/C:"" and other had ""C:"" .

The tests that are affected by this issue include - auto_join18.q,auto_join18_multi_distinct.q,
join18.q,join18_multi_distinct.q,ppd_union_view.q,create_view.q and union20.q",thejas,thejas,Major,Closed,Fixed,18/Sep/12 20:37,10/Jan/13 19:54
Bug,HIVE-3484,12608142,RetryingRawStore logic needs to be significantly reworked to support retries within transactions,The logic for retrying has been moved from RetryingRawStore to RetryingHMSHandler. ,jeanxu,jeanxu,Major,Closed,Fixed,18/Sep/12 21:39,19/Nov/14 07:41
Bug,HIVE-3485,12608186,Hive List Bucketing - Skewed DDL doesn't support skewed value with string quote,"CREATE TABLE list_bucket_single (key STRING, value STRING) SKEWED BY (key) ON ('1','5','6')

Save '1' as in map instead 1 should",gangtimliu,gangtimliu,Minor,Closed,Fixed,19/Sep/12 02:50,10/Jan/13 19:53
Bug,HIVE-3486,12608194,CTAS in database with location on non-default name node fails,"If a database has a location which is on a different name node than the default database's location, CTAS queries run in that database will fail.

This is because the intermediate location which is where the final FileSinkOperator writes to is determined based on the scheme and authority of the value of hive.metastore.warehouse.dir instead of the table's database's location.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,19/Sep/12 05:23,10/Jan/13 19:53
Bug,HIVE-3487,12608199,Some of the Metastore unit tests failing on Windows because of the static variables initialization problem in HiveConf class.,"Following unit tests are failing on windows because of the problem with static variables initialization.
TestMarkPartition
TestMarkRemotePartition
TestMetastoreEventListener

Reason – Static variables on outer class static variables are not getting initialized when we reference nested public static class variables on windows.
",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,19/Sep/12 06:57,10/Jan/13 19:53
Bug,HIVE-3490,12608367,Implement * or a.* for arguments to UDFs,"For a random UDF, we should be able to use * or a.* to refer to ""all of the columns in their natural order."" This is not currently implemented.

I'm reporting this as a bug because it is a manner in which Hive is inconsistent with the SQL spec, and because Hive claims to implement *.

hive> select all_non_null(a.*) from table a where a.ds='2012-09-01';
FAILED: ParseException line 1:25 mismatched input '*' expecting Identifier near '.' in expression specification
",navis,akramer,Major,Closed,Fixed,20/Sep/12 02:05,13/Aug/14 05:14
Bug,HIVE-3493,12608480,aggName of SemanticAnalyzer.getGenericUDAFEvaluator is generated in two different ways,"aggName in org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(String, ArrayList<ExprNodeDesc>, ASTNode, boolean, boolean) is generated by two different ways. One is String aggName = value.getChild(0).getText(); and another is String aggName = unescapeIdentifier(value.getChild(0).getText());. When a aggregation function is involved in a view, we may get a error.

You can try the query below (from create_view.q) to replay the error.
{code:sql}
set hive.map.aggr=false;
CREATE TEMPORARY FUNCTION test_max AS
'org.apache.hadoop.hive.ql.udf.UDAFTestMax';
CREATE VIEW view9(m) AS
SELECT test_max(length(value))
FROM src;
DESCRIBE EXTENDED view9;
DESCRIBE FORMATTED view9;
SELECT * FROM view9;
{code}

Here is the log
{code}
2012-09-20 07:26:15,176 DEBUG exec.FunctionRegistry (FunctionRegistry.java:getGenericUDAFResolver(849)) - Looking up GenericUDAF: `test_max`
2012-09-20 07:26:15,181 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: SemanticException Line 1:33 Function argument type mismatch '`test_max`' in definition of VIEW view9 [
SELECT `_c0` AS `m` FROM (SELECT `test_max`(length(`src`.`value`))
FROM `default`.`src`) `view9`
] used as view9 at Line 2:14: Looking for UDAF Evaluator""`test_max`"" with parameters [org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector@5afa2b2b]
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:33 Function argument type mismatch '`test_max`' in definition of VIEW view9 [
SELECT `_c0` AS `m` FROM (SELECT `test_max`(length(`src`.`value`))
FROM `default`.`src`) `view9`
] used as view9 at Line 2:14: Looking for UDAF Evaluator""`test_max`"" with parameters [org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector@5afa2b2b]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:2394)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator(SemanticAnalyzer.java:2561)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlan1MR(SemanticAnalyzer.java:3341)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:6140)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6903)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6843)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6864)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6843)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6864)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7484)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:431)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:903)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:713)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view(TestCliDriver.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)
{code}",yhuai,yhuai,Minor,Closed,Fixed,20/Sep/12 14:45,10/Jan/13 19:52
Bug,HIVE-3494,12608516,Some of the JDBC test cases are failing on Windows because of the longer class path.,If the class path size is more than 8K then we can’t set the environment variable so some of the test cases are failing on Windows. Remove the duplicate JAR entries from the class path to reduce the chance of exceeding the 8K limit.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,20/Sep/12 19:04,10/Jan/13 19:53
Bug,HIVE-3495,12608526,"For UDAFs, when generating a plan without map-side-aggregation, constant agg parameters will be replaced by ExprNodeColumnDesc","For UDAFs, when generating a plan without map-side-aggregation, constant agg parameters having ConstantObjectInspectors will be replaced by ExprNodeColumnDescs. A UDFArgumentTypeException will be thrown if a UDAF need to checkout parameters' types. 

An example used to reply the error is 
{code:sql}
set hive.map.aggr=false;
SELECT percentile_approx(cast(substr(src.value,5) AS double), 0.5) FROM src;
{code}. 

Here is the log 
{code}
2012-09-20 12:36:06,947 DEBUG exec.FunctionRegistry (FunctionRegistry.java:getGenericUDAFResolver(849)) - Looking up GenericUDAF: percentile_approx
2012-09-20 12:36:06,952 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: UDFArgumentTypeException The second argument must be a constant, but double was passed instead.
org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:149)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:774)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:2389)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator(SemanticAnalyzer.java:2561)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlan1MR(SemanticAnalyzer.java:3341)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:6140)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6903)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7484)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:431)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:903)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:713)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_replay(TestCliDriver.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)
{code}",yhuai,yhuai,Minor,Closed,Fixed,20/Sep/12 19:47,12/Feb/13 17:06
Bug,HIVE-3496,12608559,Query plan for multi-join where the third table joined is a subquery containing a map-only union with hive.auto.convert.join=true is wrong,"Take the following query as an example:

EXPLAIN SELECT * FROM 
src11 a JOIN
src12 b ON (a.key = b.key) JOIN
(SELECT * FROM (SELECT * FROM src13 UNION ALL SELECT * FROM src14)a )c ON c.value = b.value;

When hive.auto.convert.join=true, the two joins are implemented separately as conditional tasks with two mapjoins and a backup common join.  In the second join, the conditional task will be a backup task, contained in the ConditionalTask, and a root task.  This is clearly wrong, and leads to query failures.

I've traced this to the joinUnionPlan method of GenMapRedUtils.  If the union operator was performed in its own map reduce task and it could be a root task, when it is added to the mapper of the existing task which performs the join in the reducer, this task will get made a root task without first checking if the existing (non-union) task has any dependencies.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,21/Sep/12 01:24,10/Jan/13 19:53
Bug,HIVE-3497,12608573,Avoid NPE in skewed information read,Table.java reads skwed information is not safe. Add check to make is safe,gangtimliu,gangtimliu,Major,Closed,Fixed,21/Sep/12 05:02,10/Jan/13 19:53
Bug,HIVE-3498,12608677,hivetest.py fails with --revision option,"How to reproduce outside hivetest.py:
1. Clone git://git.apache.org/hive.git
2. Run ant arc-setup
3. Run arc patch <rev>

Output:
{quote}
    This diff is against commit
    https://svn.apache.org/repos/asf/hive/trunk@1382631, but the commit is
    nowhere in the working copy. Try to apply it against the current working
    copy state? (d5f66df1edfff2645f225298e225dbccc70d97ff) [Y/n] 
{quote}
If you choose 'Y' it suggests you to complete 'merge-message' and then prints:
{quote}
 Select a Default Commit Range

You're running a command which operates on a range of revisions (usually,
from some revision to HEAD) but have not specified the revision that should
determine the start of the range.

Previously, arc assumed you meant 'HEAD^' when you did not specify a start
revision, but this behavior does not make much sense in most workflows
outside of Facebook's historic git-svn workflow.

arc no longer assumes 'HEAD^'. You must specify a relative commit explicitly
when you invoke a command (e.g., `arc diff HEAD^`, not just `arc diff`) or
select a default for this working copy.

In most cases, the best default is 'origin/master'. You can also select
'HEAD^' to preserve the old behavior, or some other remote or branch. But you
almost certainly want to select 'origin/master'.

(Technically: the merge-base of the selected revision and HEAD is used to
determine the start of the commit range.)

    What default do you want to use? [origin/master]
{quote}


There isn't the same behavior for svn checkout.",ivangorbachev,ivangorbachev,Major,Closed,Fixed,21/Sep/12 18:23,10/Jan/13 19:53
Bug,HIVE-3505,12608937,log4j template has logging threshold that hides all audit logs,"With the ""template"" for log4j configuration provided in the tarball, audit logging is hidden (it's logged as ""INFO""). By making the log threshold a parameter, this information remains hidden when using the CLI (which is desired) but can be overridden when starting services to enable audit-logging.

(This is primarily so that Hive is more functional out-of-the-box as installed by Apache Bigtop).",mackrorysd,mackrorysd,Major,Closed,Fixed,24/Sep/12 22:11,16/Mar/14 03:20
Bug,HIVE-3507,12608961,Some of the tests are not deterministic,skewjoinopt* tests are not deterministic,namit,namit,Major,Closed,Fixed,25/Sep/12 04:21,10/Jan/13 19:54
Bug,HIVE-3515,12609578,metadata_export_drop.q causes failure of other tests,"metadata_export_drop.q causes failure of other tests on cleanup stage.
{quote}
Exception: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:845)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:821)
	at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:445)
	at org.apache.hadoop.hive.ql.QTestUtil.shutdown(QTestUtil.java:300)
	at org.apache.hadoop.hive.cli.TestCliDriver.tearDown(TestCliDriver.java:87)
	at junit.framework.TestCase.runBare(TestCase.java:140)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
	at org.apache.hadoop.fs.Path.initialize(Path.java:140)
	at org.apache.hadoop.fs.Path.<init>(Path.java:132)
	at org.apache.hadoop.fs.ProxyFileSystem.swizzleParamPath(ProxyFileSystem.java:56)
	at org.apache.hadoop.fs.ProxyFileSystem.mkdirs(ProxyFileSystem.java:214)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:183)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1120)
	at org.apache.hadoop.hive.ql.parse.MetaDataExportListener.export_meta_data(MetaDataExportListener.java:81)
	at org.apache.hadoop.hive.ql.parse.MetaDataExportListener.onEvent(MetaDataExportListener.java:106)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1024)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table(HiveMetaStore.java:1185)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:566)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:839)
	... 17 more
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
	at java.net.URI.checkPath(URI.java:1787)
	at java.net.URI.<init>(URI.java:735)
	at org.apache.hadoop.fs.Path.initialize(Path.java:137)
	... 28 more

{quote}",ivangorbachev,ivangorbachev,Major,Closed,Fixed,28/Sep/12 18:49,10/Jan/13 19:53
Bug,HIVE-3518,12609768,QTestUtil side-effects,"It seems that QTestUtil has side-effects. This test ([^metadata_export_drop.q]) causes failure of other tests on cleanup stage:
{quote}
Exception: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:845)
at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:821)
at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:445)
at org.apache.hadoop.hive.ql.QTestUtil.shutdown(QTestUtil.java:300)
at org.apache.hadoop.hive.cli.TestCliDriver.tearDown(TestCliDriver.java:87)
at junit.framework.TestCase.runBare(TestCase.java:140)
at junit.framework.TestResult$1.protect(TestResult.java:110)
at junit.framework.TestResult.runProtected(TestResult.java:128)
at junit.framework.TestResult.run(TestResult.java:113)
at junit.framework.TestCase.run(TestCase.java:124)
at junit.framework.TestSuite.runTest(TestSuite.java:232)
at junit.framework.TestSuite.run(TestSuite.java:227)
at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
at org.apache.hadoop.fs.Path.initialize(Path.java:140)
at org.apache.hadoop.fs.Path.<init>(Path.java:132)
at org.apache.hadoop.fs.ProxyFileSystem.swizzleParamPath(ProxyFileSystem.java:56)
at org.apache.hadoop.fs.ProxyFileSystem.mkdirs(ProxyFileSystem.java:214)
at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:183)
at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1120)
at org.apache.hadoop.hive.ql.parse.MetaDataExportListener.export_meta_data(MetaDataExportListener.java:81)
at org.apache.hadoop.hive.ql.parse.MetaDataExportListener.onEvent(MetaDataExportListener.java:106)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1024)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table(HiveMetaStore.java:1185)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:566)
at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:839)
... 17 more
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:../build/ql/test/data/exports/HIVE-3427/src.2012-09-28-11-38-17
at java.net.URI.checkPath(URI.java:1787)
at java.net.URI.<init>(URI.java:735)
at org.apache.hadoop.fs.Path.initialize(Path.java:137)
... 28 more
{quote}

Flushing 'hive.metastore.pre.event.listeners' into empty string solves the issue. During debugging I figured out this property wan't cleaned for other tests after it was set in metadata_export_drop.q.

How to reproduce:
{code} ant test -Dtestcase=TestCliDriver -Dqfile=metadata_export_drop.q,<some test>.q{code}

where <some test>.q means any test which contains CREATE statement. For example, sample10.q

",navis,ivangorbachev,Major,Closed,Fixed,01/Oct/12 16:40,10/Jan/13 19:53
Bug,HIVE-3519,12609804,partition to directory comparison in CombineHiveInputFormat needs to accept partitions dir without scheme,"TestSymlinkTextInputFormat.testCombine throws following exception. The test case is just printing out the stacktrace when that happens instead of failing.

{code}
java.io.IOException: cannot find dir = file:/Users/thejas/hive-trunk/ql/TestSymlinkTextInputFormat/datadir1/combinefile1_1 in pathToPartitionInfo: [/Users/thejas/hive-trunk/ql/TestSymlinkTextInputFormat/datadir2/combinefile2_1, /Users/thejas/hive-trunk/ql/TestSymlinkTextInputFormat/datadir1/combinefile1_1]
        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:288)
        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:256)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:289)
        at org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.testCombine(TestSymlinkTextInputFormat.java:186)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:168)
        at junit.framework.TestCase.runBare(TestCase.java:134)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:124)
        at junit.framework.TestSuite.runTest(TestSuite.java:232)
        at junit.framework.TestSuite.run(TestSuite.java:227)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)

{code}

",thejas,thejas,Major,Closed,Fixed,01/Oct/12 20:26,10/Jan/13 19:53
Bug,HIVE-3520,12609818,ivysettings.xml does not let you override .m2/repository,ivysettings.xml does not let you override .m2/repository. In other words repo.dir ivysetting should be an overridable property,raja@cmbasics.com,gkesavan,Major,Closed,Fixed,01/Oct/12 21:52,10/Jan/13 19:52
Bug,HIVE-3522,12609948,Make separator for Entity name configurable,Right now its hard-coded to '@',rsm,rsm,Trivial,Closed,Fixed,02/Oct/12 17:57,10/Jan/13 19:53
Bug,HIVE-3523,12610015,Hive info logging is broken,"Hive Info logging is broken on trunk. hive -hiveconf hive.root.logger=INFO,console doesn't print the output of LOG.info statements to the console. ",cwsteinbach,shreepadma,Major,Closed,Fixed,03/Oct/12 00:57,10/Jan/13 19:53
Bug,HIVE-3525,12610149,Avro Maps with Nullable Values fail with NPE,"When working against current trunk@1393794, using a backing Avro schema that has a Map field with nullable values causes a NPE on deserialization when the map contains a null value.",busbey,busbey,Major,Closed,Fixed,03/Oct/12 22:08,10/Jan/13 19:53
Bug,HIVE-3528,12610154,Avro SerDe doesn't handle serializing Nullable types that require access to a Schema,"Deserialization properly handles hiding Nullable Avro types, including complex types like record, map, array, etc. However, when Serialization attempts to write out these types it erroneously makes use of the UNION schema that contains NULL and the other type.

This results in Schema mis-match errors for Record, Array, Enum, Fixed, and Bytes.

Here's a [review board of unit tests that express the problem|https://reviews.apache.org/r/7431/], as well as one that supports the case that it's only when the schema is needed.",busbey,busbey,Major,Closed,Fixed,03/Oct/12 22:52,16/May/13 21:10
Bug,HIVE-3529,12610172,Incorrect partition bucket/sort metadata when overwriting partition with different metadata from table,"If you have a partition with bucket/sort metadata set, then you alter the table to have different bucket/sort metadata, and insert overwrite the partition with hive.enforce.bucketing=true and/or hive.enforce.sorting=true, the partition data will be bucketed/sorted by the table's metadata, but the partition will have the same metadata.

This could result in wrong results.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,04/Oct/12 00:51,10/Jan/13 19:52
Bug,HIVE-3533,12610374,ZooKeeperHiveLockManager does not respect the option to keep locks alive even after the current session has closed,"The HiveLockManager interface defines the following method:

public List<HiveLock> lock(List<HiveLockObj> objs,
      boolean keepAlive) throws LockException;

ZooKeeperHiveLockManager implements HiveLockManager, but the current implementation of the ""lock"" method never actually references the ""keepAlive"" parameter.  As a result, all of the locks acquired by the ""lock"" method are ephemeral.  In other words, Zookeeper-based locks only exist as long as the underlying Zookeeper session exists.  As soon as the Zookeeper session ends, any Zookeeper-based locks are automatically released.",matt.martin,matt.martin,Minor,Closed,Fixed,04/Oct/12 19:17,10/Jan/13 19:53
Bug,HIVE-3535,12610419,derby metastore upgrade script throw errors when updating from 0.7 to 0.8,"$DERBY_HOME/bin/ij upgrade.sql
ij version 10.4
ij> CONNECT 'jdbc:derby:/var/lib/hive/metastore/metastore_db';
ij> RUN '/usr/lib/hive/scripts/metastore/upgrade/derby/upgrade-0.7.0-to-0.8.0.derby.sql';
ij> -- Upgrade MetaStore schema from 0.7.0 to 0.8.0
RUN '008-HIVE-2246.derby.sql';
ij> /*
 * Creates the following tables:
 *  - CDS
 *  - COLUMNS_V2
 * The new columns table is called COLUMNS_V2
 * because many columns are removed, and the schema is changed.
 * It'd take too long to migrate and keep the same table.
 */
CREATE TABLE ""CDS"" (
  ""CD_ID"" bigint NOT NULL,
  PRIMARY KEY (""CD_ID"")
);
0 rows inserted/updated/deleted
ij> CREATE TABLE ""COLUMNS_V2"" (
  ""CD_ID"" bigint NOT NULL,
  ""COMMENT"" varchar(4000),
  ""COLUMN_NAME"" varchar(128) NOT NULL,
  ""TYPE_NAME"" varchar(4000),
  ""INTEGER_IDX"" INTEGER NOT NULL,
  PRIMARY KEY (""CD_ID"", ""COLUMN_NAME"")
);
0 rows inserted/updated/deleted
ij> ALTER TABLE ""COLUMNS_V2"" 
  ADD CONSTRAINT ""COLUMNS_V2_FK1""
  FOREIGN KEY (""CD_ID"") REFERENCES ""CDS"" (""CD_ID"")
  ON DELETE NO ACTION ON UPDATE NO ACTION
;
0 rows inserted/updated/deleted
ij> /* Alter the SDS table to:
 *  - add the column CD_ID
 *  - add a foreign key on CD_ID
 *  - create an index on CD_ID
 */ 
ALTER TABLE SDS
  ADD COLUMN ""CD_ID"" bigint
;
0 rows inserted/updated/deleted
ij> ALTER TABLE SDS
  ADD CONSTRAINT ""SDS_FK2""
  FOREIGN KEY (""CD_ID"") REFERENCES ""CDS"" (""CD_ID"")
;
0 rows inserted/updated/deleted
ij> /*
 * Migrate the TBLS table
 * Add entries into CDS.
 * Populate the CD_ID field in SDS for tables
 * Add entires to COLUMNS_V2 based on this table's sd's columns
 */ 

/* In the migration, there is a 1:1 mapping between CD_ID and SD_ID
 * for tables. For speed, just let CD_ID = SD_ID for tables 
 */
INSERT INTO CDS (CD_ID)
SELECT t.SD_ID FROM TBLS t WHERE t.SD_ID IS NOT NULL ORDER BY t.SD_ID;
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 13, column 54.
ij> UPDATE SDS
  SET CD_ID = SD_ID
WHERE SD_ID in 
(SELECT t.SD_ID FROM TBLS t WHERE t.SD_ID IS NOT NULL ORDER BY t.SD_ID);
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 4, column 55.
ij> INSERT INTO COLUMNS_V2
  (CD_ID, COMMENT, COLUMN_NAME, TYPE_NAME, INTEGER_IDX)
SELECT 
  c.SD_ID, c.COMMENT, c.COLUMN_NAME, c.TYPE_NAME, c.INTEGER_IDX
FROM
  COLUMNS c
JOIN
  TBLS t
ON
  t.SD_ID = c.SD_ID
;
ERROR 23503: INSERT on table 'COLUMNS_V2' caused a violation of foreign key constraint 'COLUMNS_V2_FK1' for key (1).  The statement has been rolled back.
ij> /*
 * Migrate the partitions.
 * Update the partitions' SDS to use the parent tables' CD_ID  BEGIN
 * Derby does not allow joins in update statements, 
 * so we have to make a temporary tableh
 */
DECLARE GLOBAL TEMPORARY TABLE ""TMP_TBL"" (
  ""SD_ID"" bigint not null,
  ""CD_ID"" bigint not null
) ON COMMIT PRESERVE ROWS NOT LOGGED;
0 rows inserted/updated/deleted
ij> INSERT INTO ""SESSION"".""TMP_TBL"" SELECT
  p.SD_ID, sds.CD_ID
  FROM PARTITIONS p
  JOIN TBLS t ON t.TBL_ID = p.TBL_ID
  JOIN SDS sds on t.SD_ID = sds.SD_ID
  WHERE p.SD_ID IS NOT NULL;
ERROR 23502: Column 'CD_ID'  cannot accept a NULL value.
ij> UPDATE SDS sd
  SET sd.CD_ID = 
    (SELECT tt.CD_ID FROM SESSION.TMP_TBL tt WHERE tt.SD_ID = sd.SD_ID)
  WHERE sd.SD_ID IN (SELECT SD_ID FROM SESSION.TMP_TBL);
0 rows inserted/updated/deleted
WARNING 02000: No row was found for FETCH, UPDATE or DELETE; or the result of a query is an empty table.
ij> /*
 * Migrate IDXS
 */
INSERT INTO CDS (CD_ID)
SELECT i.SD_ID FROM IDXS i WHERE i.SD_ID IS NOT NULL ORDER BY i.SD_ID;
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 5, column 54.
ij> UPDATE SDS
  SET CD_ID = SD_ID
WHERE SD_ID in 
(SELECT i.SD_ID FROM IDXS i WHERE i.SD_ID IS NOT NULL ORDER BY i.SD_ID);
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 4, column 55.
ij> INSERT INTO COLUMNS_V2
  (CD_ID, COMMENT, COLUMN_NAME, TYPE_NAME, INTEGER_IDX)
SELECT 
  c.SD_ID, c.COMMENT, c.COLUMN_NAME, c.TYPE_NAME, c.INTEGER_IDX
FROM
  COLUMNS c
JOIN
  IDXS i
ON
  i.SD_ID = c.SD_ID
;
ERROR 42X05: Table/View 'IDXS' does not exist.
ij> /*
 * rename the old COLUMNS table
 */
RENAME TABLE COLUMNS TO COLUMNS_OLD;
0 rows inserted/updated/deleted
ij> RUN '009-HIVE-2215.derby.sql';
ij> -- Table PARTITION_EVENTS for classes [org.apache.hadoop.hive.metastore.model.MPartitionEvent]
CREATE TABLE PARTITION_EVENTS
(
    PART_NAME_ID BIGINT NOT NULL,
    DB_NAME VARCHAR(128),
    EVENT_TIME BIGINT NOT NULL,
    EVENT_TYPE INTEGER NOT NULL,
    PARTITION_NAME VARCHAR(767),
    TBL_NAME VARCHAR(128)
);
0 rows inserted/updated/deleted
ij> ALTER TABLE PARTITION_EVENTS ADD CONSTRAINT PARTITION_EVENTS_PK PRIMARY KEY (PART_NAME_ID);
0 rows inserted/updated/deleted


7 Errors in the middle:
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 13, column 54.
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 4, column 55.
ERROR 23503: INSERT on table 'COLUMNS_V2' caused a violation of foreign key constraint 'COLUMNS_V2_FK1' for key (1).  The statement has been rolled back.
ERROR 23502: Column 'CD_ID'  cannot accept a NULL value.
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 5, column 54.
ERROR 42X01: Syntax error: Encountered ""ORDER"" at line 4, column 55.
ERROR 42X05: Table/View 'IDXS' does not exist.



",zhenxiao,zhenxiao,Major,Closed,Fixed,04/Oct/12 22:08,10/Jan/13 19:53
Bug,HIVE-3536,12610441,Output of sort merge join is no longer bucketed,"I don't know if this was a feature or a happy coincidence, but before HIVE-3230, the output of a sort merge join on two partitions would be bucketed, even if hive.enforce.bucketing was set to false.  This could potentially save a reduce phase when inserting into a bucketed table.

This would be good to have back.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,05/Oct/12 01:58,10/Jan/13 19:54
Bug,HIVE-3537,12610449,release locks at the end of move tasks,"Look at HIVE-3106 for details.

In order to make sure that concurrency is not an issue for multi-table 
inserts, the current option is to introduce a dependency task, which thereby
delays the creation of all partitions. It would be desirable to release the
locks for the outputs as soon as the move task is completed. That way, for
multi-table inserts, the concurrency can be enabled without delaying any 
table.

Currently, the movetask contains a input/output, but they do not seem to be
populated correctly.
",namit,namit,Major,Closed,Fixed,05/Oct/12 05:35,16/May/13 21:10
Bug,HIVE-3544,12610746,union involving double column with a map join subquery will fail or give wrong results,"The following query fails:
select * from (select cast(a.key as bigint) as key from src a join src b on a.key = b.key union all select cast(key as double) as key from src)a

The following query gives wrong results:
select * from (select cast(a.key as bigint) as key, cast(b.key as double) as value from src a join src b on a.key = b.key union all select cast(key as double) as key, cast(key as string) as value from src)a

But the following query runs fine:
select * from (select cast(a.key as bigint) as key from src a union all select cast(key as double) as key from src)a",kevinwilfong,kevinwilfong,Major,Closed,Fixed,07/Oct/12 08:23,02/May/13 02:29
Bug,HIVE-3556,12610901,"Test ""Path -> Alias"" for explain extended","Test framework masks output of ""Path -> Alias"" for explain extended. This makes it impossible to verify the output is right. 

Design is to add a new entry ""Truncated Path -> Alias"" to MapredWork. It has the same content as ""Path -> Alias"" except the prefix including file schema and temp dir is removed. The following config will be used for prefix-removal:
METASTOREWAREHOUSE(""hive.metastore.warehouse.dir"", ""/user/hive/warehouse""),

This will keep ""Path -> Alias"" intact and also test it's result is right.

The first use case is to verify list bucketing query's result is right.

",gangtimliu,gangtimliu,Major,Closed,Fixed,08/Oct/12 22:51,02/May/13 02:30
Bug,HIVE-3560,12611061,Hive always prints a warning message when using remote metastore,"This issue was discovered in HIVE-2585 and more details about why this issue was filed are available there.

Currently if one sets {{hive.metastore.uris}} the following error will always be displayed:

{code}
2012-07-24 15:23:58,647 [main] WARN org.apache.hadoop.hive.conf.HiveConf - DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore.
{code}

The reason is {{javax.jdo.option.ConnectionURL}} has a default value and will never be null. I set this property in {{hive-site.xml}} and walked through the configuration loading in a debugger. If the value is not empty it takes effect, and is ignored if empty.

Since {{javax.jdo.option.ConnectionURL}} has a default and cannot be unset, this warning will always be printed if someone sets {{hive.metastore.uris}}.

Per the review comments, the error message was added to reduce user confusion, and prevent surprises by using the wrong metastore (either embedded or remote). In {{HiveMetaStoreClient.java}} we see a very clear info message printed saying that a remote metastore is used.

{code}
LOG.info(""Trying to connect to metastore with URI "" + store);
...
LOG.info(""Connected to metastore."");
{code}

Since we clearly communicate to the user that a remote metastore at the given URI is being used we'll remove that message. Additionally, to further clarify a remote metastore is used I'll make the following HiveMetaStoreClient logging change:

{code}
LOG.debug(""Trying to connect to remote HiveMetaStore: "" + store);
...
LOG.info(""Connected to remote HiveMetaStore: "" + store);
{code}

The change is at debug level we print connection attempts, and always print which remote HiveMetaStore we actually connected to.",traviscrawford,traviscrawford,Major,Closed,Fixed,09/Oct/12 20:39,10/Jan/13 19:53
Bug,HIVE-3562,12611120,Some limit can be pushed down to map stage,"Queries with limit clause (with reasonable number), for example
{noformat}
select * from src order by key limit 10;
{noformat}

makes operator tree, 
TS-SEL-RS-EXT-LIMIT-FS

But LIMIT can be partially calculated in RS, reducing size of shuffling.
TS-SEL-RS(TOP-N)-EXT-LIMIT-FS
",navis,navis,Trivial,Closed,Fixed,10/Oct/12 05:12,14/Sep/21 01:37
Bug,HIVE-3563,12611241,Drop database cascade fails when there are indexes on any tables,"Drop database cascade fails if any of the table has index. 

create database db2;
use db2;

create table tab1 (id int, name string);
create index idx1 on table tab1(id) as  'COMPACT' with DEFERRED REBUILD;

drop database db2 cascade;",prasadm,prasadm,Major,Closed,Fixed,10/Oct/12 20:48,28/Oct/13 03:53
Bug,HIVE-3581,12611924,get_json_object and json_tuple return null in the presence of new line characters,This was introduced when these functions were updated to use Jackson.,kevinwilfong,kevinwilfong,Major,Closed,Fixed,15/Oct/12 22:32,10/Jan/13 19:54
Bug,HIVE-3582,12611926,NPE in union processing followed by lateral view followed by 2 group bys,"EXPLAIN 
SELECT e.key, e.arr_ele, count(1) FROM (
  SELECT d.key as key, d.arr_ele as arr_ele, d.value  as value, count(1) as cnt FROM (
    SELECT c.arr_ele as arr_ele, a.key as key, a.value as value FROM (
      SELECT key, value, array(1,2,3) as arr
      FROM src

      UNION ALL
   
      SELECT key, value, array(1,2,3) as arr
      FROM srcpart
      WHERE ds = '2008-04-08' and hr='12'
    ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele
  ) d group by d.key, d.arr_ele, d.value
) e group by e.key, e.arr_ele;",navis,namit,Major,Closed,Fixed,15/Oct/12 22:46,16/May/13 21:11
Bug,HIVE-3588,12612131,Get Hive to work with hbase 94,,vikram.dixit,vikram.dixit,Major,Closed,Fixed,17/Oct/12 00:02,15/Oct/13 23:30
Bug,HIVE-3594,12612353,"When Group by Partition Column Type is Timestamp or STRING Which Format contains ""HH:MM:SS"", It will occur URISyntaxException","create table test (no int, name string) partitioned by (pts string) row format delimited fields terminated by ' '; 
load data local inpath '/opt/files/groupbyts1.txt' into table test partition(pts='12:11:30');
load data local inpath '/opt/files/groupbyts2.txt' into table test partition(pts='21:25:12');
load data local inpath '/opt/files/groupbyts3.txt' into table test partition(pts='12:11:30');
load data local inpath '/opt/files/groupbyts4.txt' into table test partition(pts='21:25:12');

when I execute “select * from test group by pts;”, it will occur as follows exception.
 at org.apache.hadoop.fs.Path.initialize(Path.java:157)
        at org.apache.hadoop.fs.Path.<init>(Path.java:135)
        at org.apache.hadoop.hive.ql.exec.Utilities.getInputSummary(Utilities.java:1667)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.estimateNumberOfReducers(MapRedTask.java:432)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.setNumberOfReducers(MapRedTask.java:400)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:93)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:135)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1329)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1121)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:954)
        at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:198)
        at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:630)
        at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:618)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:176)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: fake-path-metadata-only-query-default.test{pts=12:11:30%7D
        at java.net.URI.checkPath(URI.java:1788)
        at java.net.URI.<init>(URI.java:734)
        at org.apache.hadoop.fs.Path.initialize(Path.java:154)
        ... 19 more
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask

When PhysicalOptimizer optimizes GroupByOperator, according to default parameters ""hive.optimize.metadataonly = true"", MetadataOnlyOptimizer will be enabled. The MetadataOnlyOptimizer will change the partition alias desc. The partition alies ""hdfs://ip:9000/user/hive/warehouse/test/pts=12%3A11%3A30"" is changed into "" 
fake-path-metadata-only-query-default.test{pts=12:11:30}"". When construct uri through new partition alies, it must occur java.net.URISyntaxException. 
 ",navis,daisy_yu,Major,Closed,Fixed,18/Oct/12 01:40,16/May/13 21:10
Bug,HIVE-3598,12612477,physical optimizer changes for auto sort-merge join,,namit,namit,Major,Closed,Fixed,18/Oct/12 19:16,24/Mar/14 20:52
Bug,HIVE-3599,12612509,missing return of compression codec to pool,The RCFile writer is currently missing a call to return of one of the compression codecs to the pool.,omalley,omalley,Major,Resolved,Fixed,18/Oct/12 23:08,09/Jan/13 10:24
Bug,HIVE-3605,12613048,HowToContribute page links to defunct hadoop wiki page,"The debugging section, under making changes links to the an old hadoop wiki page for debugging Hive. It mostly redirects to the current wiki okay, but the section anchor has changed, so it just goes to the top of the page instead of the specific section.

Currently it's a full url, if it stays that way it ought to be 
{noformat}
[Debugging Hive code|https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-DebuggingHivecode]
{noformat}

It would be better to make it an internal wiki link with 

{noformat}
[Debugging Hive code|DeveloperGuide#Debugging Hive code]
{noformat}",yhuai,busbey,Trivial,Resolved,Fixed,22/Oct/12 21:06,21/Aug/13 02:43
Bug,HIVE-3613,12613381,Implement grouping_id function,,ivangorbachev,ivangorbachev,Major,Closed,Fixed,24/Oct/12 22:16,02/May/13 02:29
Bug,HIVE-3614,12613412,TestParse_Union is failing on trunk,Test is generating output different then expected and thus failing.,,ashutoshc,Major,Closed,Fixed,25/Oct/12 00:34,10/Jan/13 19:53
Bug,HIVE-3617,12613445,Predicates pushed down to hbase is not handled properly when constant part is shown first,"Test result could not show the difference because predicates pushed down are not removed currently(HIVE-2897). So I added log message(scan.toMap()) and checked the output.

with query
select * from hbase_ppd_keyrange where key > 8 and key < 21;
timeRange=[0, 9223372036854775807], batch=-1, startRow=\x00\x00\x00\x08\x00, stopRow=\x00\x00\x00\x15, ...

but with query
select * from hbase_ppd_keyrange where 8 < key and key < 21;
timeRange=[0, 9223372036854775807], batch=-1, startRow=, stopRow=\x00\x00\x00\x15, ...",navis,navis,Minor,Resolved,Fixed,25/Oct/12 07:45,22/Jan/14 04:23
Bug,HIVE-3622,12613596,"reflect udf cannot find method which has arguments of primitive types and String, Binary, Timestamp types mixed","From http://mail-archives.apache.org/mod_mbox/hive-user/201210.mbox/%3CCANkN6JApahvYrVuiy-j4VJ0dO2tzTpePwi7LUNCp12Vwj6d6xw%40mail.gmail.com%3E
<noformat>
Query
select reflect('java.lang.Integer', 'parseInt', 'a', 16) from src limit 1;

throws java.lang.NoSuchMethodException: java.lang.Integer.parseInt(null, int)
<noformat>",navis,navis,Minor,Closed,Fixed,26/Oct/12 01:02,16/May/13 21:10
Bug,HIVE-3627,12613738,eclipse misses library:  javolution-@javolution-version@.jar,"After checking out the latest trunk, build eclipse project:

ant clean package eclipse-files

Then, import project to eclipse, you will see the error:

Description	Resource	Path	Location	Type
Project 'hive-trunk-10-26' is missing required library: 'build/ivy/lib/default/javolution-@javolution-version@.jar'	hive-trunk-10-26		Build path	Build Path Problem",gangtimliu,gangtimliu,Minor,Closed,Fixed,26/Oct/12 23:51,10/Jan/13 19:53
Bug,HIVE-3631,12613842,script_pipe.q fails when using JDK7,"Hive Runtime Error while closing operators: Hit error while closing ..

The MR job fails on this test. Unfortunately, the exception is not all that helpful.

I tracked this down to a class which attempts to close a stream that is already closed. Broken pipe exceptions are caught and not propagated further, but stream closed exception are not caught.",cdrome,cdrome,Major,Closed,Fixed,29/Oct/12 05:24,24/Jul/13 17:04
Bug,HIVE-3632,12613843,Upgrade datanucleus to support JDK7,"I found serious problems with datanucleus code when using JDK7, resulting in some sort of exception being thrown when datanucleus code is entered.

I tried source=1.7, target=1.7 with JDK7 as well as source=1.6, target=1.6 with JDK7 and there was no visible difference in that the same unit tests failed.

I tried upgrading datanucleus to 3.0.1, as per HIVE-2084.patch, which did not fix the failing tests.

I tried upgrading datanucleus to 3.1-release, as per the advise of http://www.datanucleus.org/servlet/jira/browse/NUCENHANCER-86, which suggests using ASMv4 will allow datanucleus to work with JDK7. I was not successful with this either.

I tried upgrading datanucleus to 3.1.2. I was not successful with this either.

Regarding datanucleus support for JDK7+, there is the following JIRA

http://www.datanucleus.org/servlet/jira/browse/NUCENHANCER-81

which suggests that they don't plan to actively support JDK7+ bytecode any time soon.

I also tested the following JVM parameters found on

http://veerasundar.com/blog/2012/01/java-lang-verifyerror-expecting-a-stackmap-frame-at-branch-target-jdk-7/

with no success either.

This will become a more serious problem as people move to newer JVMs. If there are other who have solved this issue, please post how this was done. Otherwise, it is a topic that I would like to raise for discussion.

Test Properties:
CLEAR LIBRARY CACHE",xuefuz,cdrome,Critical,Closed,Fixed,29/Oct/12 05:41,15/Oct/13 23:29
Bug,HIVE-3640,12614001,Reducer allocation is incorrect if enforce bucketing and mapred.reduce.tasks are both set,"When I enforce bucketing and fix the number of reducers via mapred.reduce.tasks Hive ignores my input and instead takes the largest value <= hive.exec.reducers.max that is also an even divisor of num_buckets. In other words, if I set 1024 buckets and set mapred.reduce.tasks=1024 I'll get. . . 256 reducers. If I set 1997 buckets and set mapred.reduce.tasks=1997 I'll get. . . 1 reducer. 

This is totally crazy, and it's far, far crazier when the data inputs get large. In the latter case the bucketing job will almost certainly fail because we'll most likely try to stuff several TB of input through a single reducer. We'll also drastically reduce the effectiveness of bucketing, since the buckets themselves will be larger.

If the user sets mapred.reduce.tasks in a query that inserts into a bucketed table we should either accept that value or raise an exception if it's invalid relative to the number of buckets. We should absolutely NOT override the user's direction and fall back on automatically allocating reducers based on some obscure logic dictated by completely different setting. 

I have yet to encounter a single person who expected this the first time, so it's clearly a bug.",vighnesh.avadhani,vighnesh.avadhani,Minor,Closed,Fixed,30/Oct/12 08:03,10/Jan/13 19:52
Bug,HIVE-3645,12614252,RCFileWriter does not implement the right function to support Federation,"Create a table using Hive DDL
{code}
CREATE TABLE tmp_hcat_federated_numbers_part_1 (
  id       int,  
  intnum   int,
  floatnum float
)partitioned by (
  part1    string,
  part2    string
)
STORED AS rcfile
LOCATION 'viewfs:///database/tmp_hcat_federated_numbers_part_1';
{code}

Populate it using Pig:
{code}
A = load 'default.numbers_pig' using org.apache.hcatalog.pig.HCatLoader();
B = filter A by id <=  500;
C = foreach B generate (int)id, (int)intnum, (float)floatnum;
store C into
        'default.tmp_hcat_federated_numbers_part_1'
        using org.apache.hcatalog.pig.HCatStorer
       ('part1=pig, part2=hcat_pig_insert',
        'id: int,intnum: int,floatnum: float');
{code}

Generates the following error when running on a Federated Cluster:
{quote}
2012-10-29 20:40:25,011 [main] ERROR
org.apache.pig.tools.pigstats.SimplePigStats - ERROR 2997: Unable to recreate
exception from backed error: AttemptID:attempt_1348522594824_0846_m_000000_3
Info:Error: org.apache.hadoop.fs.viewfs.NotInMountpointException:
getDefaultReplication on empty path is invalid
        at
org.apache.hadoop.fs.viewfs.ViewFileSystem.getDefaultReplication(ViewFileSystem.java:479)
        at org.apache.hadoop.hive.ql.io.RCFile$Writer.<init>(RCFile.java:723)
        at org.apache.hadoop.hive.ql.io.RCFile$Writer.<init>(RCFile.java:705)
        at
org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getRecordWriter(RCFileOutputFormat.java:86)
        at
org.apache.hcatalog.mapreduce.FileOutputFormatContainer.getRecordWriter(FileOutputFormatContainer.java:100)
        at
org.apache.hcatalog.mapreduce.HCatOutputFormat.getRecordWriter(HCatOutputFormat.java:228)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getRecordWriter(PigOutputFormat.java:84)
        at
org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:587)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:706)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:157)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)
{quote}",amalakar,viraj,Major,Closed,Fixed,31/Oct/12 18:07,16/May/13 21:10
Bug,HIVE-3647,12614314,map-side groupby wrongly due to HIVE-3432,"There seems to be a bug due to HIVE-3432.

We are converting the group by to a map side group by after only looking at
sorting columns. This can give wrong results if the data is sorted and
bucketed by different columns.

Add some tests for that scenario, verify and fix any issues.",namit,namit,Major,Closed,Fixed,01/Nov/12 05:03,11/Jan/13 06:34
Bug,HIVE-3648,12614379,HiveMetaStoreFsImpl is not compatible with hadoop viewfs,HiveMetaStoreFsImpl#deleteDir() method calls Trash#moveToTrash(). This may not work when viewfs is used. It needs to call Trash#moveToAppropriateTrash() instead.  Please note that this method is not available in hadoop versions earlier than 0.23.,amalakar,kihwal,Major,Closed,Fixed,01/Nov/12 14:49,16/May/13 21:10
Bug,HIVE-3651,12614451,bucketmapjoin?.q  tests fail with hadoop 0.23,"The hive.log show error in MR job -
Task failed!
Task ID:
  Stage-1

The job log has following error -
2012-11-01 15:51:20,253 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(479)) - job_local_0001
java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)
        at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

",prasadm,prasadm,Major,Closed,Fixed,01/Nov/12 23:31,02/May/13 02:30
Bug,HIVE-3654,12614492,block relative path access in hive,metadata only optimizer uses relative paths,namit,namit,Major,Closed,Fixed,02/Nov/12 10:29,10/Jan/13 19:53
Bug,HIVE-3658,12614533,Unable to generate the Hbase related unit tests using velocity templates on Windows,Requires to escape the “\” on windows to make it compile. So make sure to use the escaped path in the VM templates instead of actual path.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,02/Nov/12 17:02,10/Jan/13 19:53
Bug,HIVE-3659,12614541,TestHiveHistory::testQueryloglocParentDirNotExist Test fails on Windows because of some resource leaks in ZK,"Hive uses ZK for locking. In some test cases, ZK is not behaving well. In thread dumps, I saw it is waiting for locks to be released but they were not getting released. Hive tries to release locks but keeps failing, it eventually times out for its release attempts, which in default settings takes 10 mins. This is also the cause of why some queries take extra-ordinarily long to run. I suggest to disable ZK locking till ZK is certified for windows.

In this test case, I don’t see a requirement to use ZK so I am disabling the HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY to work around the issue.
",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,02/Nov/12 17:48,10/Jan/13 19:53
Bug,HIVE-3661,12614548,Remove the Windows specific “=” related swizzle path changes from Proxy FileSystems,"Because of this special conversion, Some other unit tests are failing on Windows. After some other investigation, We noticed that “=” is a valid character that can be included in the Windows paths. So I am reverting back “=” related changes from the swizzle path.",kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,02/Nov/12 18:25,10/Jan/13 19:53
Bug,HIVE-3662,12614556,TestHiveServer: testScratchDirShouldClearWhileStartup is failing on Windows,Test case is attempting to delete the ScratchDir but it is failing on Windows because one of the subfolders (local scratchdir) in use. So change the location of the local scratch directory.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,02/Nov/12 18:53,10/Jan/13 19:53
Bug,HIVE-3663,12614560,Unable to display the MR Job file path on Windows in case of MR job failures.,Because of this bunch of CLI negative tests are failing on windows.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,02/Nov/12 19:17,10/Jan/13 19:53
Bug,HIVE-3664,12614563,Avoid to create a symlink for hive-contrib.jar file in dist\lib folder.,It forces us to enumerate all the jars except this jar on Windows instead of directly referencing the “dist\lib\*.jar” folder in the class path.,kannakar@microsoft.com,kannakar@microsoft.com,Major,Closed,Fixed,02/Nov/12 19:30,10/Jan/13 19:53
Bug,HIVE-3665,12614618,Allow URIs without port to be specified in metatool,"Metatool should accept input URIs where one URI contains a port and the other doesn't. While metatool today accepts input URIs without the port when both the input URIs (oldLoc and newLoc) don't contain the port, we should make the tool a little more flexible to allow for the case where one URI contains a valid port and the other input URI doesn't. This makes more sense when transitioning to HA and a user chooses to specify the port as part of the oldLoc, but the port doesn't mean much for the newLoc.",shreepadma,shreepadma,Major,Closed,Fixed,02/Nov/12 23:48,16/May/13 21:10
Bug,HIVE-3673,12614868,Sort merge join not used when join columns have different names,"If two tables are joined on columns with different names, the sort merge join optimization is not applied.  E.g.

SELECT /*+ MAPJOIN(b) */ * FROM t1 a JOIN t2 b ON a.key = b.value;

This will not use sort merge join even if t1 and t2 are bucketed and sorted by key, value respectively.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,05/Nov/12 22:59,10/Jan/13 19:53
Bug,HIVE-3674,12614875,test case TestParse broken after recent checkin,"The below test cases fail after running svn up on my clean checkout.
    org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby1
    org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby2
    org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby3
    org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby4
    org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby5
    org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby6
The build on Nov 2 shows this issue as well.
https://builds.apache.org/job/Hive-trunk-h0.21/1770/",sambavi,sambavi,Major,Closed,Fixed,05/Nov/12 23:46,10/Jan/13 19:54
Bug,HIVE-3675,12614902,NaN does not work correctly for round(n),"It works correctly for round(n, d)",namit,namit,Major,Closed,Fixed,06/Nov/12 06:02,14/Mar/13 14:24
Bug,HIVE-3676,12614908,INSERT INTO regression caused by HIVE-3465,,navis,cwsteinbach,Major,Closed,Fixed,06/Nov/12 06:50,10/Jan/13 19:53
Bug,HIVE-3678,12615045,Add metastore upgrade scripts for column stats schema changes,Add upgrade script for column statistics schema changes for Postgres/MySQL/Oracle/Derby,shreepadma,shreepadma,Major,Closed,Fixed,06/Nov/12 23:14,10/Jan/13 19:53
Bug,HIVE-3680,12615051,Include Table information in Hive's AddPartitionEvent.,"This has to do with a minor overhaul of the HCatalog notifications that we're attempting in HCATALOG-546.

It is proposed that HCatalog's notifications (on Add/Drop of Partitions) provide details to identify the affected partitions. 

Using the Partition object in AddPartitionEvent, one is able to retrieve the values of the partition-keys and the name of the Table. However, the partition-keys themselves aren't available (since the Table instance isn't part of the AddPartitionEvent).

Adding the table-reference to the AddPartitionEvent and DropPartitionEvent classes will expose all the info we need. (The alternative is to query the metastore for the table's schema and use the partition-keys from there. :/)

I'll post a patch for this shortly.",mithun,mithun,Major,Closed,Fixed,07/Nov/12 00:30,10/Jan/13 19:53
Bug,HIVE-3681,12615077,Compile errors after HIVE-1362,Compile errors after HIVE-1362,,namit,Major,Closed,Fixed,07/Nov/12 06:34,10/Jan/13 19:53
Bug,HIVE-3686,12615197,Fix compile errors introduced by the interaction of HIVE-1362 and HIVE-3524,HIVE-3524 changed the signature of endFunction in HiveMetastore.java and was committed some hours before HIVE-1362. The change in signature broke the build after HIVE-1362 which still contained the old signature was committed. ,shreepadma,shreepadma,Blocker,Closed,Fixed,07/Nov/12 17:23,10/Jan/13 19:54
Bug,HIVE-3687,12615220,smb_mapjoin_13.q is nondeterministic,smb_mapjoin_13.q is missing an ORDER BY clause it in its queries,kevinwilfong,kevinwilfong,Minor,Closed,Fixed,07/Nov/12 20:03,10/Jan/13 19:53
Bug,HIVE-3691,12615296,TestDynamicSerDe failed with IBM JDK,"the order of the output in the gloden file are different from JDKs.
the root cause of this is the implementation of HashMap in JDK",libing,libing,Minor,Closed,Fixed,08/Nov/12 06:59,15/Oct/13 23:31
Bug,HIVE-3692,12615390,Update parallel test documentation,"https://github.com/apache/hive/blob/trunk/testutils/ptest/README has incorrect json object:
{code}
{
    ""qfile_hosts"": [
        [""hostname1"", 2],
        [""hostname2"", 4],
        [""hostname3"", 4],
    ],
    ""other_hosts"": [
        [""hostname1"", 2],
        [""hostname4"", 5]
    ],
    ""master_base_path"": ""${{HOME}}/hivetests"",
    ""host_base_path"": ""/mnt/drive/hivetests""
    ""java_home"": ""/opt/jdk""
}
{code}
Should update it with right syntax",ivangorbachev,ivangorbachev,Major,Closed,Fixed,08/Nov/12 22:13,10/Jan/13 19:53
Bug,HIVE-3693,12615410,Performance regression introduced by HIVE-3483,"https://issues.apache.org/jira/browse/HIVE-3483 introduced a performance regression in the client side during split computation.

The client side spends a lot more time in the split computation phase. The problem is checkFilterPathContains method.

While investigating, can you create a config to disable it by default?

thanks",thejas,gangtimliu,Minor,Closed,Fixed,09/Nov/12 01:31,10/Jan/13 19:54
Bug,HIVE-3695,12615425,TestParse breaks due to HIVE-3675,,namit,namit,Major,Closed,Fixed,09/Nov/12 05:10,10/Jan/13 19:53
Bug,HIVE-3696,12615429,Revert HIVE-3483 which causes performance regression,"HIVE-3483 causes performance regression.

We'd like to revert it first and find another solution for it later.

This issue is to track revert and HIVE-3693 tracks subsequent solution.",gangtimliu,gangtimliu,Critical,Closed,Fixed,09/Nov/12 06:54,10/Jan/13 19:52
Bug,HIVE-3697,12615496,External JAR files on HDFS can lead to race condition with hive.downloaded.resources.dir,"I've seen situations where utilizing JAR files on HDFS can cause job failures via CNFE or JVM crashes. 

This is difficult to replicate, seems to be related to JAR size, latency between client and HDFS cluster, but I've got some example stack traces below. Seems that the calls made to FileSystem (copyToLocal) which are static and will be executed to delete the current local copy can cause the file(s) to be removed during job processing.

We should consider changing the default for hive.downloaded.resources.dir to include some level of uniqueness per job. We should not consider hive.session.id however, as execution of multiple statements via the same user/session which might access the same JAR files will utilize the same session.

A proposal might be to utilize System.nanoTime() -- which might be enough to avoid the issue, although it's not perfect (depends on JVM and system for level of precision) as part of the default (/tmp/${user.name}/resources/System.nanoTime()/). 

If anyone else has hit this, would like to capture environment information as well. Perhaps there is something else at play here. 

Here are some examples of the errors:

for i in {0..2}; do hive -S -f query.q& done
[2] 48405
[3] 48406
[4] 48407
% #
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0x7) at pc=0x00007fb10bd931f0, pid=48407, tid=140398456698624
#
# JRE version: 6.0_31-b04
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.6-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libzip.so+0xb1f0]  __int128+0x60
#
# An error report file with more information is saved as:
# /home/.../hs_err_pid48407.log
#
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
java.lang.NoClassDefFoundError: com/example/udf/Lower
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.getUdfClass(FunctionTask.java:105)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.createFunction(FunctionTask.java:75)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:63)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1331)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1117)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:950)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:341)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:439)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:449)
        at org.apache.hadoop.hive.cli.CliDriver.processInitFiles(CliDriver.java:485)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:692)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: java.lang.ClassNotFoundException: com.example.udf.Lower
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        ... 24 more
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.FunctionTask

Another:
for i in {0..2}; do hive -S -f query.q& done
[1] 16294 
[2] 16295 
[3] 16296 
[]$ Couldn't create directory /tmp/ctm/resources/
Couldn't create directory /tmp/ctm/resources/",,ctm,Major,Closed,Fixed,09/Nov/12 15:44,16/May/13 21:10
Bug,HIVE-3698,12615576,enhanceModel.notRequired is incorrectly determined,enhanceModel.notRequired in metastore/build.xml is not correctly determined which can result in datanucleus running twice. This in turn can cause other issues when building with JDK7.,cdrome,cdrome,Major,Closed,Fixed,10/Nov/12 01:07,16/May/13 21:09
Bug,HIVE-3699,12615591,Multiple insert overwrite into multiple tables query stores same results in all tables,"(Note: This might be related to HIVE-2750)

I am doing a query with multiple INSERT OVERWRITE to multiple tables in order to scan the dataset only 1 time, and i end up having all these tables with the same content ! It seems the GROUP BY query that returns results is overwriting all the temp tables.

Weird enough, if i had further GROUP BY queries into additional temp tables, grouped by a different field, then all temp tables, even the ones that would have been wrong content are all correctly populated.

This is the misbehaving query:

    FROM nikon
    INSERT OVERWRITE TABLE e1
    SELECT qs_cs_s_aid AS Emplacements, COUNT(*) AS Impressions
    WHERE qs_cs_s_cat='PRINT' GROUP BY qs_cs_s_aid
    INSERT OVERWRITE TABLE e2
    SELECT qs_cs_s_aid AS Emplacements, COUNT(*) AS Vues
    WHERE qs_cs_s_cat='VIEW' GROUP BY qs_cs_s_aid
    ;

It launches only one MR job and here are the results. Why does table 'e1' contains results from table 'e2' ?! Table 'e1' should have been empty (see individual SELECTs further below)

    hive> SELECT * from e1;
    OK
    NULL    2
    1627575 25
    1627576 70
    1690950 22
    1690952 42
    1696705 199
    1696706 66
    1696730 229
    1696759 85
    1696893 218
    Time taken: 0.229 seconds

    hive> SELECT * from e2;
    OK
    NULL    2
    1627575 25
    1627576 70
    1690950 22
    1690952 42
    1696705 199
    1696706 66
    1696730 229
    1696759 85
    1696893 218
    Time taken: 0.11 seconds


Here is are the result to the indiviual queries (only the second query returns a result set):

    hive> SELECT qs_cs_s_aid AS Emplacements, COUNT(*) AS Impressions FROM nikon
    WHERE qs_cs_s_cat='PRINT' GROUP BY qs_cs_s_aid;
    (...)
    OK
          <- There are no results, this is normal
    Time taken: 41.471 seconds

    hive> SELECT qs_cs_s_aid AS Emplacements, COUNT(*) AS Vues FROM nikon
    WHERE qs_cs_s_cat='VIEW' GROUP BY qs_cs_s_aid;
    (...)
    OK
    NULL  2
    1627575 25
    1627576 70
    1690950 22
    1690952 42
    1696705 199
    1696706 66
    1696730 229
    1696759 85
    1696893 218
    Time taken: 39.607 seconds
    ",navis,alexfo,Major,Closed,Fixed,10/Nov/12 09:45,16/May/13 21:10
Bug,HIVE-3702,12615771,Renaming table changes table location scheme/authority,"Renaming a table changes the location of the table to the default location of the database, followed by the table name.  This means that if the default location of the database uses a different scheme/authority, an exception will get thrown attempting to move the data.

Instead, the table's location should be made the default location of the database followed by the table name, but using the original location's scheme and authority.

This only applies for managed tables, and there is already a check to ensure the new location doesn't already exist.

This is analogous to what was done for partitions in HIVE-2875",kevinwilfong,kevinwilfong,Major,Closed,Fixed,12/Nov/12 18:52,16/May/13 21:10
Bug,HIVE-3703,12615780,Hive Query Explain Plan JSON not being created properly,"There is an option to generate a JSON query plan for the hive query, however, the JSON being created is invalid and json_decoders are unable to decode it",mayankgarg,mayankgarg,Minor,Closed,Fixed,12/Nov/12 19:47,16/May/13 21:11
Bug,HIVE-3704,12615819,name of some metastore scripts are not per convention,,ashutoshc,ashutoshc,Major,Closed,Fixed,12/Nov/12 22:58,10/Jan/13 19:53
Bug,HIVE-3712,12616291,Use varbinary instead of longvarbinary to store min and max column values in column stats schema,JDBC type longvarbinary maps to BLOB SQL type in some databases. Storing min and max column values for numeric types takes up 8 bytes and hence doesn't require a BLOB. Storing these values in a BLOB will impact performance without providing much benefits. ,shreepadma,shreepadma,Major,Closed,Fixed,15/Nov/12 19:31,10/Jan/13 19:53
Bug,HIVE-3713,12616325,Metastore: Sporadic unit test failures,"For instance: https://builds.apache.org/job/Hive-trunk-h0.21/1792/testReport/org.apache.hadoop.hive.metastore/

Found the following issues:

testListener: Assumes that a certain tmp database hasn't been created yet, but doesn't enforce it

testSynchronized: Assumes that there's only one database, but doesn't enforce the fact

testDatabaseLocation: Fails if the user running the tests is root and doesn't clean up after itself.",hagleitn,hagleitn,Major,Closed,Fixed,15/Nov/12 22:29,10/Jan/13 19:54
Bug,HIVE-3714,12616364,Patch: Hive's ivy internal resolvers need to use sourceforge for sqlline,"While building hive with an internal resolver, ivy fails to resolve sqlline, which needs to be picked up from

http://sourceforge.net/projects/sqlline/files/sqlline/1.0.2/sqlline-1_0_2.jar/download

ant package -Dresolvers=internal

fails with

{code}
[ivy:resolve] 	==== datanucleus-repo: tried
[ivy:resolve] 	  -- artifact sqlline#sqlline#1.0.2;1_0_2!sqlline.jar:
[ivy:resolve] 	  http://www.datanucleus.org/downloads/maven2/sqlline/sqlline/1_0_2/sqlline-1_0_2.jar
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: sqlline#sqlline#1.0.2;1_0_2: not found
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
{code}


The attached patch adds sourceforge to the internal resolver list so that if the default sqlline version (& a hadoop snapshot) is used, the build does not fail.",gopalv,gopalv,Trivial,Closed,Fixed,16/Nov/12 08:06,16/May/13 21:10
Bug,HIVE-3717,12616469,Hive won't compile with -Dhadoop.mr.rev=20S,"ant -Dhadoop.mr.rev=20S clean package

fails with: 

{noformat}
compile:
     [echo] Project: ql
    [javac] Compiling 744 source files to /root/hive/build/ql/classes
    [javac] /root/hive/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFJson.java:67: cannot find symbol
    [javac] symbol  : variable ALLOW_UNQUOTED_CONTROL_CHARS
    [javac] location: class org.codehaus.jackson.JsonParser.Feature
    [javac]     JSON_FACTORY.enable(Feature.ALLOW_UNQUOTED_CONTROL_CHARS);
    [javac]                                ^
    [javac] /root/hive/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFJson.java:158: cannot find symbol
    [javac] symbol  : method writeValueAsString(java.lang.Object)
    [javac] location: class org.codehaus.jackson.map.ObjectMapper
    [javac]         result.set(MAPPER.writeValueAsString(extractObject));
    [javac]                          ^
    [javac] /root/hive/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFJSONTuple.java:59: cannot find symbol
    [javac] symbol  : variable ALLOW_UNQUOTED_CONTROL_CHARS
    [javac] location: class org.codehaus.jackson.JsonParser.Feature
    [javac]     JSON_FACTORY.enable(Feature.ALLOW_UNQUOTED_CONTROL_CHARS);
    [javac]                                ^
    [javac] /root/hive/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFJSONTuple.java:189: cannot find symbol
    [javac] symbol  : method writeValueAsString(java.lang.Object)
    [javac] location: class org.codehaus.jackson.map.ObjectMapper
    [javac]           retCols[i].set(MAPPER.writeValueAsString(extractObject));
    [javac]                                ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 4 errors
{noformat}

According to https://issues.apache.org/jira/browse/HADOOP-7470 hadoop 1.x has been upgraded to jackson 1.8.8 but the POM file still specifies jackson 1.0.1 which doesn't work for hive (doesn't have the ALLOW_UNQUOTED_CONTROL_CHARS).

The POM for hadoop 2.0.0-alpha (-Dhadoop.mr.rev=23) has the right dependency, hadoop 0.20.2 (-Dhadoop.mr.rev=20) doesn't depend on jackson.",hagleitn,hagleitn,Major,Closed,Fixed,16/Nov/12 20:25,16/May/13 21:10
Bug,HIVE-3722,12616827,Create index fails on CLI using remote metastore,"If the CLI uses a remote metastore and the user attempts to create an index without a comment, it will fail with a NPE.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,19/Nov/12 21:28,10/Jan/13 19:53
Bug,HIVE-3723,12616847,Hive Driver leaks ZooKeeper connections,"In certain error cases (i.e.: statement fails to compile, semantic errors) the hive driver leaks zookeeper connections.

This can be seen in the TestNegativeCliDriver test which accumulates a large number of open file handles and fails if the max allowed number of file handles isn't at least 2048.",hagleitn,hagleitn,Major,Closed,Fixed,19/Nov/12 22:28,19/Nov/13 15:14
Bug,HIVE-3724,12616856,Metastore tests use hardcoded ports,"Several of the metastore tests use hardcoded ports for remote metastore Thrift servers.  This is causing transient failures in Jenkins, e.g. https://builds.apache.org/job/Hive-trunk-h0.21/1804/

A few tests already dynamically determine free ports, and this logic can be shared.",kevinwilfong,kevinwilfong,Minor,Closed,Fixed,19/Nov/12 23:15,10/Jan/13 19:53
Bug,HIVE-3728,12617051,make optimizing multi-group by configurable,"This was done as part of https://issues.apache.org/jira/browse/HIVE-609.
This should be configurable.",namit,namit,Major,Closed,Fixed,21/Nov/12 04:45,06/Feb/15 08:42
Bug,HIVE-3729,12617124,Error in groupSetExpression rule in Hive grammar,"Here is the error:
Hive.g:1902:38: reference to rewrite element groupByExpression without reference on left of ->",rhbutani,rhbutani,Minor,Closed,Fixed,21/Nov/12 16:03,06/May/13 18:06
Bug,HIVE-3732,12617227,Multiple aggregates in query fail the job,"To reproduce:

Create the hive table:
{code}CREATE EXTERNAL TABLE Birthdays (id int, name string, birthday timestamp) ROW FORMAT DELIMITED FIELDS TERMINATED BY '	';{code}

Insert the data (from file):
{quote}
1	A	2012-10-15 08:01:00
2	B	2012-10-15 08:02:00
3	C	2012-10-15 08:03:00
4	D	2012-10-15 08:04:00
5	E	2012-10-15 08:05:00
6	F	2012-10-15 08:06:00
7	G	2012-10-15 08:07:00
8	H	2012-10-15 08:08:00
9	I	2012-10-15 08:09:00
{quote}

The first 3 queries work, the last query errors out:
{code}
SELECT * FROM Birthdays;
SELECT MIN(Birthday) AS MinBirthday FROM Birthdays;
SELECT MAX(Birthday) AS MaxBirthday FROM Birthdays;
SELECT MIN(Birthday) AS MinBirthday, MAX(Birthday) AS MaxBirthday FROM Birthdays;
{code}
",,jeromatron,Major,Closed,Fixed,21/Nov/12 22:27,10/Jan/13 19:54
Bug,HIVE-3735,12617257,PTest doesn't work due to hive snapshot version upgrade to 11,"PTest fails. Error

::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] :: UNRESOLVED DEPENDENCIES ::
[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] :: org.apache.hive#hive-builtins;0.11.0-SNAPSHOT: not found
[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::
",gangtimliu,gangtimliu,Critical,Closed,Fixed,22/Nov/12 02:13,06/May/13 18:06
Bug,HIVE-3736,12617258,hive unit test case build failure.,"Hive depend on hbase-test jar. 
since classifier is not defined in ivysettings.xml, hence hive is downloading hbase-<version>.jar as hbase-test-<version>.jar.
This causes the junit test failures.",ashishujjain,ashishujjain,Major,Closed,Fixed,22/Nov/12 02:23,10/Jan/13 19:53
Bug,HIVE-3741,12617387,Driver.validateConfVariables() should perform more validations,"Like List Bucketing, it should also check for HIVE_OPTIMIZE_UNION_REMOVE.
",gangtimliu,namit,Major,Closed,Fixed,23/Nov/12 04:16,16/May/13 21:10
Bug,HIVE-3742,12617469,The derby metastore schema script for 0.10.0 doesn't run,The hive-schema-0.10.0.derby.sql contains incorrect alter statement for SKEWED_STRING_LIST which causes the script execution to fail,prasadm,prasadm,Major,Closed,Fixed,23/Nov/12 19:29,02/May/13 02:30
Bug,HIVE-3747,12617722,Provide hive operation name for hookContext,"The hookContext exposed through ExecuteWithHookContext, does not provide the name of the Hive operation. 

The following public API should be added in HookContext.
public String getOperationName() {
    return SessionState.get().getHiveOperation().name();
}",shreepadma,sudhanshu,Major,Closed,Fixed,27/Nov/12 00:18,16/May/13 21:10
Bug,HIVE-3750,12617905,JDBCStatsPublisher fails when ID length exceeds length of ID column,"When the length of the ID field passed to JDBCStatsPublisher exceeds the length of the column in the table (currently 255 characters) stats collection fails.  This causes the entire query to fail when hive.stats.reliable is set to true.

One way to prevent this would be to calculate a deterministic, very low collision hash of the ID prefix used for aggregation and use that when the length of the ID is too long.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,28/Nov/12 00:15,23/Mar/14 04:36
Bug,HIVE-3754,12618089,Trunk hadoop 23 build fails,"check out the latest code from trunk
{code}
svn info 
{code}
{quote}
Path: .
URL: http://svn.apache.org/repos/asf/hive/trunk
Repository Root: http://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 1415005
Node Kind: directory
Schedule: normal
Last Changed Author: namit
Last Changed Rev: 1414608
Last Changed Date: 2012-11-28 01:36:27 -0800 (Wed, 28 Nov 2012)
{quote}
{code}
ant clean package -Dhadoop.version=0.23.1 -Dhadoop-0.23.version=0.23.1 -Dhadoop.mr.rev=23
{code}

{quote}
ivy-retrieve-hadoop-shim:
     [echo] Project: shims
    [javac] Compiling 2 source files to /Users/gang/hive-trunk-11-28/build/shims/classes
    [javac] /Users/gang/hive-trunk-11-28/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java:122: getDefaultBlockSize() in org.apache.hadoop.fs.FileSystem cannot be applied to (org.apache.hadoop.fs.Path)
    [javac]     return fs.getDefaultBlockSize(path);
    [javac]              ^
    [javac] /Users/gang/hive-trunk-11-28/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java:127: getDefaultReplication() in org.apache.hadoop.fs.FileSystem cannot be applied to (org.apache.hadoop.fs.Path)
    [javac]     return fs.getDefaultReplication(path);
    [javac]              ^
    [javac] 2 errors

BUILD FAILED
/Users/gang/hive-trunk-11-28/build.xml:302: The following error occurred while executing this line:
{quote}",gangtimliu,gangtimliu,Major,Resolved,Fixed,29/Nov/12 00:16,29/Nov/12 19:06
Bug,HIVE-3755,12618190,can't assign jira issue,"I can't assign jira issue even if the jira issue is created by myself.

For example, I created Hive-3754 but ""Workflow"" button is greyed out so that I can't assign it to myself.

I used to be able to do so but recently not.

Hope someone can help to check my jira previlidge or point out to me what I can do.

thanks a lot",gangtimliu,gangtimliu,Minor,Resolved,Fixed,29/Nov/12 18:11,20/Oct/21 07:15
Bug,HIVE-3756,12618191,"""LOAD DATA"" does not honor permission inheritence","When a ""LOAD DATA"" operation is performed the resulting data in hdfs for the table does not maintain permission inheritance. This remains true even with the ""hive.warehouse.subdir.inherit.perms"" set to true.

The issue is easily reproducible by creating a table and loading some data into it. After the load is complete just do a ""dfs -ls -R"" on the warehouse directory and you will see that the inheritance of permissions worked for the table directory but not for the data. ",ctang,johndee,Major,Closed,Fixed,29/Nov/12 18:11,11/Apr/14 00:13
Bug,HIVE-3757,12618203,union_remove_9.q fails in trunk (hadoop 23),"check out the latest code from trunk
{code}
svn info
{code}

{quote}
Path: .
URL: http://svn.apache.org/repos/asf/hive/trunk
Repository Root: http://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 1415321
Node Kind: directory
Schedule: normal
Last Changed Author: hashutosh
Last Changed Rev: 1415278
Last Changed Date: 2012-11-29 09:11:53 -0800 (Thu, 29 Nov 2012)
{quote}

{code}
ant -Dhadoop.version=0.23.3 -Dhadoop-0.23.version=0.23.3 -Dhadoop.mr.rev=23 test -Dtestcase=TestCliDriver -Dqfile=union_remove_9.q
{code}

{quote}
[junit] diff -a /Users/gang/hive-trunk-11-29/build/ql/test/logs/clientpositive/union_remove_9.q.out /Users/gang/hive-trunk-11-29/ql/src/test/results/clientpositive/union_remove_9.q.out
    [junit] 106c106
    [junit] <                           expr: UDFToLong(_col1)
    [junit] ---
    [junit] >                           expr: _col1
    [junit] 109,123c109,116
    [junit] <                     Select Operator
    [junit] <                       expressions:
    [junit] <                             expr: _col0
    [junit] <                             type: string
    [junit] <                             expr: _col1
    [junit] <                             type: bigint
    [junit] <                       outputColumnNames: _col0, _col1
    [junit] <                       File Output Operator
    [junit] <                         compressed: false
    [junit] <                         GlobalTableId: 1
    [junit] <                         table:
    [junit] <                             input format: 
    [junit] <                             output format: 
    [junit] <                             serde: 
    [junit] <                             name: default.outputtbl1
    [junit] ---
    [junit] >                     File Output Operator
    [junit] >                       compressed: false
    [junit] >                       GlobalTableId: 1
    [junit] >                       table:
    [junit] >                           input format: 
    [junit] >                           output format: 
    [junit] >                           serde: 
    [junit] >                           name: default.outputtbl1
    [junit] <                           expr: UDFToLong(_col1)
    [junit] ---
    [junit] >                           expr: _col1
    [junit] 149,163c142,149
    [junit] <                     Select Operator
    [junit] <                       expressions:
    [junit] <                             expr: _col0
    [junit] <                             type: string
    [junit] <                             expr: _col1
    [junit] <                             type: bigint
    [junit] <                       outputColumnNames: _col0, _col1
    [junit] <                       File Output Operator
    [junit] <                         compressed: false
    [junit] <                         GlobalTableId: 1
    [junit] <                         table:
    [junit] <                             input format: 
    [junit] <                             output format: 
    [junit] <                             serde: 
    [junit] <                             name: default.outputtbl1
    [junit] ---
    [junit] >                     File Output Operator
    [junit] >                       compressed: false
    [junit] >                       GlobalTableId: 1
    [junit] >                       table:
    [junit] >                           input format: 
    [junit] >                           output format: 
    [junit] >                           serde: 
    [junit] >                           name: default.outputtbl1
    [junit] Failed query: union_remove_9.q
{quote}",namit,gangtimliu,Major,Closed,Fixed,29/Nov/12 19:27,16/May/13 21:10
Bug,HIVE-3760,12618448,TestNegativeMinimrCliDriver_mapreduce_stack_trace.q fails on hadoop-1,"Actually functionality is working correctly, but incorrect include/exclude macro in test directive is making its .q.out comparison against incorrect results.",hagleitn,ashutoshc,Major,Closed,Fixed,01/Dec/12 19:28,16/May/13 21:10
Bug,HIVE-3763,12618527,Aggregation followed by a sort-merge join requires a new MR job,"After Hive-3633, if tbl1 and tbl2 are tables bucketiezed and sorted by key into 2 buckets, the following query:
	
	
select count(*) from (	
  select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;	

requires 2 MR job - the first one is a sort-merge join, and the second MR job
performs the count(*) on the output of the sort-merge join. 

Ideally, this should be performed in a single MR job.",namit,namit,Major,Resolved,Fixed,03/Dec/12 05:38,24/Mar/14 20:51
Bug,HIVE-3766,12618654,Enable adding hooks to hive meta store init,We will enable hooks to be added to init HMSHandler,jeanxu,jeanxu,Major,Closed,Fixed,03/Dec/12 23:33,16/May/13 21:10
Bug,HIVE-3767,12618721,BucketizedHiveInputFormat should be automatically used with Bucketized Map Joins also,It seems like even after https://issues.apache.org/jira/browse/HIVE-3219 we still need to set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat for bucket map joins (though not SMB joins),gangtimliu,namit,Major,Closed,Fixed,04/Dec/12 11:05,16/May/13 21:10
Bug,HIVE-3768,12618748,Document JDBC client configuration for secure clusters,Document the JDBC client configuration required for starting Hive on a secure cluster.,leftyl,leftyl,Major,Resolved,Fixed,04/Dec/12 16:29,08/May/13 16:30
Bug,HIVE-3769,12618789,Must publish new Hive-0.10 artifacts to apache repository.,"Hive-0.10 has successfully moved to Thrift-0.9. (HIVE-2715)

Could we please also have the Hive-0.10 artifacts published on the Apache Repository? Please note that the currently published artifacts are pretty old. I suspect HIVE-2715 isn't available in the published artifact.

https://repository.apache.org/content/groups/snapshots/org/apache/hive/hive-metastore/0.10.0-SNAPSHOT/",,mithun,Major,Closed,Fixed,04/Dec/12 22:15,10/Jan/13 19:54
Bug,HIVE-3771,12618801,HIVE-3750 broke TestParse,see title,kevinwilfong,kevinwilfong,Major,Closed,Fixed,04/Dec/12 23:58,16/May/13 21:10
Bug,HIVE-3772,12618835,Fix a concurrency bug in LazyBinaryUtils due to a static field,"Creating a JIRA for [~rxin]'s patch needed by the Shark project. https://github.com/amplab/hive/commit/17e1c3dd2f6d8eca767115dc46d5a880aed8c765
writeVLong should not use a static field due to concurrency concerns.",mikhail,mikhail,Major,Closed,Fixed,05/Dec/12 07:13,15/Oct/13 23:30
Bug,HIVE-3774,12618987,Sort merge join should work if join cols are a prefix of sort columns for each partition,"Currently, a join is converted into a sort-merge join only if the join cols exactly matches the sort cols.
This constraint can definitely be relaxed.",namit,namit,Major,Closed,Fixed,06/Dec/12 05:13,16/May/13 21:10
Bug,HIVE-3775,12619008,"Unit test failures due to unspecified order of results in ""show grant"" command","A number of unit tests (sometimes) using ""show grant"" fail, when run on windows or previous failures put the database in an unexpected state.

The reason is that the output of ""show grant"" is not specified to be in any particular order, but the golden files expect it to be.

The unit test framework should be extended to handled cases like that.",hagleitn,hagleitn,Major,Closed,Fixed,06/Dec/12 08:33,16/May/13 21:10
Bug,HIVE-3778,12622867,Add MapJoinDesc.isBucketMapJoin() as part of explain plan,"This is follow up of HIVE-3767:

Add MapJoinDesc.isBucketMapJoin() as part of explain plan",gangtimliu,gangtimliu,Minor,Closed,Fixed,07/Dec/12 06:06,16/May/13 21:11
Bug,HIVE-3779,12622878,An empty value to hive.logquery.location can't disable the creation of hive history log files,"In AdminManual Configuration (https://cwiki.apache.org/Hive/adminmanual-configuration.html), the description of hive.querylog.location mentioned that if the variable set to empty string structured log will not be created.

But it fails with the following setting,
<property>
  <name>hive.querylog.location</name>
  <value></value> 
</property>

It seems that it can NOT get an empty value from HiveConf.ConfVars.HIVEHISTORYFILELOC, but the default value.",,libing,Minor,Resolved,Fixed,07/Dec/12 08:46,06/Nov/14 17:37
Bug,HIVE-3780,12622962,RetryingMetaStoreClient Should Log the Caught Exception,Currently it logs the cause of the caught exception. It should log the caught exception itself.,bmandhani,bmandhani,Trivial,Closed,Fixed,07/Dec/12 20:05,10/Jan/13 19:54
Bug,HIVE-3781,12622982,Index related events should be delivered to metastore event listener,"An event listener must be called for any DDL activity. For example, create_index, drop_index today does not call metaevent listener.  ",navis,sudhanshu,Major,Resolved,Fixed,07/Dec/12 22:44,26/Mar/15 23:46
Bug,HIVE-3782,12623007,testCliDriver_sample_islocalmode_hook fails on hadoop-1,"Actually functionality is working correctly, but incorrect include/exclude macro in test directive is making its .q.out comparison against incorrect results.",hagleitn,hagleitn,Major,Closed,Fixed,08/Dec/12 09:09,16/May/13 21:10
Bug,HIVE-3783,12623026,stats19.q is failing on trunk,This test-case was introduced in HIVE-3750 and is failing since as soon as it was introduced. ,kevinwilfong,ashutoshc,Major,Closed,Fixed,08/Dec/12 18:37,16/May/13 21:10
Bug,HIVE-3787,12623303,Regression introduced from HIVE-3401,"By HIVE-3401, split_sample_out_of_range.q and split_sample_wrong_format.q are not showing valid 'line:loc' information for error messages.",navis,navis,Minor,Closed,Fixed,11/Dec/12 00:07,16/May/13 21:10
Bug,HIVE-3788,12623317,testCliDriver_repair fails on hadoop-1,"Actually functionality is working correctly, but incorrect include/exclude macro make cause the wrong query file to be run.",hagleitn,hagleitn,Major,Closed,Fixed,11/Dec/12 02:35,16/May/13 21:10
Bug,HIVE-3789,12623322,Patch HIVE-3648 causing the majority of unit tests to fail on branch 0.9,"Rolling back to before this patch shows that the unit tests are passing, after the patch, the majority of the unit tests are failing.",amalakar,cdrome,Major,Closed,Fixed,11/Dec/12 03:08,16/May/13 21:10
Bug,HIVE-3792,12623477,hive pom file has missing conf and scope mapping for compile configuration. ,hive-0.10.0 pom file has missing conf and scope mapping for compile configuration. ,ashishujjain,ashishujjain,Major,Closed,Fixed,12/Dec/12 00:57,10/Jan/13 19:52
Bug,HIVE-3794,12623581,Oracle upgrade script for Hive is broken,"As part of Hive configuration for Oracle I ran the schema creation script for Oracle. Here is what I observed when ran the script:
% sqlplus hive/hive@xe

SQL*Plus: Release 11.2.0.2.0 Production on Mon Dec 10 18:47:11 2012

Copyright (c) 1982, 2011, Oracle.  All rights reserved.


Connected to:
Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production

SQL> @scripts/metastore/upgrade/oracle/hive-schema-0.10.0.oracle.sql;
.....
ALTER TABLE SKEWED_STRING_LIST_VALUES ADD CONSTRAINT SKEWED_STRING_LIST_VALUES_FK1 FOREIGN KEY (STRING_LIST_ID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID) INITIALLY DEFERRED
                                                                                                                                               *
ERROR at line 1:
{color:red}ORA-00904: ""STRING_LIST_ID"": invalid identifier{color}
.....
ALTER TABLE SKEWED_STRING_LIST_VALUES ADD CONSTRAINT SKEWED_STRING_LIST_VALUES_FK1 FOREIGN KEY (STRING_LIST_ID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID) INITIALLY DEFERRED
                                                                                                                                               *
ERROR at line 1:
{color:red}ORA-00904: ""STRING_LIST_ID"": invalid identifier{color}



Table created.


Table altered.


Table altered.

CREATE TABLE SKEWED_COL_VALUE_LOCATION_MAPPING
             *
ERROR at line 1:
{color:red}ORA-00972: identifier is too long{color}



Table created.


Table created.

ALTER TABLE SKEWED_COL_VALUE_LOCATION_MAPPING ADD CONSTRAINT SKEWED_COL_VALUE_LOCATION_MAPPING_PK PRIMARY KEY (SD_ID,STRING_LIST_ID_KID)
            *
ERROR at line 1:
{color:red}ORA-00972: identifier is too long{color}


ALTER TABLE SKEWED_COL_VALUE_LOCATION_MAPPING ADD CONSTRAINT SKEWED_COL_VALUE_LOCATION_MAPPING_FK1 FOREIGN KEY (STRING_LIST_ID_KID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID) INITIALLY DEFERRED
            *
ERROR at line 1:
{color:red}ORA-00972: identifier is too long{color}


ALTER TABLE SKEWED_COL_VALUE_LOCATION_MAPPING ADD CONSTRAINT SKEWED_COL_VALUE_LOCATION_MAPPING_FK2 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID) INITIALLY DEFERRED
            *
ERROR at line 1:
{color:red}ORA-00972: identifier is too long{color}



Table created.


Table altered.

ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_FK1 FOREIGN KEY (STRING_LIST_ID_EID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID) INITIALLY DEFERRED
                                                                                                                           *
ERROR at line 1:
{color:red}ORA-00904: ""STRING_LIST_ID"": invalid identifier{color}


Basically there are two issues here with the Oracle sql script:

(1) Table ""SKEWED_STRING_LIST"" is created with the column ""SD_ID"". Later the script tries to reference ""STRING_LIST_ID"" column in ""SKEWED_STRING_LIST"" which is obviously not there. Comparing the sql with that for other flavors it seems it should be ""STRING_LIST_ID"".

(2) Table name ""SKEWED_COL_VALUE_LOCATION_MAPPING"" is too long for Oracle which limits identifier names to 30 characters. Also impacted are identifiers ""SKEWED_COL_VALUE_LOCATION_MAPPING_PK"" and ""SKEWED_COL_VALUE_LOCATION_MAPPING_FK1"".
",deepesh,deepesh,Critical,Closed,Fixed,12/Dec/12 16:44,10/Jan/13 19:54
Bug,HIVE-3795,12623588,NPE in SELECT when WHERE-clause is an and/or/not operation involving null,"Sometimes users forget to quote date constants in queries. For example, SELECT * FROM some_table WHERE ds >= 2012-12-10 and ds <= 2012-12-12; . In such cases, if the WHERE-clause contains and/or/not operation, it would throw NPE exception. That's because PcrExprProcFactory in ql/optimizer forgot to check null. ",showbufire,showbufire,Trivial,Closed,Fixed,12/Dec/12 17:17,16/May/13 21:10
Bug,HIVE-3798,12623808,Can't escape reserved keywords used as table names,"{noformat}hive (some_table)> show tables;
OK
...
comment
...
Time taken: 0.076 seconds
hive (some_table)> describe comment;
FAILED: Parse Error: line 1:0 cannot recognize input near 'describe' 'comment' '<EOF>' in describe statement
hive (some_table)> describe `comment`;             
OK
Table `comment` does not exist	 	 
Time taken: 0.042 seconds
{noformat}

Describe should honor character escaping.",jghoman,jghoman,Major,Resolved,Fixed,13/Dec/12 19:53,21/Jan/15 21:42
Bug,HIVE-3800,12623817,testCliDriver_combine2 fails on hadoop-1,"Actually functionality is working correctly, but incorrect include/exclude macro make cause the wrong query file to be run.",hagleitn,hagleitn,Major,Closed,Fixed,13/Dec/12 20:21,16/May/13 21:11
Bug,HIVE-3801,12623822,testCliDriver_loadpart_err fails on hadoop-1,"This test is marked as flaky and disabled for all versions, but hadoop-1 was missed in that list.",hagleitn,hagleitn,Major,Closed,Fixed,13/Dec/12 20:46,16/May/13 21:10
Bug,HIVE-3802,12623828,testCliDriver_input39 fails on hadoop-1,"This test is marked as flaky and disabled for all versions, but hadoop-1 was missed in that list.",hagleitn,hagleitn,Major,Closed,Fixed,13/Dec/12 21:06,16/May/13 21:10
Bug,HIVE-3803,12623919,explain dependency should show the dependencies hierarchically in presence of views,It should also include tables whose partitions are being accessed,namit,namit,Major,Closed,Fixed,14/Dec/12 09:20,16/May/13 21:10
Bug,HIVE-3806,12624097,"Ptest failing due to ""Argument list too long"" errors","ptest creates a really huge shell command to delete from each test host those .q files that it should not be running. For TestCliDriver, the command has become long enough that it is over the threshold allowed by the shell. We should rewrite it so that the same semantics is captured in a shorter command.",bmandhani,bmandhani,Minor,Closed,Fixed,15/Dec/12 04:37,16/May/13 21:11
Bug,HIVE-3809,12624152,Concurrency issue in RCFile: multiple threads can use the same decompressor,"RCFile is not thread-safe, even if each reader is only used by one thread as intended, because it is possible to return decompressors to the pool multiple times by calling close on the reader multiple times. Then, different threads can pick up the same decompressor twice from the pool, resulting in decompression failures.",mikhail,mikhail,Critical,Closed,Fixed,15/Dec/12 23:01,16/May/13 21:10
Bug,HIVE-3810,12624184,HiveHistory.log need to replace '\r' with space before writing Entry.value to historyfile,"HiveHistory.log will replace '\n' with space before writing Entry.value to history file:

    val = val.replace('\n', ' ');

but HiveHistory.parseHiveHistory use BufferedReader.readLine which takes '\n', '\r', '\r\n'  as line delimiter to parse history file

if val contains '\r', there is a high possibility that HiveHistory.parseLine will fail, in which case usually RecordTypes.valueOf(recType) will throw exception 'java.lang.IllegalArgumentException'

HiveHistory.log need to replace '\r' with space as well:

val = val.replace('\n', ' ');

changed to

val = val.replaceAll(""\r|\n"", "" "");

or

val = val.replace('\r', ' ').replace('\n', ' ');",mgrover,wsxys08@gmail.com,Minor,Closed,Fixed,16/Dec/12 12:10,15/Oct/13 23:30
Bug,HIVE-3814,12624402,Cannot drop partitions on table when using Oracle metastore,"Create a table with a partition. Try to drop the partition or the table containing the partition. Following error is seen:
FAILED: Error in metadata: MetaException(message:javax.jdo.JDODataStoreException: Error executing JDOQL query ""SELECT 'org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics' AS NUCLEUS_TYPE,THIS.AVG_COL_LEN,THIS.""COLUMN_NAME"",THIS.COLUMN_TYPE,THIS.DB_NAME,THIS.DOUBLE_HIGH_VALUE,THIS.DOUBLE_LOW_VALUE,THIS.LAST_ANALYZED,THIS.LONG_HIGH_VALUE,THIS.LONG_LOW_VALUE,THIS.MAX_COL_LEN,THIS.NUM_DISTINCTS,THIS.NUM_FALSES,THIS.NUM_NULLS,THIS.NUM_TRUES,THIS.PARTITION_NAME,THIS.""TABLE_NAME"",THIS.CS_ID FROM PART_COL_STATS THIS LEFT OUTER JOIN PARTITIONS THIS_PARTITION_PARTITION_NAME ON THIS.PART_ID = THIS_PARTITION_PARTITION_NAME.PART_ID WHERE THIS_PARTITION_PARTITION_NAME.PART_NAME = ? AND THIS.DB_NAME = ? AND THIS.""TABLE_NAME"" = ?"" : ORA-00904: ""THIS"".""PARTITION_NAME"": invalid identifier

The problem here is that the column ""PARTITION_NAME"" that the query is referring to in table ""PART_COL_STATS"" is non-existent. Looking at the hive schema scripts for mysql & derby, this should be ""PARTITION_NAME"". Postgres also suffers from the same problem.",deepesh,deepesh,Critical,Closed,Fixed,18/Dec/12 03:08,10/Jan/13 19:53
Bug,HIVE-3815,12624404,hive table rename fails if filesystem cache is disabled,"If fs.<filesyste>.impl.disable.cache  (eg fs.hdfs.impl.disable.cache) is set to true, then table rename fails.


The exception that gets thrown (though not logged!) is 
{quote}
Caused by: InvalidOperationException(message:table new location hdfs://host1:8020/apps/hive/warehouse/t2 is on a different file system than the old location hdfs://host1:8020/apps/hive/warehouse/t1. This operation is not supported)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_result$alter_table_resultStandardScheme.read(ThriftHiveMetastore.java:28825)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_result$alter_table_resultStandardScheme.read(ThriftHiveMetastore.java:28811)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_result.read(ThriftHiveMetastore.java:28753)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table(ThriftHiveMetastore.java:977)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table(ThriftHiveMetastore.java:962)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:208)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:74)
        at $Proxy7.alter_table(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:373)
        ... 18 more
{quote}",thejas,thejas,Major,Resolved,Fixed,18/Dec/12 03:34,06/Aug/14 02:34
Bug,HIVE-3817,12624530,Adding the name space for the maven task for the maven-publish target.,"maven task for the maven-publish target is missing from the build.xml.
This is causing maven deploy issues.",ashishujjain,ashishujjain,Major,Closed,Fixed,18/Dec/12 20:56,16/May/13 21:10
Bug,HIVE-3820,12624560,"Consider creating a literal like ""D"" or ""BD"" for representing Decimal type constants","When the HIVE-2693 gets committed, users are going to see this behavior:
{code}
hive> select cast(3.14 as decimal) from decimal_3 limit 1;
3.140000000000000124344978758017532527446746826171875
{code}

That's intuitively incorrect but is the case because 3.14 (double) is being converted to BigDecimal because of which there is a precision mismatch.

We should consider creating a new literal for expressing constants of Decimal type as Gunther suggested in HIVE-2693.

",hagleitn,mgrover,Major,Closed,Fixed,19/Dec/12 01:20,16/May/13 21:10
Bug,HIVE-3824,12624786,bug if different serdes are used for different partitions,"Consider the following testcase:

create table tst5 (key string, value string) partitioned by (ds string) stored as rcfile;
insert overwrite table tst5 partition (ds='1') select * from src;    
insert overwrite table tst5 partition (ds='2') select * from src;    
insert overwrite table tst5 partition (ds='3') select * from src;    

alter table tst5 stored as sequencefile; 

insert overwrite table tst5 partition (ds='4') select * from src;    
insert overwrite table tst5 partition (ds='5') select * from src;    
insert overwrite table tst5 partition (ds='6') select * from src;  

alter table tst5 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'; 

insert overwrite table tst5 partition (ds='7') select * from src;    
insert overwrite table tst5 partition (ds='8') select * from src;    
insert overwrite table tst5 partition (ds='9') select * from src;  

The following query works fine:

 select key + key, value from tst5 where ((ds = '4') or (ds = '1'));   

since both the partitions use ColumnarSerDe

But the following query fails:

select key + key, value from tst5 where ((ds = '4') or (ds = '1') or (ds='7'));

since different serdes are used.",namit,namit,Major,Closed,Fixed,20/Dec/12 09:12,16/May/13 21:10
Bug,HIVE-3826,12624899,Rollbacks and retries of drops cause org.datanucleus.exceptions.NucleusObjectNotFoundException: No such database row),"I'm not sure if this is the only cause of the exception ""org.datanucleus.exceptions.NucleusObjectNotFoundException: No such database row)"" from the metastore, but one cause seems to be related to a drop command failing, and being retried by the client.

Based on focusing on a single thread in the metastore with DEBUG level logging, I was seeing the objects that were intended to be dropped remaining in the PersistenceManager cache even after a rollback.  The steps seemed to be as follows:

1) First attempt to drop the table, the table is pulled into the PersistenceManager cache for the purposes of dropping
2) The drop fails, e.g. due to a lock wait timeout on the SQL backend, this causes a rollback of the transaction
3) The drop is retried using a different thread on the metastore Thrift server or a different server and succeeds
4) Back on the original thread of the original Thrift server someone tries to perform some write operation which produces a commit.  This causes those detached objects related to the dropped table to attempt to reattach, causing JDO to query the SQL backend for those objects which it can't find.  This causes the exception.

I was able to reproduce this regularly using the following sequence of commands:
Hive client 1 (Hive1): connected to a metastore Thrift server running a single thread, I hard coded a RuntimeException into the code to drop a table in the ObjectStore, specifically right before the commit in preDropStorageDescriptor, to induce a rollback.  I also turned off all retries at all layers of the metastore.
Hive client 2 (Hive2): connected to a separate metastore Thrift server running with standard configs and code

1: On Hive1, CREATE TABLE t1 (c STRING);
2: On Hive1, DROP TABLE t1; // This failed due to the hard coded exception
3: On Hive2, DROP TABLE t1; // Succeeds
4: On Hive1, CREATE DATABASE d1; // This database already existed, I'm not sure why this was necessary, but it didn't work without it, it seemed to have an affect on the order objects were committed in the next step
5: On Hive1, CREATE DATABASE d2; // This database didn't exist, it would fail with the NucleusObjectNotFoundException

The object that would cause the exception varied, I saw the MTable, the MSerDeInfo, and MTablePrivilege from the table that attempted to be dropped.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,20/Dec/12 21:39,16/May/13 21:10
Bug,HIVE-3828,12624948,insert overwrite fails with stored-as-dir in cluster,"The following query works fine in hive TestCliDriver test suite but not minimr because different Hadoop file system is used.

The error is
{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: .../_task_tmp.-ext-10002/key=103/_tmp.000000_0 to: .../_tmp.-ext-10002/key=103/000000_0
{code}",gangtimliu,gangtimliu,Major,Closed,Fixed,21/Dec/12 07:28,16/May/13 21:10
Bug,HIVE-3829,12625044,Hive CLI needs UNSET TBLPROPERTY command,"The Hive CLI currently supports

ALTER TABLE <table> SET TBLPROPERTIES ('key1' = 'value1', 'key2' = 'value2', ...);

To add/change the value of table properties.

It would be really useful if Hive also supported
ALTER TABLE <table> UNSET TBLPROPERTIES ('key1', 'key2', ...);

Which would remove table properties",zhenxiao,zhenxiao,Major,Closed,Fixed,21/Dec/12 19:55,16/May/13 21:10
Bug,HIVE-3832,12625129,Insert overwrite doesn't create a dir if the skewed column position doesnt match,"If skewed column doesn't match the position in table column, insert overwrite doesn't create sub-dir but put all into default directory.
",gangtimliu,gangtimliu,Major,Closed,Fixed,23/Dec/12 03:04,16/May/13 21:10
Bug,HIVE-3839,12625361,adding .gitattributes file for normalizing line endings during cross platform development,"On the lines of HADOOP-8912 .
Many developers clone the apache/hive git repository to make changes.
Adding a .gitattributes file will help in doing the right thing while checking out files on Windows (eg- adding \r\n on checkout of most text files, preserving \n in case of *.sh files ), and replacing \r\n with \n while checking in code back into a git repository.
",thejas,thejas,Major,Closed,Fixed,27/Dec/12 20:49,16/May/13 21:10
Bug,HIVE-3840,12625370,hive cli null representation in output is inconsistent,,thejas,ashutoshc,Major,Closed,Fixed,27/Dec/12 22:36,16/May/13 21:10
Bug,HIVE-3846,12625459,alter view rename NPEs with authorization on.,,teddy.choi,ashutoshc,Major,Closed,Fixed,29/Dec/12 00:33,15/Oct/13 23:31
Bug,HIVE-3847,12625501,ppd.remove.duplicatefilters removing filters too aggressively,,,ashutoshc,Major,Closed,Fixed,29/Dec/12 19:36,28/Jan/16 00:24
Bug,HDFS-2741,12536826,dfs.datanode.max.xcievers missing in 0.20.205.0,The dfs.datanode.max.xcievers configuration directive is missing in the hdfs-default.xml and documentation.,,markus17,Minor,Closed,Fixed,02/Jan/12 18:28,17/Oct/12 18:27
Bug,HDFS-2748,12537148,fstime of image file newer than fstime of editlog,"1.1 first shutdown, then restart and then the fsimage was loaded and saved to disk and editlog was cleared.
1.2 shutdown again when in safe mode to make sure no change in editlog, then restart and then the fsimage was loaded and save to disk again, but the editlog was not refreshed because it was empty.
1.3 shutdown again when in safe mode to make sure no change in editlog, the restart and then an ERROR printed in log which basically was saying fstime of fsimage was larger then fstime of editlog (which was obviously caused by saving fsimage again and again when no change in editlog), and then the editlog would be discarded (this is OK, the editlog was empty), and current fsimage would be loaded as the latest fsimage. And again save fsimage.
",,dennyy,Major,Resolved,Fixed,04/Jan/12 08:36,12/Mar/15 02:00
Bug,HDFS-2749,12537149,Wrong fsimage format while entering recovery mode,"hadoop is into a recovery mode and save namespace to disk before the system starting service. however, there are many situation will cause hadoop enter recovery mode like missing VERSION file and ckpt file exists due to last failure of checkpoint.
in recovery mode, namespace is loaded from previous fsimage, and the default numFiles of namespace.rootDir is 1. the numFiles number is read from fsimage (readInt as version, readInt as namespaceId, readLong as numFiles).
the numFiles number is not updated in namespace when saving namespace.
save namespace just after load fsimage which actually write numFiles which is default value 1 to disk.
the next time to load the saved fsimage from disk when rebooting or secondarynamenode doing checkpoint, the system will crash (OOM) because this fsimage is incorrect.",,dennyy,Critical,Resolved,Fixed,04/Jan/12 08:39,12/Mar/15 01:59
Bug,HDFS-2751,12537263,Datanode drops OS cache behind reads even for short reads,"HDFS-2465 has some code which attempts to disable the ""drop cache behind reads"" functionality when the reads are <256KB (eg HBase random access). But this check was missing in the {{close()}} function, so it always drops cache behind reads regardless of the size of the read. This hurts HBase random read performance when this patch is enabled.",tlipcon,tlipcon,Major,Closed,Fixed,04/Jan/12 23:52,10/Mar/15 04:35
Bug,HDFS-2757,12537448,Cannot read a local block that's being written to when using the local read short circuit,"When testing the tail'ing of a local file with the read short circuit on, I get:

{noformat}
2012-01-06 00:17:31,598 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal requested with incorrect offset:  Offset 0 and length 8230400 don't match block blk_-2842916025951313698_454072 ( blockLen 124 )
2012-01-06 00:17:31,598 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal: Removing blk_-2842916025951313698_454072 from cache because local file /export4/jdcryans/dfs/data/blocksBeingWritten/blk_-2842916025951313698 could not be opened.
2012-01-06 00:17:31,599 INFO org.apache.hadoop.hdfs.DFSClient: Failed to read block blk_-2842916025951313698_454072 on local machine java.io.IOException:  Offset 0 and length 8230400 don't match block blk_-2842916025951313698_454072 ( blockLen 124 )
2012-01-06 00:17:31,599 INFO org.apache.hadoop.hdfs.DFSClient: Try reading via the datanode on /10.4.13.38:51010
java.io.EOFException: hdfs://sv4r11s38:9100/hbase-1/.logs/sv4r13s38,62023,1325808100311/sv4r13s38%2C62023%2C1325808100311.1325808100818, entryStart=7190409, pos=8230400, end=8230400, edit=5
{noformat}",jdcryans,jdcryans,Major,Closed,Fixed,06/Jan/12 00:54,11/Jan/13 12:35
Bug,HDFS-2759,12537455,Pre-allocate HDFS edit log files after writing version number,"In HDFS-2709 it was discovered that there's a potential race wherein edits log files are pre-allocated before the version number is written into the header of the file. This can cause the NameNode to read an invalid HDFS layout version, and hence fail to read the edit log file. We should write the header, then pre-allocate the rest of the file after this point.",atm,atm,Major,Closed,Fixed,06/Jan/12 01:44,10/Mar/15 04:36
Bug,HDFS-2764,12537589,TestBackupNode is racy,TestBackupNode#waitCheckpointDone can spuriously fail because of a race.,atm,atm,Major,Resolved,Fixed,06/Jan/12 21:10,08/Mar/12 06:53
Bug,HDFS-2765,12537611,TestNameEditsConfigs is incorrectly swallowing IOE,"The final portion of this test case is swallowing an IOE and in so doing appearing to succeed, although it should not be succeeding as-written.",atm,atm,Major,Closed,Fixed,07/Jan/12 00:03,28/Sep/15 20:58
Bug,HDFS-2768,12537645,BackupNode stop can not close proxy connections because it is not a proxy instance.,"Observe this from BackupNode tests:

java.lang.IllegalArgumentException: not a proxy instance
	at java.lang.reflect.Proxy.getInvocationHandler(Unknown Source)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:557)
	at org.apache.hadoop.hdfs.server.namenode.BackupNode.stop(BackupNode.java:194)
	at org.apache.hadoop.hdfs.server.namenode.TestBackupNode.testCheckpoint(TestBackupNode.java:355)
	at org.apache.hadoop.hdfs.server.namenode.TestBackupNode.testBackupNode(TestBackupNode.java:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
",umamaheswararao,umamaheswararao,Major,Closed,Fixed,07/Jan/12 18:08,28/Sep/15 20:58
Bug,HDFS-2790,12538411,FSNamesystem.setTimes throws exception with wrong configuration name in the message,"the api throws this message when hdfs is not configured for accessTime

""Access time for hdfs is not configured.  Please set dfs.support.accessTime configuration parameter.""


The property name should be dfs.access.time.precision",arpitgupta,arpitgupta,Minor,Closed,Fixed,14/Jan/12 18:48,10/Mar/15 04:36
Bug,HDFS-2791,12538425,"If block report races with closing of file, replica is incorrectly marked corrupt","The following sequence of events results in a replica mistakenly marked corrupt:
1. Pipeline is open with 2 replicas
2. DN1 generates a block report but is slow in sending to the NN (eg some flaky network). It gets ""stuck"" right before the block report RPC.
3. Client closes the file.
4. DN2 is fast and sends blockReceived to the NN. NN marks the block as COMPLETE
5. DN1's block report proceeds, and includes the block in an RBW state.
6. (x) NN incorrectly marks the replica as corrupt, since it is an RBW replica on a COMPLETE block.",tlipcon,tlipcon,Major,Closed,Fixed,15/Jan/12 04:49,10/Mar/15 02:12
Bug,HDFS-2797,12538547,Fix misuses of InputStream#skip in the edit log code,"Fix some misuses of InputStream#skip in the edit log code.

Also add a test.  The existing failover tests only cover the case of graceful shutdown. We should make sure the standby NN can become active even when the final transaction in an edit log is only partially written to disk.",cmccabe,atm,Major,Closed,Fixed,16/Jan/12 18:42,11/Oct/12 17:46
Bug,HDFS-2799,12538571,Trim fs.checkpoint.dir values,"fs.checkpoint.dir values need to be trimmed like dfs.name.dir and dfs.data.dir values so eg the following works. This currently results in the directory ""HADOOP_HOME/?/home/eli/hadoop/dirs3/dfs/chkpoint1"" being created.

{noformat}
  <property>
    <name>fs.checkpoint.dir</name>
     <value>
/home/eli/hadoop/dirs3/dfs/chkpoint1,
/home/eli/hadoop/dirs3/dfs/chkpoint2
     </value>
  </property>
{noformat}
",amithdk,eli,Major,Closed,Fixed,16/Jan/12 21:40,28/Sep/15 20:58
Bug,HDFS-2800,12538582,HA: TestStandbyCheckpoints.testCheckpointCancellation is racy,"TestStandbyCheckpoints.testCheckpointCancellation is racy, have seen the following assert on line 212 fail:

{code}
assertTrue(StandbyCheckpointer.getCanceledCount() > 0);
{code}
",tlipcon,atm,Major,Closed,Fixed,16/Jan/12 23:49,07/Sep/12 21:09
Bug,HDFS-2810,12539014,Leases not properly getting renewed by clients,"We've been testing HBase on clusters running trunk and seen an issue where they seem to lose their HDFS leases after a couple of hours of runtime. We don't quite have enough data to understand what's happening, but the NN is expiring them, claiming the hard lease period has elapsed. The clients report no error until their output stream gets killed underneath them.",tlipcon,tlipcon,Critical,Closed,Fixed,19/Jan/12 18:42,10/Mar/15 04:35
Bug,HDFS-2815,12539075,Namenode is not coming out of safemode when we perform ( NN crash + restart ) .  Also FSCK report shows blocks missed.,"When tested the HA(internal) with continuous switch with some 5mins gap, found some *blocks missed* and namenode went into safemode after next switch.
   
   After the analysis, i found that this files already deleted by clients. But i don't see any delete commands logs namenode log files. But namenode added that blocks to invalidateSets and DNs deleted the blocks.
   When restart of the namenode, it went into safemode and expecting some more blocks to come out of safemode.

   Here the reason could be that, file has been deleted in memory and added into invalidates after this it is trying to sync the edits into editlog file. By that time NN asked DNs to delete that blocks. Now namenode shuts down before persisting to editlogs.( log behind)
   Due to this reason, we may not get the INFO logs about delete, and when we restart the Namenode (in my scenario it is again switch), Namenode expects this deleted blocks also, as delete request is not persisted into editlog before.

   I reproduced this scenario with bedug points. *I feel, We should not add the blocks to invalidates before persisting into Editlog*. 

    Note: for switch, we used kill -9 (force kill)

  I am currently in 0.20.2 version. Same verified in 0.23 as well in normal crash + restart  scenario.
 
",umamaheswararao,umamaheswararao,Critical,Closed,Fixed,20/Jan/12 00:14,16/Mar/15 18:39
Bug,HDFS-2816,12539192,Fix missing license header in hadoop-hdfs-project/hadoop-hdfs-httpfs/dev-support/findbugsExcludeFile.xml,,hitesh,hitesh,Trivial,Closed,Fixed,20/Jan/12 18:36,10/Mar/15 04:36
Bug,HDFS-2818,12539223,dfshealth.jsp missing space between role and node name,There seems to be a missing space in the titles of our webpages. EG: <title>Hadoop NameNodestyx01.sf.cloudera.com:8021</title>. It seems like the JSP compiler is doing something to the space which is in the .jsp. Probably a simple fix if you know something about JSP :),devaraj,tlipcon,Trivial,Closed,Fixed,20/Jan/12 23:28,23/May/12 20:50
Bug,HDFS-2822,12539245,processMisReplicatedBlock incorrectly identifies under-construction blocks as under-replicated,"When the NN processes mis-replicated blocks while exiting safemode, it considers under-construction blocks as under-replicated, inserting them into the neededReplicationsQueue. This makes them show up as corrupt in the metrics and UI momentarily, until they're pulled off the queue. At that point, it realizes that they aren't in fact under-replicated, correctly. This affects both the HA branch and trunk/23, best I can tell.",tlipcon,tlipcon,Major,Closed,Fixed,21/Jan/12 02:41,10/Mar/15 04:36
Bug,HDFS-2827,12539447,Cannot save namespace after renaming a directory above a file with an open lease,"When i execute the following operations and wait for checkpoint to complete.

fs.mkdirs(new Path(""/test1""));
FSDataOutputStream create = fs.create(new Path(""/test/abc.txt"")); //dont close
fs.rename(new Path(""/test/""), new Path(""/test1/""));

Check-pointing is failing with the following exception.

2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\HDFS-1623\hadoop-hdfs-project\hadoop-hdfs\build\test\data\dfs\name3
java.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)
	at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)
	at java.lang.Thread.run(Unknown Source)",umamaheswararao,umamaheswararao,Major,Closed,Fixed,23/Jan/12 09:34,10/Mar/15 02:26
Bug,HDFS-2835,12539662,Fix org.apache.hadoop.hdfs.tools.GetConf$Command Findbug issue,"https://builds.apache.org/job/PreCommit-HDFS-Build/1804//artifact/trunk/hadoop-hdfs-project/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html shows a findbugs warning.  It is unrelated to the patch being tested, and has shown up on a few other JIRAS as well.",revans2,revans2,Major,Closed,Fixed,24/Jan/12 20:21,10/Mar/15 04:36
Bug,HDFS-2836,12539663,HttpFSServer still has 2 javadoc warnings in trunk,"{noformat}
[WARNING] hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java:241: warning - @param argument ""override,"" is not a parameter name.
[WARNING] hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java:450: warning - @param argument ""override,"" is not a parameter name.
{noformat}

These are causing other patches to get a -1 in automated testing.",revans2,revans2,Major,Closed,Fixed,24/Jan/12 20:34,10/Mar/15 04:36
Bug,HDFS-2837,12539670,mvn javadoc:javadoc not seeing LimitedPrivate class ,"mvn javadoc:javadoc not seeing LimitedPrivate class 
{noformat}
[WARNING] org/apache/hadoop/fs/FileSystem.class(org/apache/hadoop/fs:FileSystem.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate': class file for org.apache.hadoop.classification.InterfaceAudience not found
[WARNING] org/apache/hadoop/fs/FileSystem.class(org/apache/hadoop/fs:FileSystem.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FileSystem.class(org/apache/hadoop/fs:FileSystem.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/security/Groups.class(org/apache/hadoop/security:Groups.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/security/UserGroupInformation.class(org/apache/hadoop/security:UserGroupInformation.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/security/UserGroupInformation.class(org/apache/hadoop/security:UserGroupInformation.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/security/UserGroupInformation.class(org/apache/hadoop/security:UserGroupInformation.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FSDataInputStream.class(org/apache/hadoop/fs:FSDataInputStream.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FSDataOutputStream.class(org/apache/hadoop/fs:FSDataOutputStream.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/Path.class(org/apache/hadoop/fs:Path.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/UnresolvedLinkException.class(org/apache/hadoop/fs:UnresolvedLinkException.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.class(org/apache/hadoop/fs:MD5MD5CRC32FileChecksum.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/LocalDirAllocator.class(org/apache/hadoop/fs:LocalDirAllocator.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FileContext.class(org/apache/hadoop/fs:FileContext.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FileContext.class(org/apache/hadoop/fs:FileContext.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FSOutputSummer.class(org/apache/hadoop/fs:FSOutputSummer.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FSInputStream.class(org/apache/hadoop/fs:FSInputStream.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/FSInputChecker.class(org/apache/hadoop/fs:FSInputChecker.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/DU.class(org/apache/hadoop/fs:DU.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/DF.class(org/apache/hadoop/fs:DF.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
[WARNING] org/apache/hadoop/fs/AbstractFileSystem.class(org/apache/hadoop/fs:AbstractFileSystem.class): warning: Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
{noformat}

This is causing test-patch to return a -1 for some patches.",revans2,revans2,Major,Closed,Fixed,24/Jan/12 21:16,10/Mar/15 04:36
Bug,HDFS-2840,12538544,TestHostnameFilter should work with localhost or localhost.localdomain ,"TestHostnameFilter may currently fail with the following:

{noformat}
Error Message

null expected:<localhost[.localdomain]> but was:<localhost[]>
Stacktrace

junit.framework.ComparisonFailure: null expected:<localhost[.localdomain]> but was:<localhost[]>
	at junit.framework.Assert.assertEquals(Assert.java:81)
	at junit.framework.Assert.assertEquals(Assert.java:87)
	at org.apache.hadoop.lib.servlet.TestHostnameFilter$1.doFilter(TestHostnameFilter.java:50)
	at org.apache.hadoop.lib.servlet.HostnameFilter.doFilter(HostnameFilter.java:68)
	at org.apache.hadoop.lib.servlet.TestHostnameFilter.hostname(TestHostnameFilter.java:58)
{noformat}",tucu00,eli,Major,Closed,Fixed,16/Jan/12 18:18,05/Mar/12 02:49
Bug,HDFS-2869,12540715,Error in Webhdfs documentation for mkdir,"Reported over the lists by user Stuti Awasthi:

{quote}

I have tried the webhdfs functionality of Hadoop-1.0.0 and it is working fine.
Just a small change is required in the documentation :

Make a Directory declaration in documentation:
curl -i -X PUT ""http://<HOST>:<PORT>/<PATH>?op=MKDIRS[&permission=<OCTAL>]""

Gives following error :
HTTP/1.1 405 HTTP method PUT is not supported by this URL
Content-Length: 0
Server: Jetty(6.1.26)

Correction Required : This works for me
curl -i -X PUT ""http://<host>:<port>/*webhdfs/v1/*<PATH>?op=MKDIRS""
{quote}",qwertymaniac,qwertymaniac,Minor,Closed,Fixed,01/Feb/12 13:23,10/Mar/15 02:13
Bug,HDFS-2877,12540850,"If locking of a storage dir fails, it will remove the other NN's lock file on exit","In {{Storage.tryLock()}}, we call {{lockF.deleteOnExit()}} regardless of whether we successfully lock the directory. So, if another NN has the directory locked, then we'll fail to lock it the first time we start another NN. But our failed start attempt will still remove the other NN's lockfile, and a second attempt will erroneously start.",tlipcon,tlipcon,Major,Closed,Fixed,02/Feb/12 00:20,10/Mar/15 04:36
Bug,HDFS-2878,12540847,TestBlockRecovery does not compile,Looks like HDFS-2563 introduced a compilation error in TestBlockRecovery. We didn't catch this because of HDFS-2876.,tlipcon,eli,Blocker,Closed,Fixed,01/Feb/12 23:59,28/Sep/15 20:58
Bug,HDFS-2882,12540965,"DN continues to start up, even if block pool fails to initialize","I started a DN on a machine that was completely out of space on one of its drives. I saw the following:

2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-12978
42002148) service to styx01.sf.cloudera.com/172.29.5.192:8021
java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp
        at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)

but the DN continued to run, spewing NPEs when it tried to do block reports, etc. This was on the HDFS-1623 branch but may affect trunk as well.",vinayakumarb,tlipcon,Major,Closed,Fixed,02/Feb/12 18:11,19/Sep/18 16:34
Bug,HDFS-2889,12541132,getNumCurrentReplicas is package private but should be public on 0.23 (see HDFS-2408),"See https://issues.apache.org/jira/browse/HDFS-2408
HDFS-2408 was not committed to 0.23 (or trunk it looks like).

This is breaking HBase unit tests with ""-Dhadoop.profile=23""",gchanan,gchanan,Major,Closed,Fixed,03/Feb/12 20:47,10/Mar/15 02:12
Bug,HDFS-2893,12541217,The start/stop scripts don't start/stop the 2NN when using the default configuration,"HDFS-1703 changed the behavior of the start/stop scripts so that the masters file is no longer used to indicate which hosts to start the 2NN on. The 2NN is now started, when using start-dfs.sh, on hosts only when dfs.namenode.secondary.http-address is configured with a non-wildcard IP. This means you can not start a NN using an http-address specified using a wildcard IP. We should allow a 2NN to be started with the default config, ie start-dfs.sh should start a NN, 2NN and DN. The packaging already works this way (it doesn't use start-dfs.sh, it uses hadoop-daemon.sh directly w/o first checking getconf) so let's bring start-dfs.sh in line with this behavior.

",eli,eli2,Minor,Closed,Fixed,04/Feb/12 23:08,22/Jun/12 05:02
Bug,HDFS-2914,12541729,HA: Standby should not enter safemode when resources are low,"When shared edits dir is bounced, standby NN is put into safemode by the NameNodeResourceMonitor(). However, there is no path for it to exit out of safe mode when shared edits dir reappears.",vinayakumarb,harip,Major,Closed,Fixed,08/Feb/12 01:20,11/Oct/12 17:46
Bug,HDFS-2923,12541928,Namenode IPC handler count uses the wrong configuration key,"In HDFS-1763, there was a typo introduced which causes the namenode to use dfs.datanode.handler.count to set the number of IPC threads instead of the correct dfs.namenode.handler.count. This results in bad performance under high load, since there are not nearly enough handlers.",tlipcon,tlipcon,Critical,Closed,Fixed,09/Feb/12 05:15,10/Mar/15 02:12
Bug,HDFS-2938,12542274,Recursive delete of a large directory makes namenode unresponsive,"When deleting a large directory with millions of files, namenode holding FSNamesystem lock will make it unresponsive for other request. In this scenario HDFS-173 added a mechanism to delete blocks in smaller chunks holding the locks. With new read/write lock changes, the mechanism from HDFS-173 is lost. Need to resurrect the mechanism back. Also a good unit test/update to existing unit test is needed to catch future errors with this functionality.",harip,sureshms,Major,Resolved,Fixed,12/Feb/12 02:50,06/Jun/12 00:29
Bug,HDFS-2944,12542435,Typo in hdfs-default.xml causes dfs.client.block.write.replace-datanode-on-failure.enable to be mistakenly disabled,"Here, ""ture"" should be ""true"" if we really do want this feature enabled by default.

{code}
<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
  <value>ture</value>
  ...
</property>
{code}",atm,atm,Major,Resolved,Fixed,13/Feb/12 19:39,10/Mar/15 04:35
Bug,HDFS-2950,12542614,Secondary NN HTTPS address should be listed as a NAMESERVICE_SPECIFIC_KEY,"This config was mistakenly left out of the NAMESERVICE_SPECIFIC_KEY map, which makes it tougher to configure federated clusters with security.",tlipcon,tlipcon,Minor,Resolved,Fixed,14/Feb/12 23:42,28/Mar/12 09:23
Bug,HDFS-2956,12542781,calling fetchdt without a --renewer argument throws NPE,"If I call ""bin/hdfs fetchdt /tmp/mytoken"" without a ""--renewer foo"" argument, then it will throw a NullPointerException:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:830)

this is because getDelegationToken is being called with a null renewer",vinayakumarb,tlipcon,Major,Resolved,Fixed,15/Feb/12 23:02,30/Aug/16 01:42
Bug,HDFS-2963,12543011,Console Output is confusing while executing metasave (dfsadmin command) ,"While giving the command 
./hdfs dfsadmin -metasave /filename
In the console the following message is displayed
"" created file /filename on  server hdfs://ip:port""

it is not created as hdfs file but will be created on local machine under log folder",andrew.wang,andreina,Minor,Closed,Fixed,17/Feb/12 08:41,11/Oct/12 17:46
Bug,HDFS-2966,12543077,TestNameNodeMetrics tests can fail under load,"I've managed to recreate HDFS-540 and HDFS-2434 by the simple technique of running the HDFS tests on a desktop with out enough memory for all the programs trying to run. Things got swapped out and the tests failed as the DN heartbeats didn't come in on time.

the tests both rely on {{waitForDeletion()}} to block the tests until the delete operation has completed, but all it does is sleep for the same number of seconds as there are datanodes. This is too brittle -it may work on a lightly-loaded system, but not on a system under heavy load where it is taking longer to replicate than expect.

Immediate fix: double, triple, the sleep time?
Better fix: have the thread block until all the DN heartbeats have finished.",stevel@apache.org,stevel@apache.org,Minor,Closed,Fixed,17/Feb/12 18:07,11/Oct/12 17:46
Bug,HDFS-2968,12543094,Protocol translator for BlockRecoveryCommand broken when multiple blocks need recovery,"If there are multiple blocks to be recovered, it ends up translating to N copies of the first block instead of the N different blocks.",tlipcon,tlipcon,Blocker,Closed,Fixed,17/Feb/12 20:10,28/Sep/15 20:58
Bug,HDFS-2969,12543095,ExtendedBlock.equals is incorrectly implemented,"The {{ExtendedBlock.equals}} method incorrectly returns true for any two blocks in the same block pool, regardless of block ID.",tlipcon,tlipcon,Critical,Resolved,Fixed,17/Feb/12 20:20,28/Mar/12 09:23
Bug,HDFS-2975,12543301,Rename with overwrite flag true can make NameNode to stuck in safemode on NN (crash + restart).,"When we rename the file with overwrite flag as true, it will delete the destination file blocks. After deleting the blocks, whenever it releases the fsNameSystem lock, NN can give the invalidation work to corresponding DNs to delete the blocks.
Parallaly it will sync the rename related edits to editlog file. At this step before NN sync the edits if NN crashes, NN can stuck into safemode on restart. This is because block already deleted from the DN as part of invalidations. But dst file still exist as rename edits not persisted in log file and no DN will report that blocks now.

This is similar to HDFS-2815
 ",hitliuyi,umamaheswararao,Major,Closed,Fixed,20/Feb/12 07:34,01/Dec/14 03:11
Bug,HDFS-2976,12543334,Remove unnecessary method (tokenRefetchNeeded) in DFSClient,,umamaheswararao,umamaheswararao,Trivial,Closed,Fixed,20/Feb/12 12:47,10/Mar/15 04:35
Bug,HDFS-2982,12543600,Startup performance suffers when there are many edit log segments,"For every one of the edit log segments, it seems like we are calling listFiles on the edit log directory inside of {{findMaxTransaction}}. This is killing performance, especially when there are many log segments and the directory is stored on NFS. It is taking several minutes to start up the NN when there are several thousand log segments present.",cmccabe,tlipcon,Critical,Closed,Fixed,22/Feb/12 00:44,11/Oct/12 17:46
Bug,HDFS-2991,12543786,failure to load edits: ClassCastException,"In doing scale testing of trunk at r1291606, I hit the following:

java.io.IOException: Error replaying edit log at offset 1354251
Recent opcode offsets: 1350014 1350176 1350312 1354251
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)
...
Caused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)
        ... 13 more
",tlipcon,tlipcon,Blocker,Resolved,Fixed,23/Feb/12 02:06,10/Mar/15 04:35
Bug,HDFS-2994,12543807,If lease soft limit is recovered successfully the append can fail,"I saw the following logs on my test cluster:
{code}
2012-02-22 14:35:22,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: startFile: recover lease [Lease.  Holder: DFSClient_attempt_1329943893604_0007_m_000376_0_453973131_1, pendingcreates: 1], src=/benchmarks/TestDFSIO/io_data/test_io_6 from client DFSClient_attempt_1329943893604_0007_m_000376_0_453973131_1
2012-02-22 14:35:22,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_attempt_1329943893604_0007_m_000376_0_453973131_1, pendingcreates: 1], src=/benchmarks/TestDFSIO/io_data/test_io_6
2012-02-22 14:35:22,888 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.
2012-02-22 14:35:22,888 WARN org.apache.hadoop.hdfs.StateChange: DIR* FSDirectory.replaceNode: failed to remove /benchmarks/TestDFSIO/io_data/test_io_6
2012-02-22 14:35:22,888 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.startFile: FSDirectory.replaceNode: failed to remove /benchmarks/TestDFSIO/io_data/test_io_6
{code}
It seems like, if {{recoverLeaseInternal}} succeeds in {{startFileInternal}}, then the INode will be replaced with a new one, meaning the later {{replaceNode}} call can fail.",taoluo,tlipcon,Major,Closed,Fixed,23/Feb/12 07:05,28/Sep/15 20:50
Bug,HDFS-2995,12543808,start-dfs.sh should only start the 2NN for namenodes with dfs.namenode.secondary.http-address configured,"When I run ""start-dfs.sh"" it tries to start a 2NN on every node in the cluster. This despite:

[todd@c1120 hadoop-active]$ ./bin/hdfs getconf -secondaryNameNodes
Incorrect configuration: secondary namenode address dfs.namenode.secondary.http-address is not configured.

Thankfully they do not start :)",eli,tlipcon,Major,Closed,Fixed,23/Feb/12 07:22,28/Sep/15 20:58
Bug,HDFS-3005,12543892,ConcurrentModificationException in FSDataset$FSVolume.getDfsUsed(..),"Saw this in [build #1888|https://builds.apache.org/job/PreCommit-HDFS-Build/1888//testReport/org.apache.hadoop.hdfs.server.datanode/TestMulitipleNNDataBlockScanner/testBlockScannerAfterRestart/].
{noformat}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolume.getDfsUsed(FSDataset.java:557)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.getDfsUsed(FSDataset.java:809)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.access$1400(FSDataset.java:774)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.getDfsUsed(FSDataset.java:1124)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.sendHeartBeat(BPOfferService.java:406)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.offerService(BPOfferService.java:490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.run(BPOfferService.java:635)
	at java.lang.Thread.run(Thread.java:662)
{noformat}
",szetszwo,szetszwo,Major,Closed,Fixed,23/Feb/12 20:08,28/Sep/15 20:58
Bug,HDFS-3006,12543946,"Webhdfs ""SETOWNER"" call returns incorrect content-type","The SETOWNER call returns an empty body. But the header has ""Content-Type: application/json"", which is a contradiction (empty string is not valid json). This appears to happen for SETTIMES and SETPERMISSION as well.",szetszwo,bcwalrus,Major,Closed,Fixed,24/Feb/12 03:08,04/Jul/12 00:12
Bug,HDFS-3008,12543963,Negative caching of local addrs doesn't work,"HDFS-2653 added negative caching of local addrs, however it still goes through the fall through path every time if the address is non-local. ",eli,eli2,Major,Closed,Fixed,24/Feb/12 07:46,17/Oct/12 18:27
Bug,HDFS-3009,12544019,DFSClient islocaladdress() can use similar routine in netutils,isLocalAddress() in dfsclient can use similar function in netutils,harip,harip,Trivial,Closed,Fixed,24/Feb/12 17:22,10/Mar/15 04:36
Bug,HDFS-3012,12541495,Exception while renewing delegation token,,revans2,rramya,Critical,Resolved,Fixed,06/Feb/12 20:27,23/Sep/15 20:39
Bug,HDFS-3019,12544349,TestEditLogJournalFailures silently failing,"Because we use a crappy version of surefire, currently, we didn't notice this. But, TestEditLogJournalFailures seems to be calling System.exit() -- Surefire reports no tests run, errored, or skipped, but it's lying. I'll file a separate JIRA to upgrade Surefire on trunk, but we also need to investigate this test failure.",tlipcon,tlipcon,Critical,Resolved,Fixed,28/Feb/12 00:22,29/Feb/12 13:05
Bug,HDFS-3020,12544358,Auto-logSync based on edit log buffer size broken,"HDFS-1112 added a feature whereby the edit log automatically calls logSync() if the buffered data crosses a threshold. However, the code checks {{bufReady.size()}} rather than {{bufCurrent.size()}} -- which is incorrect since the writes themselves go into {{bufCurrent}}.",tlipcon,tlipcon,Critical,Closed,Fixed,28/Feb/12 03:41,28/Sep/15 20:58
Bug,HDFS-3026,12544475,HA: Handle failure during HA state transition,This JIRA is to address a TODO in NameNode about handling the possibility of an incomplete HA state transition.,atm,atm,Major,Closed,Fixed,28/Feb/12 20:12,28/Sep/15 20:58
Bug,HDFS-3031,12544686,HA: Fix complete() and getAdditionalBlock() RPCs to be idempotent.,"I executed section 3.4 of Todd's HA test plan. https://issues.apache.org/jira/browse/HDFS-1623
1. A large file upload is started.
2. While the file is being uploaded, the administrator kills the first NN and performs a failover.
3. After the file finishes being uploaded, it is verified for correct length and contents.

For the test, I have a vm_template styx01:/home/schu/centos64-2-5.5.qcow2. styx01 hosted the active NN and styx02 hosted the standby NN.

In the log files I attached, you can see that on styx01 I began file upload.
hadoop fs -put centos64-2.5.5.qcow2

After waiting several seconds, I kill -9'd the active NN on styx01 and manually failed over to the NN on styx02. I ran into exception below. (rest of the stacktrace in the attached file styx01_uploadLargeFile)

12/02/29 14:12:52 WARN retry.RetryInvocationHandler: A failover has occurred since the start of this method invocation attempt.
put: Failed on local exception: java.io.EOFException; Host Details : local host is: ""styx01.sf.cloudera.com/172.29.5.192""; destination host is: """"styx01.sf.cloudera.com""\
:12020;
12/02/29 14:12:52 ERROR hdfs.DFSClient: Failed to close file /user/schu/centos64-2-5.5.qcow2._COPYING_
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: ""styx01.sf.cloudera.com/172.29.5.192""; destination host is: """"styx01.\
sf.cloudera.com"":12020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
        at org.apache.hadoop.ipc.Client.call(Client.java:1145)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:188)
        at $Proxy9.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:302)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
        at $Proxy10.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1097)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:973)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:455)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:830)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:762)",tlipcon,schu,Major,Closed,Fixed,29/Feb/12 22:48,10/Mar/15 04:36
Bug,HDFS-3032,12544694,Lease renewer tries forever even if renewal is not possible,"When LeaseRenewer gets an IOException while attempting to renew for a client, it retries after sleeping 500ms. If the exception is caused by a condition that will never change, it keeps talking to the name node until the DFSClient object is closed or aborted.  With the FileSystem cache, a DFSClient can stay alive for very long time. We've seen the cases in which node managers and long living jobs flooding name node with this type of calls.

The current proposal is to abort the client when RemoteException is caught during renewal. LeaseRenewer already does abort on all clients when it sees a SocketTimeoutException.",kihwal,kihwal,Major,Resolved,Fixed,29/Feb/12 23:42,16/Mar/15 18:40
Bug,HDFS-3037,12544849,TestMulitipleNNDataBlockScanner#testBlockScannerAfterRestart is racy,"In this test, we restart a DN in a running cluster, call MiniDFS#waitActive, and then assert some things about the DN. Trouble is, MiniDFSCluster#waitActive won't wait any time at all, since the DN had previously registered with the NN and the NN never had time to realize the DN was dead.",atm,atm,Minor,Closed,Fixed,01/Mar/12 22:02,10/Mar/15 04:35
Bug,HDFS-3038,12544850,Add FSEditLog.metrics to findbugs exclude list,"to fix a findbugs error on trunk -- this field is only re-set by tests, no need to be worried about its synchronization.",tlipcon,tlipcon,Trivial,Closed,Fixed,01/Mar/12 22:09,28/Sep/15 20:58
Bug,HDFS-3048,12545229,Small race in BlockManager#close,"There's a small race in BlockManager#close, we close the BlocksMap before the replication monitor, which means the replication monitor can NPE if it tries to access the blocks map. We need to swap the order (close the blocks map after shutting down the repl monitor).",adi2,eli2,Major,Closed,Fixed,05/Mar/12 20:21,11/Oct/12 17:46
Bug,HDFS-3054,12545395,distcp -skipcrccheck has no effect,"Using distcp with '-skipcrccheck' still seems to cause CRC checksums to happen. 

Ran into this while debugging an issue associated with source and destination having different blocksizes, and not using the preserve blocksize parameter (-pb). In both 23.1 and 23.2 builds, trying to bypass the checksum verification by using the '-skipcrcrcheck' parameter had no effect, the distcp still failed on checksum errors.

Test scenario to reproduce;
do not use '-pb' and try a distcp from 20.205 (default blksize=128M) to .23 (default blksize=256M), the distcp fails on checksum errors, which is expected due to checksum calculation (tiered aggregation of all blks). Trying the same distcp only providing '-skipcrccheck' still fails with the same checksum error, it is expected that checksum would now be bypassed and the distcp would proceed.
",cmccabe,patwhitey2007,Major,Closed,Fixed,06/Mar/12 19:12,04/Sep/14 00:59
Bug,HDFS-3057,12545444,httpfs and hdfs launcher scripts should honor CATALINA_HOME and HADOOP_LIBEXEC_DIR,"In sbin/httpfs.sh the following should use CATALINA_HOME:

{noformat}
if [ ""${HTTPFS_SILENT}"" != ""true"" ]; then
  ${CATALINA_BASE:-""${BASEDIR}/share/hadoop/httpfs/tomcat""}/bin/catalina.sh ""$@""
else
  ${CATALINA_BASE:-""${BASEDIR}/share/hadoop/httpfs/tomcat""}/bin/catalina.sh ""$@"" > /dev/null
fi
{noformat}

and the following should honor HADOOP_LIBEXEC_DIR:

{noformat}
source ${BASEDIR}/libexec/httpfs-config.sh
{noformat}",rvs,rvs,Major,Closed,Fixed,07/Mar/12 02:31,28/Sep/15 20:58
Bug,HDFS-3059,12545531,ssl-server.xml causes NullPointer,"If ssl is enabled (dfs.https.enable) but ssl-server.xml is not available, a DN will crash during startup while setting up an SSL socket with a NullPointerException:

{noformat}12/03/07 17:08:36 DEBUG security.Krb5AndCertsSslSocketConnector: useKerb = false, useCerts = true
jetty.ssl.password : jetty.ssl.keypassword : 12/03/07 17:08:36 INFO mortbay.log: jetty-6.1.26.cloudera.1
12/03/07 17:08:36 INFO mortbay.log: Started SelectChannelConnector@p-worker35.alley.sara.nl:1006
12/03/07 17:08:36 DEBUG security.Krb5AndCertsSslSocketConnector: Creating new KrbServerSocket for: 0.0.0.0
12/03/07 17:08:36 WARN mortbay.log: java.lang.NullPointerException
12/03/07 17:08:36 WARN mortbay.log: failed Krb5AndCertsSslSocketConnector@0.0.0.0:50475: java.io.IOException: !JsseListener: java.lang.NullPointerException
12/03/07 17:08:36 WARN mortbay.log: failed Server@604788d5: java.io.IOException: !JsseListener: java.lang.NullPointerException
12/03/07 17:08:36 INFO mortbay.log: Stopped Krb5AndCertsSslSocketConnector@0.0.0.0:50475
12/03/07 17:08:36 INFO mortbay.log: Stopped SelectChannelConnector@p-worker35.alley.sara.nl:1006
12/03/07 17:08:37 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0{noformat}

The same happens if I set an absolute path to an existing dfs.https.server.keystore.resource - in this case the file cannot be found but not even a WARN is given.

Since in dfs.https.server.keystore.resource we know we need to have 4 properties specified (ssl.server.truststore.location, ssl.server.keystore.location, ssl.server.keystore.password, and ssl.server.keystore.keypassword) we should check if they are set and throw an IOException if they are not.",xiaochen,evertlammerts,Minor,Resolved,Fixed,07/Mar/12 16:33,30/Aug/16 01:42
Bug,HDFS-3061,12545579,Backport HDFS-1487 to branch-1,"It appears that there's a condition under which a HDFS directory with a space quota set can get to a point where the cached size for the directory can permanently differ from the computed value.  When this happens the following command:

{code}
hadoop fs -count -q /tmp/quota-test
{code}

results in the following output in the NameNode logs:

{code}
WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Inconsistent diskspace for directory quota-test. Cached: 6000 Computed: 6072
{code}

I've observed both transient and persistent instances of this happening.  In the transient instances this warning goes away, but in the persistent instances every invocation of the {{fs -count -q}} command yields the above warning.

I've seen instances where the actual disk usage of a directory is 25% of the cached value in INodeDirectory, which creates problems since the quota code uses this cached value to determine whether block write requests are permitted.

This isn't easy to reproduce - I am able to (inconsistently) get HDFS into this state with a simple program which:

#  Writes files into HDFS
#  When a DSQuotaExceededException is encountered removes all files created in step 1
#  Repeat step 1

I'm going to try and come up with a more repeatable test case to reproduce this issue.",kihwal,alex.holmes,Blocker,Closed,Fixed,08/Mar/12 00:11,12/Jun/12 05:30
Bug,HDFS-3062,12545583,Fail to submit mapred job on a secured-HA-HDFS: logic URI cannot be picked up by job submission.,"When testing the combination of NN HA + security + yarn, I found that the mapred job submission cannot pick up the logic URI of a nameservice. 

I have logic URI configured in core-site.xml
{code}
<property>
 <name>fs.defaultFS</name>
 <value>hdfs://ns1</value>
</property>
{code}

HDFS client can work with the HA deployment/configs:
{code}
[root@nn1 hadoop]# hdfs dfs -ls /
Found 6 items
drwxr-xr-x   - hbase  hadoop          0 2012-03-07 20:42 /hbase
drwxrwxrwx   - yarn   hadoop          0 2012-03-07 20:42 /logs
drwxr-xr-x   - mapred hadoop          0 2012-03-07 20:42 /mapred
drwxr-xr-x   - mapred hadoop          0 2012-03-07 20:42 /mr-history
drwxrwxrwt   - hdfs   hadoop          0 2012-03-07 21:57 /tmp
drwxr-xr-x   - hdfs   hadoop          0 2012-03-07 20:42 /user
{code}

but cannot submit a mapred job with security turned on
{code}
[root@nn1 hadoop]# /usr/lib/hadoop/bin/yarn --config ./conf jar share/hadoop/mapreduce/hadoop-mapreduce-examples-0.24.0-SNAPSHOT.jar randomwriter out
Running 0 maps.
Job started: Wed Mar 07 23:28:23 UTC 2012
java.lang.IllegalArgumentException: java.net.UnknownHostException: ns1
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:431)
	at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:312)
	at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:217)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:119)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:97)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:137)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:411)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:326)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1221)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)
....
{code}0.24",mingjielai,mingjielai,Critical,Closed,Fixed,08/Mar/12 00:58,28/Sep/15 20:58
Bug,HDFS-3067,12545729,NPE in DFSInputStream.readBuffer if read is repeated on corrupted block,"With a singly-replicated block that's corrupted, issuing a read against it twice in succession (e.g. if ChecksumException is caught by the client) gives a NullPointerException.

Here's the body of a test that reproduces the problem:

{code}

    final short REPL_FACTOR = 1;
    final long FILE_LENGTH = 512L;
    cluster.waitActive();
    FileSystem fs = cluster.getFileSystem();

    Path path = new Path(""/corrupted"");

    DFSTestUtil.createFile(fs, path, FILE_LENGTH, REPL_FACTOR, 12345L);
    DFSTestUtil.waitReplication(fs, path, REPL_FACTOR);

    ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, path);
    int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);
    assertEquals(""All replicas not corrupted"", REPL_FACTOR, blockFilesCorrupted);

    InetSocketAddress nnAddr =
        new InetSocketAddress(""localhost"", cluster.getNameNodePort());
    DFSClient client = new DFSClient(nnAddr, conf);
    DFSInputStream dis = client.open(path.toString());
    byte[] arr = new byte[(int)FILE_LENGTH];
    boolean sawException = false;
    try {
      dis.read(arr, 0, (int)FILE_LENGTH);
    } catch (ChecksumException ex) {     
      sawException = true;
    }
    
    assertTrue(sawException);
    sawException = false;
    try {
      dis.read(arr, 0, (int)FILE_LENGTH); // <-- NPE thrown here
    } catch (ChecksumException ex) {     
      sawException = true;
    } 
{code}

The stack:

{code}
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:492)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:545)
        [snip test stack]
{code}

and the problem is that currentNode is null. It's left at null after the first read, which fails, and then is never refreshed because the condition in read that protects blockSeekTo is only triggered if the current position is outside the block's range. 
",henryr,henryr,Major,Closed,Fixed,08/Mar/12 23:08,10/Mar/15 04:36
Bug,HDFS-3070,12545749,HDFS balancer doesn't ensure that hdfs-site.xml is loaded,"I TeraGenerated data into DataNodes styx01 and styx02. Looking at the web UI, both have over 3% disk usage.
Attached is a screenshot of the Live Nodes web UI.

On styx01, I run the _hdfs balancer_ command with threshold 1% and don't see the blocks being balanced across all 4 datanodes (all blocks on styx01 and styx02 stay put).

HA is currently enabled.

[schu@styx01 ~]$ hdfs haadmin -getServiceState nn1
active
[schu@styx01 ~]$ hdfs balancer -threshold 1
12/03/08 10:10:32 INFO balancer.Balancer: Using a threshold of 1.0
12/03/08 10:10:32 INFO balancer.Balancer: namenodes = []
12/03/08 10:10:32 INFO balancer.Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=1.0]
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
Balancing took 95.0 milliseconds
[schu@styx01 ~]$ 

I believe with a threshold of 1% the balancer should trigger blocks being moved across DataNodes, right? I am curious about the ""namenode = []"" from the above output.

[schu@styx01 ~]$ hadoop version
Hadoop 0.24.0-SNAPSHOT
Subversion git://styx01.sf.cloudera.com/home/schu/hadoop-common/hadoop-common-project/hadoop-common -r f6a577d697bbcd04ffbc568167c97b79479ff319
Compiled by schu on Thu Mar  8 15:32:50 PST 2012
From source with checksum ec971a6e7316f7fbf471b617905856b8

From http://hadoop.apache.org/hdfs/docs/r0.21.0/api/org/apache/hadoop/hdfs/server/balancer/Balancer.html:
The threshold parameter is a fraction in the range of (0%, 100%) with a default value of 10%. The threshold sets a target for whether the cluster is balanced. A cluster is balanced if for each datanode, the utilization of the node (ratio of used space at the node to total capacity of the node) differs from the utilization of the (ratio of used space in the cluster to total capacity of the cluster) by no more than the threshold value. The smaller the threshold, the more balanced a cluster will become. It takes more time to run the balancer for small threshold values. Also for a very small threshold the cluster may not be able to reach the balanced state when applications write and delete files concurrently.",atm,schu,Major,Closed,Fixed,09/Mar/12 00:54,28/Sep/15 20:58
Bug,HDFS-3078,12546141,2NN https port setting is broken,"The code in SecondaryNameNode.java to set the https port is broken, if the port is set it sets the bind addr to ""addr:addr:port"" which is bogus. Even if it did work it uses port 0 instead of port 50490 (default listed in ./src/packages/templates/conf/hdfs-site.xml).

",eli,eli2,Major,Closed,Fixed,12/Mar/12 19:36,17/Oct/12 18:27
Bug,HDFS-3083,12546187,Cannot run an MR job with HA and security enabled when second-listed NN active,"Steps to reproduce:
- turned on ha and security
- run a mapred job, and wait to finish
- failover to another namenode
- run the mapred job again, it fails. ",atm,mingjielai,Critical,Closed,Fixed,13/Mar/12 06:37,28/Sep/15 20:58
Bug,HDFS-3087,12546303,Decomissioning on NN restart can complete without blocks being replicated,"If a data node is added to the exclude list and the name node is restarted, the decomissioning happens right away on the data node registration. At this point the initial block report has not been sent, so the name node thinks the node has zero blocks and the decomissioning completes very quick, without replicating the blocks on that node.",shahrs87,kihwal,Critical,Closed,Fixed,13/Mar/12 21:18,09/Apr/15 18:32
Bug,HDFS-3093,12546475,TestAllowFormat is trying to be interactive,"HDFS-2731 broke TestAllowFormat such that it now tries to prompt the user, which of course hangs forever.",tlipcon,tlipcon,Critical,Closed,Fixed,14/Mar/12 20:53,28/Sep/15 20:58
Bug,HDFS-3099,12546632,SecondaryNameNode does not properly initialize metrics system,"The SecondaryNameNode is not properly initializing its metrics system. This results in the UgiMetrics, Metrics subsystem stats, and JvmMetrics not being output.",atm,atm,Major,Closed,Fixed,15/Mar/12 18:02,28/Sep/15 20:58
Bug,HDFS-3100,12546643,failed to append data,"STEP：
1, deploy a single node hdfs  0.23.1 cluster and configure hdfs as:
A) enable webhdfs
B) enable append
C) disable permissions
2, start hdfs
3, run the test script as attached

RESULT:
expected: a file named testFile should be created and populated with 32K * 5000 zeros, HDFS should be OK.
I got: script cannot be finished, file has been created but not be populated as expected, actually append operation failed.

Datanode log shows that, blockscaner report a bad replica and nanenode decide to delete it. Since it is a single node cluster, append fail. It makes no sense that the script failed every time.

Datanode and Namenode logs are attached.",brandonli,wangzw,Major,Closed,Fixed,15/Mar/12 19:07,28/Sep/15 20:58
Bug,HDFS-3101,12546644,cannot read empty file using webhdfs,"STEP:
1, create a new EMPTY file
2, read it using webhdfs.

RESULT:
expected: get a empty file
I got: {""RemoteException"":{""exception"":""IOException"",""javaClassName"":""java.io.IOException"",""message"":""Offset=0 out of the range [0, 0); OPEN, path=/testFile""}}

First of all, [0, 0) is not a valid range, and I think read a empty file should be OK.",szetszwo,wangzw,Major,Closed,Fixed,15/Mar/12 19:11,16/Mar/15 18:14
Bug,HDFS-3109,12546814,Remove hsqldb exclusions from pom.xml,Related to MAPREDUCE-3621,raviprak,raviprak,Major,Closed,Fixed,16/Mar/12 16:25,28/Sep/15 20:58
Bug,HDFS-3116,12547094,Typo in fetchdt error message,"In {{DelegationTokenFetcher.java}} there's the following typo of the word ""exactly"":

{code}
System.err.println(""ERROR: Must specify exacltly one token file"");
{code}",aoe,atm,Trivial,Resolved,Fixed,19/Mar/12 21:26,12/May/16 18:16
Bug,HDFS-3119,12547201,Overreplicated block is not deleted even after the replication factor is reduced after sync follwed by closing that file,"cluster setup:
--------------

1NN,2 DN,replication factor 2,block report interval 3sec ,block size-256MB

step1: write a file ""filewrite.txt"" of size 90bytes with sync(not closed) 
step2: change the replication factor to 1  using the command: ""./hdfs dfs -setrep 1 /filewrite.txt""
step3: close the file

* At the NN side the file ""Decreasing replication from 2 to 1 for /filewrite.txt"" , logs has occured but the overreplicated blocks are not deleted even after the block report is sent from DN

* while listing the file in the console using ""./hdfs dfs -ls "" the replication factor for that file is mentioned as 1

* In fsck report for that files displays that the file is replicated to 2 datanodes
",ashish singhi,andreina,Minor,Closed,Fixed,20/Mar/12 14:15,28/Sep/15 20:58
Bug,HDFS-3127,12547630,failure in recovering removed storage directories should not stop checkpoint process,"When a restore fails, rollEditLog() also fails even if there are healthy directories. Any exceptions from recovering the removed directories should not fail checkpoint process.",brandonli,brandonli,Major,Closed,Fixed,22/Mar/12 16:52,16/May/12 20:45
Bug,HDFS-3128,12547681,Unit tests should not use a test root in /tmp,"Saw this on jenkins, TestResolveHdfsSymlink#testFcResolveAfs creates /tmp/alpha which interferes with other executors on the same machine.",andrew.wang,eli2,Minor,Closed,Fixed,22/Mar/12 20:51,11/Apr/14 18:18
Bug,HDFS-3132,12547835,Findbugs warning on HDFS trunk,"""Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.NameNode.state; locked 50% of time"". I think this was probably one of my recent cross-project HA patches in which the QA bot was unable to run. Will investigate.",tlipcon,tlipcon,Minor,Closed,Fixed,23/Mar/12 17:08,28/Sep/15 20:58
Bug,HDFS-3136,12547882,Multiple SLF4J binding warning,"This is the HDFS portion of HADOOP-8005.  HDFS no longer depends upon slf4j, so removing it from the assembly will eliminate the HDFS-portion of the multiple SLF4J warnings.
",jlowe,jlowe,Major,Closed,Fixed,23/Mar/12 20:44,11/Oct/12 17:46
Bug,HDFS-3142,12547972,TestHDFSCLI.testAll is failing,TestHDFSCLI.testAll is failing in the latest trunk/23 builds. Last good build was Mar 23rd.,brandonli,eli2,Blocker,Closed,Fixed,24/Mar/12 22:45,28/Sep/15 20:58
Bug,HDFS-3143,12547973,TestGetBlocks.testGetBlocks is failing,TestGetBlocks.testGetBlocks is failing in the latest trunk/23 builds. Last good build was Mar 23rd.,arpitgupta,eli2,Major,Closed,Fixed,24/Mar/12 22:51,28/Sep/15 20:58
Bug,HDFS-3156,12548443,TestDFSHAAdmin is failing post HADOOP-8202,"TestDFSHAAdmin mocks a protocol object without implementing Closeable, which is now required.",atm,atm,Major,Closed,Fixed,28/Mar/12 07:17,28/Sep/15 20:58
Bug,HDFS-3157,12548499,Error in deleting block is keep on coming from DN even after the block report and directory scanning has happened,"Cluster setup:

1NN,Three DN(DN1,DN2,DN3),replication factor-2,""dfs.blockreport.intervalMsec"" 300,""dfs.datanode.directoryscan.interval"" 1

step 1: write one file ""a.txt"" with sync(not closed)
step 2: Delete the blocks in one of the datanode say DN1(from rbw) to which replication happened.
step 3: close the file.

Since the replication factor is 2 the blocks are replicated to the other datanode.

Then at the NN side the following cmd is issued to DN from which the block is deleted
-------------------------------------------------------------------------------------
{noformat}
2012-03-19 13:41:36,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_2903555284838653156 to add as corrupt on XX.XX.XX.XX by /XX.XX.XX.XX because reported RBW replica with genstamp 1002 does not match COMPLETE block's genstamp in block map 1003
2012-03-19 13:41:39,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* Removing block blk_2903555284838653156_1003 from neededReplications as it has enough replicas.
{noformat}

From the datanode side in which the block is deleted the following exception occured


{noformat}
2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block blk_2903555284838653156_1003. BlockInfo not found in volumeMap.
2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Error in deleting blocks.
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)
	at java.lang.Thread.run(Thread.java:619)
{noformat}",ashish singhi,andreina,Major,Closed,Fixed,28/Mar/12 14:35,10/Mar/15 04:36
Bug,HDFS-3160,12548576,httpfs should exec catalina instead of forking it,In Bigtop we would like to start supporting constant monitoring of the running daemons (BIGTOP-263). It would be nice if Oozie can support that requirement by execing Catalina instead of forking it off. Currently we have to track down the actual process being monitored through the script that still hangs around.,rvs,rvs,Major,Resolved,Fixed,28/Mar/12 23:15,16/Mar/15 18:41
Bug,HDFS-3165,12548799,HDFS Balancer scripts are refering to wrong path of hadoop-daemon.sh,"HDFS Balancer scripts are refering to wrong path of hadoop-daemon.sh

HOST-xx-xx-xx-xx:/home/amith/V1R2/namenode1/sbin # ./start-balancer.sh
./start-balancer.sh: line 27: /home/amith/V1R2/namenode1/bin/hadoop-daemon.sh: No such file or directory

currently hadoop-daemon.sh script is in sbin and not bin.
",amithdk,amithdk,Minor,Resolved,Fixed,30/Mar/12 08:43,17/Apr/12 14:03
Bug,HDFS-3166,12548520,Hftp connections do not have a timeout,"Hftp connections do not have read timeouts.  This leads to indefinitely hung sockets when there is a network outage during which time the remote host closed the socket.

This may also affect WebHdfs, etc.",daryn,daryn,Critical,Closed,Fixed,28/Mar/12 16:40,12/May/16 18:12
Bug,HDFS-3174,12549036,Fix assert in TestPendingDataNodeMessages,The assert in the text in TestPendingDataNodeMessages is missing the DatanodeID port number.,eli,eli2,Major,Resolved,Fixed,02/Apr/12 01:35,22/Jun/12 05:02
Bug,HDFS-3176,12549176,JsonUtil should not parse the MD5MD5CRC32FileChecksum bytes on its own.,Currently JsonUtil used by webhdfs parses MD5MD5CRC32FileChecksum binary bytes on its own and contructs a MD5MD5CRC32FileChecksum. It should instead call MD5MD5CRC32FileChecksum.readFields().,kihwal,kihwal,Major,Closed,Fixed,02/Apr/12 16:19,11/Oct/12 17:46
Bug,HDFS-3177,12549184,Allow DFSClient to find out and use the CRC type being used for a file.,"To support HADOOP-8060, DFSClient should be able to find out the checksum type being used for files in hdfs.
In my prototype, DataTransferProtocol was extended to include the checksum type in the blockChecksum() response. DFSClient uses it in getFileChecksum() to determin the checksum type. Also append() can be configured to use the existing checksum type instead of the configured one.",kihwal,kihwal,Major,Closed,Fixed,02/Apr/12 16:43,11/Oct/12 17:46
Bug,HDFS-3180,12549228,Add socket timeouts to webhdfs,WebHDFS connections may indefinitely hang due to no timeouts on the connection.  WebHDFS should be adapted in a similar fashion to HDFS-3166 for hftp.,cnauroth,daryn,Major,Closed,Fixed,02/Apr/12 21:09,27/Aug/13 22:08
Bug,HDFS-3181,12549266,testHardLeaseRecoveryAfterNameNodeRestart fails when length before restart is 1 byte less than CRC chunk size,"org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart seems to be failing intermittently on jenkins.

{code}
org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart
Failing for the past 1 build (Since Failed#2163 )
Took 8.4 sec.
Error Message

Lease mismatch on /hardLeaseRecovery owned by HDFS_NameNode but is accessed by DFSClient_NONMAPREDUCE_1147689755_1  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2076)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2051)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:1983)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:492)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:311)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42604)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:417)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1661)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1657)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:396)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1205)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1655) 

Stacktrace

org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on /hardLeaseRecovery owned by HDFS_NameNode but is accessed by DFSClient_NONMAPREDUCE_1147689755_1
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2076)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2051)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:1983)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:492)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:311)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42604)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:417)
        ...
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at $Proxy15.getAdditionalDatanode(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolTranslatorPB.java:317)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:828)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:930)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:741)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:416)
{code}",szetszwo,cmccabe,Minor,Closed,Fixed,03/Apr/12 00:59,28/Sep/15 20:58
Bug,HDFS-3194,12549470,DataNode block scanner is running too frequently,"Block scanning interval by default should be taken as 21 days(3 weeks) and each block scanning should happen once in 21 days.
Here the block is being scanned continuosly.

2012-04-03 10:44:47,056 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-241703115-xx.xx.xx.55-1333086229434:blk_-2666054955039014473_1003
2012-04-03 10:45:02,064 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-241703115-xx.xx.xx.55-1333086229434:blk_-2666054955039014473_1003
2012-04-03 10:45:17,071 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-241703115-xx.xx.xx.55-1333086229434:blk_-2666054955039014473_1003
2012-04-03 10:45:32,079 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-241703115-xx.xx.xx.55-1333086229434:blk_-2666054955039014473",adi2,suja,Major,Closed,Fixed,04/Apr/12 09:33,11/Oct/12 17:46
Bug,HDFS-3195,12549520,Compilation error in FSNamesystem,"Build fails when trying to build branch-2:

[ERROR] /hadoop/src/branch-2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:[2741,32] unreported exception java.io.IOException; must be caught or declared to be thrown
",eli,jlowe,Blocker,Resolved,Fixed,04/Apr/12 15:34,22/Jun/12 05:02
Bug,HDFS-3197,12549575,Incorrect class comments in a few tests,"TestFileCreationClient, TestDatanodeDeath, TestReplaceDatanodeOnFailure, and TestDatanodeRegistration all have the following class comment:

{noformat}
/**
 * This class tests that a file need not be closed before its
 * data can be read by another client.
 */
{noformat}

The javadoc for the classes need to be updated to describe the tests.",adi2,atm,Trivial,Resolved,Fixed,04/Apr/12 22:03,12/May/16 18:16
Bug,HDFS-3199,12549587,TestValidateConfigurationSettings is failing,TestValidateConfigurationSettings is failing on every run.,tlipcon,eli2,Major,Closed,Fixed,04/Apr/12 23:54,28/Sep/15 20:58
Bug,HDFS-3202,12549600,NamespaceInfo PB translation drops build version,"The PBHelper#convert(NamespaceInfoProto) function doesn't pass the build version from the NamespaceInfoProto to the created NamespaceInfo object. Instead, the NamespaceInfo constructor gets the build version using the static function Storage#getBuildVersion. DNs also use this static function to determine their own build version. This means that the check the DN does to compare its own build version to that of the NN always passes, regardless of what build version exists on the NN.",atm,atm,Major,Closed,Fixed,05/Apr/12 01:29,28/Sep/15 20:58
Bug,HDFS-3208,12549789,Bogus entries in hosts files are incorrectly displayed in the report ,"DM#getDatanodeListForReport incorrectly creates the DatanodeID for the ""dead"" report for bogus entries in the host files (eg that an invalid hostname).",eli,eli2,Major,Closed,Fixed,05/Apr/12 20:28,28/Sep/15 20:58
Bug,HDFS-3210,12549797,"JsonUtil#toJsonMap for for a DatanodeInfo should use ""ipAddr"" instead of ""name""","In HDFS-3144 I missed a spot when renaming the ""name"" field. Let's fix that and add a test.",eli,eli2,Major,Closed,Fixed,05/Apr/12 21:40,28/Sep/15 20:58
Bug,HDFS-3214,12549826,InterDatanodeProtocolServerSideTranslatorPB doesn't handle null response from initReplicaRecovery,"The initReplicaRecovery function may return null to indicate that the block doesn't exist on the local node. However, the translator doesn't handle this case, which results in NPEs.",tlipcon,tlipcon,Blocker,Closed,Fixed,06/Apr/12 00:49,28/Sep/15 20:58
Bug,HDFS-3222,12549917,DFSInputStream#openInfo should not silently get the length as 0 when locations length is zero for last partial block.,"I have seen one situation with Hbase cluster.

Scenario is as follows:

1)1.5 blocks has been written and synced.

2)Suddenly cluster has been restarted.

Reader opened the file and trying to get the length., By this time partial block contained DNs are not reported to NN. So, locations for this partial block would be 0. In this case, DFSInputStream assumes that, 1 block size as final size.

But reader also assuming that, 1 block size is the final length and setting his end marker. Finally reader ending up reading only partial data. Due to this, HMaster could not replay the complete edits. 

Actually this happend with 20 version. Looking at the code, same should present in trunk as well.


{code}
    int replicaNotFoundCount = locatedblock.getLocations().length;
    
    for(DatanodeInfo datanode : locatedblock.getLocations()) {
..........
..........
 // Namenode told us about these locations, but none know about the replica
    // means that we hit the race between pipeline creation start and end.
    // we require all 3 because some other exception could have happened
    // on a DN that has it.  we want to report that error
    if (replicaNotFoundCount == 0) {
      return 0;
    }

{code}",umamaheswararao,umamaheswararao,Major,Closed,Fixed,06/Apr/12 18:49,12/May/16 18:18
Bug,HDFS-3224,12549930,Bug in check for DN re-registration with different storage ID,"DatanodeManager#registerDatanode checks the host to node map using an IP:port key, however the map is keyed on IP, so this check will always fail. It's performing the check to determine if a DN with the same IP and storage ID has already registered, and if so to remove this DN from the map and indicate that eg it's no longer hosting these blocks. This bug has been here forever.",jlowe,eli2,Minor,Closed,Fixed,06/Apr/12 20:17,06/Feb/13 17:05
Bug,HDFS-3234,12550291,Accidentally left log message in GetConf after HDFS-3226,"I accidentally left a debug printout in. It doesn't cause a functionality regression, but it does cause noisy output on the command line:

$ ./bin/hdfs getconf -confKey fs.defaultFS
key: fs.defaultFS
hdfs://nameserviceId1
",tlipcon,tlipcon,Trivial,Closed,Fixed,09/Apr/12 23:17,28/Sep/15 20:58
Bug,HDFS-3235,12550293,MiniDFSClusterManager doesn't correctly support -format option,"MiniDFSClusterManager.java correctly honours -format for setting StartupOption.FORMAT, but does not set .format(true) on the MiniDFSClusterBuilder. This means the datanodes' data dirs will be formatted every time. ",henryr,henryr,Minor,Closed,Fixed,09/Apr/12 23:32,12/May/16 18:11
Bug,HDFS-3236,12550297,NameNode does not initialize generic conf keys when started with -initializeSharedEditsDir,This means that configurations that scope the location of the name/edits/shared edits dirs by nameserice or namenode won't work with `hdfs namenode -initializeSharedEdits'.,atm,atm,Minor,Closed,Fixed,09/Apr/12 23:45,28/Sep/15 20:58
Bug,HDFS-3243,12550402,TestParallelRead timing out on jenkins,Trunk builds have been failing recently due to a TestParallelRead timeout. It doesn't report in the Jenkins failure list because surefire handles timeouts really poorly.,henryr,tlipcon,Major,Closed,Fixed,10/Apr/12 16:38,11/Oct/12 17:46
Bug,HDFS-3248,12550439,bootstrapstanby repeated twice in hdfs namenode usage message,"The HDFS usage message repeats ""bootstrapStandby"" twice.

{code}
Usage: java NameNode [-backup] | [-checkpoint] | [-format[-clusterid cid ]] | [-upgrade] | [-rollback] | [-finalize] | [-importCheckpoint] | [-bootstrapStandby] | [-initializeSharedEdits] | [-bootstrapStandby] | [-recover [ -force ] ]
{code}",cmccabe,cmccabe,Minor,Closed,Fixed,10/Apr/12 20:32,28/Sep/15 20:58
Bug,HDFS-3254,12550556,Branch-2 build broken due to wrong version number in fuse-dfs' pom.xml,,anupamseth,anupamseth,Major,Closed,Fixed,11/Apr/12 16:10,28/Sep/15 20:58
Bug,HDFS-3255,12550578,HA DFS returns wrong token service,"{{fs.getCanonicalService()}} must be equal to {{fs.getDelegationToken(renewer).getService()}}.  When HA is enabled, the DFS token's service is a logical uri, but {{dfs.getCanonicalService()}} is only returning the hostname of the logical uri.",daryn,daryn,Critical,Closed,Fixed,11/Apr/12 20:31,28/Sep/15 20:58
Bug,HDFS-3256,12550589,HDFS considers blocks under-replicated if topology script is configured with only 1 rack,"HDFS treats the mere presence of a topology script being configured as evidence that there are multiple racks. If there is in fact only a single rack, the NN will try to place the blocks on at least two racks, and thus blocks will be considered to be under-replicated.",atm,atm,Major,Closed,Fixed,11/Apr/12 21:32,28/Sep/15 20:58
Bug,HDFS-3260,12550622,TestDatanodeRegistration should set minimum DN version in addition to minimum NN version,"This test doesn't actually fail on trunk, but it does fail on branch-2 because of the static version number of branch-2. Regardless, this patch should be committed to both trunk and branch-2 so that if/when trunk changes version numbers things still work as expected.",atm,atm,Major,Closed,Fixed,12/Apr/12 03:59,28/Sep/15 20:58
Bug,HDFS-3261,12550628,TestHASafeMode fails on HDFS-3042 branch,"TestHASafeMode started failing on the HDFS-3042 branch after the commit of HADOOP-8247. The reason is that testEnterSafeModeInANNShouldNotThrowNPE restarts the active node, and then tries to make an RPC to it right after restarting. The RPC picks up a cached connection to the old (restarted) NN, which causes an EOFException. This was just due to a test change that was made in HADOOP-8247, not due to any change made by the actual patch.",tlipcon,tlipcon,Trivial,Resolved,Fixed,12/Apr/12 06:49,13/Apr/12 22:46
Bug,HDFS-3265,12550713,PowerPc Build error.,"When attempting to build branch-1, the following error is seen and ant exits.
[exec] configure: error: Unsupported CPU architecture ""powerpc64""

The following command was used to build hadoop-common

ant -Dlibhdfs=true -Dcompile.native=true -Dfusedfs=true -Dcompile.c++=true -Dforrest.home=$FORREST_HOME compile-core-native compile-c++ compile-c++-examples task-controller tar record-parser compile-hdfs-classes package -Djava5.home=/opt/ibm/ibm-java2-ppc64-50/ ",kumarr,kumarr,Major,Closed,Fixed,12/Apr/12 16:49,12/May/16 18:13
Bug,HDFS-3266,12550729,DFSTestUtil#waitCorruptReplicas doesn't sleep between checks,"DFSTestUtil#waitCorruptReplicas runs a loop waiting for an expected number of corrupt replicas. Unfortunately, it doesn't sleep between iterations of this loop, causing tests to occasionally fail spuriously.",phatak.dev,atm,Minor,Closed,Fixed,12/Apr/12 19:53,07/Mar/13 12:42
Bug,HDFS-3268,12550738,Hdfs mishandles token service & incompatible with HA,"The {{Hdfs AbstractFileSystem}} is overwriting the token service set by the {{DFSClient}}.  The service is not necessarily the correct one since {{DFSClient}} is responsible for the service.  Most importantly, this improper behavior is overwriting the HA logical service which indirectly renders {{FileContext}} incompatible with HA.",daryn,daryn,Critical,Closed,Fixed,12/Apr/12 20:41,28/Sep/15 20:58
Bug,HDFS-3275,12550894,Format command overwrites contents of non-empty shared edits dir if name dirs are empty without any prompting,"To reproduce:
# Configure a NameNode with namedirs and a shared edits dir, all of which are empty.
# Run hdfs namenode -format. Namedirs and shared edits dir gets populated.
# Delete the contents of the namedirs. Leave the shared edits dir as is. Check the timestamps of the shared edits dir contents.
# Run format again. The namedirs as well as the shared edits dir get formatted. The shared edits dir's contents have been replaced without any prompting.",amithdk,vinithra,Major,Closed,Fixed,13/Apr/12 22:48,28/Sep/15 20:58
Bug,HDFS-3277,12550899,fail over to loading a different FSImage if the first one we try to load is corrupt,"Most users store multiple copies of the FSImage in order to prevent catastrophic data loss if a hard disk fails.  However, our image loading code is currently not set up to start reading another FSImage if loading the first one does not succeed.  We should add this capability.

We should also be sure to remove the FSImage directory that failed from the list of FSImage directories to write to, in the way we normally do when a write (as opopsed to read) fails.",andrew.wang,cmccabe,Major,Closed,Fixed,13/Apr/12 23:29,12/May/16 18:13
Bug,HDFS-3280,12550919,DFSOutputStream.sync should not be synchronized,"HDFS-895 added an optimization to make hflush() much faster by unsynchronizing it. But, we forgot to un-synchronize the deprecated {{sync()}} wrapper method. This makes the HBase WAL really slow on 0.23+ since it doesn't take advantage of HDFS-895 anymore.",tlipcon,tlipcon,Critical,Closed,Fixed,14/Apr/12 02:32,28/Sep/15 20:58
Bug,HDFS-3284,12551110,bootstrapStandby fails in secure cluster,"HDFS-3247 improved bootstrapStandby to check if the other NN is in active state before trying to bootstrap. But, it forgot to set up the kerberos principals in the config before doing so. So, bootstrapStandby now fails with ""Failed to specify server's Kerberos principal name"" in a secure cluster. (Credit to Stephen Chu for finding this)",tlipcon,tlipcon,Minor,Closed,Fixed,16/Apr/12 18:20,28/Sep/15 20:58
Bug,HDFS-3286,12551257,"When the threshold value for balancer is 0(zero) ,unexpected output is displayed","Replication factor =1
Step 1: Start NN,DN1.write 4 GB of data
Step 2: Start DN2
Step 3: issue the balancer command(./hdfs balancer -threshold 0)

The threshold parameter is a fraction in the range of (0%, 100%) with a default value of 10%
When the above scenario is executed the Source DN and Target DN is choosen and the number of bytes to be moved from source to target DN is also calculated .

Then the balancer is exiting with the following message ""No block can be moved. Exiting..."" which is not expected.

{noformat}
HOST-xx-xx-xx-xx:/home/Andreina/APril10/install/hadoop/namenode/bin # ./hdfs balancer -threshold 0
12/04/16 16:22:07 INFO balancer.Balancer: Using a threshold of 0.0
12/04/16 16:22:07 INFO balancer.Balancer: namenodes = [hdfs://HOST-xx-xx-xx-xx:9000]
12/04/16 16:22:07 INFO balancer.Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=0.0]
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
12/04/16 16:22:10 INFO net.NetworkTopology: Adding a new node: /default-rack/yy.yy.yy.yy:50176
12/04/16 16:22:10 INFO net.NetworkTopology: Adding a new node: /default-rack/xx.xx.xx.xx:50010
12/04/16 16:22:10 INFO balancer.Balancer: 1 over-utilized: [Source[xx.xx.xx.xx:50010, utilization=7.212458091389678]]
12/04/16 16:22:10 INFO balancer.Balancer: 1 underutilized: [BalancerDatanode[yy.yy.yy.yy:50176, utilization=4.650670324367203E-5]]
12/04/16 16:22:10 INFO balancer.Balancer: Need to move 1.77 GB to make the cluster balanced.
No block can be moved. Exiting...
Balancing took 5.142 seconds
{noformat}",ashish singhi,andreina,Major,Closed,Fixed,17/Apr/12 15:59,28/Sep/15 20:58
Bug,HDFS-3303,12551642,RemoteEditLogManifest doesn't need to implements Writable,"Since we are using protocol buffers, RemoteEditLogManifest doesn't need to implements Writable.",brandonli,brandonli,Minor,Closed,Fixed,19/Apr/12 16:21,28/Sep/15 20:58
Bug,HDFS-3305,12551662,GetImageServlet should consider SBN a valid requestor in a secure HA setup,Right now only the NN and 2NN are considered valid requestors. This won't work if the ANN and SBN use distinct principal names.,atm,atm,Major,Closed,Fixed,19/Apr/12 19:08,28/Sep/15 20:58
Bug,HDFS-3306,12551701,fuse_dfs: don't lock release operations,"There's no need to lock release operations in FUSE, because release can only be called once on a fuse_file_info structure.",cmccabe,cmccabe,Minor,Closed,Fixed,20/Apr/12 00:32,11/Oct/12 17:46
Bug,HDFS-3308,12551813,hftp/webhdfs can't get tokens if authority has no port,"Token acquisition fails if a hftp or webhdfs filesystem is obtained with no port in the authority.  Building a token service requires a port, and the renewer needs the port.  The default port is not being used when there is no port in the uri.",daryn,daryn,Critical,Closed,Fixed,20/Apr/12 16:44,10/Mar/15 04:35
Bug,HDFS-3309,12551942,HttpFS (Hoop) chmod not supporting octal and sticky bit permissions,"HttpFs supports only the permissions: [0-7][0-7][0-7]

In order to be compatible with webhdfs in needs to understand octal and sticky bit permissions (e.g. 0777, 01777...)

Example of error:
curl -L -X PUT ""http://localhost:14000/webhdfs/v1/user/romain/test?permission=01777&op=SETPERMISSION&user.name=romain"" 
{""RemoteException"":{""message"":""java.lang.IllegalArgumentException: Parameter [permission], invalid value [01777], value must be [default|(-[-r][-w][-x][-r][-w][-x][-r][-w][-x])|[0-7][0-7][0-7]]"",""exception"":""QueryParamException"",""javaClassName"":""com.sun.jersey.api.ParamException$QueryParamException""}}

Works with WebHdfs:
curl -L -X PUT ""http://localhost:50070/webhdfs/v1/user/romain/test?permission=01777&op=SETPERMISSION&user.name=romain"" 
echo $?
0



curl -L -X PUT ""http://localhost:14000/webhdfs/v1/user/romain/test?permission=999999&op=SETPERMISSION&user.name=romain"" 
{""RemoteException"":{""message"":""java.lang.IllegalArgumentException: Parameter [permission], invalid value [999999], value must be [default|(-[-r][-w][-x][-r][-w][-x][-r][-w][-x])|[0-7][0-7][0-7]]"",""exception"":""QueryParamException"",""javaClassName"":""com.sun.jersey.api.ParamException$QueryParamException""}}",tucu00,romainr,Major,Closed,Fixed,20/Apr/12 21:59,28/Sep/15 20:58
Bug,HDFS-3310,12551959,Make sure that we abort when no edit log directories are left,We should make sure to abort when there are no edit log directories left to write to.  It seems that there is at least one case that is slipping through the cracks right now in branch-1.,cmccabe,cmccabe,Major,Closed,Fixed,21/Apr/12 00:57,16/May/12 20:45
Bug,HDFS-3312,12552148,Hftp selects wrong token service,"Hftp tries to select a token based on the non-secure port in the uri, instead of the secure-port.  This breaks hftp on a secure cluster and there is no workaround.",daryn,daryn,Blocker,Closed,Fixed,23/Apr/12 15:29,10/Mar/15 04:36
Bug,HDFS-3314,12552238,HttpFS operation for getHomeDirectory is incorrect,HttpFS is using GETHOMEDIR when it should be using GETHOMEDIRECTORY based on the WebHdfs HTTP API spec,tucu00,tucu00,Major,Closed,Fixed,24/Apr/12 03:35,12/May/16 18:12
Bug,HDFS-3318,12552440,Hftp hangs on transfers >2GB,"Hftp transfers >2GB hang after the transfer is complete.  The problem appears to be caused by java internally using an int for the content length.  When it overflows 2GB, it won't check the bounds of the reads on the input stream.  The client continues reading after all data is received, and the client blocks until the server times out the connection -- _many_ minutes later.  In conjunction with hftp timeouts, all transfers >2G fail with a read timeout.",daryn,daryn,Blocker,Closed,Fixed,24/Apr/12 21:52,10/Mar/15 04:35
Bug,HDFS-3321,12552653,Error message for insufficient data nodes to come out of safemode is wrong.,"Courtesy Andreina,

{quote}
When ""dfs.namenode.safemode.min.datanodes"" is configured to n
So namenode will be in safemode until n DN are registered with the namenode .
But in UI it displays that n+1 datanodes are additionally needed. And ""safe mode will be turned off automatically "" message is also displayed .
==========
""dfs.namenode.safemode.min.datanodes"" =2
Started the NN
The UI report displays that
 
""Safe mode is on.The number of live datanodes 0 needs additional 3 live inimum number 2. Safe mode will turned off automatically"".
{quote}
",raviprak,raviprak,Major,Closed,Fixed,25/Apr/12 13:54,11/Oct/12 17:46
Bug,HDFS-3325,12552784,"When configuring ""dfs.namenode.safemode.threshold-pct"" to a value greater or equal to 1 there is mismatch in the UI report","When dfs.namenode.safemode.threshold-pct is configured to n
Namenode will be in safemode until n percentage of blocks that should satisfy 
    the minimal replication requirement defined by dfs.namenode.replication.min is reported to namenode

But in UI it displays that n percentage of total blocks + 1 blocks  are additionally needed
to come out of the safemode

Scenario 1:
============
Configurations:
dfs.namenode.safemode.threshold-pct = 2
dfs.replication = 2
dfs.namenode.replication.min =2
Step 1: Start NN,DN1,DN2
Step 2: Write a file ""a.txt"" which has got 167 blocks
step 3: Stop NN,DN1,DN2
Step 4: start NN
In UI report the Number of blocks needed to come out of safemode and number of blocks actually present is different.

{noformat}
Cluster Summary
Security is OFF 
Safe mode is ON. The reported blocks 0 needs additional 335 blocks to reach the threshold 2.0000 of total blocks 167. Safe mode will be turned off automatically.
2 files and directories, 167 blocks = 169 total.
Heap Memory used 57.05 MB is 2% of Commited Heap Memory 2 GB. Max Heap Memory is 2 GB. 
Non Heap Memory used 23.37 MB is 17% of Commited Non Heap Memory 130.44 MB. Max Non Heap Memory is 176 MB.{noformat}

Scenario 2:
===========
Configurations:
dfs.namenode.safemode.threshold-pct = 1
dfs.replication = 2
dfs.namenode.replication.min =2
Step 1: Start NN,DN1,DN2
Step 2: Write a file ""a.txt"" which has got 167 blocks
step 3: Stop NN,DN1,DN2
Step 4: start NN
In UI report the Number of blocks needed to come out of safemode and number of blocks actually present is different

{noformat}
Cluster Summary
Security is OFF 
Safe mode is ON. The reported blocks 0 needs additional 168 blocks to reach the threshold 1.0000 of total blocks 167. Safe mode will be turned off automatically.
2 files and directories, 167 blocks = 169 total.
Heap Memory used 56.2 MB is 2% of Commited Heap Memory 2 GB. Max Heap Memory is 2 GB. 
Non Heap Memory used 23.37 MB is 17% of Commited Non Heap Memory 130.44 MB. Max Non Heap Memory is 176 MB.{noformat}
",andreina,andreina,Minor,Resolved,Fixed,26/Apr/12 04:07,21/Dec/18 02:53
Bug,HDFS-3326,12552821,Append enabled log message uses the wrong variable,"""dfs.support.append"" is set to true
started NN in non-HA mode

At the NN side log the append enable is set to false.

This is because in code append enabled is set to HA enabled value.Since Started NN in non-HA mode the value for append is false
Code:
=====
{noformat}
this.supportAppends = conf.getBoolean(DFS_SUPPORT_APPEND_KEY, DFS_SUPPORT_APPEND_DEFAULT);
      LOG.info(""Append Enabled: "" + haEnabled);{noformat}
NN logs
========
{noformat}
2012-04-25 21:11:09,693 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2012-04-25 21:11:09,702 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: false{noformat}
",mjacobs,andreina,Trivial,Closed,Fixed,26/Apr/12 09:33,28/Sep/15 20:58
Bug,HDFS-3328,12552885,NPE in DataNode.getIpcPort,"While running the tests, I have seen this exceptions.Tests passed. 
Not sure this is a problem.

{quote}
2012-04-26 23:15:51,763 WARN  hdfs.DFSClient (DFSOutputStream.java:run(710)) - DFSOutputStream ResponseProcessor exception  for block BP-1372255573-49.249.124.17-1335462329685:blk_-8435040801555580201_1005
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:657)
Exception in thread ""DataXceiver for client /127.0.0.1:52323 [Cleaning up]"" java.lang.NullPointerException
	at org.apache.hadoop.ipc.Server$Listener.getAddress(Server.java:669)
	at org.apache.hadoop.ipc.Server.getListenerAddress(Server.java:1988)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getIpcPort(DataNode.java:882)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getDisplayName(DataNode.java:863)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:171)
	at java.lang.Thread.run(Unknown Source){quote}

",eli,umamaheswararao,Minor,Closed,Fixed,26/Apr/12 17:51,28/Sep/15 20:58
Bug,HDFS-3330,12553066,"If GetImageServlet throws an Error or RTE, response has HTTP ""OK"" status","Currently in GetImageServlet, we catch Exception but not other Errors or RTEs. So, if the code ends up throwing one of these exceptions, the ""response.sendError()"" code doesn't run, but the finally clause does run. This results in the servlet returning HTTP 200 OK and an empty response, which causes the client to think it got a successful image transfer.",tlipcon,tlipcon,Critical,Closed,Fixed,26/Apr/12 23:59,17/Oct/12 18:27
Bug,HDFS-3331,12553068,setBalancerBandwidth do not checkSuperuserPrivilege,"- setBalancerBandwidth from HDFS-2202 should checkSuperuserPrivilege
- finalizeUpgrade should acquire the write lock.",szetszwo,szetszwo,Major,Closed,Fixed,27/Apr/12 00:14,11/Oct/12 17:46
Bug,HDFS-3332,12553084,NullPointerException in DN when directoryscanner is trying to report bad blocks,"There is 1 NN and 1 DN (NN is started with HA conf)
I corrupted 1 block and found 
{code}
2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(401)) - BlockReport of 2 blocks took 0 msec to generate and 5 msecs for RPC and NN processing
2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(420)) - sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b756db3
2012-04-27 09:59:01,726 INFO  datanode.DirectoryScanner (DirectoryScanner.java:scan(390)) - BlockPool BP-2087868617-10.18.40.95-1335500488012 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:1
2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1366)) - Updating size of block -4466699320171028643 from 1024 to 1034
2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1374)) - Reporting the block blk_-4466699320171028643_1004 as corrupt due to length mismatch
2012-04-27 09:59:01,728 DEBUG ipc.Client (Client.java:sendParam(807)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root sending #257
2012-04-27 09:59:01,730 DEBUG ipc.Client (Client.java:receiveResponse(848)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root got value #257
2012-04-27 09:59:01,730 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(193)) - Call: reportBadBlocks 2
2012-04-27 09:59:01,731 ERROR datanode.DirectoryScanner (DirectoryScanner.java:run(288)) - Exception during DirectoryScanner execution - will continue next cycle
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)
	at org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)
	at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)
	at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{code}

Here when Directory scanner is trying to report badblock we got a NPE.",amithdk,amithdk,Major,Closed,Fixed,27/Apr/12 04:42,12/May/16 18:14
Bug,HDFS-3334,12553138,ByteRangeInputStream leaks streams,{{HftpFileSystem.ByteRangeInputStream}} does not implement {{close}} so it leaks the underlying stream(s).,daryn,daryn,Major,Closed,Fixed,27/Apr/12 13:29,10/Mar/15 04:36
Bug,HDFS-3336,12553504,hdfs launcher script will be better off not special casing namenode command with regards to hadoop.security.logger,"All of the launcher scripts (except hdfs when called with a namenode argument) are defaulting to INFO,NullAppender setting for hadoop.security.logger. The only codepath where the default is INFO,RFAS is via hadoop-daemon.sh. This is exactly the right thing to do since only daemons are guaranteed to be executed under the proper user account (the one that has write/create access to the locations specified in ${hadoop.log.dir}/*).

It would be nice for 'hdfs namenode' to follow the suit and not produce spurious warnings about not being able to write to log locations when executed from under regular accounts.",rvs,rvs,Minor,Closed,Fixed,01/May/12 16:27,28/Sep/15 20:58
Bug,HDFS-3344,12553566,Unreliable corrupt blocks counting in TestProcessCorruptBlocks,"It failed in [build #2356|https://builds.apache.org/job/PreCommit-HDFS-Build/2356//testReport/org.apache.hadoop.hdfs.server.namenode/TestProcessCorruptBlocks/testWithReplicationFactorAsOne/]:
{noformat}
java.lang.AssertionError: expected:<0> but was:<1>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.junit.Assert.assertEquals(Assert.java:454)
	at org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks.testWithReplicationFactorAsOne(TestProcessCorruptBlocks.java:192)
	...
{noformat}",kihwal,szetszwo,Major,Closed,Fixed,01/May/12 23:07,04/Sep/14 00:54
Bug,HDFS-3350,12553664,findbugs warning: INodeFileUnderConstruction doesn't override INodeFile.equals(Object),"Somehow this findbugs warning got triggered in the recent builds, e.g. [build #2361|https://builds.apache.org/job/PreCommit-HDFS-Build/2361/artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html].  It probably was triggered by HDFS-3339 although HDFS-3339 did not introduce the bug.",szetszwo,szetszwo,Major,Closed,Fixed,02/May/12 20:36,28/Sep/15 20:58
Bug,HDFS-3351,12553671,NameNode#initializeGenericKeys should always set fs.defaultFS regardless of whether HA or Federation is enabled,"NameNode#initializeGenericKeys exits early if neither a nameservice nor NN ID is passed. However, this method also serves to set fs.defaultFS in the configuration object stored by the NN to the NN RPC address after generic keys have been configured. This should be done in all cases.",atm,atm,Major,Closed,Fixed,02/May/12 21:35,28/Sep/15 20:58
Bug,HDFS-3357,12553709,DataXceiver reads from client socket with incorrect/no timeout,"In DataXceiver, we currently use Socket.setSoTimeout to try to manage the read timeout when switching between reading the initial opCode, reading a keepalive opcode, and reading the status after a successfully sent block. However, since all of these reads use the same underlying DataInputStream, the change to the socket timeout isn't respected. Thus, they all occur with whatever timeout is set on the socket at the time of DataXceiver construction. In practice this turns out to be 0, which can cause infinitely hung xceivers.",tlipcon,tlipcon,Critical,Closed,Fixed,03/May/12 05:15,28/Sep/15 20:58
Bug,HDFS-3359,12553728,DFSClient.close should close cached sockets,"Some applications like the TT/JT (pre-2.0) and probably the RM/NM cycle through DistributedFileSystem objects reasonably frequently. So long as they call close() it isn't a big problem, except that currently DFSClient.close() doesn't explicitly close the SocketCache. So unless a full GC runs (causing the references to get finalized), many SocketCaches can get orphaned, each with many open sockets inside. We should fix the close() function to close all cached sockets.",tlipcon,tlipcon,Critical,Closed,Fixed,03/May/12 09:43,02/Feb/15 19:55
Bug,HDFS-3368,12553875,Missing blocks due to bad DataNodes coming up and down.,All replicas of a block can be removed if bad DataNodes come up and down during cluster restart resulting in data loss.,shv,shv,Major,Closed,Fixed,04/May/12 07:11,12/May/16 18:17
Bug,HDFS-3373,12553982,FileContext HDFS implementation can leak socket caches,"As noted by Nicholas in HDFS-3359, FileContext doesn't have a close() method, and thus never calls DFSClient.close(). This means that, until finalizers run, DFSClient will hold on to its SocketCache object and potentially have a lot of outstanding sockets/fds held on to.",johnvijoe,tlipcon,Major,Closed,Fixed,04/May/12 18:40,12/May/16 18:17
Bug,HDFS-3374,12554011,hdfs' TestDelegationToken fails intermittently with a race condition,"The testcase is failing because the MiniDFSCluster is shutdown before the secret manager can change the key, which calls system.exit with no edit streams available.

{code}

    [junit] 2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1
    [junit] 2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible
    [junit] java.lang.Exception: No edit streams are accessible
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)
    [junit]     at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] Running org.apache.hadoop.hdfs.security.TestDelegationToken
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hdfs.security.TestDelegationToken FAILED (crashed)
{code}",omalley,omalley,Major,Resolved,Fixed,04/May/12 22:08,13/Feb/13 23:19
Bug,HDFS-3376,12554060,DFSClient fails to make connection to DN if there are many unusable cached sockets,"After fixing the datanode side of keepalive to properly disconnect stale clients, (HDFS-3357), the client side has the following issue: when it connects to a DN, it first tries to use cached sockets, and will try a configurable number of sockets from the cache. If there are more cached sockets than the configured number of retries, and all of them have been closed by the datanode side, then the client will throw an exception and mark the replica node as dead.",tlipcon,tlipcon,Critical,Closed,Fixed,05/May/12 21:11,28/Sep/15 20:58
Bug,HDFS-3384,12554295,DataStreamer thread should be closed immediatly when failed to setup a PipelineForAppendOrRecovery,"Scenraio:
=========
write a file
corrupt block manually
call append..

{noformat}

2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)
2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing
java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
{noformat}",umamaheswararao,brahmareddy,Major,Resolved,Fixed,08/May/12 09:01,30/Aug/16 01:42
Bug,HDFS-3385,12554317,ClassCastException when trying to append a file,"When I try to append a file I got 

{noformat}
2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty
Exception in thread ""main"" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)
        ...
	at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)
	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)
	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)
	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)
	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)
	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)
	at org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)
{noformat}",szetszwo,brahmareddy,Major,Closed,Fixed,08/May/12 12:45,07/Sep/12 21:09
Bug,HDFS-3391,12554421,TestPipelinesFailover#testLeaseRecoveryAfterFailover is failing,"Running org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.208 sec <<< FAILURE!
--
Running org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 81.195 sec <<< FAILURE!
--",tlipcon,acmurthy,Critical,Closed,Fixed,09/May/12 03:17,07/Sep/12 21:09
Bug,HDFS-3395,12554621,NN doesn't start with HA+security enabled and HTTP address set to 0.0.0.0,"DFSUtil#substituteForWildcardAddress subs in a default hostname if the given hostname is 0.0.0.0. However, this function throws an exception if the given hostname is set to 0.0.0.0 and security is enabled, regardless of whether the default hostname is also 0.0.0.0. This function shouldn't throw an exception unless both addresses are set to 0.0.0.0.",atm,atm,Major,Closed,Fixed,10/May/12 05:01,28/Sep/15 20:58
Bug,HDFS-3396,12554628,FUSE build fails on Ubuntu 12.04,"The HDFS FUSE-dfs build fails on Ubuntu 12.04 (and probably other OSes) with a message like this:
{code}
/home/petru/work/ubeeko/hadoo.apache.org/0.23/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.c:27:

undefined reference to `fuse_get_context'
{code}",cmccabe,cmccabe,Minor,Closed,Fixed,10/May/12 05:58,28/Sep/15 20:58
Bug,HDFS-3398,12554693,Client will not retry when primaryDN is down once it's just got pipeline,"Scenario:
=========
Start NN and three DN""S


Get the datanode to which blocks has to be replicated.
from 
{code}
nodes = nextBlockOutputStream(src);

{code}
Before start writing to the DN ,kill the primary DN.
{code}
// write out data to remote datanode
          blockStream.write(buf.array(), buf.position(), buf.remaining());
          blockStream.flush();
{code}

Now write will fail with the exception 

{noformat}
2012-05-10 14:21:47,993 WARN  hdfs.DFSClient (DFSOutputStream.java:run(552)) - DataStreamer Exception
java.io.IOException: An established connection was aborted by the software in your host machine
	at sun.nio.ch.SocketDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(Unknown Source)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(Unknown Source)
	at sun.nio.ch.IOUtil.write(Unknown Source)
	at sun.nio.ch.SocketChannelImpl.write(Unknown Source)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:513)

{noformat}


.

",amithdk,brahmareddy,Minor,Closed,Fixed,10/May/12 14:56,04/Sep/14 00:59
Bug,HDFS-3402,12554585,Fix hdfs scripts for secure datanodes,"Starting secure datanode gives out the following error :

Error thrown :
09/04/2012 12:09:30 2524 jsvc error: Invalid option -server
09/04/2012 12:09:30 2524 jsvc error: Cannot parse command line arguments",benoyantony,benoyantony,Minor,Closed,Fixed,09/May/12 23:17,15/May/13 05:16
Bug,HDFS-3413,12555043,TestFailureToReadEdits timing out,"HDFS-3026 made it so that an HA NN that does not fully complete an HA state transition will exit immediately. TestFailureToReadEdits has a test case which causes an incomplete state transition, thus causing a JVM exit in the test.",atm,tlipcon,Critical,Closed,Fixed,13/May/12 01:17,12/May/16 18:14
Bug,HDFS-3414,12555049,Balancer does not find NameNode if rpc-address or servicerpc-address are not set in client configs,The balancer should also try to balance the NN if it's only specified via fs.defaultFS in the client config.,atm,atm,Minor,Closed,Fixed,13/May/12 03:31,07/Sep/12 21:09
Bug,HDFS-3415,12555068,"During NameNode starting up, it may pick wrong storage directory inspector when the layout versions of the storage directories are different","Scenario:
=========
start Namenode and datanode by configuring three storage dir's for namenode
write 10 files
edit version file of one of the storage dir and give layout version as 123 which different with default(-40).
Stop namenode
start Namenode.


Then I am getting follwong exception...


{noformat}
2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)
	at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)
	at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)
	at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)
2012-05-13 19:01:41,485 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 

{noformat}
",brandonli,brahmareddy,Major,Closed,Fixed,13/May/12 13:44,12/May/16 18:14
Bug,HDFS-3422,12555290,TestStandbyIsHot timeouts too aggressive,"I've seen TestStandbyIsHot timeout a few times waiting for replication to change, but when I look at the logs, it appears everything is fine. The block deletions are just a bit slow in being processed.

To improve the test we should either increase the timeouts, or explicitly trigger heartbeats and replication work after changing the desired replication levels.",tlipcon,tlipcon,Minor,Closed,Fixed,15/May/12 06:24,07/Sep/12 21:09
Bug,HDFS-3424,12555833,TestDatanodeBlockScanner and TestReplication fail intermittently on Windows,"The tests change the block length to corrupt the data block. If the block file is opened by the datanode then the test can concurrently modify it on Linux but such concurrent modification is not allowed by the default permissions on Windows. Since this is more of a test issue, the fix would be to have the tests make sure that the block is not open concurrently.",bikassaha,bikassaha,Major,Resolved,Fixed,15/May/12 17:21,06/Jun/12 01:55
Bug,HDFS-3428,12555920,move DelegationTokenRenewer to common,"Currently DelegationTokenRenewer is in HDFS, it seems more appropriate for it to be in common so it could be used by other FS implementations not necessary depending on HDFS (ie HttpFS)",tucu00,tucu00,Major,Closed,Fixed,15/May/12 22:41,11/Oct/12 17:46
Bug,HDFS-3429,12555960,DataNode reads checksums even if client does not need them,"Currently, even if the client does not want to verify checksums, the datanode reads them anyway and sends them over the wire. This means that performance improvements like HBase's application-level checksums don't have much benefit when reading through the datanode, since the DN is still causing seeks into the checksum file.

(Credit goes to Dhruba for discovering this - filing on his behalf)",tlipcon,tlipcon,Major,Closed,Fixed,16/May/12 06:10,03/Sep/14 23:09
Bug,HDFS-3432,12556144,TestDFSZKFailoverController tries to fail over too early,"This test starts a cluster, then starts the ZKFCs, then tries to issue a manual failover. But, it doesn't wait until the ZKFCs are fully done starting up. So, I see a test run occasionally fail because the failover says that the NN is not ready yet. The test just needs to wait until the ZKFCs are in ""healthy"" state before proceeding.",tlipcon,tlipcon,Minor,Resolved,Fixed,16/May/12 22:58,17/May/12 00:37
Bug,HDFS-3433,12556155,GetImageServlet should allow administrative requestors when security is enabled,"Currently the GetImageServlet only allows the NN and checkpointing nodes to connect. Since we now have the fetchImage command in DFSAdmin, we should also allow administrative requests as well.",atm,atm,Major,Closed,Fixed,17/May/12 00:41,07/Sep/12 21:09
Bug,HDFS-3434,12556158,InvalidProtocolBufferException when visiting DN browseDirectory.jsp,"The nnaddr field on dfsnodelist.jsp is getting set incorrectly. When selecting the ""haus04"" under the ""Node"" table I get a link with the http address which is bogus (the wildcard/http port not the nn rpc addr), which results in an error of ""Call From haus04.mtv.cloudera.com/172.29.122.94 to 0.0.0.0:10070 failed on connection exception: java.net.ConnectException: Connection refused"". The browse this file system link works.",eli,eli2,Major,Resolved,Fixed,17/May/12 00:48,22/Jun/12 05:10
Bug,HDFS-3436,12556220,adding new datanode to existing  pipeline fails in case of Append/Recovery,"Scenario:
=========

1. Cluster with 4 DataNodes.
2. Written file to 3 DNs, DN1->DN2->DN3
3. Stopped DN3,
Now Append to file is failing due to addDatanode2ExistingPipeline is failed.

 *CLinet Trace* 
{noformat}
2012-04-24 22:06:09,947 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1063)) - Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as *******:50010
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
2012-04-24 22:06:09,947 WARN  hdfs.DFSClient (DFSOutputStream.java:setupPipelineForAppendOrRecovery(916)) - Error Recovery for block BP-1023239-10.18.40.233-1335275282109:blk_296651611851855249_1253 in pipeline *****:50010, ******:50010, *****:50010: bad datanode ******:50010
2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1515)) - Error while syncing
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
{noformat}

 *DataNode Trace*  

{noformat}

2012-05-17 15:39:12,261 ERROR datanode.DataNode (DataXceiver.java:run(193)) - host0.foo.com:49744:DataXceiver error processing TRANSFER_BLOCK operation  src: /127.0.0.1:49811 dest: /127.0.0.1:49744
java.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW
  getNumBytes()     = 1024
  getBytesOnDisk()  = 1024
  getVisibleLength()= 1024
  getVolume()       = E:\MyWorkSpace\branch-2\Test\build\test\data\dfs\data\data1\current
  getBlockFile()    = E:\MyWorkSpace\branch-2\Test\build\test\data\dfs\data\data1\current\BP-2001850558-xx.xx.xx.xx-1337249347060\current\rbw\blk_-8165642083860293107
  bytesAcked=1024
  bytesOnDisk=102
at org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)
	at java.lang.Thread.run(Unknown Source)
{noformat}",vinayakumarb,brahmareddy,Major,Closed,Fixed,17/May/12 10:36,12/May/16 18:18
Bug,HDFS-3440,12556330,should more effectively limit stream memory consumption when reading corrupt edit logs,"Currently, we do in.mark(100MB) before reading an opcode out of the edit log.  However, this could result in us usin all of those 100 MB when reading bogus data, which is not what we want.  It also could easily make some corrupt edit log files unreadable.

We should have a stream limiter interface, that causes a clean IOException when we're in this situation, and does not result in huge memory consumption.",cmccabe,cmccabe,Minor,Closed,Fixed,18/May/12 00:27,03/Mar/14 19:16
Bug,HDFS-3442,12556364,Incorrect count for Missing Replicas in FSCK report,"Scenario:
Cluster running in HA mode with 2 DNs. Files are written with replication factor as 3.
There are 7 blocks in cluster.
FSCK report is including all blocks in UnderReplicated Blocks as well as Missing Replicas.

HOST-XX-XX-XX-102:/home/Apr4/hadoop-2.0.0-SNAPSHOT/bin # ./hdfs fsck /
Connecting to namenode via http://XX.XX.XX.55:50070
FSCK started by root (auth:SIMPLE) from /XX.XX.XX.102 for path / at Wed Apr 04 17:28:37 IST 2012
.
/1:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_2551710840802340037_1002. Target Replicas is 3 but found 2 replica(s).
.
/2:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_-3851276776144500288_1004. Target Replicas is 3 but found 2 replica(s).
.
/3:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_-3210606555285049524_1006. Target Replicas is 3 but found 2 replica(s).
.
/4:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_4028835120510075310_1008. Target Replicas is 3 but found 2 replica(s).
.
/5:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_-5238093749956876969_1010. Target Replicas is 3 but found 2 replica(s).
.
/testrenamed/file1renamed:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_-5669194716756513504_1012. Target Replicas is 3 but found 2 replica(s).
.
/testrenamed/file2:  Under replicated BP-534619337-XX.XX.XX.55-1333526344705:blk_8510284478280941311_1014. Target Replicas is 3 but found 2 replica(s).
Status: HEALTHY
 Total size:    33215 B
 Total dirs:    3
 Total files:   7 (Files currently being written: 1)
 Total blocks (validated):      7 (avg. block size 4745 B)
 Minimally replicated blocks:   7 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       7 (100.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     2.0
 Corrupt blocks:                0
 Missing replicas:              7 (50.0 %)
 Number of data-nodes:          2
 Number of racks:               1
FSCK ended at Wed Apr 04 17:28:37 IST 2012 in 2 milliseconds


The filesystem under path '/' is HEALTHY

Also it indicates a measure as 50% in brackets (There are only 7 blocks in cluster and so if all 7 are included as Missing replicas it should be 100%)",andrew.wang,suja,Minor,Closed,Fixed,18/May/12 09:17,11/Oct/12 17:46
Bug,HDFS-3443,12556383,Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer,"Start NN
Let NN standby services be started.
Before the editLogTailer is initialised start ZKFC and allow the activeservices start to proceed further.


Here editLogTailer.catchupDuringFailover() will throw NPE.

void startActiveServices() throws IOException {
    LOG.info(""Starting services required for active state"");
    writeLock();
    try {
      FSEditLog editLog = dir.fsImage.getEditLog();
      
      if (!editLog.isOpenForWrite()) {
        // During startup, we're already open for write during initialization.
        editLog.initJournalsForWrite();
        // May need to recover
        editLog.recoverUnclosedStreams();
        
        LOG.info(""Catching up to latest edits from old active before "" +
            ""taking over writer role in edits logs."");
        editLogTailer.catchupDuringFailover();


{noformat}
2012-05-18 16:51:27,585 WARN org.apache.hadoop.ipc.Server: IPC Server Responder, call org.apache.hadoop.ha.HAServiceProtocol.getServiceStatus from XX.XX.XX.55:58003: output error
2012-05-18 16:51:27,586 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from XX.XX.XX.55:58004: error: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)
2012-05-18 16:51:27,586 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2092)
	at org.apache.hadoop.ipc.Server.access$2000(Server.java:107)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:930)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:994)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1738)
{noformat}",vinayakumarb,suja,Major,Closed,Fixed,18/May/12 11:30,30/Aug/16 01:42
Bug,HDFS-3444,12556435,hdfs groups command doesn't work with security enabled,"When one tries to run `hdfs groups' with security enabled, you'll get an error like the following:

{noformat}
java.io.IOException: Failed to specify server's Kerberos principal name;
{noformat}",atm,atm,Major,Closed,Fixed,18/May/12 18:48,07/Sep/12 21:09
Bug,HDFS-3446,12556495,HostsFileReader silently ignores bad includes/excludes,"The HostsFileReader silently fails if the includes or excludes files do
not exist or are not readable. The current behavior is to overwrite the
existing set of hosts or excluded hosts, regardless of whether or not
the includes/excludes file exists.

This behavior was introduced in HADOOP-5643 to support updating the job
tracker's node lists. The HostsFileReader is no longer used by the job
tracker. If this behavior was intentional, it no longer seems necessary
for any reason. The HostsFileReader is still used by NodeListManager as
well as the DatanodeManager, and in both cases, throwing an exception
when the include/exclude files aren't found/readable is desirable.

We should validate the given includes and excludes files before using
them.",mjacobs,mjacobs,Major,Closed,Fixed,19/May/12 00:06,11/Oct/12 17:46
Bug,HDFS-3453,12556860,HDFS does not use ClientProtocol in a backward-compatible way,"HDFS-617 was brought into branch-0.20-security/branch-1 to support non-recursive create, along with HADOOP-6840 and HADOOP-6886. However, the changes in HDFS was done in an incompatible way, making the client unusable against older clusters, even when plain old create() is called. This is because DFS now internally calls create() through the newly introduced method. By simply changing how the methods are wired internally, we can remove this limitation. We may eventually switch back to the approach in HDFS-617 when the majority of users adopt branch-1 based releases.",kihwal,kihwal,Major,Closed,Fixed,22/May/12 13:35,17/Oct/12 18:27
Bug,HDFS-3460,12557071,HttpFS proxyuser validation with Kerberos ON uses full principal name,"The HttpFSServer.getEffectiveUser() method uses the principal name for proxy user verification. If the Kerberos is ON and the proxy user is a service principal (NAME/HOST) then the verification fails, instead the short name (just NAME) should be used.",tucu00,tucu00,Critical,Closed,Fixed,23/May/12 21:05,11/Oct/12 17:46
Bug,HDFS-3461,12557093,HFTP should use the same port & protocol for getting the delegation token,"Currently, hftp uses http to the Namenode's https port, which doesn't work.",omalley,omalley,Major,Closed,Fixed,23/May/12 22:44,26/Sep/13 21:52
Bug,HDFS-3462,12558043,TestDFSClientRetries.busyTest() should restore default xceiver count in the config.,"TestDFSClientRetries.busyTest() sets maximum xceiver count in the config to a small value, but does not restore it back to the default. As the result subsequent tests can fail, especially on slower or busy machines.",phatak.dev,shv,Major,Resolved,Fixed,25/May/12 06:25,12/May/16 18:17
Bug,HDFS-3466,12558201,The SPNEGO filter for the NameNode should come out of the web keytab file,"Currently, the spnego filter uses the DFS_NAMENODE_KEYTAB_FILE_KEY to find the keytab. It should use the DFS_WEB_AUTHENTICATION_KERBEROS_KEYTAB_KEY to do it.",omalley,omalley,Major,Closed,Fixed,25/May/12 21:55,11/Oct/12 17:46
Bug,HDFS-3469,12558240,"start-dfs.sh will start zkfc, but stop-dfs.sh will not stop zkfc similarly.","start-dfs.sh will start the zkfc process. 
Similarly stop-dfs.sh should stop zkfc process.",vinayakumarb,vinayakumarb,Minor,Closed,Fixed,26/May/12 11:30,12/May/16 18:18
Bug,HDFS-3480,12558686,Multiple SLF4J binding warning,"Looks like HDFS-3136 was re-introduced on trunk, don't see it on branch-2.

From a trunk build from today:

hadoop-3.0.0-SNAPSHOT $ ./bin/hadoop fs -ls -R /
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/eli/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/eli/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
",vinayakumarb,eli2,Major,Closed,Fixed,30/May/12 22:24,11/Oct/12 17:46
Bug,HDFS-3482,12558818,hdfs balancer throws ArrayIndexOutOfBoundsException if option is specified without arguments,"When running the hdfs balancer with an option but no argument, we run into an ArrayIndexOutOfBoundsException. It's preferable to print the usage.

{noformat}
bash-3.2$ hdfs balancer -threshold
Usage: java Balancer
    [-policy <policy>]	the balancing policy: datanode or blockpool
    [-threshold <threshold>]	Percentage of disk capacity
Balancing took 261.0 milliseconds
12/05/31 09:38:46 ERROR balancer.Balancer: Exiting balancer due an exception
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.parse(Balancer.java:1505)
	at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:1482)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:1555)
bash-3.2$ hdfs balancer -policy
Usage: java Balancer
    [-policy <policy>]	the balancing policy: datanode or blockpool
    [-threshold <threshold>]	Percentage of disk capacity
Balancing took 261.0 milliseconds
12/05/31 09:39:03 ERROR balancer.Balancer: Exiting balancer due an exception
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.parse(Balancer.java:1520)
	at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:1482)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:1555)
{noformat}",phatak.dev,schu,Minor,Closed,Fixed,31/May/12 16:48,01/Dec/14 03:07
Bug,HDFS-3484,12558839,hdfs fsck doesn't work if NN HTTP address is set to 0.0.0.0 even if NN RPC address is configured,"The default NN HTTP address is 0.0.0.0. Clients which need to connect to the HTTP address (e.g. fsck and fetchImage) need an address which is actually resolvable, however. If the configured NN HTTP address is set to 0.0.0.0, these clients should fall back on using the hostname configured for the RPC address, with the port configured for the HTTP address.",atm,atm,Minor,Closed,Fixed,31/May/12 18:33,11/Oct/12 17:46
Bug,HDFS-3485,12558841,DataTransferThrottler will over-throttle when currentTimeMillis jumps,"When the system clock is set backwards, DataTransferThrottler will simply pause until the clock reaches the end of the previously calculated transfer period:
{code}
    this.curPeriodStart = System.currentTimeMillis();
...
    while (curReserve <= 0) {
      long now = System.currentTimeMillis();
      long curPeriodEnd = curPeriodStart + period;
      if ( now < curPeriodEnd ) {
        try {
          wait( curPeriodEnd - now );
{code}

Instead of using currentTimeMillis() which is affected by system-clock-changes, this code should use nanoTime which ticks forward monotonically.",adi2,adi2,Minor,Closed,Fixed,31/May/12 18:41,11/Oct/12 17:46
Bug,HDFS-3486,12558864,offlineimageviewer can't read fsimage files that contain persistent delegation tokens,"OfflineImageViewer (oiv) crashes when trying to read fsimage files that contain persistent delegation tokens.

Example stack trace:
{code}
Caused by: java.lang.IndexOutOfBoundsException
        at java.io.DataInputStream.readFully(DataInputStream.java:175)
        at org.apache.hadoop.io.Text.readFields(Text.java:284)
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.readFields(AbstractDelegationTokenIdentifier.java:178)
        at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDelegationTokens(ImageLoaderCurrent.java:222)
        at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:186)
        at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:129)
{code}

The oiv and loadFSImage code paths are separate.  The issue here seems to be that the loadFSImage code path has diverged from the oiv code path.

On the loadFSImage code path (from FSImageFormat#loadCurrentTokens):
{code}
  /**
   * Private helper methods to load Delegation tokens from fsimage
   */
  private synchronized void loadCurrentTokens(DataInputStream in)
      throws IOException {
    int numberOfTokens = in.readInt();
    for (int i = 0; i < numberOfTokens; i++) {
      DelegationTokenIdentifier id = new DelegationTokenIdentifier();
      id.readFields(in);
      long expiryTime = in.readLong();
      addPersistedDelegationToken(id, expiryTime);
    }
  }
{code}
Notice how it loads a 4-byte int after every DelegationTokenIdentifier.

On the oiv code path (from ImageLoaderCurrent#processDelegationTokens):
{code}
    int numDTokens = in.readInt();
    v.visitEnclosingElement(ImageElement.DELEGATION_TOKENS,
        ImageElement.NUM_DELEGATION_TOKENS, numDTokens);
    for(int i=0; i<numDTokens; i++){
      DelegationTokenIdentifier id = new  DelegationTokenIdentifier();
      id.readFields(in);
      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER, id.toString());
    }
{code}

Notice how it does *not* load a 4-byte int after every DelegationTokenIdentifier.

This bug seems to have been introduced by change 916534, the same change which introduced persistent delegation tokens.  So I don't think oiv was ever able to decode them in the past.",cmccabe,cmccabe,Minor,Closed,Fixed,31/May/12 21:09,25/Feb/13 18:56
Bug,HDFS-3487,12558867,offlineimageviewer should give byte offset information when it encounters an exception,"OfflineImageViewer should identify the byte offset that it failed at when it encounters an exception.  Otherwise, we don't know which part of the fsimage file was at fault.",cmccabe,cmccabe,Minor,Closed,Fixed,31/May/12 21:20,11/Oct/12 17:46
Bug,HDFS-3490,12558996,DN WebHDFS methods throw NPE if Namenode RPC address param not specified,"If NamenodeRpcAddressParam is not passed to the datanode webhdfs methods, an NPE will be generated trying to use it. We should check the parameter and throw a more appropriate error message when it is missing.",szetszwo,tlipcon,Minor,Closed,Fixed,01/Jun/12 19:17,11/Oct/12 17:46
Bug,HDFS-3491,12559008,HttpFs does not set permissions correctly,"HttpFs seems to have these problems:
# can't set permissions to 777 at file creation or 1777 with setpermission
# does not accept 01777 permissions (which is valid in WebHdfs)

WebHdfs
curl -X PUT ""http://localhost:50070/webhdfs/v1/tmp/test-perm-webhdfs?permission=1777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}
curl  ""http://localhost:50070/webhdfs/v1/tmp/test-perm-webhdfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1338581075040,""owner"":""hue"",""pathSuffix"":"""",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""}}

curl -X PUT ""http://localhost:50070/webhdfs/v1/tmp/test-perm-webhdfs?permission=01777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}


HttpFs
curl -X PUT ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?permission=1777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}
curl  ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""pathSuffix"":"""",""type"":""DIRECTORY"",""length"":0,""owner"":""hue"",""group"":""supergroup"",""permission"":""755"",""accessTime"":0,""modificationTime"":1338580912205,""blockSize"":0,""replication"":0}}

curl -X PUT  ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=SETPERMISSION&PERMISSION=1777&user.name=hue&doas=hue""
curl  ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""pathSuffix"":"""",""type"":""DIRECTORY"",""length"":0,""owner"":""hue"",""group"":""supergroup"",""permission"":""777"",""accessTime"":0,""modificationTime"":1338581075040,""blockSize"":0,""replication"":0}}

curl -X PUT ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?permission=01777&op=MKDIRS&user.name=hue&doas=hue""
{""RemoteException"":{""message"":""java.lang.IllegalArgumentException: Parameter [permission], invalid value [01777], value must be [default|[0-1]?[0-7][0-7][0-7]]"",""exception"":""QueryParamException"",""javaClassName"":""com.sun.jersey.api.ParamException$QueryParamException""}}
",tucu00,romainr,Major,Closed,Fixed,01/Jun/12 21:10,11/Oct/12 17:46
Bug,HDFS-3492,12559099,fix some misuses of InputStream#skip,"It seems that we have a few cases where programmers are calling InputStream#skip and not handling ""short skips.""  Unfortunately, the skip method is documented and implemented so that it doesn't actually skip the requested number of bytes, but simply tries to skip at most that amount of bytes.  A better name probably would have been trySkip or similar.

It seems like most of the time when the argument to skip is small enough, we'll succeed almost all of the time.  This is no doubt an implementation artifact of some of the popular stream implementations.  This tends to hide the bug-- however, it is still waiting to emerge at some point if those implementations ever change or if buffer sizes are adjusted, etc.

All of these cases can be fixed by calling IOUtils#skipFully to get the behavior that the programmer expects-- i.e., skipping by the specified amount.",cmccabe,cmccabe,Minor,Resolved,Fixed,03/Jun/12 20:05,14/Jul/12 14:04
Bug,HDFS-3493,12559116,Invalidate excess corrupted blocks as long as minimum replication is satisfied,"replication factor= 3, block report interval= 1min and start NN and 3DN

Step 1:Write a file without close and do hflush (Dn1,DN2,DN3 has blk_ts1)
Step 2:Stopped DN3
Step 3:recovery happens and time stamp updated(blk_ts2)
Step 4:close the file
Step 5:blk_ts2 is finalized and available in DN1 and Dn2
Step 6:now restarted DN3(which has got blk_ts1 in rbw)

From the NN side there is no cmd issued to DN3 to delete the blk_ts1 . But ask DN3 to make the block as corrupt .
Replication of blk_ts2 to DN3 is not happened.

NN logs:
========
{noformat}
INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_3927215081484173742 to add as corrupt on XX.XX.XX.XX:50276 by /XX.XX.XX.XX because reported RWR replica with genstamp 1007 does not match COMPLETE block's genstamp in block map 1008
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* processReport: from DatanodeRegistration(XX.XX.XX.XX, storageID=DS-443871816-XX.XX.XX.XX-50276-1336829714197, infoPort=50275, ipcPort=50277, storageInfo=lv=-40;cid=CID-e654ac13-92dc-4f82-a22b-c0b6861d06d7;nsid=2063001898;c=0), blocks: 2, processing time: 1 msecs
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* Removing block blk_3927215081484173742_1008 from neededReplications as it has enough replicas.

INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_3927215081484173742 to add as corrupt on XX.XX.XX.XX:50276 by /XX.XX.XX.XX because reported RWR replica with genstamp 1007 does not match COMPLETE block's genstamp in block map 1008
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* processReport: from DatanodeRegistration(XX.XX.XX.XX, storageID=DS-443871816-XX.XX.XX.XX-50276-1336829714197, infoPort=50275, ipcPort=50277, storageInfo=lv=-40;cid=CID-e654ac13-92dc-4f82-a22b-c0b6861d06d7;nsid=2063001898;c=0), blocks: 2, processing time: 1 msecs
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 1 to reach 1
For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
{noformat}

fsck Report
===========
{noformat}
/file21:  Under replicated BP-1008469586-XX.XX.XX.XX-1336829603103:blk_3927215081484173742_1008. Target Replicas is 3 but found 2 replica(s).
.Status: HEALTHY
 Total size:	495 B
 Total dirs:	1
 Total files:	3
 Total blocks (validated):	3 (avg. block size 165 B)
 Minimally replicated blocks:	3 (100.0 %)
 Over-replicated blocks:	0 (0.0 %)
 Under-replicated blocks:	1 (33.333332 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	1
 Average block replication:	2.0
 Corrupt blocks:		0
 Missing replicas:		1 (14.285714 %)
 Number of data-nodes:		3
 Number of racks:		1
FSCK ended at Sun May 13 09:49:05 IST 2012 in 9 milliseconds
The filesystem under path '/' is HEALTHY
{noformat}
",jyu@cloudera.com,andreina,Major,Closed,Fixed,04/Jun/12 05:01,30/Apr/15 15:09
Bug,HDFS-3499,12559240,Make NetworkTopology support user specified topology class,,junping_du,junping_du,Major,Closed,Fixed,04/Jun/12 18:22,27/Aug/13 22:07
Bug,HDFS-3501,12559252,Checkpointing with security enabled will stop working after ticket lifetime expires,"The StandbyCheckpointer and 2NN currently do the right thing in renewing their krb5 creds before attempting to perform a checkpoint to the active NN, but the active NN makes no attempt to renew its own krb5 creds before connecting to the standby NN or 2NN to fetch the new merged fsimage file.",atm,atm,Major,Closed,Fixed,04/Jun/12 19:10,11/Oct/12 17:46
Bug,HDFS-3505,12559298,DirectoryScanner does not join all threads in shutdown,"DirectoryScanner does not join all the threads it spawned during DirectoryScanner#shutdown.

This can lead to rare failures in TestDirectoryScanner when some threads linger past their designated expiration date and cause interference between different test runs.

The fix is to call ExecutorService#awaitTermination.",cmccabe,cmccabe,Minor,Closed,Fixed,05/Jun/12 00:19,11/Oct/12 17:46
Bug,HDFS-3507,12559376,DFS#isInSafeMode needs to execute only on Active NameNode,"Currently DFS#isInSafeMode is not Checking for the NN state. It can be executed on any of the NNs.

But HBase will use this API to check for the NN safemode before starting up its service.

If first NN configured is in standby then DFS#isInSafeMode will check standby NNs safemode but hbase want state of Active NN.",vinayakumarb,vinayakumarb,Critical,Closed,Fixed,05/Jun/12 13:03,12/Jun/14 09:03
Bug,HDFS-3510,12559457,Improve FSEditLog pre-allocation,"It is good to avoid running out of space in the middle of writing a batch of edits, because when it happens, we often get partial edits at the end of the log.
Edit log preallocation can solve this problem (see HADOOP-2330 for a full description of edit log preallocation).

The current pre-allocation code was introduced for performance reasons, not for preventing partial edits.  As a consequence, we sometimes do a write without using pre-allocation.  We should change the pre-allocation code so that it always preallocates at least enough space before writing out the edits.",cmccabe,cmccabe,Major,Closed,Fixed,05/Jun/12 22:15,03/Sep/14 23:09
Bug,HDFS-3517,12559892,TestStartup should bind ephemeral ports,TestStartup starts a DN but doesn't bind to ephemeral ports.,eli,eli2,Minor,Closed,Fixed,08/Jun/12 20:10,11/Oct/12 17:46
Bug,HDFS-3518,12559897,Provide API to check HDFS operational state,This will improve the usability of JobTracker safe mode.,szetszwo,bikassaha,Major,Closed,Fixed,08/Jun/12 21:33,14/Nov/12 09:15
Bug,HDFS-3519,12559906,Checkpoint upload may interfere with a concurrent saveNamespace,"TestStandbyCheckpoints failed in [precommit build 2620|https://builds.apache.org/job/PreCommit-HDFS-Build/2620//testReport/] due to the following issue:
- both nodes were in Standby state, and configured to checkpoint ""as fast as possible""
- NN1 starts to save its own namespace
- NN2 starts to upload a checkpoint for the same txid. So, both threads are writing to the same file fsimage.ckpt_12, but the actual file contents correspond to the uploading thread's data.
- NN1 finished its saveNamespace operation while NN2 was still uploading. So, it renamed the ckpt file. However, the contents of the file are still empty since NN2 hasn't sent any bytes
- NN2 finishes the upload, and the rename() call fails, which causes the directory to be marked failed, etc.

The result is that there is a file fsimage_12 which appears to be a finalized image but in fact is incompletely transferred. When the transfer completes, the problem ""heals itself"" so there wouldn't be persistent corruption unless the machine crashes at the same time. And even then, we'd still have the earlier checkpoint to restore from.

This same race could occur in a non-HA setup if a user puts the NN in safe mode and issues saveNamespace operations concurrent with a 2NN checkpointing, I believe.",mingma,tlipcon,Critical,Closed,Fixed,08/Jun/12 22:48,10/Apr/15 20:29
Bug,HDFS-3522,12560119,"If NN is in safemode, it should throw SafeModeException when getBlockLocations has zero locations",,brandonli,brandonli,Major,Closed,Fixed,11/Jun/12 04:57,12/May/16 18:17
Bug,HDFS-3524,12560295,TestFileLengthOnClusterRestart failed due to error message change,"This test failed on a recent jenkins run with the following:
 
org.junit.ComparisonFailure: expected:<[Could not obtain the last block locations.]> but was:<[Zero blocklocations for /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/test. Name node is in safe mode.

The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2. Safe mode will be turned off automatically.",brandonli,eli2,Major,Closed,Fixed,12/Jun/12 04:56,10/Mar/15 04:36
Bug,HDFS-3531,12559735,EditLogFileOutputStream#preallocate should check for incomplete writes,We need to check for short writes when using WritableByteChannel#write and related methods in EditLogFileOutputStream#preallocate so we ensure we've preallocated sufficient space.,cmccabe,cmccabe,Minor,Closed,Fixed,07/Jun/12 18:38,11/Oct/12 17:46
Bug,HDFS-3539,12560809,libhdfs code cleanups,"Fix some compiler warnings, improperly exposed private data types, and extraneous includes in libhdfs.",cmccabe,cmccabe,Minor,Closed,Fixed,15/Jun/12 17:44,11/Oct/12 17:46
Bug,HDFS-3540,12560837,Further improvement on recovery mode and edit log toleration in branch-1,"*Recovery Mode*: HDFS-3479 backported HDFS-3335 to branch-1.  However, the recovery mode feature in branch-1 is dramatically different from the recovery mode in trunk since the edit log implementations in these two branch are different.  For example, there is UNCHECKED_REGION_LENGTH in branch-1 but not in trunk.

*Edit Log Toleration*: HDFS-3521 added this feature to branch-1 to remedy UNCHECKED_REGION_LENGTH and to tolerate edit log corruption.

There are overlaps between these two features.  We study potential further improvement in this issue.",szetszwo,szetszwo,Major,Closed,Fixed,15/Jun/12 21:51,15/May/13 05:15
Bug,HDFS-3541,12594925,"Deadlock between recovery, xceiver and packet responder","Block Recovery initiated while write in progress at Datanode side. Found a lock between recovery, xceiver and packet responder.
",vinayakumarb,suja,Major,Closed,Fixed,18/Jun/12 11:25,04/Sep/14 00:59
Bug,HDFS-3548,12595311,NamenodeFsck.copyBlock fails to create a Block Reader,"NamenodeFsck.copyBlock creates a Socket using {{new Socket()}}, and thus that socket doesn't have an associated Channel. Then, it fails to create a BlockReader since RemoteBlockReader2 needs a socket channel.
(thanks to Hiroshi Yokoi for reporting)",cmccabe,tlipcon,Critical,Closed,Fixed,20/Jun/12 17:59,11/Oct/12 17:46
Bug,HDFS-3549,12595317,dist tar build fails in hadoop-hdfs-raid project,"Trying to build the distribution tarball in a clean tree via {{mvn install -Pdist -Dtar -DskipTests -Dmaven.javadoc.skip}} fails with this error:

{noformat}
main:
     [exec] tar: hadoop-hdfs-raid-3.0.0-SNAPSHOT: Cannot stat: No such file or directory
     [exec] tar: Exiting with failure status due to previous errors
{noformat}",jlowe,jlowe,Critical,Resolved,Fixed,20/Jun/12 18:29,12/May/16 18:12
Bug,HDFS-3550,12595340,raid added javadoc warnings,"hdfs raid which I believe was introduced by MAPREDUCE-3868 has added the following javadoc warnings and now all the builds complain about them:

[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/Decoder.java:180: warning - @param argument ""parityFile"" is not a parameter name.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/Encoder.java:340: warning - @param argument ""srcFile"" is not a parameter name.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/RaidConfigurationException.java:24: warning - Tag @link: reference not found: CronNode
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/RaidConfigurationException.java:24: warning - Tag @link: reference not found: CronNode
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/RaidConfigurationException.java:24: warning - Tag @link: reference not found: CronNode
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.",jlowe,tgraves,Minor,Resolved,Fixed,20/Jun/12 20:48,12/May/16 18:11
Bug,HDFS-3551,12595372,WebHDFS CREATE does not use client location for redirection,CREATE currently redirects client to a random datanode but not using the client location information.,szetszwo,szetszwo,Major,Closed,Fixed,21/Jun/12 01:39,11/Oct/12 17:46
Bug,HDFS-3553,12595429,Hftp proxy tokens are broken,"Proxy tokens are broken for hftp.  The impact is systems using proxy tokens, such as oozie jobs, cannot use hftp.",daryn,daryn,Blocker,Closed,Fixed,21/Jun/12 13:43,12/May/16 18:11
Bug,HDFS-3555,12595303,idle client socket triggers DN ERROR log (should be INFO or DEBUG),"Datanode service is logging java.net.SocketTimeoutException at ERROR level.
This message indicates that the datanode is not able to send data to the client because the client has stopped reading. This message is not really a cause for alarm and should be INFO level.

2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]
at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)
at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)
at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)
at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)
at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)",adi2,jlord,Major,Closed,Fixed,20/Jun/12 17:36,11/Oct/12 17:46
Bug,HDFS-3559,12595647,DFSTestUtil: use Builder class to construct DFSTestUtil instances,"The number of parameters in DFSTestUtil's constructor has grown over time.  It would be nice to have a Builder class similar to MiniDFSClusterBuilder, which could construct an instance of DFSTestUtil.",cmccabe,cmccabe,Minor,Closed,Fixed,22/Jun/12 22:50,11/Oct/12 17:46
Bug,HDFS-3572,12596012,Cleanup code which inits SPNEGO in HttpServer,Currently the code which inits the SPNEGO filter is duplicated between the 2NN and NN. We should move this into the HttpServer utility class to clean it up.,tlipcon,tlipcon,Minor,Closed,Fixed,27/Jun/12 01:45,11/Oct/12 17:46
Bug,HDFS-3574,12596018,Fix small race and do some cleanup in GetImageServlet,"There's a very small race window in GetImageServlet, if the following interleaving occurs:
- The Storage object returns some local file in the storage directory (eg an edits file or image file)
- *Race*: some other process removes the file
- GetImageServlet calls file.length() which returns 0, since it doesn't exist. It thus faithfully sets the Content-Length header to 0
- getFileClient() throws FileNotFoundException when trying to open the file. But, since we call response.getOutputStream() before this, the headers have already been sent, so we fail to send the ""404"" or ""500"" response that we should.

Thus, the client sees a 0-length Content-Length followed by 0 lengths of content, and thinks it successfully has downloaded the target file, where in fact it downloads an empty one.

I saw this in practice during the ""edits synchronization"" phase of recovery while working on HDFS-3077, though it could apply on existing code paths, as well, I believe.",tlipcon,tlipcon,Minor,Closed,Fixed,27/Jun/12 02:27,12/May/16 18:14
Bug,HDFS-3575,12596098,HttpFS does not log Exception Stacktraces,"In the 'log' method of the HttpFSExceptionProvider we log exceptions as ""warn"" but the stacktrace itself is not logged:

    LOG.warn(""[{}:{}] response [{}] {}"", new Object[]{method, path, status, message, throwable});

We should log the exception here.",brocknoland,brocknoland,Minor,Closed,Fixed,27/Jun/12 14:09,11/Oct/12 17:46
Bug,HDFS-3577,12596327,WebHdfsFileSystem can not read files larger than 24KB,"If reading a file large enough for which the httpserver running webhdfs/httpfs uses chunked transfer encoding (more than 24K in the case of webhdfs), then the WebHdfsFileSystem client fails with an IOException with message *Content-Length header is missing*.

It looks like WebHdfsFileSystem is delegating opening of the inputstream to *ByteRangeInputStream.URLOpener* class, which checks for the *Content-Length* header, but when using chunked transfer encoding the *Content-Length* header is not present and  the *URLOpener.openInputStream()* method thrown an exception.
",szetszwo,tucu00,Blocker,Closed,Fixed,28/Jun/12 21:18,18/Apr/13 21:29
Bug,HDFS-3579,12596332,libhdfs: fix exception handling,"libhdfs does not consistently handle exceptions.  Sometimes we don't free the memory associated with them (memory leak).  Sometimes we invoke JNI functions that are not supposed to be invoked when an exception is active.

Running a libhdfs test program with -Xcheck:jni shows the latter problem clearly:
{code}
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
Exception in thread ""main"" java.io.IOException: ...
{code}",cmccabe,cmccabe,Major,Closed,Fixed,28/Jun/12 22:09,12/Jul/13 22:54
Bug,HDFS-3580,12596456,incompatible types; no instance(s) of type variable(s) V exist so that V conforms to boolean compiling HttpFSServer.java with OpenJDK,"{quote}
[ERROR] /home/lars/dev/hadoop-2/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java:[407,36] incompatible types; no instance(s) of type variable(s) V exist so that V conforms to boolean
{quote}
{quote}
$ javac -version
javac 1.6.0_24


$ java -version
java version ""1.6.0_24""
OpenJDK Runtime Environment (IcedTea6 1.11.3) (fedora-67.1.11.3.fc16-x86_64)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
{quote}",adi2,adi2,Minor,Closed,Fixed,29/Jun/12 21:39,11/Oct/12 17:46
Bug,HDFS-3581,12596482,FSPermissionChecker#checkPermission sticky bit check missing range check ,The checkStickyBit call in FSPermissionChecker#checkPermission is missing a range check which results in an index out of bounds when accessing root.,eli,eli,Major,Closed,Fixed,30/Jun/12 00:56,11/Oct/12 17:46
Bug,HDFS-3591,12596687,Backport HDFS-3357 to branch-0.23,"I would like to have HDFS-3357 in branch-0.23, but it is not a trivial upmerge.",revans2,revans2,Major,Closed,Fixed,02/Jul/12 18:11,11/Oct/12 17:46
Bug,HDFS-3595,12597338,TestEditLogLoading fails in branch-1,"TestEditLogLoading currently fails in branch-1, with this error message:
{code}
Testcase: testDisplayRecentEditLogOpCodes took 1.965 sec
    FAILED
error message contains opcodes message
junit.framework.AssertionFailedError: error message contains opcodes message
    at org.apache.hadoop.hdfs.server.namenode.TestEditLogLoading.testDisplayRecentEditLogOpCodes(TestEditLogLoading.java:75)
{code}",cmccabe,cmccabe,Major,Closed,Fixed,03/Jul/12 20:44,15/May/13 05:15
Bug,HDFS-3597,12597375,SNN can fail to start on upgrade,"When upgrading from 1.x to 2.0.0, the SecondaryNameNode can fail to start up:
{code}
2012-06-16 09:52:33,812 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -40 namespaceID = 64415959 cTime = 1339813974990 ; clusterId = CID-07a82b97-8d04-4fdd-b3a1-f40650163245 ; blockpoolId = BP-1792677198-172.29.121.67-1339813967723.
Expecting respectively: -19; 64415959; 0; ; .
at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:120)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:454)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:334)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:301)
at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:438)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:297)
at java.lang.Thread.run(Thread.java:662)
{code}
The error check we're hitting came from HDFS-1073, and it's intended to verify that we're connecting to the correct NN.  But the check is too strict and considers ""different metadata version"" to be the same as ""different clusterID"".

I believe the check in {{doCheckpoint}} simply needs to explicitly check for and handle the update case.",adi2,adi2,Minor,Closed,Fixed,04/Jul/12 00:13,11/Oct/12 17:46
Bug,HDFS-3603,12597642,Decouple TestHDFSTrash from TestTrash,TestHDFSTrash is failing pretty regularly during test builds.,jlowe,jlowe,Major,Closed,Fixed,05/Jul/12 20:11,11/Oct/12 17:46
Bug,HDFS-3605,12597814,Block mistakenly marked corrupt during edit log catchup phase of failover,"Open file for append
Write data and sync.
After next log roll and editlog tailing in standbyNN close the append stream.
Call append multiple times on the same file, before next editlog roll.
Now abruptly kill the current active namenode.

Here block is missed..

this may be because of All latest blocks were queued in StandBy Namenode. 
During failover, first OP_CLOSE was processing the pending queue and adding the block to corrupted block. ",tlipcon,brahmareddy,Major,Closed,Fixed,06/Jul/12 18:39,04/Sep/14 00:59
Bug,HDFS-3608,12597877,fuse_dfs: detect changes in UID ticket cache,"Currently in fuse_dfs, if one kinits as some principal ""foo"" and then does some operation on fuse_dfs, then kdestroy and kinit as some principal ""bar"", subsequent operations done via fuse_dfs will still use cached credentials for ""foo"". The reason for this is that fuse_dfs caches Filesystem instances using the UID of the user running the command as the key into the cache.  This is a very uncommon scenario, since it's pretty uncommon for a single user to want to use credentials for several different principals on the same box.

However, we can use inotify to detect changes in the Kerberos ticket cache file and force the next operation to create a new FileSystem instance in that case.  This will also require a reference counting mechanism in fuse_dfs so that we can free the FileSystem classes when they refer to previous Kerberos ticket caches.

Another mechanism is to run a stat periodically on the ticket cache file.  This is a good fallback mechanism if inotify does not work on the file (for example, because it's on an NFS mount.)",cmccabe,cmccabe,Minor,Closed,Fixed,07/Jul/12 01:21,02/May/18 18:14
Bug,HDFS-3609,12597878,libhdfs: don't force the URI to look like hdfs://hostname:port,"Currently, libhdfs forces the URI to look like hdfs://<hostname>:port.

For configurations like HA or federation this is not ideal.",cmccabe,cmccabe,Major,Closed,Fixed,07/Jul/12 01:59,11/Oct/12 17:46
Bug,HDFS-3611,12597940,NameNode prints unnecessary WARNs about edit log normally skipping a few bytes,"The NameNode currently warns these form of lines at every startup, even if there's no trouble really. For instance, the below is from a NN startup that was only just freshly formatted.

{code}
12/07/08 20:00:22 WARN namenode.EditLogInputStream: skipping 1048563 bytes at the end of edit log  '/Users/harshchouraria/Work/installs/temp-space/tmp-default/dfs-cdh4/data/current/edits_0000000000000000003-0000000000000000003': reached txid 3 out of 3
{code}

If this skipping is not really a cause for warning, we should not log it at a WARN level but at an INFO or even DEBUG one. Avoids users getting unnecessarily concerned.",cmccabe,qwertymaniac,Trivial,Closed,Fixed,08/Jul/12 14:33,11/Oct/12 17:46
Bug,HDFS-3614,12597947,Revert unused MiniDFSCluster constructor from HDFS-3049,Added a unused parameter to ctor of MiniDFSCluster.,acmurthy,acmurthy,Blocker,Resolved,Fixed,08/Jul/12 17:01,12/May/16 18:13
Bug,HDFS-3615,12597953,Two BlockTokenSecretManager findbugs warnings,"Looks like two findbugs warnings were introduced recently (see these across a couple recent patches). Unclear what change introduced it as the file hasn't been modified and recent committed changes pass the findbugs check.

IS	Inconsistent synchronization of org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.keyUpdateInterval; locked 75% of time

IS	Inconsistent synchronization of org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.serialNo; locked 75% of time
",atm,eli,Major,Closed,Fixed,08/Jul/12 17:55,11/Oct/12 17:46
Bug,HDFS-3616,12597955,TestWebHdfsWithMultipleNameNodes fails with ConcurrentModificationException in DN shutdown,"I have seen this in precommit build #2743

{noformat}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:209)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.shutdown(FsVolumeList.java:168)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:1214)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1105)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1324)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1304)
	at org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes.shutdownCluster(TestWebHdfsWithMultipleNameNodes.java:100)
{noformat}

",jingzhao,umamaheswararao,Major,Closed,Fixed,08/Jul/12 18:15,12/May/16 18:13
Bug,HDFS-3622,12598091,Backport HDFS-3541 to branch-0.23,"HDFS-3541 Deadlock between recovery, xceiver and packet responder 

does not apply directly to branch-0.23, but the bug exists there too.",revans2,revans2,Major,Closed,Fixed,09/Jul/12 21:05,02/May/13 02:30
Bug,HDFS-3625,12598229,Fix TestBackupNode by properly initializing edit log,"TestBackupNode#testCheckpointNode fails because the following code in FSN#startActiveServices NPEs (resulting in a System.exit) because editLogTailer is set when starting standby services and if ha is not enabled we go directly to the active state. Looks like it should be wrapped with an haEnabled check.

{code}
LOG.info(""Catching up to latest edits from old active before "" +
   ""taking over writer role in edits logs."");
editLogTailer.catchupDuringFailover();
{code}",junping_du,eli,Blocker,Closed,Fixed,10/Jul/12 17:19,28/Sep/15 20:58
Bug,HDFS-3626,12598232,Creating file with invalid path can corrupt edit log,"Joris Bontje reports the following:

The following command results in a corrupt NN editlog (note the double slash and reading from stdin):
$ cat /usr/share/dict/words | hadoop fs -put - hdfs://localhost:8020//path/file

After this, restarting the namenode will result into the following fatal exception:
{code}
2012-07-10 06:29:19,910 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /var/lib/hadoop-hdfs/cache/hdfs/dfs/name/current/edits_0000000000000000173-0000000000000000188 expecting start txid #173
2012-07-10 06:29:19,912 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation MkdirOp [length=0, path=/, timestamp=1341915658216, permissions=cloudera:supergroup:rwxr-xr-x, opCode=OP_MKDIR, txid=182]
java.lang.ArrayIndexOutOfBoundsException: -1
{code}",tlipcon,tlipcon,Blocker,Closed,Fixed,10/Jul/12 17:26,03/Sep/14 23:21
Bug,HDFS-3628,12598258,The dfsadmin -setBalancerBandwidth command on branch-1 does not check for superuser privileges,"The changes from HDFS-2202 for 0.20.x/1.x failed to add in a checkSuperuserPrivilege();, and hence any user (not admins alone) can reset the balancer bandwidth across the cluster if they wished to.",qwertymaniac,qwertymaniac,Blocker,Closed,Fixed,10/Jul/12 19:54,15/May/13 05:16
Bug,HDFS-3629,12598275,fix the typo in the error message about inconsistent storage layout version,"Brahma Reddy Battula pointed out a typo in the following error message:
{noformat}
if (multipleLV) {             
      throw new IOException( 
          ""Storage directories containe multiple layout versions: "" 
              + layoutVersions); 
{noformat}",brandonli,brandonli,Trivial,Closed,Fixed,10/Jul/12 20:54,11/Oct/12 17:46
Bug,HDFS-3630,12598283,Modify TestPersistBlocks to use both flush and hflush,,sanjay.radia,sanjay.radia,Major,Resolved,Fixed,10/Jul/12 21:27,12/May/16 18:16
Bug,HDFS-3633,12598298,libhdfs: hdfsDelete should pass JNI_FALSE or JNI_TRUE,"In libhdfs in hdfsDelete, the header file says any non-zero argument to hdfsDelete will be interpreted as true.  However, the hdfsDelete function does not translate these non-zero values to JNI_FALSE and JNI_TRUE, potentially leading to undefined or JVM-specific behavior.",cmccabe,cmccabe,Minor,Closed,Fixed,11/Jul/12 00:26,11/Oct/12 17:46
Bug,HDFS-3646,12598556,LeaseRenewer can hold reference to inactive DFSClient instances forever,"If {{LeaseRenewer#closeClient()}} is not called, {{LeaseRenewer}} keeps the reference to a {{DFSClient}} instance in {{dfsclients}} forever. This prevents {{DFSClient}}, {{LeaseRenewer}}, conf, etc. from being garbage collected, leading to memory leak.

{{LeaseRenewer}} should remove the reference after some delay, if a {{DFSClient}} instance no longer has active streams.",kihwal,kihwal,Critical,Closed,Fixed,12/Jul/12 13:43,11/Oct/12 17:46
Bug,HDFS-3652,12598643,1.x: FSEditLog failure removes the wrong edit stream when storage dirs have same name,"In {{FSEditLog.removeEditsForStorageDir}}, we iterate over the edits streams trying to find the stream corresponding to a given dir. To check equality, we currently use the following condition:
{code}
      File parentDir = getStorageDirForStream(idx);
      if (parentDir.getName().equals(sd.getRoot().getName())) {
{code}
... which is horribly incorrect. If two or more storage dirs happen to have the same terminal path component (eg /data/1/nn and /data/2/nn) then it will pick the wrong stream(s) to remove.",tlipcon,tlipcon,Blocker,Closed,Fixed,12/Jul/12 23:16,17/Oct/12 18:25
Bug,HDFS-3658,12598768,TestDFSClientRetries#testNamenodeRestart failed,"Saw the following fail on a jenkins run:

{noformat}
Error Message

expected:<MD5-of-0MD5-of-512CRC32:f397fb3d9133d0a8f55854ea2bb268b0> but was:<MD5-of-0MD5-of-0CRC32:70bc8f4b72a86921468bf8e8441dce51>
Stacktrace

junit.framework.AssertionFailedError: expected:<MD5-of-0MD5-of-512CRC32:f397fb3d9133d0a8f55854ea2bb268b0> but was:<MD5-of-0MD5-of-0CRC32:70bc8f4b72a86921468bf8e8441dce51>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:283)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:71)
	at org.apache.hadoop.hdfs.TestDFSClientRetries.testNamenodeRestart(TestDFSClientRetries.java:886)
{noformat}",szetszwo,eli,Major,Closed,Fixed,13/Jul/12 23:27,19/Nov/12 07:20
Bug,HDFS-3664,12598830,BlockManager race when stopping active services,"Looks like there's a race hit by the test where we try to do block management while shutting down (eg allocate a block or compute replication work after BlocksMap#close, resulting in an NPE).

",cmccabe,eli,Major,Closed,Fixed,14/Jul/12 23:41,11/Oct/12 17:46
Bug,HDFS-3669,12598992,'mvn clean' should delete native build directories,Running maven clean (mvn clean) should delete native build directories.  This is useful in cases where you need to re-run cmake from scratch.  One example of a case like this is where the set of installed system libraries have changed.,cmccabe,cmccabe,Minor,Resolved,Fixed,16/Jul/12 16:30,12/Oct/12 23:09
Bug,HDFS-3673,12599044,libhdfs: fix some compiler warnings,"Fix some compiler warnings.  Some of these are misuses of fprintf (forgetting the FILE* parameter), const mismatch warnings, format specifier warnings.",cmccabe,cmccabe,Minor,Closed,Fixed,16/Jul/12 21:20,11/Oct/12 17:46
Bug,HDFS-3675,12599191,libhdfs: follow documented return codes,"libhdfs should follow its own documentation for return codes.  This means always setting errno, and in most cases returning -1 (not some other value) on error.",cmccabe,cmccabe,Minor,Closed,Fixed,17/Jul/12 23:15,11/Oct/12 17:46
Bug,HDFS-3677,12599206,dfs.namenode.edits.dir.required missing from hdfs-default.xml,This config should be documented. It's useful for cases where the user would like to ensure that (eg) an NFS filer always has the most up-to-date edits.,mark_yang,tlipcon,Major,Resolved,Fixed,18/Jul/12 01:51,30/Aug/16 01:42
Bug,HDFS-3678,12599207,Edit log files are never being purged from 2NN,"Seeing the following NPE in the SecondaryNameNode when it tries to purge storage after a checkpoint:

12/07/17 19:05:22 WARN namenode.FSImage: Unable to purge old storage
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.purgeLogsOlderThan(FSEditLog.java:1013)
        at org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.purgeOldStorage(NNStorageRetentionManager.java:98)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.purgeOldStorage(FSImage.java:925)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:479)
",atm,tlipcon,Critical,Closed,Fixed,18/Jul/12 02:05,12/May/16 18:18
Bug,HDFS-3679,12599316,fuse_dfs notrash option sets usetrash,"fuse_dfs sets usetrash option when the ""notrash"" flag is given. This is the exact opposite of the desired behavior. The ""usetrash"" flag sets usetrash as well, but this is correct. Here are the relevant lines from fuse_options.c, in latest HDFS HEAD[0]:

123	  case KEY_USETRASH:
124	    options.usetrash = 1;
125	    break;
126	  case KEY_NOTRASH:
127	    options.usetrash = 1;
128	    break;

This is a pretty trivial bug to fix. I'm not familiar with the process here, but I can attach a patch if needed.

[0]: https://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c?view=markup",cmeyerisi,cmeyerisi,Minor,Closed,Fixed,18/Jul/12 18:35,11/Oct/12 17:46
Bug,HDFS-3683,12599504,Edit log replay progress indicator shows >100% complete,"When loading a large edit log, the NameNode prints a percentage-complete progress indicator:
{code}
              int percent = Math.round((float)lastAppliedTxId / numTxns * 100);
              LOG.info(""replaying edit log: "" + lastAppliedTxId + ""/"" + numTxns
                  + "" transactions completed. ("" + percent + ""%)"");
{code}
But the percentage is calculated incorrectly, since it divides the transaction ID by the number of expected transactions. This only works when starting at txid 1. (eg if we are loading 1000 transaction starting at txid 1000, the indicator will start at 100% and go to 200%)",zero45,tlipcon,Minor,Closed,Fixed,19/Jul/12 17:42,11/Oct/12 17:46
Bug,HDFS-3688,12599524,Namenode loses datanode hostname if datanode re-registers,"If a datanode ever re-registers with the namenode (e.g.: namenode restart, temporary network cut, etc.) then the datanode ends up registering with an IP address as the datanode name rather than the hostname.
",jlowe,jlowe,Major,Closed,Fixed,19/Jul/12 20:46,11/Oct/12 17:46
Bug,HDFS-3690,12599544,BlockPlacementPolicyDefault incorrectly casts LOG,"The hadoop-tools tests, eg TestCopyListing, fails with

{noformat}
Caused by: java.lang.ClassCastException: org.apache.commons.logging.impl.SimpleLog cannot be cast to org.apache.commons.logging.impl.Log4JLogger
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.<clinit>(BlockPlacementPolicyDefault.java:55)
{noformat}

caused by this cast

{code}
  private static final String enableDebugLogging =
    ""For more information, please enable DEBUG log level on ""
    + ((Log4JLogger)LOG).getLogger().getName();
{code}
",eli,eli,Major,Closed,Fixed,19/Jul/12 22:12,11/Oct/12 17:46
Bug,HDFS-3696,12599627,Create files with WebHdfsFileSystem goes OOM when file size is big,"When doing ""fs -put"" to a WebHdfsFileSystem (webhdfs://), the FsShell goes OOM if the file size is large. When I tested, 20MB files were fine, but 200MB didn't work.  

I also tried reading a large file by issuing ""-cat"" and piping to a slow sink in order to force buffering. The read path didn't have this problem. The memory consumption stayed the same regardless of progress.
",szetszwo,kihwal,Critical,Closed,Fixed,20/Jul/12 14:23,27/Mar/13 10:40
Bug,HDFS-3698,12599704,TestHftpFileSystem is failing in branch-1 due to changed default secure port,This test is failing since the default secure port changed to the HTTP port upon the commit of HDFS-2617.,atm,atm,Major,Closed,Fixed,21/Jul/12 08:32,17/Oct/12 18:27
Bug,HDFS-3701,12599834,HDFS may miss the final block when reading a file opened for writing if one of the datanode is dead,"When the file is opened for writing, the DFSClient calls one of the datanode owning the last block to get its size. If this datanode is dead, the socket exception is shallowed and the size of this last block is equals to zero. This seems to be fixed on trunk, but I didn't find a related Jira. On 1.0.3, it's not fixed. It's on the same area as HDFS-1950 or HDFS-3222.
",nkeywal,nkeywal,Critical,Closed,Fixed,23/Jul/12 10:50,12/Mar/13 00:35
Bug,HDFS-3707,12599873,TestFSInputChecker: improper use of skip,"TestFSInputChecker assumes that FSDataInputStream#skip always skips the full amount of bytes.  However, FSDataInputStream inherits this method from InputStream.  InputStream#skip skips ""up to"" the requested number of bytes.  It is not an error if it skips fewer.",cmccabe,cmccabe,Minor,Closed,Fixed,23/Jul/12 17:39,11/Oct/12 17:46
Bug,HDFS-3710,12599921,libhdfs misuses O_RDONLY/WRONLY/RDWR,"The {{O_RDONLY}} / {{O_WRONLY}} / {{O_RDWR}} macros in {{fcntl.h}} are not a bitmask; they are an enum stored in the low bits of the flag word.  The proper way to use them is
{code}
if ((flags & O_ACCMODE) == O_RDONLY)
{code}
rather than
{code}
if ((flags & O_RDONLY) == 0)
{code}

There are many examples of this misuse in {{hdfs.c}}.

As a result of this incorrect testing, erroneous code may be accepted without error and correct code might not work correctly.",adi2,adi2,Minor,Closed,Fixed,23/Jul/12 23:39,11/Oct/12 17:46
Bug,HDFS-3715,12599939,Fix TestFileCreation#testFileCreationNamenodeRestart,"TestFileCreation#testFileCreationNamenodeRestart is ignored due to the following. We should (a) modify this test to test the current expected behavior for leases on restart and (b) file any jiras for necessary fixes to close the gap between current and desired behavior.

{code}
  /**
   * Test that file leases are persisted across namenode restarts.
   * This test is currently not triggered because more HDFS work is 
   * is needed to handle persistent leases.
   */
  @Ignore
  @Test
  public void xxxtestFileCreationNamenodeRestart() throws IOException {
{code}",andrew.wang,eli,Major,Closed,Fixed,24/Jul/12 03:36,11/Oct/12 17:46
Bug,HDFS-3716,12599949,Purger should remove stale fsimage ckpt files,"NN got killed while checkpointing in progress before renaming the ckpt file to actual file.
Since the checkpointing process is not completed, on next NN startup it will load previous fsimage and apply rest of the edits.
Functionally there's no harm but this ckpt file will be retained as is.
Purger will not remove the ckpt file though other old fsimage files will be taken care.",andreina,suja,Minor,Resolved,Fixed,24/Jul/12 06:29,24/Jan/17 22:50
Bug,HDFS-3718,12600035,Datanode won't shutdown because of runaway DataBlockScanner thread,"Datanode sometimes does not shutdown because the block pool scanner thread keeps running. It prints out ""Starting a new period"" every five seconds, even after {{shutdown()}} is called.  Somehow the interrupt is missed.

{{DataBlockScanner}} will also terminate if {{datanode.shouldRun}} is false, but in {{DataNode#shutdown}}, {{DataBlockScanner#shutdown()}} is invoked before it is being set to false.

Is there any reason why {{datanode.shouldRun}} is set to false later? ",kihwal,kihwal,Critical,Closed,Fixed,24/Jul/12 17:02,19/Aug/13 02:44
Bug,HDFS-3720,12600064,hdfs.h must get packaged,"hdfs.h should be packaged, but it currently is not.  This was broken when some header files got renamed by HDFS-3537.",cmccabe,cmccabe,Major,Closed,Fixed,24/Jul/12 20:56,11/Oct/12 17:46
Bug,HDFS-3721,12600066,hsync support broke wire compatibility,"HDFS-744 added support for hsync to the data transfer wire protocol. However, it actually broke wire compatibility: if the client has hsync support but the server does not, the client cannot read or write data on the old cluster.",atm,tlipcon,Critical,Closed,Fixed,24/Jul/12 20:58,11/Oct/12 17:46
Bug,HDFS-3724,12600241,add InterfaceAudience annotations to HttpFS classes and making inner enum static,,tucu00,tucu00,Major,Closed,Fixed,26/Jul/12 00:17,11/Oct/12 17:46
Bug,HDFS-3727,12600251,"When using SPNEGO, NN should not try to log in using KSSL principal","When performing a checkpoint with security enabled, the NN will attempt to relogin from its keytab before making an HTTP request back to the 2NN to fetch the newly-merged image. However, it always attempts to log in using the KSSL principal, even if SPNEGO is configured to be used.

This issue was discovered by Stephen Chu.",atm,atm,Major,Closed,Fixed,26/Jul/12 01:10,06/Mar/13 09:55
Bug,HDFS-3731,12600331,2.0 release upgrade must handle blocks being written from 1.0,"Release 2.0 upgrades must handle blocks being written to (bbw) files from 1.0 release. Problem reported by @brahmareddy
The {{DataNode}} will only have one block pool after upgrading from a 1.x release.  (This is because in the 1.x releases, there were no block pools-- or equivalently, everything was in the same block pool).  During the upgrade, we should hardlink the block files from the {{blocksBeingWritten}} directory into the {{rbw}} directory of this block pool.  Similarly, on {{-finalize}}, we should delete the {{blocksBeingWritten}} directory.",kihwal,sureshms,Blocker,Closed,Fixed,26/Jul/12 15:24,06/Nov/12 10:22
Bug,HDFS-3732,12600399,fuse_dfs: incorrect configuration value checked for connection expiry timer period,"In fuse_dfs, we check an incorrect hdfs configuration value checked for the connection expiry timer period.

{code}
gTimerPeriod = FUSE_CONN_DEFAULT_TIMER_PERIOD;
ret = hdfsConfGetInt(HADOOP_FUSE_CONNECTION_TIMEOUT, &gTimerPeriod);
if (ret) {
  fprintf(stderr, ""Unable to determine the configured value for %s."",
        HADOOP_FUSE_TIMER_PERIOD);
  return -EINVAL
}
{code}

We should be checking HADOOP_FUSE_TIMER_PERIOD.",cmccabe,cmccabe,Minor,Closed,Fixed,26/Jul/12 22:13,11/Oct/12 17:46
Bug,HDFS-3733,12600416,Audit logs should include WebHDFS access,"Access via WebHdfs does not result in audit log entries.  It should.

{noformat}
% curl ""http://nn1:50070/webhdfs/v1/user/adi/hello.txt?op=GETFILESTATUS""
{""FileStatus"":{""accessTime"":1343351432395,""blockSize"":134217728,""group"":""supergroup"",""length"":12,""modificationTime"":1342808158399,""owner"":""adi"",""pathSuffix"":"""",""permission"":""644"",""replication"":1,""type"":""FILE""}}
{noformat}
and observe that no audit log entry is generated.

Interestingly, OPEN requests do not generate audit log entries when the NN generates the redirect, but do generate audit log entries when the second phase against the DN is executed.
{noformat}
% curl -v 'http://nn1:50070/webhdfs/v1/user/adi/hello.txt?op=OPEN'
...
< HTTP/1.1 307 TEMPORARY_REDIRECT
< Location: http://dn01:50075/webhdfs/v1/user/adi/hello.txt?op=OPEN&namenoderpcaddress=nn1:8020&offset=0
...
% curl -v 'http://dn01:50075/webhdfs/v1/user/adi/hello.txt?op=OPEN&namenoderpcaddress=nn1:8020'
...
< HTTP/1.1 200 OK
< Content-Type: application/octet-stream
< Content-Length: 12
< Server: Jetty(6.1.26.cloudera.1)
< 
hello world
{noformat}
This happens because {{DatanodeWebHdfsMethods#get}} uses {{DFSClient#open}} thereby triggering the existing {{logAuditEvent}} code.",adi2,adi2,Major,Closed,Fixed,27/Jul/12 01:37,11/Oct/12 17:46
Bug,HDFS-3735,12600634,"NameNode WebUI should allow sorting live datanode list by fields Block Pool Used, Block Pool Used(%) and Failed Volumes. ","Live datanode list is not correctly sorted for columns Block Pool Used (GB),
Block Pool Used (%)  and Failed Volumes. Read comments for more details.",brahmareddy,brahmareddy,Major,Resolved,Fixed,30/Jul/12 09:22,12/May/16 18:17
Bug,HDFS-3737,12600687,retry support for webhdfs is breaking HttpFS support,"HDFS-3667 is breaking HttpFS testcases:

{code}
Running org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem
Tests run: 30, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 14.585 sec <<< FAILURE!

Results :

Tests in error: 
  testOperation[1](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem): Unexpected HTTP response: code=200 != 307, op=OPEN, message=OK
  testOperationDoAs[1](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem): Unexpected HTTP response: code=200 != 307, op=OPEN, message=OK
  testOperation[2](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem)
  testOperationDoAs[2](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem)
  testOperation[3](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem): Unexpected HTTP response: code=400 != 200, op=APPEND, message=Data upload requests must have content-type set to 'application/octet-stream'
  testOperationDoAs[3](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem): Unexpected HTTP response: code=400 != 200, op=APPEND, message=Data upload requests must have content-type set to 'application/octet-stream'
  testOperation[13](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem): Unexpected HTTP response: code=200 != 307, op=GETFILECHECKSUM, message=OK
  testOperationDoAs[13](org.apache.hadoop.fs.http.client.TestWebhdfsFileSystem): Unexpected HTTP response: code=200 != 307, op=GETFILECHECKSUM, message=OK

Tests run: 30, Failures: 0, Errors: 8, Skipped: 0
{code}
",szetszwo,tucu00,Blocker,Resolved,Fixed,30/Jul/12 17:25,01/Aug/12 01:48
Bug,HDFS-3738,12600724,TestDFSClientRetries#testFailuresArePerOperation sets incorrect timeout config,"TestDFSClientRetries#testFailuresArePerOperation involves testing retries by making use of expected timeouts. However, this test sets the wrong config to lower the timeout, and thus takes far longer than it should.",atm,atm,Minor,Closed,Fixed,30/Jul/12 20:49,11/Oct/12 17:46
Bug,HDFS-3740,12600739,Multiple test cases in TestWebhdfsFileSystem are failing,"After HDFS-3667, 7-8 cases have been failing in 2.0 build.
These are with Clover enabled.

",szetszwo,kihwal,Major,Resolved,Fixed,30/Jul/12 22:21,07/Sep/12 21:09
Bug,HDFS-3750,12601120,API docs don't include HDFS,[The javadocs|http://hadoop.apache.org/common/docs/current/api/index.html] don't include HDFS.,jolly,eli,Critical,Resolved,Fixed,01/Aug/12 20:47,12/May/16 18:19
Bug,HDFS-3753,12601249,Tests don't run with native libraries,"Test execution when run with the native flag and native libraries have been built don't actually use the native libs because NativeCodeLoader is unable to load native-hadoop. Eg run {{mvn compile -Pnative}} then {{mvn -Dtest=TestSeekBug test -Pnative}} and check the test output. This is because the test's java.library.path is looking for the lib in hdfs (
hadoop-hdfs-project/hadoop-hdfs/target/native/target/usr/local/lib) however the native lib lives in common. I confirmed copying the lib to the appropriate directory fixes things. We need to update the java.library.path for test execution to include the common lib dir.  This may be an issue with MR as well.",cmccabe,eli,Major,Closed,Fixed,02/Aug/12 17:19,15/Feb/13 13:12
Bug,HDFS-3754,12601250,BlockSender doesn't shutdown ReadaheadPool threads,The BlockSender doesn't shutdown the ReadaheadPool threads so when tests are run with native libraries some tests fail (time out) because shutdown hangs waiting for the outstanding threads to exit.,eli,eli,Major,Closed,Fixed,02/Aug/12 17:26,11/Oct/12 17:46
Bug,HDFS-3755,12601268,Creating an already-open-for-write file with overwrite=true fails,"If a file is already open for write by one client, and another client calls {{fs.create()}} with {{overwrite=true}}, the file should be deleted and the new file successfully created. Instead, it is currently throwing AlreadyBeingCreatedException.

This is a regression since branch-1.",tlipcon,tlipcon,Major,Closed,Fixed,02/Aug/12 21:02,08/May/15 06:39
Bug,HDFS-3756,12601273,"DelegationTokenFetcher creates 2 HTTP connections, the second one not properly configured","When doing the forwardport of HDFS-2617 to trunk I've missed deleting the original connection creation when converting it to use the SecurityUtil class.

{code}
       connection = (HttpURLConnection) SecurityUtil.openSecureHttpConnection(url);
      connection = (HttpURLConnection)URLUtils.openConnection(url);
{code}
",tucu00,tucu00,Critical,Closed,Fixed,02/Aug/12 21:50,11/Oct/12 17:46
Bug,HDFS-3758,12601400,TestFuseDFS test failing,"TestFuseDFS.java has two bugs:

* there is a race condition between mounting the filesystem and testing it
* it doesn't clear the mount directory before it tries to mount there",cmccabe,cmccabe,Minor,Closed,Fixed,03/Aug/12 18:15,11/Oct/12 17:46
Bug,HDFS-3760,12601424,"primitiveCreate is a write, not a read","{code}
DistributedFileSystem.java:

246   protected HdfsDataOutputStream primitiveCreate(Path f,
247     FsPermission absolutePermission, EnumSet<CreateFlag> flag, int bufferSize,
248     short replication, long blockSize, Progressable progress,
249     int bytesPerChecksum) throws IOException {
250     statistics.incrementReadOps(1);
{code}
A {{create}} (whether primitive or not) is a write operation, not a read operation.",adi2,adi2,Minor,Closed,Fixed,03/Aug/12 20:37,11/Oct/12 17:46
Bug,HDFS-3763,12601438,TestNameNodeMXBean fails on Windows,"One step of the test is to change the edit directory permission to ""000"" and then do rolledit. On linux rolledit operation will fail and the edit directory will be considered as failed by NN.

However, changing the directory permission to 000 on Windows doesn't affect some accessibility of its sub-directories and files. Therefore, the edit files in the edit directory is still accessible and thus is not considered as ""failed"" by NN, which eventually causes test failure.
",brandonli,brandonli,Major,Resolved,Fixed,03/Aug/12 22:44,14/Aug/12 05:03
Bug,HDFS-3766,12601581,Fix TestStorageRestore on Windows,"When a storage directory is removed, namenode doesn't close the stream and storage directory is remained locked. This could fail later on the restoring storage directory function because namenode will not be able to format original directory. Unlike Linux, Windows doesn't allow deleting a file or directory which is opened with no share/delete permission by a different process.

Similar problem also caused TestStorageRestore to fail because it can't delete the directories/files being used by the test itself. ",brandonli,brandonli,Major,Resolved,Fixed,06/Aug/12 17:01,14/Aug/12 05:02
Bug,HDFS-3773,12602263,TestNNWithQJM fails after HDFS-3741,Looks like the change of visibility of one of the QuorumJournalManager constructors fouls up the instantiation via reflection.,atm,atm,Major,Resolved,Fixed,08/Aug/12 16:41,08/Aug/12 17:22
Bug,HDFS-3788,12603151,distcp can't copy large files using webhdfs due to missing Content-Length header,"The following command fails when data1 contains a 3gb file. It passes when using hftp or when the directory just contains smaller (<2gb) files, so looks like a webhdfs issue with large files.

{{hadoop distcp webhdfs://eli-thinkpad:50070/user/eli/data1 hdfs://localhost:8020/user/eli/data2}}
",szetszwo,eli,Critical,Closed,Fixed,12/Aug/12 21:50,30/Jul/13 05:29
Bug,HDFS-3790,12603292,test_fuse_dfs.c doesn't compile on centos 5,"test_fuse_dfs.c uses execvpe, which doesn't exist in the version of glibc shipped on CentOS 5.",cmccabe,cmccabe,Minor,Closed,Fixed,13/Aug/12 17:49,11/Oct/12 17:46
Bug,HDFS-3791,12603298,Backport HDFS-173 to Branch-1 :  Recursively deleting a directory with millions of files makes NameNode unresponsive for other commands until the deletion completes,"Backport HDFS-173. 
see the [comment|https://issues.apache.org/jira/browse/HDFS-2815?focusedCommentId=13422007&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13422007] for more details",umamaheswararao,umamaheswararao,Major,Closed,Fixed,13/Aug/12 18:34,03/Dec/12 07:33
Bug,HDFS-3792,12603322,Fix two findbugs introduced by HDFS-3695,Accidentally introduced two trivial findbugs warnings in HDFS-3695. This JIRA is to fix them.,tlipcon,tlipcon,Trivial,Closed,Fixed,13/Aug/12 20:58,12/May/16 18:18
Bug,HDFS-3794,12603339,WebHDFS Open used with Offset returns the original (and incorrect) Content Length in the HTTP Header.,"When an offset is specified, the HTTP header Content Length still contains the original file size. e.g. if the original file is 100 bytes, and the offset specified it 10, then HTTP Content Length ought to be 90. Currently it is still returned as 100.
This causes curl to give error 18, and JAVA to throw ConnectionClosedException",raviprak,raviprak,Major,Closed,Fixed,13/Aug/12 21:41,19/Jul/13 19:36
Bug,HDFS-3803,12603524,BlockPoolSliceScanner new work period notice is very chatty at INFO level,One line of ~140 chars logged every 5 seconds. ,,apurtell,Minor,Closed,Fixed,14/Aug/12 22:24,04/Sep/14 00:59
Bug,HDFS-3804,12603536,TestHftpFileSystem fails intermittently with JDK7,"For example:

  testFileNameEncoding(org.apache.hadoop.hdfs.TestHftpFileSystem): Filesystem closed
  testDataNodeRedirect(org.apache.hadoop.hdfs.TestHftpFileSystem): Filesystem closed

This test case sets up a filesystem that is used by the first half of the test methods (in declaration order), but the second half of the tests start by calling {{FileSystem.closeAll}}. With JDK7, test methods are run in an arbitrary order, so if any first half methods run after any second half methods, they fail.",scurrilous,scurrilous,Major,Closed,Fixed,14/Aug/12 23:33,03/Sep/14 23:09
Bug,HDFS-3808,12603666,fuse_dfs: postpone libhdfs intialization until after fork,"{{libhdfs}} may create threads or initialize internal JNI data structures when it is used.  So we should be careful to {{daemonize()}} before initializing {{libhdfs}}, not after.

Unfortunately, fuse_dfs is doing just this -- initializing libhdfs in its {{main}} function, prior to calling {{fuse_main}} which does a {{daemonize}}.

{{daemonize()}} does not preserve threads, because it is implemented by {{fork()}}ing and then continuing execution in the child and allowing the parent to {{exit()}}.
",cmccabe,cmccabe,Critical,Closed,Fixed,15/Aug/12 23:04,11/Oct/12 17:46
Bug,HDFS-3816,12603933,Invalidate work percentage default value should be 0.32f instead of 32,HDFS-3475 used blocksInvalidateWorkPct/DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION_DEFAULT as a percentage float number. Thus DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION_DEFAULT should be defined as a float number with value 0.32f.,jingzhao,jingzhao,Major,Closed,Fixed,18/Aug/12 00:50,12/May/16 18:13
Bug,HDFS-3823,12604145,QJM: TestQJMWithFaults fails occasionally because of missed setting of HTTP port,"TestQJMWithFaults#testRandomized will fail if the IPCLoggerChannel.httpPort instance variable isn't set before a call to IPCLoggerChannel#prepareRecovery is made, since this is necessary to build URLs to the returned logs.

Credit to Todd Lipcon for discovering this issue.",atm,atm,Minor,Resolved,Fixed,20/Aug/12 23:28,20/Aug/12 23:45
Bug,HDFS-3824,12604147,TestHftpDelegationToken fails intermittently with JDK7,"{{testHdfsDelegationToken}} fails if not run before {{testSelectHftpDelegationToken}} and {{testSelectHsftpDelegationToken}}:

{noformat}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.541 sec <<< FAILURE!
testHdfsDelegationToken(org.apache.hadoop.hdfs.TestHftpDelegationToken)  Time elapsed: 0.039 sec  <<< FAILURE!
java.lang.AssertionError: wrong token expected same:<Kind: HDFS_DELEGATION_TOKEN, Service: 127.0.0.1:8020, Ident: > was not:<null>
{noformat}

Debug output:

{noformat}
2012-08-20 18:46:54,742 INFO  fs.FileSystem (HftpFileSystem.java:run(251)) - Couldn't get a delegation token from http://localhost:50470 using http.
2012-08-20 18:46:54,743 DEBUG fs.FileSystem (HftpFileSystem.java:run(254)) - error was
java.io.IOException: Unable to obtain remote token
        at org.apache.hadoop.hdfs.tools.DelegationTokenFetcher.getDTfromRemote(DelegationTokenFetcher.java:239)
        at org.apache.hadoop.hdfs.HftpFileSystem$2.run(HftpFileSystem.java:249)
        at org.apache.hadoop.hdfs.HftpFileSystem$2.run(HftpFileSystem.java:243)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:416)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
        at org.apache.hadoop.hdfs.HftpFileSystem.getDelegationToken(HftpFileSystem.java:243)
        at org.apache.hadoop.hdfs.HftpFileSystem.initDelegationToken(HftpFileSystem.java:196)
        at org.apache.hadoop.hdfs.HftpFileSystem.initialize(HftpFileSystem.java:185)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2284)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:85)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2318)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2300)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:315)
        at org.apache.hadoop.hdfs.TestHftpDelegationToken$1.run(TestHftpDelegationToken.java:131)
        at org.apache.hadoop.hdfs.TestHftpDelegationToken$1.run(TestHftpDelegationToken.java:128)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:416)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
        at org.apache.hadoop.hdfs.TestHftpDelegationToken.testHdfsDelegationToken(TestHftpDelegationToken.java:61)
...
Caused by: java.net.ConnectException: Connection refused
...
{noformat}
",scurrilous,scurrilous,Major,Closed,Fixed,20/Aug/12 23:43,03/Sep/14 23:19
Bug,HDFS-3827,12604257,TestHASafeMode#assertSafemode should be made static,"TestHASafeMode#assertSafemode should be made static. The parameters of assertSafemode contains NameNode nn, thus should change ""String status = nn1.getNamesystem().getSafemode();"" to ""String status = nn.getNamesystem().getSafemode();"".",jingzhao,jingzhao,Trivial,Resolved,Fixed,21/Aug/12 18:11,12/May/16 18:14
Bug,HDFS-3828,12604270,Block Scanner rescans blocks too frequently,"{{BlockPoolSliceScanner#scan}} calls cleanUp every time it's invoked from {{DataBlockScanner#run}} via {{scanBlockPoolSlice}}.  But cleanUp unconditionally roll()s the verificationLogs, so after two iterations we have lost the first iteration of block verification times.  As a result a cluster with just one block repeatedly rescans it every 10 seconds:
{noformat}
2012-08-16 15:59:57,884 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:verifyBlock(391)) - Verification succeeded for BP-2101131164-172.29.122.91-1337906886255:blk_7919273167187535506_4915
2012-08-16 16:00:07,904 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:verifyBlock(391)) - Verification succeeded for BP-2101131164-172.29.122.91-1337906886255:blk_7919273167187535506_4915
2012-08-16 16:00:17,925 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:verifyBlock(391)) - Verification succeeded for BP-2101131164-172.29.122.91-1337906886255:blk_7919273167187535506_4915
{noformat}
{quote}

To fix this, we need to avoid roll()ing the logs multiple times per period.
",adi2,adi2,Major,Closed,Fixed,21/Aug/12 20:09,15/Aug/14 05:41
Bug,HDFS-3829,12604274,TestHftpURLTimeouts fails intermittently with JDK7,"{{testHftpSocketTimeout}} fails if run after {{testHsftpSocketTimeout}}:

{noformat}
testHftpSocketTimeout(org.apache.hadoop.hdfs.TestHftpURLTimeouts): expected:<connect timed out> but was:<null>
{noformat}
",scurrilous,scurrilous,Major,Closed,Fixed,21/Aug/12 20:39,03/Sep/14 23:19
Bug,HDFS-3830,12604275,test_libhdfs_threaded: use forceNewInstance,"test_libhdfs_threaded should create new FileSystem instances with newInstance, to make sure that the test doesn't break because of interference between the different threads.",cmccabe,cmccabe,Minor,Closed,Fixed,21/Aug/12 20:54,11/Oct/12 17:46
Bug,HDFS-3831,12604280,Failure to renew tokens due to test-sources left in classpath,,jlowe,jlowe,Critical,Closed,Fixed,21/Aug/12 21:15,03/Sep/14 23:20
Bug,HDFS-3832,12604281,Remove protocol methods related to DistributedUpgrade,"As part of further DistributedUpgrade code deletion, this jira proposes to remove ClientProtocol#distributedUpgradeProgress() and ClientProtocol#processUpgradeCommand
* related protobuf methods, requests and responses
* related java methods, request and responses

These are non-compatible changes. It should be okay to delete them, because this functionality has not been used for a long time(last used in 0.14)",sureshms,sureshms,Major,Closed,Fixed,21/Aug/12 21:16,12/May/16 18:14
Bug,HDFS-3833,12604304,TestDFSShell fails on Windows due to file concurrent read write,"TestDFSShell sometimes fails due to the race between the write issued by the test and blockscanner. Example stack trace:
{noformat}
Error Message

c:\A\HM\build\test\data\dfs\data\data1\current\blk_-7735708801221347790 (The requested operation cannot be performed on a file with a user-mapped section open)

Stacktrace

java.io.FileNotFoundException: c:\A\HM\build\test\data\dfs\data\data1\current\blk_-7735708801221347790 (The requested operation cannot be performed on a file with a user-mapped section open)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:194)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:145)
	at java.io.PrintWriter.<init>(PrintWriter.java:218)
	at org.apache.hadoop.hdfs.TestDFSShell.corrupt(TestDFSShell.java:1133)
	at org.apache.hadoop.hdfs.TestDFSShell.testGet(TestDFSShell.java:1231)
{noformat}",brandonli,brandonli,Major,Closed,Fixed,21/Aug/12 23:47,12/May/16 18:14
Bug,HDFS-3835,12604314,Long-lived 2NN cannot perform a checkpoint if security is enabled and the NN restarts with outstanding delegation tokens,"When the 2NN wants to perform a checkpoint, it figures out the highest transaction ID of the fsimage files on the NN, and if the 2NN has a copy of that fsimage file (because it created that merged fsimage file the last time it did a checkpoint) then the 2NN won't download the fsimage file from the NN, and instead only gets the new edits files from the NN. In this case, the 2NN also doesn't even bother reloading the fsimage file it has from disk, since it has all of the namespace state in-memory. This all works just fine.

When the 2NN _doesn't_ have a copy of the relevant fsimage file (for example, if the NN had restarted since the last checkpoint) then the 2NN blows away its in-memory namespace state, downloads the fsimage file from the NN, and loads the newly-downloaded fsimage file from disk. The bug is that when the 2NN clears its in-memory state, it only resets the namespace, but not the delegation token map.

The fix is pretty simple - just make the delegation token map get cleared as well as the namespace state when a running 2NN needs to load a new fsimage from disk.

Credit to Stephen Chu for identifying this issue.",atm,atm,Major,Closed,Fixed,22/Aug/12 01:18,11/Oct/12 17:46
Bug,HDFS-3837,12604452,Fix DataNode.recoverBlock findbugs warning,"HDFS-2686 introduced the following findbugs warning:

{noformat}
Call to equals() comparing different types in org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(BlockRecoveryCommand$RecoveringBlock)
{noformat}

Both are using DatanodeID#equals but it's a different method because DNR#equals overrides equals for some reason (doesn't change behavior).",eli,eli,Major,Closed,Fixed,22/Aug/12 16:10,09/Nov/15 17:57
Bug,HDFS-3841,12604481,Port HDFS-3835 to branch-0.23,HDFS-3835 does not cleanly merge into branch-0.23.  This is to port it over.,revans2,revans2,Major,Closed,Fixed,22/Aug/12 19:57,11/Oct/12 17:46
Bug,HDFS-3846,12604579,Namenode deadlock in branch-1,"Jitendra found out the following problem:
1. Handler : Acquires namesystem lock waits on SafemodeInfo lock at SafeModeInfo.isOn()
2. SafemodeMonitor : Calls SafeModeInfo.canLeave() which is synchronized so SafemodeInfo lock is acquired, but this method also causes following call sequence needEnter() -> getNumLiveDataNodes() -> getNumberOfDatanodes() -> getDatanodeListForReport() -> getDatanodeListForReport() . The getDatanodeListForReport is synchronized with FSNamesystem lock.",brandonli,szetszwo,Major,Closed,Fixed,23/Aug/12 09:09,16/Oct/13 09:45
Bug,HDFS-3848,12604638,A Bug in recoverLeaseInternal method of FSNameSystem class,"This is a bug in logic of the method recoverLeaseInternal. In line 1322 it checks if the owner of the file is trying to recreate the file. The condition of the if statement is 

(leaseFile != null && leaseFile.equals(lease)) || lease.getHolder().equals(holder)

As it can be seen, there are two operands (conditions) connected with an ""or"" operator. The first operand is straight and will be true only if the holder of the file is the new holder. But the problem is the second operand which will be always true since the ""lease"" object is the one found by the ""holder"" by calling ""Lease lease = leaseManager.getLease(holder);"" in line 1315.

To fix this I think the if statement only should contain the following the condition:
(leaseFile != null && leaseFile.getHolder().equals(holder))",airbots,shps,Major,Closed,Fixed,23/Aug/12 16:43,03/Sep/14 20:34
Bug,HDFS-3849,12604674,"When re-loading the FSImage, we should clear the existing genStamp and leases.","When re-loading the FSImage, we should clear the existing genStamp and leases.

This is an issue in the 2NN, because it sometimes clears the existing FSImage and reloads a new one in order to get back in sync with the NN.",cmccabe,cmccabe,Critical,Closed,Fixed,23/Aug/12 19:46,11/Oct/12 17:46
Bug,HDFS-3852,12604910,TestHftpDelegationToken is broken after HADOOP-8225,It's been failing in all builds for the last 2 days or so. Git bisect indicates that it's due to HADOOP-8225.,daryn,atm,Major,Closed,Fixed,24/Aug/12 19:06,11/Oct/12 17:46
Bug,HDFS-3853,12604939,Port MiniDFSCluster enableManagedDfsDirsRedundancy option to branch-2,"MiniDFSCluster's enabledManagedDfsDirsRedunancy flag should be ported to branch-2.  Certain tests rely on it to get full coverage-- for example,  {{TestNameNodeRecovery}} and {{TestFSEditLogLoader}}.

Just to give an example, if we're using redundant edit log directories, a test like this may succeed for the wrong reasons:
1. screw up edit log directory
2. run recovery
3. test that we can start NN

With redundant edit log directories and edit log failover, step 1 is a no-op because we can just fail over to the other directory.  So we're not really testing what we need to test.  This was supposed to be ported over to branch-2 from trunk with the other stuff from HDFS-3049, but was not.",cmccabe,cmccabe,Minor,Closed,Fixed,24/Aug/12 23:32,11/Oct/12 17:46
Bug,HDFS-3856,12605084,TestHDFSServerPorts failure is causing surefire fork failure,"We have been seeing the hdfs tests on trunk and branch-2 error out with fork failures.  I see the hadoop jenkins trunk build is also seeing these:
https://builds.apache.org/view/Hadoop/job/Hadoop-trunk/lastCompletedBuild/console

",eli,tgraves,Blocker,Closed,Fixed,27/Aug/12 14:58,11/Oct/12 17:46
Bug,HDFS-3860,12605159,HeartbeatManager#Monitor may wrongly hold the writelock of namesystem,"In HeartbeatManager#heartbeatCheck, if some dead datanode is found, the monitor thread will acquire the write lock of namesystem, and recheck the safemode. If it is in safemode, the monitor thread will return from the heartbeatCheck function without release the write lock. This may cause the monitor thread wrongly holding the write lock forever.

The attached test case tries to simulate this bad scenario.",jingzhao,jingzhao,Major,Closed,Fixed,27/Aug/12 23:39,12/May/16 18:17
Bug,HDFS-3861,12605253,Deadlock in DFSClient,"The deadlock is between DFSOutputStream#close() and DFSClient#close().

",kihwal,kihwal,Blocker,Closed,Fixed,28/Aug/12 16:55,12/May/16 18:18
Bug,HDFS-3864,12605303,NN does not update internal file mtime for OP_CLOSE when reading from the edit log,"When logging an OP_CLOSE to the edit log, the NN writes out an updated file mtime and atime. However, when reading in an OP_CLOSE from the edit log, the NN does not apply these values to the in-memory FS data structure. Because of this, a file's mtime or atime may appear to go back in time after an NN restart, or an HA failover.

Most of the time this will be harmless and folks won't notice, but in the event one of these files is being used in the distributed cache of an MR job when an HA failover occurs, the job might notice that the mtime of a cache file has changed, which in MR2 will cause the job to fail with an exception like the following:

{noformat}
java.io.IOException: Resource hdfs://ha-nn-uri/user/jenkins/.staging/job_1341364439849_0513/libjars/snappy-java-1.0.3.2.jar changed on src filesystem (expected 1342137814599, was 1342137814473
	at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)
	at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)
	at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

Credit to Sujay Rau for discovering this issue.",atm,atm,Major,Closed,Fixed,28/Aug/12 21:58,11/Oct/12 17:46
Bug,HDFS-3873,12605559,Hftp assumes security is disabled if token fetch fails,"Hftp ignores all exceptions generated while trying to get a token, based on the assumption that it means security is disabled.  Debugging problems is excruciatingly difficult when security is enabled but something goes wrong.  Job submissions succeed, but tasks fail because the NN rejects the user as unauthenticated.",daryn,daryn,Major,Closed,Fixed,30/Aug/12 18:20,12/May/16 18:12
Bug,HDFS-3874,12605609,Exception when client reports bad checksum to NN,"We see the following exception in our logs on a cluster:

{code}
2012-08-27 16:34:30,400 INFO org.apache.hadoop.hdfs.StateChange: *DIR* NameNode.reportBadBlocks
2012-08-27 16:34:30,400 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:SIMPLE) cause:java.io.IOException: Cannot mark blk_8285012733733669474_140475196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.29.97.219:50010|RBW]]}(same as stored) as corrupt because datanode :0 does not exist
2012-08-27 16:34:30,400 INFO org.apache.hadoop.ipc.Server: IPC Server handler 46 on 8020, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.reportBadBlocks from 172.29.97.219:43805: error: java.io.IOException: Cannot mark blk_8285012733733669474_140475196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.29.97.219:50010|RBW]]}(same as stored) as corrupt because datanode :0 does not exist
java.io.IOException: Cannot mark blk_8285012733733669474_140475196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.29.97.219:50010|RBW]]}(same as stored) as corrupt because datanode :0 does not exist
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.markBlockAsCorrupt(BlockManager.java:1001)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.findAndMarkBlockAsCorrupt(BlockManager.java:994)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.reportBadBlocks(FSNamesystem.java:4736)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.reportBadBlocks(NameNodeRpcServer.java:537)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.reportBadBlocks(DatanodeProtocolServerSideTranslatorPB.java:242)
        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:20032)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
{code}",kihwal,tlipcon,Critical,Resolved,Fixed,30/Aug/12 22:10,28/Jan/13 18:41
Bug,HDFS-3875,12605611,Issue handling checksum errors in write pipeline,"We saw this issue with one block in a large test cluster. The client is storing the data with replication level 2, and we saw the following:
- the second node in the pipeline detects a checksum error on the data it received from the first node. We don't know if the client sent a bad checksum, or if it got corrupted between node 1 and node 2 in the pipeline.
- this caused the second node to get kicked out of the pipeline, since it threw an exception. The pipeline started up again with only one replica (the first node in the pipeline)
- this replica was later determined to be corrupt by the block scanner, and unrecoverable since it is the only replica",kihwal,tlipcon,Critical,Closed,Fixed,30/Aug/12 22:48,03/Sep/14 23:00
Bug,HDFS-3879,12605805,Fix findbugs warning in TransferFsImage on branch-2 ,"The trunk patch for HDFS-3190 fixed the following findbugs warning but the branch-2 merge missed it.

{noformat}
Dead store to buf in org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(String, String, List, Storage, boolean)
{noformat}
",eli,eli,Minor,Closed,Fixed,31/Aug/12 22:29,11/Oct/12 17:46
Bug,HDFS-3888,12606170,BlockPlacementPolicyDefault code cleanup,"BlockPlacementPolicyDefault#LOG should be removed as it hides LOG from the base class BlockPlacementPolicy. Also, in BlockPlacementPolicyDefault#chooseTarget method, the logic computing the maxTargetPerLoc can be made a separate method.",jingzhao,jingzhao,Minor,Closed,Fixed,04/Sep/12 22:18,12/May/16 18:13
Bug,HDFS-3890,12606263,filecontext mkdirs doesn't apply umask as expected,"I was attempting to set the umask of my fileContext and then do a mkdirs, but the umask wasn't applied as expected. 

doneDirFc = FileContext.getFileContext(doneDirPrefixPath.toUri(), conf);
doneDirFc.setUMask(JobHistoryUtils.HISTORY_DONE_DIR_UMASK);
doneDirFc.mkdir(path, fsp, true);

It appears to be using the default umask set in the conf (fs.permissions.umask-mode) and overrode the umask I set in fileContext. I had the default umask set to 077 and set the filecontext umask to 007.  The permissions on the directories it created were all rwx------.

",tgraves,tgraves,Critical,Closed,Fixed,05/Sep/12 15:57,11/Oct/12 17:46
Bug,HDFS-3895,12606453,hadoop-client must include commons-cli,"httpfs WAR is excluding commons-cli, the same for hadoop-client. The motivation of this exclusion was that because the usage is programmatic there was no need for having commons-cli. 

This was true until not long ago but is seem that recent changes in DFSClient are now importing commons-cli classes thus failing DistributedFileSystem class to load if commons-cli is not around.
",tucu00,tucu00,Major,Closed,Fixed,06/Sep/12 18:07,11/Oct/12 17:46
Bug,HDFS-3897,12606519,QJM: TestBlockToken fails after HDFS-3893,"HDFS-3893 caused the NN to log in using its configured Kerberos credentials when formatting the NN. This caused TestBlockToken#testBlockTokenInLastLocatedBlock to begin failing, since the test enables Kerberos but doesn't configure the NN principal or keytab.",atm,atm,Major,Resolved,Fixed,07/Sep/12 01:40,07/Sep/12 02:02
Bug,HDFS-3902,12606614,TestDatanodeBlockScanner#testBlockCorruptionPolicy is broken,"Since HDFS-3828 fixed the block scanner to not repeatedly rescan small blockpools, TestDatanodeBlockScanner times out after 13 minutes in {{waitReplication}}.",adi2,adi2,Minor,Closed,Fixed,07/Sep/12 16:38,11/Oct/12 17:46
Bug,HDFS-3905,12606683,Secure cluster cannot use hftp to an insecure cluster,"HDFS-3873 fixed the case where all exceptions acquiring tokens for hftp were ignored.  Jobs would be submitted sans tokens, and then the tasks would eventually all fail trying to get the missing token.  HDFS-3873 made jobs fail to submit if the remote cluster is secure.

Unfortunately it regressed the ability for a secure cluster to access an insecure cluster over hftp.  The issue is unique to 23 due to KSSL.",daryn,daryn,Critical,Closed,Fixed,07/Sep/12 21:22,29/Jul/14 23:02
Bug,HDFS-3916,12606980,libwebhdfs (C client) code cleanups,"Code cleanups in libwebhdfs.

* don't duplicate exception.c, exception.h, expect.h, jni_helper.c.  We have one copy of these files; we don't need 2.
* remember to set errno in all public library functions (this is part of the API)
* fix undefined symbols (if a function is not implemented, it should return ENOTSUP, but still exist)
* don't expose private data structures in the (end-user visible) public headers
* can't re-use hdfsBuilder as hdfsFS, because the strings in hdfsBuilder are not dynamically allocated.",cmccabe,cmccabe,Major,Closed,Fixed,11/Sep/12 01:27,15/Feb/13 13:12
Bug,HDFS-3919,12607120,MiniDFSCluster:waitClusterUp can hang forever,"A test run hung due to a known system config issue, but the hang was interesting:
{noformat}
2012-09-11 13:22:41,888 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
2012-09-11 13:22:42,889 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
2012-09-11 13:22:43,889 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
2012-09-11 13:22:44,890 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
{noformat}
The MiniDFSCluster should give up after a few seconds.",adi2,adi2,Minor,Closed,Fixed,11/Sep/12 20:26,06/Feb/13 17:05
Bug,HDFS-3921,12607136,NN will prematurely consider blocks missing when entering active state while still in safe mode,"I shut down all the HDFS daemons in an Highly Available (automatic failover) cluster.

Then I started one NN and it transitioned it to active. No DNs were started, and I saw the red warning link on the NN web UI:
WARNING : There are 36 missing blocks. Please check the logs or run fsck in order to identify the missing blocks.

I clicked this to go to the corrupt_files.jsp page, which ran into the following error:

{noformat}
HTTP ERROR 500

Problem accessing /corrupt_files.jsp. Reason:

    Cannot run listCorruptFileBlocks because replication queues have not been initialized.
Caused by:

java.io.IOException: Cannot run listCorruptFileBlocks because replication queues have not been initialized.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.listCorruptFileBlocks(FSNamesystem.java:5035)
	at org.apache.hadoop.hdfs.server.namenode.corrupt_005ffiles_jsp._jspService(corrupt_005ffiles_jsp.java:78)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1039)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}
",atm,schu,Major,Closed,Fixed,11/Sep/12 22:01,15/Feb/13 13:11
Bug,HDFS-3922,12607144,0.22 and 0.23 namenode throws away blocks under construction on restart,"When reading edits on startup, namenode may throw away blocks under construction. This is because the file inode is turned into a ""under construction"" one, but nothing is done to the last block. 

With append/hsync, this is not acceptable because it may drop sync'ed partial blocks.  In branch 2 and trunk, HDFS-1623 (HA) fixed this issue.",kihwal,kihwal,Critical,Resolved,Fixed,11/Sep/12 23:05,28/Sep/12 12:40
Bug,HDFS-3924,12607165,Multi-byte id in HdfsVolumeId,"Currently, the {{id}} in {{HdfsVolumeId}} is just a single byte. It'd be nice to make this larger, for situations where there's a large number of disks.",andrew.wang,andrew.wang,Major,Closed,Fixed,12/Sep/12 03:11,15/Feb/13 13:12
Bug,HDFS-3928,12607309,MiniDFSCluster should reset the first ExitException on shutdown,MiniDFSCluster doesn't reset the first exception on shutdown so if one tests fails due to an ExitException subsequent tests will as well.,eli,eli,Major,Closed,Fixed,12/Sep/12 17:27,11/Oct/12 17:46
Bug,HDFS-3931,12607320,TestDatanodeBlockScanner#testBlockCorruptionPolicy2 is broken,"Per Andy's comment on HDFS-3902:

TestDatanodeBlockScanner still fails about 1/5 runs in testBlockCorruptionRecoveryPolicy2. That's due to a separate test issue also uncovered by HDFS-3828.
The failure scenario for this one is a bit more tricky. I think I've captured the scenario below:

- The test corrupts 2/3 replicas.
- client reports a bad block.
- NN asks a DN to re-replicate, and randomly picks the other corrupt replica.
- DN notices the incoming replica is corrupt and reports it as a bad block, but does not inform the NN that re-replication failed.
- NN keeps the block on pendingReplications.
- BP scanner wakes up on both DNs with corrupt blocks, both report corruption. NN reports both as duplicates, one from the client and one from the DN report above.
since block is on pendingReplications, NN does not schedule another replication.",adi2,eli,Minor,Closed,Fixed,12/Sep/12 18:38,15/Feb/13 13:11
Bug,HDFS-3932,12607364,NameNode Web UI broken if the rpc-address is set to the wildcard   ,"If {{dfs.namenode.rpc-address}} is set to the wildcard some of links in the dfsnodelist.jsp and browseDirectory.jsp pages are broken because the nnaddr field is passed verbatim (eg nnaddr=0.0.0.0:8021).
",eli,eli,Major,Closed,Fixed,12/Sep/12 23:35,15/Feb/13 13:12
Bug,HDFS-3934,12607487,duplicative dfs_hosts entries handled wrong,"A dead DN listed in dfs_hosts_allow.txt by IP and in dfs_hosts_exclude.txt by hostname ends up being displayed twice in {{dfsnodelist.jsp?whatNodes=DEAD}} after the NN restarts because {{getDatanodeListForReport}} does not handle such a ""pseudo-duplicate"" correctly:
# the ""Remove any nodes we know about from the map"" loop no longer has the knowledge to remove the spurious entries
# the ""The remaining nodes are ones that are referenced by the hosts files"" loop does not do hostname lookups, so does not know that the IP and hostname refer to the same host.

Relatedly, such an IP-based dfs_hosts entry results in a cosmetic problem in the JSP output:  The *Node* column shows "":50010"" as the nodename, with HTML markup {{<a href=""http://:50075/browseDirectory.jsp?namenodeInfoPort=50070&amp;dir=%2F&amp;nnaddr=172.29.97.196:8020"" title=""172.29.97.216:50010"">:50010</a>}}.",cmccabe,adi2,Minor,Closed,Fixed,13/Sep/12 17:36,03/Sep/14 23:02
Bug,HDFS-3936,12607570,MiniDFSCluster shutdown races with BlocksMap usage,"Looks like HDFS-3664 didn't fix the whole issue because the added join times out because the thread closing the BM (FSN#stopCommonServices) holds the FSN lock while closing the BM and the BM is block uninterruptedly trying to aquire the FSN lock.

{noformat}
2012-09-13 18:54:12,526 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit
org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null
stack trace
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)
	at java.lang.Thread.run(Thread.java:662)
{noformat}",eli,eli,Major,Closed,Fixed,14/Sep/12 05:35,15/Feb/13 13:11
Bug,HDFS-3938,12607668,remove current limitations from HttpFS docs,"the docs still state that HttpFS does not support delegation tokens, we should remove that.",tucu00,tucu00,Major,Closed,Fixed,14/Sep/12 16:08,15/Feb/13 13:11
Bug,HDFS-3948,12608092,TestWebHDFS#testNamenodeRestart occasionally fails,"After fixing HDFS-3936 I noticed that TestWebHDFS#testNamenodeRestart fails when looping it, on my system it takes about 40 runs. WebHdfsFileSystem#close is racing with restart and resulting in an add block after the edit log is closed.",jingzhao,eli,Minor,Closed,Fixed,18/Sep/12 16:49,09/Oct/18 15:15
Bug,HDFS-3949,12608099,NameNodeRpcServer#join should join on both client and server RPC servers,"Per HDFS-3939 NameNodeRpcServer#join should join on both the client and server RPC servers, otherwise NN may continue to shutdown (for tests) while the server RPC server is still running.",eli,eli,Minor,Closed,Fixed,18/Sep/12 17:41,15/Feb/13 13:11
Bug,HDFS-3951,12608136,datanode web ui does not work over HTTPS when datanode is started in secure mode,"When a datanode is configured to work in secure mode, the HttpServer connector listener is created by the SecureDataNodeStarter class in a privileged port and given to the HttpServer constructor to use.

When enabling SSL for the web ui the connector listener is created by the HttpServer if no connector is given in the constructor.

This means that in secure mode the DataNode HttpServer starts always in HTTP ",tucu00,tucu00,Major,Closed,Fixed,18/Sep/12 21:13,01/Aug/13 17:32
Bug,HDFS-3961,12608509,FSEditLog preallocate() needs to reset the position of PREALLOCATE_BUFFER when more than 1MB size is needed,"In the new preallocate() function, when the required size is larger 1MB, we need to reset the position for PREALLOCATION_BUFFER every time when we have allocated 1MB. Otherwise seems only 1MB can be allocated even if need is larger than 1MB.",jingzhao,jingzhao,Major,Closed,Fixed,20/Sep/12 18:26,15/May/13 05:16
Bug,HDFS-3963,12608550,backport namenode/datanode serviceplugin to branch-1,backport namenode/datanode serviceplugin to branch-1,brandonli,brandonli,Major,Closed,Fixed,21/Sep/12 00:04,15/May/13 05:15
Bug,HDFS-3964,12608571,Make NN log of fs.defaultFS debug rather than info,"HDFS-3939 added a log where the NN clobbers fs.defaultFS, this is used to determine the address of remote NNs as well, not just logged on NN startup, so better to have it as a debug option.",eli,eli,Minor,Closed,Fixed,21/Sep/12 04:20,15/Feb/13 13:12
Bug,HDFS-3966,12608684,"For branch-1, TestFileCreation should use JUnit4 to make assumeTrue work","Currently in TestFileCreation for branch-1, assumeTrue() is used by two test cases in order to check if the OS is Linux. Thus JUnit 4 should be used to enable assumeTrue.",jingzhao,jingzhao,Minor,Closed,Fixed,21/Sep/12 19:50,02/Apr/15 15:58
Bug,HDFS-3969,12608948,Small bug fixes and improvements for disk locations API,"The new disk block locations API has a configurable timeout, but it's used inconsistently: the invokeAll() call to the thread pool assumes the timeout is in seconds, but the RPC timeout is set in milliseconds.

Also, we can improve the wire protocol for this API to be a lot more efficient.",tlipcon,tlipcon,Major,Closed,Fixed,25/Sep/12 00:02,10/Apr/14 13:12
Bug,HDFS-3970,12609034,BlockPoolSliceStorage#doRollback(..) should use BlockPoolSliceStorage instead of DataStorage to read prev version file.,"{code}    // read attributes out of the VERSION file of previous directory
    DataStorage prevInfo = new DataStorage();
    prevInfo.readPreviousVersionProperties(bpSd);{code}

In the above code snippet BlockPoolSliceStorage instance should be used. other wise rollback results in 'storageType' property missing which will not be there in initial VERSION file.",vinayakumarb,vinayakumarb,Major,Closed,Fixed,25/Sep/12 13:32,12/May/16 18:18
Bug,HDFS-3972,12609107,Trash emptier fails in secure HA cluster,"In a secure HA cluster, we're seeing the following issue on the NN when the trash emptier tries to run:

WARN org.apache.hadoop.fs.TrashPolicyDefault: Trash can't list homes: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host \
is: ""xxxxx""; destination host is: ""xxxx"":8020;  Sleeping.

The issue seems to be that the trash emptier thread sends RPCs back to itself, but isn't wrapped in a doAs. Credit goes to Stephen Chu for discovering this.",tlipcon,tlipcon,Critical,Closed,Fixed,25/Sep/12 19:29,12/May/16 18:17
Bug,HDFS-3979,12609173,Fix hsync semantics,"See discussion in HDFS-744. The actual sync/flush operation in BlockReceiver is not on a synchronous path from the DFSClient, hence it is possible that a DN loses data that it has already acknowledged as persisted to a client.

Edit: Spelling.
",larsh,larsh,Major,Closed,Fixed,26/Sep/12 04:52,15/Feb/13 13:12
Bug,HDFS-3981,12609296,access time is set without holding FSNamesystem write lock,"Incorrect condition in {{FSNamesystem.getBlockLocatoins()}} can lead to updating times without write lock. In most cases this condition will force {{FSNamesystem.getBlockLocatoins()}} to hold write lock, even if times do not need to be updated.",teledriver,teledriver,Major,Closed,Fixed,26/Sep/12 19:29,03/Sep/14 23:34
Bug,HDFS-3985,12609406,Add timeouts to TestMulitipleNNDataBlockScanner,Saw TestMulitipleNNDataBlockScanner hang in a jenkins job recently. Let's add the relevant test timeouts so we can see what's going on.,,eli,Major,Closed,Fixed,27/Sep/12 16:38,15/Feb/13 13:12
Bug,HDFS-3990,12609538,NN's health report has severe performance problems,"The dfshealth page will place a read lock on the namespace while it does a dns lookup for every DN.  On a multi-thousand node cluster, this often results in 10s+ load time for the health page.  10 concurrent requests were found to cause 7m+ load times during which time write operations blocked.",daryn,daryn,Critical,Closed,Fixed,28/Sep/12 13:40,12/May/16 18:12
Bug,HDFS-3992,12609256,Method org.apache.hadoop.hdfs.TestHftpFileSystem.tearDown() sometimes throws NPEs,"Recommended to add null-checking.
Suggested patch is attached.",iveselovsky,iveselovsky,Minor,Closed,Fixed,26/Sep/12 14:44,15/Feb/13 13:11
Bug,HDFS-3996,12609952,Add debug log removed in HDFS-3873 back,Per HDFS-3873 let's add the debug log back.,eli,eli,Minor,Closed,Fixed,02/Oct/12 18:12,06/Feb/13 17:05
Bug,HDFS-3997,12609973,OfflineImageViewer incorrectly passes value of imageVersion when visiting IS_COMPRESSED element,"Rumen's processing of FSImage logs reports the value of ""IS_COMPRESSED"" incorrectly as ""-39"" (or whatever the image-version is).

The problem is in ImageLoaderCurrent, where the FSIMAGE_COMPRESSION node is visited using the imageVersion value instead of the value of isCompressed.) A fix is forthcoming.",mithun,mithun,Trivial,Closed,Fixed,02/Oct/12 20:12,15/Feb/13 13:12
Bug,HDFS-3999,12609995,"HttpFS OPEN operation expects len parameter, it should be length","WebHDFS API defines *length* as the parameter for partial length for OPEN operations, HttpFS is using *len*",tucu00,tucu00,Major,Closed,Fixed,02/Oct/12 22:07,15/Feb/13 13:12
Bug,HDFS-4001,12610035,TestSafeMode#testInitializeReplQueuesEarly may time out,"Saw this failure on a recent branch-2 jenkins run, has also been seen on trunk.

{noformat}
java.util.concurrent.TimeoutException: Timed out waiting for condition
	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:107)
	at org.apache.hadoop.hdfs.TestSafeMode.testInitializeReplQueuesEarly(TestSafeMode.java:191)
{noformat}
",,eli,Major,Resolved,Fixed,03/Oct/12 06:55,20/Jul/15 18:49
Bug,HDFS-4003,12610162,test-patch should build the common native libs before running hdfs tests ,"HDFS-3753 added a dependency on common's native build, let's modify test-patch to run the common native build before running Hdfs tests so Hdfs tests can depend on libhadoop (eg TestHdfsNativeCodeLoader).",cmccabe,eli,Major,Resolved,Fixed,03/Oct/12 23:37,12/May/16 18:13
Bug,HDFS-4006,12610418,TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit,"TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit, due to an NPE while checkpointing. It looks like the background checkpoint fails, conflicts with the explicit checkpoints done by the tests (note the backtrace is not for the doCheckpoint calls in the tests.

{noformat}
2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit
org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null
stack trace
java.lang.NullPointerException
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)
at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)
at java.lang.Thread.run(Thread.java:662)
{noformat}",tlipcon,eli,Major,Closed,Fixed,04/Oct/12 22:05,15/Feb/13 13:11
Bug,HDFS-4013,12610669,TestHftpURLTimeouts throws NPE,"The case fails at line 116, where message is null. I guess this may be an openjdk-specific behavior, but it would be nice to have it fixed although openjdk is not officially supported.

FYI: The exception is thrown with null message at java.net.SocksSocketImpl.

{code}
    private static int remainingMillis(long deadlineMillis) throws IOException {
        if (deadlineMillis == 0L)
            return 0;

        final long remaining = deadlineMillis - System.currentTimeMillis();
        if (remaining > 0)
            return (int) remaining;

        throw new SocketTimeoutException();
    }
{code}
",stepinto,stepinto,Trivial,Closed,Fixed,06/Oct/12 00:45,27/Aug/13 22:07
Bug,HDFS-4016,12610800,back-port HDFS-3582 to branch-0.23,We suggest a patch that back-ports the change https://issues.apache.org/jira/browse/HDFS-3582 to branch 0.23.,iveselovsky,iveselovsky,Minor,Closed,Fixed,08/Oct/12 07:22,06/Feb/13 17:05
Bug,HDFS-4018,12610843,TestDataNodeMultipleRegistrations#testMiniDFSClusterWithMultipleNN is missing some cluster cleanup,"TestDataNodeMultipleRegistrations#testMiniDFSClusterWithMultipleNN does not unconditionally shutdown the cluster if a call to addNameNode fails, so if one of these calls fails other tests may failed due to the NN directory already being locked.",eli,eli,Minor,Closed,Fixed,08/Oct/12 16:56,15/Feb/13 13:12
Bug,HDFS-4020,12610863,TestRBWBlockInvalidation may time out,I've seen TestRBWBlockInvalidation time out mvn hard on trunk recently. Let's make sure the methods have time outs so it fails cleanly.,eli,eli,Major,Closed,Fixed,08/Oct/12 19:56,15/Feb/13 13:12
Bug,HDFS-4021,12610881,Misleading error message when resources are low on the NameNode,"When resources are low on the namenode, it enters SafeMode with a message like this:
{code}
14:00:10,666  WARN NameNodeResourceChecker:89 - Space available on
volume '/dev/hda1' is 104357888, which is below the configured
reserved amount 104857600
14:00:10,670  WARN FSNamesystem:3190 - NameNode low on available disk
space. Entering safe mode.
14:00:10,670  INFO StateChange:3836 - STATE* Safe mode is ON.
Resources are low on NN. Safe mode must be turned off manually.
{code}

However, turning off safe mode manually has no effect, since it immediately puts itself into Safe Mode again with a log message like this:

{code}
 14:00:10,666  WARN NameNodeResourceChecker:89 - Space available on
volume '/dev/hda1' is 104357888, which is below the configured
reserved amount 104857600
14:00:10,670  WARN FSNamesystem:3190 - NameNode low on available disk
space. Entering safe mode.
14:00:10,670  INFO StateChange:3836 - STATE* Safe mode is ON.
Resources are low on NN. Safe mode must be turned off manually.
{code}

From the shell, it looks like this:
{code}
 [cmccabe@vm1 h]$ ./bin/hdfs dfsadmin -safemode get
Safe mode is ON
[cmccabe@vm1 h]$ ./bin/hdfs dfsadmin -safemode leave
Safe mode is OFF
[cmccabe@vm1 h]$ ./bin/hdfs dfsadmin -safemode get
Safe mode is ON
{code}

It seems like we should change the message about turning off safe mode manually, if turning off safe mode manually does not actually work for the case where resources are low.  Probably we need to explain that safe mode should be turned off manually *after* adding more resources.  As it is, the error message seems misleading.",cconner,cmccabe,Minor,Closed,Fixed,08/Oct/12 21:18,15/Feb/13 13:11
Bug,HDFS-4022,12610975,Replication not happening for appended block,"Block written and finalized
Later append called. Block GenTS got changed.

DN side log 
""Can't send invalid block BP-407900822-192.xx.xx.xx-1348830837061:blk_-9185630731157263852_108738"" logged continously

NN side log
""INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Error report from DatanodeRegistration(192.xx.xx.xx, storageID=DS-2040532042-192.xx.xx.xx-50010-1348830863443, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=123456;nsid=116596173;c=0): Can't send invalid block BP-407900822-192.xx.xx.xx-1348830837061:blk_-9185630731157263852_108738"" also logged continuosly.

The block checked for tansfer is the one with old genTS whereas the new block with updated genTS exist in the data dir.
",vinayakumarb,suja,Blocker,Closed,Fixed,09/Oct/12 12:18,12/May/16 18:16
Bug,HDFS-4036,12611289,FSDirectory.unprotectedAddFile(..) should not throw UnresolvedLinkException,"The code in FSDirectory.unprotectedAddFile(..) does not throw UnresolvedLinkException, we should remove ""throws UnresolvedLinkException"" from the declaration.",jingzhao,szetszwo,Major,Closed,Fixed,11/Oct/12 03:17,12/May/16 18:17
Bug,HDFS-4043,12611474,Namenode Kerberos Login does not use proper hostname for host qualified hdfs principal name.,"The Namenode uses the loginAsNameNodeUser method in NameNode.java to login using the hdfs principal. This method in turn invokes SecurityUtil.login with a hostname (last parameter) obtained via a call to InetAddress.getHostName. This call does not always return the fully qualified host name, and thus causes the namenode to login to fail due to kerberos's inability to find a matching hdfs principal in the hdfs.keytab file. Instead it should use InetAddress.getCanonicalHostName. This is consistent with what is used internally by SecurityUtil.java to login in other services, such as the DataNode. ",svaughan,ahadr,Major,Resolved,Fixed,12/Oct/12 00:43,22/Aug/22 21:35
Bug,HDFS-4044,12611486,Duplicate ChecksumType definition in HDFS .proto files,"Both hdfs.proto and datatransfer.proto define ChecksumType enum, datatransfer.proto already includes hdfs.proto, so it should reuse ChecksumTypeProto enum.",decster,decster,Major,Closed,Fixed,12/Oct/12 06:20,15/Feb/13 13:11
Bug,HDFS-4046,12611553,ChecksumTypeProto use NULL as enum value which is illegal in C/C++,"I tried to write a native hdfs client using protobuf based protocol, when I generate c++ code using hdfs.proto, the generated file can not compile, because NULL is an already defined macro.
I am thinking two solutions:
1. refactor all DataChecksum.Type.NULL references to NONE, which should be fine for all languages, but this may breaking compatibility.
2. only change protobuf definition ChecksumTypeProto.NULL to NONE, and use enum integer value(DataChecksum.Type.id) to convert between ChecksumTypeProto and DataChecksum.Type, and make sure enum integer values are match(currently already match).
I can make a patch for solution 2.
 ",decster,decster,Minor,Closed,Fixed,12/Oct/12 16:27,03/Sep/14 23:09
Bug,HDFS-4049,12611781,hflush performance regression due to nagling delays,"HDFS-3721 reworked the way that packets are mirrored through the pipeline in the datanode. This caused two write() calls where there used to be one, which interacts badly with nagling so that there are 40ms bubbles on hflush() calls. We didn't notice this in the tests because the hflush perf test only uses a single datanode.",tlipcon,tlipcon,Critical,Closed,Fixed,15/Oct/12 05:17,12/May/16 18:11
Bug,HDFS-4055,12611898,TestAuditLogs is flaky,"{code}
    InputStream istream = webfs.open(file);
    int val = istream.read();
    istream.close();

    verifyAuditLogsRepeat(true, 3);
    assertTrue(""failed to read from file"", val >= 0);
{code}
{code}
    byte[] toWrite = new byte[bufferLen];
    Random rb = new Random(seed);
    long bytesToWrite = fileLen;
    while (bytesToWrite>0) {
      rb.nextBytes(toWrite);
{code}
InputStream.read() return the first byte of the file, the bytes in the file is generated in using Random.nextBytes(), so you get 1/256 chance the first byte is 0, so some times it may fail.",decster,decster,Major,Closed,Fixed,15/Oct/12 20:32,15/Feb/13 13:11
Bug,HDFS-4061,12611954,TestBalancer and TestUnderReplicatedBlocks need timeouts,"Saw TestBalancer and TestUnderReplicatedBlocks timeout hard on a jenkins job recently, let's annotate the relevant tests with timeouts.",eli,eli,Major,Closed,Fixed,16/Oct/12 02:57,15/Feb/13 13:12
Bug,HDFS-4065,12612114,TestDFSShell.testGet sporadically fails attempting to corrupt block files due to race condition,"TestDFSShell.testGet attempts to simulate corruption of block files in order to test hadoop fs -get with the -ignoreCrc option.  It is possible that the data node's DataXceiver thread has not yet closed the block file.  This causes a locking violation on Windows, so the test fails.",cnauroth,cnauroth,Major,Resolved,Fixed,16/Oct/12 22:25,18/Oct/12 00:13
Bug,HDFS-4067,12612148,TestUnderReplicatedBlocks may fail due to ReplicaAlreadyExistsException,"After adding the timeout to TestUnderReplicatedBlocks in HDFS-4061 we can see the root cause of the failure is ReplicaAlreadyExistsException:

{noformat}
org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-1541130889-172.29.121.238-1350435573411:blk_-3437032108997618258_1002 already exists in state FINALIZED and thus cannot be created.
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary(FsDatasetImpl.java:799)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary(FsDatasetImpl.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:155)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:393)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:98)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:66)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:219)
{noformat}",jingzhao,eli,Major,Resolved,Fixed,17/Oct/12 04:53,13/May/16 05:20
Bug,HDFS-4072,12612310,On file deletion remove corresponding blocks pending replication,"Currently when deleting a file, blockManager does not remove records that are corresponding to the file's blocks from pendingRelications. These records can only be removed after timeout (5~10 min).",jingzhao,jingzhao,Minor,Closed,Fixed,17/Oct/12 20:44,12/May/16 18:13
Bug,HDFS-4075,12612480,Reduce recommissioning overhead,"When datanodes are recommissioned, {BlockManager#processOverReplicatedBlocksOnReCommission()} is called for each rejoined node and excess blocks are added to the invalidate list. The problem is this is done while the namesystem write lock is held.
",kihwal,kihwal,Critical,Closed,Fixed,18/Oct/12 19:31,03/Sep/14 23:19
Bug,HDFS-4080,12612521,Add a separate logger for block state change logs to enable turning off those logs,"Although the block-level logging in namenode is useful for debugging, it can add a significant overhead to busy hdfs clusters since they are done while the namespace write lock is held. One example is shown in HDFS-4075. In this example, the write lock was held for 5 minutes while logging 11 million log messages for 5.5 million block invalidation events. 

It will be useful if we have an option to disable these block-level log messages and keep other state change messages going.  If others feel that they can turned into DEBUG (with addition of isDebugEnabled() checks), that may also work too, but there might be people depending on the messages.",kihwal,kihwal,Major,Closed,Fixed,19/Oct/12 01:34,12/May/16 18:14
Bug,HDFS-4090,12612784,getFileChecksum() result incompatible when called against zero-byte files.,"When getFileChecksum() is called against a zero-byte file, the branch-1 client returns MD5MD5CRC32FileChecksum with crcPerBlock=0, bytePerCrc=0 and md5=70bc8f4b72a86921468bf8e8441dce51, whereas a null is returned in trunk.

The null makes sense since there is no actual block checksums, but this breaks the compatibility when doing distCp and calling getFileChecksum() via webhdfs or hftp.

This JIRA is to make the client to return the same 'magic' value that the branch-1 and earlier clients return.",kihwal,kihwal,Critical,Closed,Fixed,19/Oct/12 21:29,03/Sep/14 23:19
Bug,HDFS-4093,12612799,"In branch-1-win, AzureBlockPlacementPolicy#chooseTarget only returns one DN when replication factor is greater than 3. ","In branch-1-win, when AzureBlockPlacementPolicy (which extends the BlockPlacementPolicyDefault) is used, if the client increases the number of replicas (e.g., from 3 to 10), AzureBlockPlacementPolicy#chooseTarget will return only 1 Datanode each time. Thus in FSNameSystem#computeReplicationWorkForBlock, it is possible that the replication monitor may choose a datanode that has been chosen as target but still in the pendingReplications (because computeReplicationWorkForBlock does not check the pending replication before doing the chooseTarget). 

To avoid this ""hit-the-same-datanode"" scenario, we modify the AzureBlockPlacementPolicy#chooseTarget to make it return multiple DN. ",jingzhao,jingzhao,Major,Resolved,Fixed,19/Oct/12 22:23,24/Oct/12 00:02
Bug,HDFS-4099,12612901,Clean up replication code and add more javadoc,"- FSNamesystem.checkReplicationFactor(..) should be combined with BlockManager.checkReplication(..).
- Add javadoc to the replication related method in BlockManager.
- Also clean up those methods.",szetszwo,szetszwo,Minor,Closed,Fixed,22/Oct/12 00:16,15/Feb/13 13:11
Bug,HDFS-4104,12613058,dfs -test -d prints inappropriate error on nonexistent directory,"Running {{hdfs dfs -test -d foo}} should return 0 or 1 as appropriate. It should not generate any output due to missing files.  Alas, it prints an error message when {{foo}} does not exist.

{code}
$ hdfs dfs -test -d foo; echo $?
test: `foo': No such file or directory
1
{code}
",adi2,adi2,Minor,Closed,Fixed,22/Oct/12 22:01,03/Sep/14 23:09
Bug,HDFS-4105,12613073,the SPNEGO user for secondary namenode should use the web keytab,"This is similar to HDFS-3466 where we made sure the namenode checks for the web keytab before it uses the namenode keytab.

The same needs to be done for secondary namenode as well.

{code}
String httpKeytab = 
              conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);
            if (httpKeytab != null && !httpKeytab.isEmpty()) {
              params.put(""kerberos.keytab"", httpKeytab);
            }
{code}",arpitgupta,arpitgupta,Major,Closed,Fixed,22/Oct/12 23:19,03/Sep/14 23:06
Bug,HDFS-4106,12613080,"BPServiceActor#lastHeartbeat, lastBlockReport and lastDeletedReport should be declared as volatile","All these variables may be assigned/read by a testing thread (through BPServiceActor#triggerXXX) while also assigned/read by the actor thread. Thus they should be declared as volatile to make sure the ""happens-before"" consistency.",jingzhao,jingzhao,Minor,Closed,Fixed,23/Oct/12 00:24,12/May/16 18:16
Bug,HDFS-4107,12613096,Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction,"In the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.  Let's add utility methods for such checks. ",szetszwo,szetszwo,Major,Closed,Fixed,23/Oct/12 03:34,17/Nov/14 08:56
Bug,HDFS-4112,12613590,A few improvements on INodeDirectory,"Below are a few improvements on INodeDirectory
- Add a utility method for casting from INode;
- avoid creating new empty list and INode arrays;
- renames getChildren() and getChildrenRaw(), rewrite the javadoc.

Also,
- clean up isDirEmpty(..) and isEmpty() in FSDirectory",szetszwo,szetszwo,Major,Closed,Fixed,25/Oct/12 23:57,15/Feb/13 13:12
Bug,HDFS-4115,12613680,TestHDFSCLI.testAll fails one test due to number format,"This test fails repeatedly on only one of my machines:

{noformat}
Failed tests:   testAll(org.apache.hadoop.cli.TestHDFSCLI): One of the tests failed. See the Detailed results to identify the command that failed

           Test ID: [587]
  Test Description: [report: Displays the report about the Datanodes]
     Test Commands: [-fs hdfs://localhost:35254 -report]
        Comparator: [RegexpComparator]
Comparision result:   [fail]
   Expected output:   [Configured Capacity: [0-9]+ \([0-9]+\.[0-9]+ [BKMGT]+\)]
     Actual output:   [Configured Capacity: 472446337024 (440 GB)
{noformat}

The problem appears to be that {{StringUtils.byteDesc}} calls {{limitDecimalTo2}} which calls {{DecimalFormat.format}} with a pattern of {{#.##}}. This pattern does not include trailing zeroes, so the expected regex is incorrect in requiring a decimal.",scurrilous,scurrilous,Minor,Resolved,Fixed,26/Oct/12 16:32,12/May/16 18:12
Bug,HDFS-4122,12613770,Cleanup HDFS logs and reduce the size of logged messages,"I have attached the patch that removes unnecessary information from the log such as ""Namesystem."" and prefixing file information with ""file"" and block information with ""block"" etc.

On a branch-1 log I saw it reduce the log size by ~10%.",sureshms,sureshms,Major,Closed,Fixed,27/Oct/12 19:00,03/Sep/14 23:11
Bug,HDFS-4127,12613921,Log message is not correct in case of short of replica,"For some reason that block cannot be placed with enough replica (like no enough available data nodes), it will throw a warning with wrong number of replica in short.",junping_du,junping_du,Minor,Closed,Fixed,29/Oct/12 17:24,03/Sep/14 23:09
Bug,HDFS-4128,12613955,2NN gets stuck in inconsistent state if edit log replay fails in the middle,"We saw the following issue in a cluster:
- The 2NN downloads an edit log segment:
{code}
2012-10-29 12:30:57,433 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /xxxxxxx/current/edits_0000000000049136809-0000000000049176162 expecting start txid #49136809
{code}
- It fails in the middle of replay due to an OOME:
{code}
2012-10-29 12:31:21,021 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation AddOp [length=0, path=/xxxxxxxx
java.lang.OutOfMemoryError: Java heap space
{code}
- Future checkpoints then fail because the prior edit log replay only got halfway through the stream:
{code}
2012-10-29 12:32:21,214 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /xxxxx/current/edits_0000000000049176163-0000000000049177224 expecting start txid #49144432
2012-10-29 12:32:21,216 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: There appears to be a gap in the edit log.  We expected txid 49144432, but got txid 49176163.
{code}",kihwal,tlipcon,Major,Closed,Fixed,29/Oct/12 21:24,03/Sep/14 22:55
Bug,HDFS-4132,12614289,"when libwebhdfs is not enabled, nativeMiniDfsClient frees uninitialized memory ","When libwebhdfs is not enabled, nativeMiniDfsClient frees uninitialized memory.

Details: jconfStr is declared uninitialized...
{code}
struct NativeMiniDfsCluster* nmdCreate(struct NativeMiniDfsConf *conf)
{
    struct NativeMiniDfsCluster* cl = NULL;
    jobject bld = NULL, bld2 = NULL, cobj = NULL;
    jvalue  val;
    JNIEnv *env = getJNIEnv();
    jthrowable jthr;
    jstring jconfStr;
{code}

and only initialized later if conf->webhdfsEnabled:
{code}
    ...
    if (conf->webhdfsEnabled) {
        jthr = newJavaStr(env, DFS_WEBHDFS_ENABLED_KEY, &jconfStr);
        if (jthr) {
            printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
    ...
{code}

Then we try to free this uninitialized memory at the end, usually resulting in a crash.
{code}
    (*env)->DeleteLocalRef(env, jconfStr);
    return cl;
{code}",cmccabe,cmccabe,Major,Closed,Fixed,31/Oct/12 22:51,03/Sep/14 23:09
Bug,HDFS-4134,12614381,hadoop namenode & datanode entry points should return negative exit code on bad arguments,"When you go  {{hadoop namenode start}} (or some other bad argument to the namenode), a usage message is generated -but the script returns 0. 

This stops it being a robust command to invoke from other scripts -and is inconsistent with the JT & TT entry points, that do return -1 on a usage message",,stevel@apache.org,Minor,Closed,Fixed,01/Nov/12 15:35,03/Dec/12 07:33
Bug,HDFS-4138,12614432,BackupNode startup fails due to uninitialized edit log,"It was notices by TestBackupNode.testCheckpointNode failure. When a backup node is getting started, it tries to enter active state and start common services. But when it fails to start services and exits, which is caught by the exit util.",kihwal,kihwal,Major,Resolved,Fixed,01/Nov/12 21:17,02/May/13 02:29
Bug,HDFS-4139,12614437,fuse-dfs RO mode still allows file truncation,"Mounting a fuse-dfs in readonly mode with ""-oro"" still allows the user to truncate files.
{noformat}
$ grep fuse /etc/fstab
hadoop-fuse-dfs#dfs://ubu-cdh-0.local /export/hdfs fuse noauto,ro 0 0
$ sudo mount /export/hdfs
$ hdfs dfs -ls /tmp
...
-rw-r--r--   3 ubuntu hadoop          4 2012-11-01 14:18 /tmp/blah.txt
$ echo foo > /export/hdfs/tmp/blah.txt
-bash: /export/hdfs/tmp/blah.txt: Permission denied
$  hdfs dfs -ls /tmp
...
-rw-r--r--   3 ubuntu hadoop          0 2012-11-01 14:28 /tmp/blah.txt
$ ps ax | grep dfs
...
13639 ?        Ssl    0:02 /usr/lib/hadoop/bin/fuse_dfs dfs://ubu-cdh-0.local /export/hdfs -o ro,dev,suid
{noformat}",cmccabe,adi2,Major,Closed,Fixed,01/Nov/12 22:16,13/Mar/15 23:14
Bug,HDFS-4140,12614438,fuse-dfs handles open(O_TRUNC) poorly,"fuse-dfs handles open(O_TRUNC) poorly.

It is converted to multiple fuse operations.  Those multiple fuse operations often fail (for example, calling fuse_truncate_impl() while a file is also open for write results in a ""multiple writers!"" exception.)

One easy way to see the problem is to run the following sequence of shell commands:
{noformat}
ubuntu@ubu-cdh-0:~$ echo foo > /export/hdfs/tmp/a/t1.txt
ubuntu@ubu-cdh-0:~$ ls -l /export/hdfs/tmp/a
total 0
-rw-r--r-- 1 ubuntu hadoop 4 Nov  1 15:21 t1.txt
ubuntu@ubu-cdh-0:~$ hdfs dfs -ls /tmp/a
Found 1 items
-rw-r--r--   3 ubuntu hadoop          4 2012-11-01 15:21 /tmp/a/t1.txt
ubuntu@ubu-cdh-0:~$ echo bar > /export/hdfs/tmp/a/t1.txt
ubuntu@ubu-cdh-0:~$ ls -l /export/hdfs/tmp/a
total 0
-rw-r--r-- 1 ubuntu hadoop 0 Nov  1 15:22 t1.txt
ubuntu@ubu-cdh-0:~$ hdfs dfs -ls /tmp/a
Found 1 items
-rw-r--r--   3 ubuntu hadoop          0 2012-11-01 15:22 /tmp/a/t1.txt
{noformat}",cmccabe,adi2,Major,Closed,Fixed,01/Nov/12 22:23,15/Feb/13 13:12
Bug,HDFS-4145,12614478,Merge hdfs cmd line scripts from branch-1-win,Tracking Jira for merging hdfs cmd line scripts from branch-1-win to trunk. Scripts also have to be updated to reflect their unix equivalents.,ivanmi,ivanmi,Major,Resolved,Fixed,02/Nov/12 07:35,12/Nov/12 19:59
Bug,HDFS-4156,12615026,Seeking to a negative position should throw an IOE,"Credit to [~vanzin] for finding this. If you seek() to a position before the start of the file, the seek() call succeeds, and the following read() call throws an NPE. A more friendly approach would be to fail the seek() call.

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:670)
at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:685)
at java.io.DataInputStream.read(DataInputStream.java:83)
at fstest.main(fstest.java:12)
{noformat}",initialcontext,eli,Major,Closed,Fixed,06/Nov/12 21:43,15/Feb/13 13:12
Bug,HDFS-4161,12615237,HDFS keeps a thread open for every file writer,"In 1.0 release DFSClient uses a thread per file writer. In some use cases (dynamic partions in hive) that use a large number of file writers a large number of threads are created. The file writer thread has the following stack:
{noformat}
at java.lang.Thread.sleep(Native Method)
at org.apache.hadoop.hdfs.DFSClient$LeaseChecker.run(DFSClient.java:1462)
at java.lang.Thread.run(Thread.java:662)
{noformat}

This problem has been fixed in later releases. This jira will post a consolidated patch from various jiras that addresses the issue.",szetszwo,sureshms,Major,Closed,Fixed,07/Nov/12 22:14,04/Dec/12 19:01
Bug,HDFS-4162,12615240,Some malformed and unquoted HTML strings are returned from datanode web ui,"When browsing to the datanode at /browseDirectory.jsp, if a path with HTML characters is requested, the resulting error page echos back the input unquoted.

Example:

http://localhost:50075/browseDirectory.jsp?dir=/<xss>&go=go&namenodeInfoPort=50070&nnaddr=localhost%3A9000

Writes an input element as part of the response:

<input name=""dir"" type=""text"" width=""50"" id""dir"" value=""/<xss>"">

- The value of the ""value"" attribute is not quoted. 
- An = must follow the ""id"" attribute name.
- Element ""input"" should have a closing tag.

The output should be something like:

<input name=""dir"" type=""text"" width=""50"" id=""dir"" value=""/&lt;xss&gt;""/>


In addition, if one creates a directory:

hdfs dfs -put '/some/path/to/<xss>'

Then browsing to the parent of directory '<xss>' prints unquoted HTML in the directory names.
",dagit,dagit,Minor,Closed,Fixed,07/Nov/12 22:17,03/Sep/14 23:19
Bug,HDFS-4163,12615253,HDFS distribution build fails on Windows,Distribution build relies on sh scripts that do not work on Windows.,cnauroth,cnauroth,Major,Resolved,Fixed,07/Nov/12 23:47,13/Nov/12 22:28
Bug,HDFS-4164,12615266,fuse_dfs: add -lrt to the compiler command line on Linux,We need to add -ltr to the compiler command line on Linux in order to use clock_gettime on OpenSuSE 12.1.,cmccabe,cmccabe,Minor,Closed,Fixed,08/Nov/12 01:23,15/Feb/13 13:12
Bug,HDFS-4165,12615267,Faulty sanity check in FsDirectory.unprotectedSetQuota,"According to the documentation:

The quota can have three types of values : (1) 0 or more will set 
the quota to that value, (2) {@link HdfsConstants#QUOTA_DONT_SET}  implies 
the quota will not be changed, and (3) {@link HdfsConstants#QUOTA_RESET} 
implies the quota will be reset. Any other value is a runtime error.

sanity check in FsDirectory.unprotectedSetQuota should use 

{code}
nsQuota != HdfsConstants.QUOTA_RESET
{code}

rather than

{code}
nsQuota < HdfsConstants.QUOTA_RESET
{code}

Since HdfsConstants.QUOTA_RESET is defined to be -1, there is not any problem for this code, but it is better to do it right.
",decster,decster,Trivial,Closed,Fixed,08/Nov/12 01:27,12/May/16 18:17
Bug,HDFS-4168,12615370,TestDFSUpgradeFromImage fails in branch-1,"{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeBlocks(FSNamesystem.java:2212)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removePathAndBlocks(FSNamesystem.java:2225)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedDelete(FSDirectory.java:645)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:833)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1024)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:841)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:402)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:367)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:104)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:420)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:388)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:285)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:546)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1444)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:173)
	at org.apache.hadoop.hdfs.TestDFSUpgradeFromImage.testUpgradeFromImage(TestDFSUpgradeFromImage.java:185)
{noformat}",jingzhao,szetszwo,Major,Closed,Fixed,08/Nov/12 19:36,15/May/13 05:16
Bug,HDFS-4171,12615527,WebHDFS and HttpFs should accept only valid Unix user names,"HttpFs tries to use UserProfile.USER_PATTERN to match all usernames before a doAs impersonation function. This regex is too strict for most usernames, as it disallows any special character at all. We should relax it more or ditch needing to match things there.

WebHDFS currently has no such limitations.",tucu00,qwertymaniac,Major,Closed,Fixed,09/Nov/12 17:59,15/Feb/13 13:11
Bug,HDFS-4172,12615557,namenode does not URI-encode parameters when building URI for datanode request,"Param values such as

foo&bar

or 

foo=bar

Are not escaped in Param.toSortedString()

When these are given as, say, token parameter values, a string like

&token=foo&bar&token=foo=bar

is returned.",dagit,dagit,Minor,Closed,Fixed,09/Nov/12 22:40,03/Sep/14 23:19
Bug,HDFS-4176,12615772,EditLogTailer should call rollEdits with a timeout,"When the EditLogTailer thread calls rollEdits() on the active NN via RPC, it currently does so without a timeout. So, if the active NN has frozen (but not actually crashed), this call can hang forever. This can then potentially prevent the standby from becoming active.

This may actually considered a side effect of HADOOP-6762 -- if the RPC were interruptible, that would also fix the issue.",eddyxu,tlipcon,Major,Resolved,Fixed,12/Nov/12 19:11,30/Aug/16 01:42
Bug,HDFS-4178,12615786,shell scripts should not close stderr,"The {{start-dfs.sh}} and {{stop-dfs.sh}} scripts close stderr for some subprocesses using the construct
bq. {{2>&-}}
This is dangerous because child processes started up under this scenario will re-use filedescriptor 2 for opened files.  Since libc and many other codepaths assume that filedescriptor 2 can be written to in error conditions, this can potentially result in data corruption.

Much better to redirect stderr using the construct {{2>/dev/null}}.",adi2,adi2,Major,Closed,Fixed,12/Nov/12 20:18,03/Sep/14 23:09
Bug,HDFS-4179,12615789,"BackupNode: allow reads, fix checkpointing, safeMode",BackupNode should be allowed to accept read command. Needs some adjustments in checkpointing and with safe mode.,shv,shv,Major,Closed,Fixed,12/Nov/12 20:37,15/Feb/13 13:12
Bug,HDFS-4180,12615796,TestFileCreation fails in branch-1 but not branch-1.1,"{noformat}
Testcase: testFileCreation took 3.419 sec
	Caused an ERROR
java.io.IOException: Cannot create /test_dir; already exists as a directory
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1374)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1334)
	...
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)

org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot create /test_dir; already exists as a directory
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1374)
	...
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:443)
	at org.apache.hadoop.hdfs.TestFileCreation.checkFileCreation(TestFileCreation.java:249)
	at org.apache.hadoop.hdfs.TestFileCreation.testFileCreation(TestFileCreation.java:179)
{noformat}
",jingzhao,szetszwo,Minor,Closed,Fixed,12/Nov/12 20:57,15/May/13 05:15
Bug,HDFS-4181,12615801,LeaseManager tries to double remove and prints extra messages,"When checkLeases() runs, internalReleaseLease() is called on the expired ones. When it returns true, the lease is already removed, yet it is tried again in checkLease(). This causes unnecessary ERROR messages to be logged. The line doing {{removing.add(p)}} should be removed.

The internalReleaseLease() method logs a detailed message per call, so the extra INFO log message from checkLease() is redundant. 

The error message from removeLease() can be very big and needs to be cut down. When the namenode itself is holding a lot of leases for block recovery, hitting this error is very expensive. In one instance, slow block recovery caused the namenode to hold more than 42K leases. The one log line in this case was over 4 MB.  The dump of data structure should be only enabled in debug mode.",kihwal,kihwal,Critical,Closed,Fixed,12/Nov/12 21:17,03/Sep/14 23:19
Bug,HDFS-4182,12615810,SecondaryNameNode leaks NameCache entries,"We recently saw an issue where a 2NN ran out of memory, even though it had a relatively small fsimage. When we looked at the heap dump, we saw that all of the memory had gone to entries in the NameCache.

It appears that the NameCache is staying in ""initializing"" mode forever, and therefore a long running 2NN leaks entries.",revans2,tlipcon,Critical,Closed,Fixed,12/Nov/12 22:16,12/May/16 18:11
Bug,HDFS-4186,12616009,logSync() is called with the write lock held while releasing lease,"As pointed out in HDFS-4183, when the lease monitor calls internalReleaseLease(), it acquires the namespace write lock. Inside internalReleaseLease(), if a block recovery is needed, the lease is reassigned to the namenode itself and this is logged & synced in logReassignLease().

Since this is done while the write lock is held, log syncing is blocked. When a large number of leases are expired and blocks are recovered, namenode can slow down.",kihwal,kihwal,Critical,Closed,Fixed,14/Nov/12 02:31,03/Feb/16 03:27
Bug,HDFS-4201,12616476,NPE in BPServiceActor#sendHeartBeat,"Saw the following NPE in a log.

Think this is likely due to {{dn}} or {{dn.getFSDataset()}} being null, (not {{bpRegistration}}) due to a configuration or local directory failure.

{code}
2012-09-25 04:33:20,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode svsrs00127/11.164.162.226:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000
2012-09-25 04:33:20,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1678908700-11.164.162.226-1342785481826 (storage id DS-1031100678-11.164.162.251-5010-1341933415989) service to svsrs00127/11.164.162.226:8020
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)
        at java.lang.Thread.run(Thread.java:722)
{code}",jxiang,eli,Critical,Closed,Fixed,16/Nov/12 21:28,24/Feb/14 20:58
Bug,HDFS-4205,12616509,fsck fails with symlinks,"I created a symlink using
{code}
...
    FileContext fc = FileContext.getFileContext(dst.fs.getUri());
    for (PathData src : srcs) {
      fc.createSymlink(src.path, dst.path, false);
    }
{code}
After doing this to create a symlink {{/foo/too.txt -> /foo/hello.txt}}, I tried to {{hdfs fsck}} and got the following:
{code}
[adi@host01 ~]$ hdfs fsck /
Connecting to namenode via http://host01:21070
FSCK started by adi (auth:SIMPLE) from /172.29.122.91 for path / at Fri Nov 16 15:59:18 PST 2012
FSCK ended at Fri Nov 16 15:59:18 PST 2012 in 3 milliseconds
hdfs://host01:21020/foo/hello.txt


Fsck on path '/' FAILED
{code}

It's very surprising that an unprivileged user can run code which so easily causes a fundamental administration tool to fail.",jlowe,adi2,Major,Closed,Fixed,17/Nov/12 00:54,12/May/16 18:14
Bug,HDFS-4207,12616536,All hadoop fs operations fail if the default fs is down even if a different file system is specified in the command,"you can't do any {{hadoop fs}} commands against any hadoop filesystem (e.g, s3://, a remote hdfs://, webhdfs://) if the default FS of the client is offline. Only operations that need the local fs should be expected to fail in this situation",jingzhao,stevel@apache.org,Minor,Closed,Fixed,17/Nov/12 17:26,15/May/13 05:16
Bug,HDFS-4208,12616663,NameNode could be stuck in SafeMode due to never-created blocks,"In one test case, NameNode allocated a block and then was killed before the client got the addBlock response. After NameNode restarted, it couldn't get out of SafeMode waiting for the block which was never created. In trunk, NameNode can get out of SafeMode since it only counts complete blocks. However branch-1 doesn't have the clear notion of under-constructioned-block in Namenode. 

JIRA HDFS-4212 is to track the never-created-block issue and this JIRA is to fix NameNode in branch-1 so it can get out of SafeMode when never-created-block exists.

The proposed idea is for SafeMode not to count the zero-sized last block in an under-construction file as part of total blcok count.",brandonli,brandonli,Critical,Closed,Fixed,18/Nov/12 22:45,06/Mar/13 09:55
Bug,HDFS-4209,12616673,Clean up the addNode/addChild/addChildNoQuotaCheck methods in FSDirectory,"- Returning the same INode back is not useful.  It is better to simply return a boolean to indicate whether the inode is added.

- The value of childDiskspace parameter is always UNKNOWN_DISK_SPACE.

- In most cases, the value of the pos parameter is length - 1.",szetszwo,szetszwo,Major,Closed,Fixed,19/Nov/12 01:06,03/Sep/14 22:46
Bug,HDFS-4210,12616736,Throw helpful exception when DNS entry for JournalNode cannot be resolved,"Setting  : 
  qjournal://cdh4master01:8485;cdh4master02:8485;cdh4worker03:8485/hdfscluster
  cdh4master01 and cdh4master02 JournalNode up and running, 
  cdh4worker03 not yet provisionning (no DNS entrie)

With :
`hadoop namenode -format` fails with :
  12/11/19 14:42:42 FATAL namenode.NameNode: Exception in namenode join
java.lang.IllegalArgumentException: Unable to construct journal, qjournal://cdh4master01:8485;cdh4master02:8485;cdh4worker03:8485/hdfscluster
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1235)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals(FSEditLog.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournalsForWrite(FSEditLog.java:193)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:745)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1099)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1204)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1233)
	... 5 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannelMetrics.getName(IPCLoggerChannelMetrics.java:107)
	at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannelMetrics.create(IPCLoggerChannelMetrics.java:91)
	at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.<init>(IPCLoggerChannel.java:161)
	at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$1.createLogger(IPCLoggerChannel.java:141)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createLoggers(QuorumJournalManager.java:353)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createLoggers(QuorumJournalManager.java:135)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:104)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:93)
	... 10 more

I suggest that if quorum is up format should not fails.",jzhuge,dam_ned,Trivial,Resolved,Fixed,19/Nov/12 14:00,22/Feb/18 00:04
Bug,HDFS-4216,12617020,Adding symlink should not ignore QuotaExceededException,FSDirectory.unprotectedSymlink(..) catches IOException and ignores it.  As a consequence it ignores QuotaExceededException.,szetszwo,szetszwo,Major,Closed,Fixed,20/Nov/12 22:29,15/Feb/13 13:12
Bug,HDFS-4222,12617248,NN is unresponsive and loses heartbeats of DNs when Hadoop is configured to use LDAP and LDAP has issues,"For Hadoop clusters configured to access directory information by LDAP, the FSNamesystem calls on behave of DFS clients might hang due to LDAP issues (including LDAP access issues caused by networking issues) while holding the single lock of FSNamesystem. That will result in the NN unresponsive and loss of the heartbeats from DNs.

The places LDAP got accessed by FSNamesystem calls are the instantiation of FSPermissionChecker, which could be moved out of the lock scope since the instantiation does not need the FSNamesystem lock. After the move, a DFS client hang will not affect other threads by hogging the single lock. This is especially helpful when we use separate RPC servers for ClientProtocol and DatanodeProtocol since the calls for DatanodeProtocol do not need to access LDAP. So even if DFS clients hang due to LDAP issues, the NN will still be able to process the requests (including heartbeats) from DNs.",teledriver,teledriver,Minor,Closed,Fixed,22/Nov/12 01:07,15/May/13 05:15
Bug,HDFS-4227,12617663,Document dfs.namenode.resource.*  ,"Let's document {{dfs.namenode.resource.*}} in hdfs-default.xml and a section the in the HDFS docs that covers local directories.

{{dfs.namenode.resource.check.interval}} - the interval in ms at which the NameNode resource checker runs (default is 5000)

{{dfs.namenode.resource.du.reserved}} - the amount of space to reserve/require for a NN storage directory (default is 100mb)

{{dfs.namenode.resource.checked.volumes}} - a list of local directories for the NN resource checker to check in addition to the local edits directories (default is empty).

{{dfs.namenode.resource.checked.volumes.minimum}} - the minimum number of redundant NN storage volumes required (default is 1). If no redundant resources are available we don't enter SM if there are sufficient required resources.",daisuke.kobayashi,eli,Major,Closed,Fixed,26/Nov/12 18:38,01/Dec/14 03:09
Bug,HDFS-4232,12618057,NN fails to write a fsimage with stale leases,"The reading of a fsimage will ignore leases for non-existent files, but the writing of an image will fail if there are leases for non-existent files.  If the image contains leases that reference a non-existent file, then the NN will fail to start, and the 2NN will start but fail to ever write an image.",daryn,daryn,Blocker,Closed,Fixed,28/Nov/12 20:46,03/Sep/14 23:24
Bug,HDFS-4233,12618061,NN keeps serving even after no journals started while rolling edit,"We've seen namenode keeps serving even after rollEditLog() failure. Instead of taking a corrective action or regard this condition as FATAL, it keeps on serving and modifying its file system state. No logs are written from this point, so if the namenode is restarted, there will be data loss.
",kihwal,kihwal,Blocker,Resolved,Fixed,28/Nov/12 21:03,02/May/13 02:29
Bug,HDFS-4235,12618101,"when outputting XML, OfflineEditsViewer can't handle some edits containing non-ASCII strings","It seems that when outputting XML, OfflineEditsViewer can't handle some edits containing non-ASCII strings.

Example:

{code}
cmccabe@keter:/h> ./bin/hdfs oev -i ~/Downloads/current2/edits -o /tmp/u.xml                                         
17:11:24,662 ERROR OfflineEditsBinaryLoader:82 - Got IOException at position 10593
Encountered exception. Exiting: SAX error: The character '�' is an invalid XML character
java.io.IOException: SAX error: The character '�' is an invalid XML character
        at org.apache.hadoop.hdfs.tools.offlineEditsViewer.XmlEditsVisitor.visitOp(XmlEditsVisitor.java:119)
        at org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader.loadEdits(OfflineEditsBinaryLoader.java:78)
        at org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer.go(OfflineEditsViewer.java:142)
        at org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer.run(OfflineEditsViewer.java:228)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer.main(OfflineEditsViewer.java:237)
{code}

Probably, we forgot to properly escape and/or re-encode a filename before putting it into the XML.  The other processors (stats, binary) don't have this problem, so it is purely an XML encoding issue.",cmccabe,cmccabe,Minor,Closed,Fixed,29/Nov/12 01:21,27/Aug/13 22:08
Bug,HDFS-4236,12618111,Regression: HDFS-4171 puts artificial limit on username length,"HDFS-4171 made the invalid assumption that there is a common limit on user names at the UNIX level.  Almost all modern systems, when running under a 64-bit kernel, use a pointer instead of a char array in the passwd struct.  This makes usernames essentially unlimited.

Additionally, IIRC, the only places where HDFS and the OS interact where the username is touched is during group lookup.

This limit is artificial and should be removed.  There is a very high risk that we will break users, especially service accounts used for automated processes.",tucu00,aw,Blocker,Closed,Fixed,29/Nov/12 03:33,15/Feb/13 13:12
Bug,HDFS-4238,12618137,[HA] Standby namenode should not do purging of shared storage edits.,"This happened in our cluster,

>> Standby NN was keep doing checkpoint every one hour and uploading to Active NN was continuously failing due to some kerberos issue and nobody noticed this, since Active was servicing properly.

>> Active NN was up for long time with fsimage having very least transaction.

>> Standby NN has saved the checkpoint in its name dir and purged the txns > 1000000 from shared storage ( includes edits which are not present in Active NN's fsimage)

>> After some time Active NN is restarted and StandBy NN switched to Active.

Now current Standby not able to load any edits from shared storage, as expected edits are not present in shared storage. Its keep running idle.


So {{editLog.purgeLogsOlderThan(purgeLogsFrom);}} always should be called from Active NameNode.
",tlipcon,vinayakumarb,Major,Closed,Fixed,29/Nov/12 10:52,12/May/16 18:12
Bug,HDFS-4240,12616257,"In nodegroup-aware case, make sure nodes are avoided to place replica if some replica are already under the same nodegroup","In previous implementation for HADOOP-8468, 3rd replica is avoid to place on the same nodegroup of 2nd replica. But it didn't provide check on nodegroup of 1st replica, so if 2nd replica's rack is not efficient to place replica, then it is possible to place 3rd and 1st replica within the same node group. We need a change to remove all nodes from available nodes for placing replica if there already replica on the same nodegroup.",junping_du,junping_du,Major,Closed,Fixed,15/Nov/12 16:14,27/Aug/13 22:08
Bug,HDFS-4241,12618223,DataNode unit test failures on Windows,,arp,arp,Major,Resolved,Fixed,29/Nov/12 20:41,02/May/13 02:30
Bug,HDFS-4242,12618231,Map.Entry is incorrectly used in LeaseManager,"As Daryn pointed out, the behavior of [Map.Entry|http://docs.oracle.com/javase/6/docs/api/java/util/Map.Entry.html] is undefined after the iteration or modifications of the map. ",szetszwo,szetszwo,Major,Closed,Fixed,29/Nov/12 21:06,15/Feb/13 13:11
Bug,HDFS-4243,12618242,INodeDirectory.replaceChild(..) does not update parent,"The method replaces an existing child with a new child.  However, it does not take care the case the child also an INodeDirectory.  In such case, the parent pointers of the children of the child have to be updated to the new child.

Only FSDirectory.unprotectedSetQuota(..) calls this method so that the child is always an INodeDirectory.",jingzhao,szetszwo,Major,Closed,Fixed,29/Nov/12 22:14,12/May/16 18:13
Bug,HDFS-4254,12618400,testAllEditsDirsFailOnFlush makes subsequent test cases fail (0.23.6 only),"This jira is about a broken test case in branch-0.23 only.

After HDFS-4233, testAllEditsDirsFailOnFlush started making subsequent cases in the same file fail. This is because MiniDFSCluster won't shutdown cleanly due to logSync() failure caused by lack of active journal streams. Without HDFS-4233, the condition was not detected and logSync() incorrectly ran without any error. To make MiniDFSCluster shutdown cleanly after a crash, the edit log needs to be aborted.

",kihwal,kihwal,Major,Resolved,Fixed,30/Nov/12 23:44,01/Dec/12 12:42
Bug,HDFS-4260,12618625,Fix HDFS tests to set test dir to a valid HDFS path as opposed to the local build path,"Multiple HDFS test suites fail early during initialization on Windows because of inclusion of the drive spec in the test root path.  The ':' gets rejected as an invalid character by the logic of isValidName.
",cnauroth,cnauroth,Major,Resolved,Fixed,03/Dec/12 20:02,12/May/16 18:14
Bug,HDFS-4261,12618630,TestBalancerWithNodeGroup times out,"When I manually ran TestBalancerWithNodeGroup, it always timed out in my machine.  Looking at the Jerkins report [build #3573|https://builds.apache.org/job/PreCommit-HDFS-Build/3573//testReport/org.apache.hadoop.hdfs.server.balancer/], TestBalancerWithNodeGroup somehow was skipped so that the problem was not detected.",junping_du,szetszwo,Major,Closed,Fixed,03/Dec/12 20:53,17/Aug/14 08:44
Bug,HDFS-4268,12618818,Remove redundant enum NNHAStatusHeartbeat.State,"NNHAStatusHeartbeat.State is redundant and can be replaced with commonly used enum HAServiceState.
We should also consistently use HAServiceState to check if the state is ACTIVE or STANDBY rather than using {{instanceof}}.",shv,shv,Major,Closed,Fixed,05/Dec/12 02:57,15/Feb/13 13:11
Bug,HDFS-4269,12618826,DatanodeManager#registerDatanode rejects all datanode registrations from localhost in single-node developer setup,"HDFS-3990 is a change that optimized some redundant DNS lookups.  As part of that change, {{DatanodeManager#registerDatanode}} now rejects attempts to register a datanode for which the name has not been resolved.  Unfortunately, this broke single-node developer setups on Windows, because Windows does not resolve 127.0.0.1 to ""localhost"".",cnauroth,cnauroth,Major,Closed,Fixed,05/Dec/12 04:40,12/May/16 18:15
Bug,HDFS-4270,12618845,Replications of the highest priority should be allowed to choose a source datanode that has reached its max replication limit,"Blocks that have been identified as under-replicated are placed on one of several priority queues.  The highest priority queue is essentially reserved for situations in which only one replica of the block exists, meaning it should be replicated ASAP.

The ReplicationMonitor periodically computes replication work, and a call to BlockManager#chooseUnderReplicatedBlocks selects a given number of under-replicated blocks, choosing blocks from the highest-priority queue first and working down to the lowest priority queue.

In the subsequent call to BlockManager#computeReplicationWorkForBlocks, a source for the replication is chosen from among datanodes that have an available copy of the block needed.  This is done in BlockManager#chooseSourceDatanode.


chooseSourceDatanode's job is to choose the datanode for replication.  It chooses a random datanode from the available datanodes that has not reached its replication limit (preferring datanodes that are currently decommissioning).

However, the priority queue of the block does not inform the logic.  If a datanode holds the last remaining replica of a block and has already reached its replication limit, the node is dismissed outright and the replication is not scheduled.

In some situations, this could lead to data loss, as the last remaining replica could disappear if an opportunity is not taken to schedule a replication.  It would be better to waive the max replication limit in cases of highest-priority block replication.
",dagit,dagit,Minor,Closed,Fixed,05/Dec/12 09:00,12/May/16 18:16
Bug,HDFS-4274,12618919,BlockPoolSliceScanner does not close verification log during shutdown,{{BlockPoolSliceScanner}} holds open a handle to a verification log.  This file is not getting closed during process shutdown.,cnauroth,cnauroth,Minor,Closed,Fixed,05/Dec/12 18:20,02/May/13 02:29
Bug,HDFS-4275,12618920,MiniDFSCluster-based tests fail on Windows due to failure to delete test namenode directory,"Multiple HDFS test suites fail on Windows during initialization of {{MiniDFSCluster}} due to ""Could not fully delete"" the name testing data directory.",cnauroth,cnauroth,Major,Resolved,Fixed,05/Dec/12 18:25,23/Mar/18 01:06
Bug,HDFS-4279,12618963,NameNode does not initialize generic conf keys when started with -recover,This means that configurations that scope the location of the name/edits/shared edits dirs by nameserice or namenode won't work with `hdfs namenode -recover`,cmccabe,cmccabe,Minor,Closed,Fixed,05/Dec/12 23:05,15/Feb/13 13:12
Bug,HDFS-4282,12618994,TestEditLog.testFuzzSequences FAILED in all pre-commit test,"Caught non-IOException throwable java.lang.RuntimeException: java.io.IOException: Invalid UTF8 at 9871b370d70a  at org.apache.hadoop.io.UTF8.toString(UTF8.java:154)  at org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readString(FSImageSerialization.java:200)  at org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TimesOp.readFields(FSEditLogOp.java:1439)  at org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.decodeOp(FSEditLogOp.java:2399)  at org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.readOp(FSEditLogOp.java:2290)  at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOpImpl(EditLogFileInputStream.java:177)  at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOpImpl(EditLogFileInputStream.java:175)  at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOp(EditLogFileInputStream.java:217)  at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:72)  at org.apache.hadoop.hdfs.server.namenode.TestEditLog.validateNoCrash(TestEditLog.java:1233)  at org.apache.hadoop.hdfs.server.namenode.TestEditLog.testFuzzSequences(TestEditLog.java:1272)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)  at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)  at org.junit.runners.ParentRunner.run(ParentRunner.java:236)  at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)  at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)  at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)  at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)  at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)  at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)  at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75) Caused by: java.io.IOException: Invalid UTF8 at 9871b370d70a  at org.apache.hadoop.io.UTF8.readChars(UTF8.java:277)  at org.apache.hadoop.io.UTF8.toString(UTF8.java:151)  ... 39 more 
",tlipcon,junping_du,Major,Closed,Fixed,06/Dec/12 06:49,12/May/16 18:15
Bug,HDFS-4287,12622943,HTTPFS tests fail on Windows,The HTTPFS tests have some platform-specific assumptions that cause the tests to fail when run on Windows.,cnauroth,cnauroth,Major,Closed,Fixed,07/Dec/12 18:32,12/May/16 18:16
Bug,HDFS-4288,12622964,NN accepts incremental BR as IBR in safemode,"If a DN is ready to send an incremental BR and the NN goes down, the DN will repeatedly try to reconnect.  The NN will then process the DN's incremental BR as an initial BR.  The NN now thinks the DN has only a few blocks, and will ignore all subsequent BRs from that DN until out of safemode -- which it may never do because of all the ""missing"" blocks on the affected DNs.",daryn,daryn,Critical,Closed,Fixed,07/Dec/12 20:17,12/May/16 18:16
Bug,HDFS-4291,12622993,edit log unit tests leave stray test_edit_log_file around,"Some of the edit log tests leave a stray {{test_edit_log_file}} around.  These should be put in the test directory, and cleaned up after the tests succeed.",cmccabe,cmccabe,Minor,Closed,Fixed,07/Dec/12 23:58,03/Sep/14 23:09
Bug,HDFS-4292,12623023,Sanity check not correct in RemoteBlockReader2.newBlockReader,"{code}
    if ( firstChunkOffset < 0 || firstChunkOffset > startOffset ||
        firstChunkOffset >= (startOffset + checksum.getBytesPerChecksum())) {
{code}
should be:
{code}
    if ( firstChunkOffset < 0 || firstChunkOffset > startOffset ||
        firstChunkOffset <= (startOffset - checksum.getBytesPerChecksum())) {
{code}
",decster,decster,Minor,Closed,Fixed,08/Dec/12 15:43,03/Sep/14 23:09
Bug,HDFS-4294,12623200,Backwards compatibility is not maintained for TestVolumeId,HDFS-4199 uses untyped (wildcard) generic that is not functional in earlier versions of Java 1.6.,robsparker,robsparker,Major,Closed,Fixed,10/Dec/12 19:36,12/May/16 18:17
Bug,HDFS-4295,12623201,Using port 1023 should be valid when starting Secure DataNode,"In SecureDataNodeStarter:
{code}
if ((ss.getLocalPort() >= 1023 || listener.getPort() >= 1023) &&
        UserGroupInformation.isSecurityEnabled()) {
      throw new RuntimeException(""Cannot start secure datanode with unprivileged ports"");
}
{code}

This prohibits using port 1023, but this should be okay because only root can listen to ports below 1024.

We can change the >= to >.",schu,schu,Major,Closed,Fixed,10/Dec/12 19:58,03/Sep/14 23:09
Bug,HDFS-4296,12623209,Add layout version for HDFS-4256 for release 1.2.0,"Backport of concat requires editlog change and incrementing layout version. See [this comment|https://issues.apache.org/jira/browse/HDFS-4256?focusedCommentId=13525989&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13525989].

Version number -41 will be reserved for release 1.2.0.",sureshms,sureshms,Major,Closed,Fixed,10/Dec/12 20:50,12/May/16 18:17
Bug,HDFS-4297,12623214,Port datanode concurrent reading and writing fixes to trunk-win,Investigate and port any applicable changes from HADOOP-8564.,arp,arp,Major,Resolved,Fixed,10/Dec/12 21:22,02/May/13 02:30
Bug,HDFS-4298,12623305,StorageRetentionManager spews warnings when used with QJM,"When the NN is configured with a QJM, we see the following warning message every time a checkpoint is made or uploaded:
12/12/10 16:07:52 WARN namenode.FSEditLog: Unable to determine input streams from QJM to [127.0.0.1:13001, 127.0.0.1:13002, 127.0.0.1:13003]. Skipping.
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:
127.0.0.1:13002: Asked for firstTxId 114837 which is in the middle of file /tmp/jn-2/myjournal/current/edits_0000000000000095185-0000000000000114846
...

This is because, since HDFS-2946, the NN calls {{selectInputStreams}} to determine the number of log segments and put a cap on the number. This API throws an exception in the case of QJM if the argument falls in the middle of an edit log boundary.",atm,tlipcon,Major,Closed,Fixed,11/Dec/12 00:37,12/May/16 18:16
Bug,HDFS-4300,12623432,TransferFsImage.downloadEditsToStorage should use a tmp file for destination,"Currently, in TransferFsImage.downloadEditsToStorage, we download the edits file directly to its finalized path. So, if the transfer fails in the middle, a half-written file is left and cannot be distinguished from a correct file. So, future checkpoints by the 2NN will fail, since the file is truncated in the middle -- but it won't ever download a good copy because it thinks it already has the proper file.",andrew.wang,tlipcon,Critical,Closed,Fixed,11/Dec/12 19:05,27/Aug/13 22:08
Bug,HDFS-4302,12623443,"Precondition in EditLogFileInputStream's length() method is checked too early in NameNode startup, causing fatal exception","When bringing up a namenode in standby mode, where DEBUG is enabled for namenode, the namenode will hit the following code in {{hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java}}:

{code}
 if (LOG.isDebugEnabled()) {
      LOG.debug(""edit log length: "" + in.length() + "", start txid: ""
          + expectedStartingTxId + "", last txid: "" + lastTxId);
    }
{code}.

However, if {{in}} has an {{EditLogFileInputStream}} as its {{streams[0]}}, this code is hit before the {{EditLogFileInputStream}}'s {{advertizedSize}} is initialized (before the HTTP client connects to the remote edit log server (i.e. the journal node)). This causes the following precondition to fail in {{EditLogFileInputStream:length()}}:

{code}
      Preconditions.checkState(advertisedSize != -1,
          ""must get input stream before length is available"");
{code}

which shuts down the namenode with the following log messages and stack trace:

{code}
2012-12-11 10:45:33,319 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getEditLogManifest took 88ms
2012-12-11 10:45:33,336 DEBUG client.QuorumJournalManager (QuorumJournalManager.java:selectInputStreams(459)) - selectInputStream manifests:
172.16.175.1:8485: [[1,3]]
2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(605)) - Planning to load image :
FSImageFile(file=/tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(607)) - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9
2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(168)) - Loading image file /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000 using no compression
2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(171)) - Number of files = 1
2012-12-11 10:45:33,356 INFO  namenode.FSImage (FSImageFormat.java:loadFilesUnderConstruction(383)) - Number of files under construction = 0
2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImageFormat.java:load(193)) - Image file of size 119 loaded in 0 seconds.
2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImage.java:loadFSImage(753)) - Loaded image for txid 0 from /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000
2012-12-11 10:45:33,357 DEBUG namenode.FSImage (FSImage.java:loadEdits(686)) - About to load edits:
  org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9
2012-12-11 10:45:33,359 INFO  namenode.FSImage (FSImage.java:loadEdits(694)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9 expecting start txid #1
2012-12-11 10:45:33,361 DEBUG ipc.Client (Client.java:stop(1060)) - Stopping client
2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:close(1016)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: closed
2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:run(848)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: stopped, remaining connections 0
2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join
java.lang.IllegalStateException: must get input stream before length is available
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)
        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)
2012-12-11 10:45:33,470 INFO  util.ExitUtil (ExitUtil.java:terminate(84)) - Exiting with status 1
2012-12-11 10:45:33,471 INFO  namenode.NameNode (StringUtils.java:run(620)) - SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at Eugenes-MacBook-Pro.local/172.16.175.1
************************************************************/
{code}


",ekoontz,ekoontz,Major,Closed,Fixed,11/Dec/12 19:44,03/Sep/14 23:09
Bug,HDFS-4305,12623456,"Add a configurable limit on number of blocks per file, and min block size","We recently had an issue where a user set the block size very very low and managed to create a single file with hundreds of thousands of blocks. This caused problems with the edit log since the OP_ADD op was so large (HDFS-4304). I imagine it could also cause efficiency issues in the NN. To prevent users from making such mistakes, we should:
- introduce a configurable minimum block size, below which requests are rejected
- introduce a configurable maximum number of blocks per file, above which requests to add another block are rejected (with a suitably high default as to not prevent legitimate large files)",andrew.wang,tlipcon,Minor,Closed,Fixed,11/Dec/12 21:02,28/Jan/14 17:26
Bug,HDFS-4306,12623494,PBHelper.convertLocatedBlock miss convert BlockToken,PBHelper.convertLocatedBlock(from protobuf array to primitive array) miss convert BlockToken.,decster,decster,Major,Closed,Fixed,12/Dec/12 07:26,15/Feb/13 13:12
Bug,HDFS-4307,12623600,SocketCache should use monotonic time,"{{SocketCache}} should use monotonic time, not wall-clock time.  Otherwise, if the time is adjusted by ntpd or a system administrator, sockets could be either abrupbtly expired, or left in the cache indefinitely.",cmccabe,cmccabe,Minor,Closed,Fixed,12/Dec/12 18:38,15/Feb/13 13:12
Bug,HDFS-4308,12623653,addBlock() should persist file blocks once,"FSNamesystem.getAdditionalBlock() persists all file blocks twice, first in {{dir.removeBlock()}} using OP_ADD, then immediately after that in {{dir.persistBlocks()}} using OP_UPDATE_BLOCKS.
This should be aggregated in one call to logEdit().",zero45,shv,Major,Closed,Fixed,13/Dec/12 02:01,15/Feb/13 13:11
Bug,HDFS-4310,12623800,fix test org.apache.hadoop.hdfs.server.datanode.TestStartSecureDataNode,the test org/apache/hadoop/hdfs/server/datanode/TestStartSecureDataNode catches exceptions and does not re-throw them. Due to that it passes even if it actually failed.,iveselovsky,iveselovsky,Major,Resolved,Fixed,13/Dec/12 19:28,12/May/16 18:18
Bug,HDFS-4315,12624080,DNs with multiple BPs can have BPOfferServices fail to start due to unsynchronized map access,"In some nightly test runs we've seen pretty frequent failures of TestWebHdfsWithMultipleNameNodes. I've traced the root cause to an unsynchronized map access in the DataStorage class.

More details in the first comment.",atm,atm,Major,Closed,Fixed,15/Dec/12 00:48,15/Feb/13 13:12
Bug,HDFS-4316,12624151,branch-trunk-win contains test code accidentally added during work on fixing tests on Windows,branch-trunk-win currently contains some test code that was not meant to be committed.,cnauroth,cnauroth,Major,Resolved,Fixed,15/Dec/12 23:00,16/Dec/12 23:22
Bug,HDFS-4328,12624648,TestLargeBlock#testLargeBlockSize is timing out,"For some time now TestLargeBlock#testLargeBlockSize has been timing out on trunk.  It is getting hung up during cluster shutdown, and after 15 minutes surefire kills it and causes the build to fail since it exited uncleanly.

In addition to fixing the hang, we should consider adding a timeout parameter to the @Test decorator for this test.",cnauroth,jlowe,Major,Resolved,Fixed,19/Dec/12 15:04,12/Jan/13 13:21
Bug,HDFS-4329,12624877,DFSShell issues with directories with spaces in name,"This bug was discovered by Casey Ching.

The command {{dfs -put /foo/hello.txt dir}} is supposed to create {{dir/hello.txt}} on HDFS.  It doesn't work right if ""dir"" has a space in it:

{code}
[adi@haus01 ~]$ hdfs dfs -mkdir 'space cat'
[adi@haus01 ~]$ hdfs dfs -put /etc/motd 'space cat'
[adi@haus01 ~]$ hdfs dfs -cat 'space cat/motd'
cat: `space cat/motd': No such file or directory
[adi@haus01 ~]$ hdfs dfs -ls space\*
Found 1 items
-rw-r--r--   2 adi supergroup        251 2012-12-20 11:16 space%2520cat/motd
[adi@haus01 ~]$ hdfs dfs -cat 'space%20cat/motd'
Welcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-30-generic x86_64)
...
{code}

Note that the {{dfs -ls}} output wrongly encodes the wrongly encoded directory name, turning {{%20}} into {{%2520}}.  It does the same thing with space:
{code}
[adi@haus01 ~]$ hdfs dfs -touchz 'space cat/foo'
[adi@haus01 ~]$ hdfs dfs -ls 'space cat'
Found 1 items
-rw-r--r--   2 adi supergroup          0 2012-12-20 11:36 space%20cat/foo
{code}",cabad,adi2,Major,Closed,Fixed,20/Dec/12 19:37,12/May/16 18:14
Bug,HDFS-4337,12625245,Backport HDFS-4240 to branch-1: Make sure nodes are avoided to place replica if some replica are already under the same nodegroup.,Update affects version from 1.0.0 to 1.2.0.,mgong@vmware.com,junping_du,Major,Closed,Fixed,26/Dec/12 09:08,15/May/13 05:15
Bug,HDFS-4338,12625343,TestNameNodeMetrics#testCorruptBlock is flaky,"Ran some background cpuburn threads, got this stack trace:

{noformat}
Running org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 15.287 sec <<< FAILURE!
testCorruptBlock(org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics)  Time elapsed: 14922 sec  <<< FAILURE!
java.lang.AssertionError: Bad value for metric ScheduledReplicationBlocks expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.hadoop.test.MetricsAsserts.assertGauge(MetricsAsserts.java:190)
	at org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCorruptBlock(TestNameNodeMetrics.java:229)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:242)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:137)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)


Results :

Failed tests:   testCorruptBlock(org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics): Bad value for metric ScheduledReplicationBlocks expected:<1> but was:<0>

Tests run: 1, Failures: 1, Errors: 0, Skipped: 0
{noformat}",andrew.wang,andrew.wang,Major,Resolved,Fixed,27/Dec/12 16:41,12/May/16 18:16
Bug,HDFS-4341,12625221,TestBalancerWithNodeGroup fails on Windows,"Test added with the support for ""NetworkTopology with NodeGroup"" fails on Windows. 

Relevant section from the test log:

2012-12-24 23:51:01,835 WARN  datanode.DataNode (DataNode.java:makeInstance(1582)) - Invalid directory in dfs.data.dir: Incorrect permission for I:/git/project/hadoop-monarch/build/test/data/dfs/data/data1, expected: rwxr-xr-x, while actual: rwx------
2012-12-24 23:51:01,911 WARN  datanode.DataNode (DataNode.java:makeInstance(1582)) - Invalid directory in dfs.data.dir: Incorrect permission for I:/git/project/hadoop-monarch/build/test/data/dfs/data/data2, expected: rwxr-xr-x, while actual: rwx------
2012-12-24 23:51:01,911 ERROR datanode.DataNode (DataNode.java:makeInstance(1588)) - All directories in dfs.data.dir are invalid.

Default permissions on Windows are 700 while datanode expects 755.

We already fixed this in MiniDFSCluster in branch-1-win. A similar fix is needed in MiniDFSClusterWithNodeGroup.",ivanmi,ivanmi,Minor,Resolved,Fixed,25/Dec/12 23:21,28/Dec/12 08:18
Bug,HDFS-4342,12625378,Edits dir in dfs.namenode.edits.dir.required will be silently ignored if it is not in dfs.namenode.edits.dir,"Edits dir must be listed in both dfs.namenode.edits.dir and dfs.namenode.edits.dir.required for the later to take effect. 
We should throw an invalid configuration error when a dir is in .required but not in dfs.namenode.edits.dir",arp,mark_yang,Major,Closed,Fixed,28/Dec/12 03:01,27/Aug/13 22:07
Bug,HDFS-4344,12625389,dfshealth.jsp throws NumberFormatException when dfs.hosts/dfs.hosts.exclude includes port number,"dfs.hosts and dfs.hosts.exclude files cannot contain a port number of host.
If contained, and access a dfshealth.jsp on a webui, we got a NumberFormatException.

How to reproduce:
{noformat}
$ cat /tmp/include.txt
salve-host1:9999

$ cat /tmp/exclude.txt
slave-host1:9999

$ hdfs namenode -Ddfs.hosts=/tmp/include.txt -Ddfs.hosts.exclude=/tmp/exclude.txt
{noformat}

Error:
{noformat}
Problem accessing /dfshealth.jsp. Reason:

    For input string: "":9999""
Caused by:

java.lang.NumberFormatException: For input string: "":9999""
     at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
     at java.lang.Integer.parseInt(Integer.java:449)
     at java.lang.Integer.valueOf(Integer.java:554)
     at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.parseDNFromHostsEntry(DatanodeManager.java:970)
     at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeListForReport(DatanodeManager.java:1039)
     at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.fetchDatanodes(DatanodeManager.java:892)
     at org.apache.hadoop.hdfs.server.namenode.NamenodeJspHelper$HealthJsp.generateHealthReport(NamenodeJspHelper.java:288)
     at org.apache.hadoop.hdfs.server.namenode.dfshealth_jsp._jspService(dfshealth_jsp.java:109)
     at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)
     at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
     at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
     at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
     at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1071)
     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
     at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
     at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
     at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
     at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
     at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
     at org.mortbay.jetty.Server.handle(Server.java:326)
     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
     at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}

It's probably because DatanodeManager.parseDNFromHostsEntry() doesn't parse host:port string correctly.

{noformat}
  private DatanodeID parseDNFromHostsEntry(String hostLine) {
    DatanodeID dnId;
    String hostStr;
    int port;
    int idx = hostLine.indexOf(':');

    if (-1 == idx) {
      hostStr = hostLine;
      port = DFSConfigKeys.DFS_DATANODE_DEFAULT_PORT;
    } else {
      hostStr = hostLine.substring(0, idx);
      port = Integer.valueOf(hostLine.substring(idx)); // <- HERE!!
    }
{noformat}

correct it as the below.

{noformat}
port = Integer.valueOf(hostLine.substring(idx + 1));
{noformat}
",adi2,tamtam180,Major,Closed,Fixed,28/Dec/12 05:26,12/May/16 18:16
Bug,HDFS-4347,12625457,"TestBackupNode can go into infinite loop ""Waiting checkpoint to complete.""",{{waitCheckpointDone()}} can loop forever if txid to wait for is calculated after {{startBackupNode()}}.,zero45,shv,Major,Closed,Fixed,28/Dec/12 23:51,15/Feb/13 13:12
Bug,HBASE-5114,12536761,TestMetaReaderEditor can timeout during test execution,"Likely because it belongs to the too long list of flaky tests. A test should not fail randomly. Here is the list, from 2974 to 2598, with a timeouted TestMetaReaderEditor
https://builds.apache.org/job/HBase-TRUNK/2597/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/HBase-TRUNK/2588/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/HBase-TRUNK/2585/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/HBase-TRUNK/2584/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/HBase-TRUNK/2582/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/HBase-TRUNK/2577/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/HBase-TRUNK/2574/testReport/org.apache.hadoop.hbase.catalog/

It happens on hadoop-qa as well:
https://builds.apache.org/job/PreCommit-HBASE-Build/640/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/PreCommit-HBASE-Build/638/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/PreCommit-HBASE-Build/630/testReport/org.apache.hadoop.hbase.catalog/
https://builds.apache.org/job/PreCommit-HBASE-Build/622/testReport/org.apache.hadoop.hbase.catalog/



Looking at the source code, main issues are:
- in testRetrying, some threads are started, but will not be stopped if an assertion fails. As a consequence, the process will not stop
- still in testRetrying, it's not possible to use @Test(timeout=180000), because if this timeout is reached, the thread won't be stopped.

This does not explain why the test fails sometimes, but explain why we have a timeout instead of a clean failure.


I also fixed some stuff that seems unrelated, and I cannot reproduce the error anymore locally after 10 tries. So I will try the patch on hadoop-qa.

The patch includes as well the removal of TestAdmin#testHundredsOfTable because this test takes 2 minutes to execute; but proves nothing on success as there is no check on ressources used.",nkeywal,nkeywal,Major,Closed,Fixed,01/Jan/12 16:46,12/Jun/22 20:02
Bug,HBASE-5120,12537133,Timeout monitor races with table disable handler,"Here is what J-D described here:
https://issues.apache.org/jira/browse/HBASE-5119?focusedCommentId=13179176&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13179176

I think I will retract from my statement that it ""used to be extremely racy and caused more troubles than it fixed"", on my first test I got a stuck region in transition instead of being able to recover. The timeout was set to 2 minutes to be sure I hit it.

First the region gets closed
{quote}
2012-01-04 00:16:25,811 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to sv4r5s38,62023,1325635980913 for region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.
{quote}

2 minutes later it times out:

{quote}
2012-01-04 00:18:30,026 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=PENDING_CLOSE, ts=1325636185810, server=null
2012-01-04 00:18:30,026 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.
2012-01-04 00:18:30,027 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. (offlining)
{quote}

100ms later the master finally gets the event:

{quote}
2012-01-04 00:18:30,129 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=sv4r5s38,62023,1325635980913, region=1a4b111bcc228043e89f59c4c3f6a791, which is more than 15 seconds late
2012-01-04 00:18:30,129 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for 1a4b111bcc228043e89f59c4c3f6a791
2012-01-04 00:18:30,129 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Table being disabled so deleting ZK node and removing from regions in transition, skipping assignment of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.
2012-01-04 00:18:30,129 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x134589d3db03587 Deleting existing unassigned node for 1a4b111bcc228043e89f59c4c3f6a791 that is in expected state RS_ZK_REGION_CLOSED
2012-01-04 00:18:30,166 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x134589d3db03587 Successfully deleted unassigned node for region 1a4b111bcc228043e89f59c4c3f6a791 in expected state RS_ZK_REGION_CLOSED
{quote}

At this point everything is fine, the region was processed as closed. But wait, remember that line where it said it was going to force an unassign?

{quote}
2012-01-04 00:18:30,322 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x134589d3db03587 Creating unassigned node for 1a4b111bcc228043e89f59c4c3f6a791 in a CLOSING state
2012-01-04 00:18:30,328 INFO org.apache.hadoop.hbase.master.AssignmentManager: Server null returned java.lang.NullPointerException: Passed server is null for 1a4b111bcc228043e89f59c4c3f6a791
{quote}

Now the master is confused, it recreated the RIT znode but the region doesn't even exist anymore. It even tries to shut it down but is blocked by NPEs. Now this is what's going on.

The late ZK notification that the znode was deleted (but it got recreated after):

{quote}
2012-01-04 00:19:33,285 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: The znode of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. has been deleted.
{quote}

Then it prints this, and much later tries to unassign it again:

{quote}
2012-01-04 00:19:46,607 DEBUG org.apache.hadoop.hbase.master.handler.DeleteTableHandler: Waiting on  region to clear regions in transition; test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=PENDING_CLOSE, ts=1325636310328, server=null
...
2012-01-04 00:20:39,623 DEBUG org.apache.hadoop.hbase.master.handler.DeleteTableHandler: Waiting on  region to clear regions in transition; test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=PENDING_CLOSE, ts=1325636310328, server=null
2012-01-04 00:20:39,864 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=PENDING_CLOSE, ts=1325636310328, server=null
2012-01-04 00:20:39,864 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.
2012-01-04 00:20:39,865 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. (offlining)
2012-01-04 00:20:39,865 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Attempted to unassign region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. but it is not currently assigned anywhere
{quote}

And this is still ongoing.",ram_krish,zhihyu@ebaysf.com,Blocker,Closed,Fixed,04/Jan/12 02:45,12/Oct/12 05:34
Bug,HBASE-5121,12537140,MajorCompaction may affect scan's correctness,"In our test, there are two families' keyvalue for one row.

But we could find a infrequent problem when doing scan's next if majorCompaction happens concurrently.
In the client's two continuous doing scan.next():
1.First time, scan's next returns the result where family A is null.
2.Second time, scan's next returns the result where family B is null.
The two next()'s result have the same row.

If there are more families, I think the scenario will be more strange...

We find the reason is that storescanner.peek() is changed after majorCompaction if there are delete type KeyValue.
This change causes the PriorityQueue<KeyValueScanner> of RegionScanner's heap is not sure to be sorted.",zjushch,zjushch,Critical,Closed,Fixed,04/Jan/12 06:39,12/Oct/12 05:34
Bug,HBASE-5129,12537307,book is inconsistent regarding disabling - major compaction,"It seems that the book has some inconsistencies regarding the way to disable major compactions

According to the book in chapter 2.6.1.1. HBase Default Configuration

hbase.hregion.majorcompaction - The time (in miliseconds) between 'major' compactions of all HStoreFiles in a region. Default: 1 day. Set to 0 to disable automated major compactions.
Default: 86400000 (http://hbase.apache.org/book.html#hbase_default_configurations)

According to the book at chapter 2.8.2.8. Managed Compactions
""A common administrative technique is to manage major compactions manually, rather than letting HBase do it. By default, HConstants.MAJOR_COMPACTION_PERIOD is one day and major compactions may kick in when you least desire it - especially on a busy system. To ""turn off"" automatic major compactions set the value to Long.MAX_VALUE.""

According to the code org.apache.hadoop.hbase.regionserver.Store.java, ""0"" is the right answer. 

(affect all documentation from 0.90.1)",dmeil,mikaels,Minor,Closed,Fixed,05/Jan/12 12:31,12/Jun/22 20:03
Bug,HBASE-5137,12537543,MasterFileSystem.splitLog() should abort even if waitOnSafeMode() throws IOException,"I am not sure if this bug was already raised in JIRA.
In our test cluster we had a scenario where the RS had gone down and ServerShutDownHandler started with splitLog.
But as the HDFS was down the check waitOnSafeMode throws IOException.
{code}
try {
        // If FS is in safe mode, just wait till out of it.
        FSUtils.waitOnSafeMode(conf,
          conf.getInt(HConstants.THREAD_WAKE_FREQUENCY, 1000));  
        splitter.splitLog();
      } catch (OrphanHLogAfterSplitException e) {
{code}
We catch the exception
{code}
} catch (IOException e) {
      checkFileSystem();
      LOG.error(""Failed splitting "" + logDir.toString(), e);
    }
{code}
So the HLog split itself did not happen. We encontered like 4 regions that was recently splitted in the crashed RS was lost.

Can we abort the Master in such scenarios? Pls suggest.",ram_krish,ram_krish,Major,Closed,Fixed,06/Jan/12 16:53,20/Nov/15 11:52
Bug,HBASE-5141,12537607,Memory leak in MonitoredRPCHandlerImpl,"I got a pretty reliable way of OOME'ing my region servers. Using a big payload (64MB in my case), a default heap and default number of handlers, it's not too long that all the MonitoredRPCHandlerImpl hold on a 64MB reference and once a compaction kicks in it kills everything.

The issue is that even after the RPC call is done, the packet still lives in MonitoredRPCHandlerImpl.

Will attach a screen shot of jprofiler's analysis in a moment.

This is a blocker for 0.92.0, anyone using a high number of handlers and bigish values will kill themselves.",jdcryans,jdcryans,Blocker,Closed,Fixed,06/Jan/12 23:08,12/Oct/12 05:34
Bug,HBASE-5152,12537711,"Region is on service before completing initialization when doing rollback of split, it will affect read correctness ",,zjushch,zjushch,Major,Closed,Fixed,09/Jan/12 05:38,12/Oct/12 05:35
Bug,HBASE-5153,12537717,Add retry logic in HConnectionImplementation#resetZooKeeperTrackers,"HBASE-4893 is related to this issue. In that issue, we know, if multi-threads share a same connection, once this connection got abort in one thread, the other threads will got a ""HConnectionManager$HConnectionImplementation@18fb1f7 closed"" exception.

It solve the problem of ""stale connection can't removed"". But the orignal HTable instance cann't be continue to use. The connection in HTable should be recreated.

Actually, there's two aproach to solve this:
1. In user code, once catch an IOE, close connection and re-create HTable instance. We can use this as a workaround.
2. In HBase Client side, catch this exception, and re-create connection.
",jeason,jeason,Major,Closed,Fixed,09/Jan/12 08:52,20/Nov/15 11:53
Bug,HBASE-5155,12537741,ServerShutDownHandler And Disable/Delete should not happen parallely leading to recreation of regions that were deleted,"ServerShutDownHandler and disable/delete table handler races.  This is not an issue due to TM.
-> A regionserver goes down.  In our cluster the regionserver holds lot of regions.
-> A region R1 has two daughters D1 and D2.
-> The ServerShutdownHandler gets called and scans the META and gets all the user regions
-> Parallely a table is disabled. (No problem in this step).
-> Delete table is done.
-> The tables and its regions are deleted including R1, D1 and D2.. (So META is cleaned)
-> Now ServerShutdownhandler starts to processTheDeadRegion
{code}
 if (hri.isOffline() && hri.isSplit()) {
      LOG.debug(""Offlined and split region "" + hri.getRegionNameAsString() +
        ""; checking daughter presence"");
      fixupDaughters(result, assignmentManager, catalogTracker);
{code}
As part of fixUpDaughters as the daughers D1 and D2 is missing for R1 
{code}
    if (isDaughterMissing(catalogTracker, daughter)) {
      LOG.info(""Fixup; missing daughter "" + daughter.getRegionNameAsString());
      MetaEditor.addDaughter(catalogTracker, daughter, null);

      // TODO: Log WARN if the regiondir does not exist in the fs.  If its not
      // there then something wonky about the split -- things will keep going
      // but could be missing references to parent region.

      // And assign it.
      assignmentManager.assign(daughter, true);
{code}
we call assign of the daughers.  
Now after this we again start with the below code.
{code}
        if (processDeadRegion(e.getKey(), e.getValue(),
            this.services.getAssignmentManager(),
            this.server.getCatalogTracker())) {
          this.services.getAssignmentManager().assign(e.getKey(), true);
{code}
Now when the SSH scanned the META it had R1, D1 and D2.
So as part of the above code D1 and D2 which where assigned by fixUpDaughters
is again assigned by 
{code}
this.services.getAssignmentManager().assign(e.getKey(), true);
{code}
Thus leading to a zookeeper issue due to bad version and killing the master.
The important part here is the regions that were deleted are recreated which i think is more critical.",ram_krish,ram_krish,Blocker,Closed,Fixed,09/Jan/12 13:16,12/Jun/22 20:04
Bug,HBASE-5156,12537749,Backport HBASE-4899 -  Region would be assigned twice easily with continually killing server and moving region in testing environment,Need to backport to 0.90.6 considering the criticality of the issue,,ram_krish,Major,Closed,Fixed,09/Jan/12 13:38,20/Nov/15 11:54
Bug,HBASE-5158,12537752,Backport HBASE-4878 - Master crash when splitting hlog may cause data loss,Backporting to 0.90.6 considering the importance of the issue.,,ram_krish,Major,Closed,Fixed,09/Jan/12 13:45,20/Nov/15 11:54
Bug,HBASE-5159,12537753,Backport HBASE-4079 - HTableUtil - helper class for loading data ,Backporting to 0.90.6 considering the usefulness of the feature.,,ram_krish,Major,Closed,Fixed,09/Jan/12 13:47,20/Nov/15 11:54
Bug,HBASE-5160,12537757,"Backport HBASE-4397 - -ROOT-, .META. tables stay offline for too long in recovery phase after all RSs are shutdown at the same time",Backporting to 0.90.6 considering the importance of the issue.,,ram_krish,Major,Closed,Fixed,09/Jan/12 13:55,20/Nov/15 11:53
Bug,HBASE-5163,12537843,"TestLogRolling#testLogRollOnDatanodeDeath fails sometimes on Jenkins or hadoop QA (""The directory is already locked."")","The stack is typically:
{noformat}
    <error message=""Cannot lock storage /tmp/19e3e634-8980-4923-9e72-a5b900a71d63/dfscluster_32a46f7b-24ef-488f-bd33-915959e001f4/dfs/data/data3. The directory is already locked."" type=""java.io.IOException"">java.io.IOException: Cannot lock storage /tmp/19e3e634-8980-4923-9e72-a5b900a71d63/dfscluster_32a46f7b-24ef-488f-bd33-915959e001f4/dfs/data/data3. The directory is already locked.
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:602)
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:455)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:111)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:376)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.&lt;init&gt;(DataNode.java:290)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1553)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1492)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1467)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:417)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:460)
	at org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.testLogRollOnDatanodeDeath(TestLogRolling.java:470)
        // ...
{noformat}

It can be reproduced without parallelization or without executing the other tests in the class. It seems to fail about 5% of the time.

This comes from the naming policy for the directories in MiniDFSCluster#startDataNode. It depends on the number of nodes *currently* in the cluster, and does not take into account previous starts/stops:
{noformat}
   for (int i = curDatanodesNum; i < curDatanodesNum+numDataNodes; i++) {
      if (manageDfsDirs) {
        File dir1 = new File(data_dir, ""data""+(2*i+1));
        File dir2 = new File(data_dir, ""data""+(2*i+2));
        dir1.mkdirs();
        dir2.mkdirs();
      // [...]
{noformat}

This means that it if we want to stop/start a datanode, we should always stop the last one, if not the names will conflict. This test exhibits the behavior:
{noformat}
  @Test
  public void testMiniDFSCluster_startDataNode() throws Exception {

    assertTrue( dfsCluster.getDataNodes().size() == 2 );


    // Works, as we kill the last datanode, we can now start a datanode
    dfsCluster.stopDataNode(1);
    dfsCluster
      .startDataNodes(TEST_UTIL.getConfiguration(), 1, true, null, null);


    // Fails, as it's not the last datanode, the directory will conflict on
    //  creation
    dfsCluster.stopDataNode(0);
    try {
      dfsCluster
        .startDataNodes(TEST_UTIL.getConfiguration(), 1, true, null, null);
      fail(""There should be an exception because the directory already exists"");
    } catch (IOException e) {
      assertTrue( e.getMessage().contains(""The directory is already locked.""));
      LOG.info(""Expected (!) exception caught "" + e.getMessage());
    }

    // Works, as we kill the last datanode, we can now restart 2 datanodes
    // This makes us back with 2 nodes
    dfsCluster.stopDataNode(0);
    dfsCluster
      .startDataNodes(TEST_UTIL.getConfiguration(), 2, true, null, null);
  }
{noformat}


And then this behavior is randomly triggered in testLogRollOnDatanodeDeath because when we do
{noformat}
DatanodeInfo[] pipeline = getPipeline(log);
assertTrue(pipeline.length == fs.getDefaultReplication());
{noformat}

and then kill the datanodes in the pipeline, we will have:
 - most of the time: pipeline = 1 & 2, so after killing 1&2 we can start a new datanode that will reuse the available 2's directory.
 - sometimes: pipeline = 1 & 3. In this case,when we try to launch the new datanode, it fails because it wants to use the same directory as the still alive '2'.

There are two ways of fixing the test:
1) Fix the naming rule in MiniDFSCluster#startDataNode, for example to ensure that the directory names will not be reused. But I wonder if there is not a testCase somewhere (may be not in HBase) depending on this behavior.
2) Kill explicitly the first and second datanode without using the pipeline to be sure that the names won't conflict.

Feedback welcome and the choice to make here...

",nkeywal,nkeywal,Minor,Closed,Fixed,10/Jan/12 00:43,12/Oct/12 05:35
Bug,HBASE-5168,12537887,Backport HBASE-5100 - Rollback of split could cause closed region to be opened again,Considering the importance of the defect merging it to 0.90.6,,ram_krish,Major,Closed,Fixed,10/Jan/12 11:03,20/Nov/15 11:55
Bug,HBASE-5172,12537938,HTableInterface should extend java.io.Closeable,Ioan Eugen Stan found this issue.,stack,zhihyu@ebaysf.com,Major,Closed,Fixed,10/Jan/12 18:21,12/Oct/12 05:35
Bug,HBASE-5176,12538007,AssignmentManager#getRegion: logging nit  adds a redundant '+'  ,"From the logs of HMaster: 


2012-01-10 17:28:24,370 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan for -ROOT-,,0.70236052 destination server is + localhost,60020,1326242475275

Was the '+' intended to be there , as part of some token for log verification or just being redundant , w.r.t the following string append ? ",kaykay.unique,kaykay.unique,Minor,Closed,Fixed,11/Jan/12 01:34,12/Oct/12 05:34
Bug,HBASE-5178,12538017,Backport HBASE-4101 - Regionserver Deadlock,Critical issue not merged to 0.90.  ,,ram_krish,Major,Closed,Fixed,11/Jan/12 04:13,20/Nov/15 11:54
Bug,HBASE-5179,12538023,"Concurrent processing of processFaileOver and ServerShutdownHandler may cause region to be assigned before log splitting is completed, causing data loss","If master's processing its failover and ServerShutdownHandler's processing happen concurrently, it may appear following  case.
1.master completed splitLogAfterStartup()
2.RegionserverA restarts, and ServerShutdownHandler is processing.
3.master starts to rebuildUserRegions, and RegionserverA is considered as dead server.
4.master starts to assign regions of RegionserverA because it is a dead server by step3.

However, when doing step4(assigning region), ServerShutdownHandler may be doing split log, Therefore, it may cause data loss.",zjushch,zjushch,Critical,Closed,Fixed,11/Jan/12 06:53,12/Jun/22 20:07
Bug,HBASE-5180,12538109,[book] book.xml - fixed scanner example,book.xml - the scanner example wasn't closing the ResultScanner!  that's bad practice.,dmeil,dmeil,Major,Closed,Fixed,11/Jan/12 21:51,12/Jun/22 20:07
Bug,HBASE-5182,12538121,TBoundedThreadPoolServer threadKeepAliveTimeSec is not configured properly,TBoundedThreadPoolServer does not take the configured threadKeepAliveTimeSec. It uses the default value instead.,schen,schen,Minor,Closed,Fixed,11/Jan/12 22:42,12/Oct/12 05:35
Bug,HBASE-5184,12538147,"Backport HBASE-5152 - Region is on service before completing initialization when doing rollback of split, it will affect read correctness ",Important issue to be merged into 0.90.,,ram_krish,Major,Closed,Fixed,12/Jan/12 04:31,20/Nov/15 11:52
Bug,HBASE-5191,12538284,Fix compilation error against hadoop 0.23.1,"From Mahadev:

I just checked out 0.92 branch and tried running:

mvn -Dhadoop.profile=23 clean test
-Dtest=org.apache.hadoop.hbase.mapreduce.TestTableMapReduce

Looks like a compilation issue:

------------
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile
(default-testCompile) on project hbase: Compilation failure
[ERROR] /Users/mahadev/workspace/hbase-workspace/hbase-git/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:[341,33]
cannot find symbol
[ERROR] symbol  : variable dnRegistration
[ERROR] location: class org.apache.hadoop.hdfs.server.datanode.DataNode
[ERROR] -> [Help 1]
[ERROR]",,zhihyu@ebaysf.com,Major,Closed,Fixed,13/Jan/12 04:09,20/Nov/15 11:52
Bug,HBASE-5192,12538295,Backport HBASE-4236 Don't lock the stream while serializing the response,"Backporting to 0.90.6

",,ram_krish,Major,Closed,Fixed,13/Jan/12 09:16,20/Nov/15 11:56
Bug,HBASE-5195,12538349,[Coprocessors] preGet hook does not allow overriding or wrapping filter on incoming Get,"Without the ability to wrap the internal Scan on the Get, we can't override (or protect, in the case of access control) Gets as we can Scans. The result is inconsistent behavior.",apurtell,apurtell,Major,Closed,Fixed,13/Jan/12 18:30,12/Oct/12 05:35
Bug,HBASE-5196,12538352,Failure in region split after PONR could cause region hole,"If region split fails after PONR, it relies on the master ServerShutdown handler to fix it.  However, if the master doesn't get a chance to fix it.  There will be a hole in the region chain.",jxiang,jxiang,Major,Closed,Fixed,13/Jan/12 19:05,12/Oct/12 05:34
Bug,HBASE-5200,12538389,AM.ProcessRegionInTransition() and AM.handleRegion() race thus leaving the region assignment inconsistent,"This is the scenario
Consider a case where the balancer is going on thus trying to close regions in a RS.
Before we could close a master switch happens.  
On Master switch the set of nodes that are in RIT is collected and we first get Data and start watching the node
After that the node data is added into RIT.
Now by this time (before adding to RIT) if the RS to which close was called does a transition in AM.handleRegion() we miss the handling saying RIT state was null.
{code}
2012-01-13 10:50:46,358 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region a66d281d231dfcaea97c270698b26b6f from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,358 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region c12e53bfd48ddc5eec507d66821c4d23 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,358 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 59ae13de8c1eb325a0dd51f4902d2052 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region f45bc9614d7575f35244849af85aa078 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region cc3ecd7054fe6cd4a1159ed92fd62641 from server HOST-192-168-47-204,20020,1326342744518 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 3af40478a17fee96b4a192b22c90d5a2 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region e6096a8466e730463e10d3d61f809b92 from server HOST-192-168-47-204,20020,1326342744518 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 4806781a1a23066f7baed22b4d237e24 from server HOST-192-168-47-204,20020,1326342744518 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region d69e104131accaefe21dcc01fddc7629 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states

{code}

In branch the CLOSING node is created by RS thus leading to more inconsistency.
",ram_krish,ram_krish,Major,Closed,Fixed,14/Jan/12 10:07,12/Oct/12 05:35
Bug,HBASE-5204,12538465,Backward compatibility fixes for 0.92,"Attached are 3 patches that are necessary to allow compatibility between HBase 0.90.x (and previous releases) and HBase 0.92.0.

First of all, I'm well aware that 0.92.0 RC4 has been thumbed up by a lot of people and would probably wind up being released as 0.92.0 tomorrow, so I sincerely apologize for creating this issue so late in the process.  I spent a lot of time trying to work around the quirks of 0.92 but once I realized that with a few very quasi-trivial changes compatibility would be made significantly easier, I immediately sent these 3 patches to Stack, who suggested I create this issue.

The first patch is required as without it clients sending a 0.90-style RPC to a 0.92-style server causes the server to die uncleanly.  It seems that 0.92 ships with {{\-XX:OnOutOfMemoryError=""kill \-9 %p""}}, and when a 0.92 server fails to deserialize a 0.90-style RPC, it attempts to allocate a large buffer because it doesn't read fields of 0.90-style RPCs properly.  This allocation attempt immediately triggers an OOME, which causes the JVM to die abruptly of a {{SIGKILL}}.  So whenever a 0.90.x client attempts to connect to HBase, it kills whichever RS is hosting the {{\-ROOT-}} region.

The second patch fixes a bug introduced by HBASE-2002, which added support for letting clients specify what ""protocol"" they want to speak.  If a client doesn't properly specify what protocol to use, the connection's {{protocol}} field will be left {{null}}, which causes any subsequent RPC on that connection to trigger an NPE in the server, even though the connection was successfully established from the client's point of view.  The fix is to simply give the connection a default protocol, by assuming the client meant to speak to a RegionServer.

The third patch fixes an oversight that slipped in HBASE-451, where a change to {{HbaseObjectWritable}} caused all the codes used to serialize {{Writables}} to shift by one.  This was carefully avoided in other changes such as HBASE-1502, which cleanly removed entries for {{HMsg}} and {{HMsg[]}}, so I don't think this breakage in HBASE-451 was intended.",tsuna,tsuna,Blocker,Closed,Fixed,16/Jan/12 00:32,20/Nov/15 11:54
Bug,HBASE-5206,12538474,"Port HBASE-5155 to 0.92, 0.94, and TRUNK",This JIRA ports HBASE-5155 (ServerShutDownHandler And Disable/Delete should not happen parallely leading to recreation of regions that were deleted) to 0.92 and TRUNK,ashutosh_jindal,zhihyu@ebaysf.com,Major,Closed,Fixed,16/Jan/12 06:20,26/Feb/13 08:12
Bug,HBASE-5212,12538581,Fix test TestTableMapReduce against 0.23.,"As reported by Andrew on the hadoop mailing list, mvn -Dhadoop.profile=23 clean test -Dtest=org.apache.hadoop.hbase.mapreduce.TestTableMapReduce fails on 0.92 branch. There are minor changes to HBase poms required to fix that.",gchanan,mahadev,Major,Closed,Fixed,16/Jan/12 23:16,12/Oct/12 05:34
Bug,HBASE-5213,12538595,"""hbase master stop"" does not bring down backup masters","Typing ""hbase master stop"" produces the following message:

""stop   Start cluster shutdown; Master signals RegionServer shutdown""
It seems like backup masters should be considered part of the cluster, but they are not brought down by ""hbase master stop"".
""stop-hbase.sh"" does correctly bring down the backup masters.

The same behavior is observed when a client app makes use of the client API HBaseAdmin.shutdown() http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#shutdown() -- this isn't too surprising since I think ""hbase master stop"" just calls this API.

It seems like HBASE-1448 address this; perhaps there was a regression?",gchanan,gchanan,Minor,Closed,Fixed,17/Jan/12 02:38,09/May/13 22:40
Bug,HBASE-5225,12538745,Backport HBASE-3845 -data loss because lastSeqWritten can miss memstore edits,Critical defect. Patch from HBASE-3845 was not integrated to 0.90.,ram_krish,ram_krish,Major,Closed,Fixed,18/Jan/12 08:16,20/Nov/15 11:54
Bug,HBASE-5228,12538882,"[REST] Rip out ""transform"" feature","The 'transform' feature, where REST can be instructed, via a table attribute, to apply a transformation (e.g. base64 encoding or decoding) to a (sub)set of column values before serving them up to a client or storing them into HBase, was added some time ago at the request of Jack Levin. I have since come to regret it, it was not a well thought out feature:

  - This is really an application concern.

  - It adds significant overhead to request processing: Periodically a HBaseAdmin is used to retrieve the table descriptor, in order to scan through table attributes for transformation directives. 

I think it is best to rip it out, its a real problem area, and REST should be no more concerned about data formats than the Java API. 

I doubt anyone uses this, not even Jack. Will need to follow up with him to confirm.",apurtell,apurtell,Major,Closed,Fixed,18/Jan/12 22:18,12/Oct/12 05:34
Bug,HBASE-5235,12539141,HLogSplitter writer thread's streams not getting closed when any of the writer threads has exceptions.,"Pls find the analysis.  Correct me if am wrong
{code}
2012-01-15 05:14:02,374 FATAL org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: WriterThread-9 Got while writing log entry to log
java.io.IOException: All datanodes 10.18.40.200:50010 are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3373)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2811)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:3026)

{code}
Here we have an exception in one of the writer threads. If any exception we try to hold it in an Atomic variable 
{code}
  private void writerThreadError(Throwable t) {
    thrown.compareAndSet(null, t);
  }
{code}
In the finally block of splitLog we try to close the streams.
{code}
      for (WriterThread t: writerThreads) {
        try {
          t.join();
        } catch (InterruptedException ie) {
          throw new IOException(ie);
        }
        checkForErrors();
      }
      LOG.info(""Split writers finished"");
      
      return closeStreams();
{code}
Inside checkForErrors
{code}
  private void checkForErrors() throws IOException {
    Throwable thrown = this.thrown.get();
    if (thrown == null) return;
    if (thrown instanceof IOException) {
      throw (IOException)thrown;
    } else {
      throw new RuntimeException(thrown);
    }
  }
So once we throw the exception the DFSStreamer threads are not getting closed.
{code}
",ram_krish,ram_krish,Major,Closed,Fixed,20/Jan/12 14:26,20/Nov/15 11:54
Bug,HBASE-5237,12539155,Addendum for HBASE-5160 and HBASE-4397,"As part of HBASE-4397 there is one more scenario where the patch has to be applied.
{code}
RegionPlan plan = getRegionPlan(state, forceNewPlan);
      if (plan == null) {
        debugLog(state.getRegion(),
            ""Unable to determine a plan to assign "" + state);
        return; // Should get reassigned later when RIT times out.
      }
{code}
I think in this scenario also 
{code}
this.timeoutMonitor.setAllRegionServersOffline(true);
{code}
this should be done.",ram_krish,ram_krish,Major,Closed,Fixed,20/Jan/12 15:40,20/Nov/15 11:55
Bug,HBASE-5243,12539275,LogSyncerThread not getting shutdown waiting for the interrupted flag,"In the LogSyncer run() we keep looping till this.isInterrupted flag is set.

But in some cases the DFSclient is consuming the Interrupted exception.  So
we are running into infinite loop in some shutdown cases.

I would suggest that as we are the ones who tries to close down the
LogSyncerThread we can introduce a variable like

Close or shutdown and based on the state of this flag along with
isInterrupted() we can make the thread stop.",ram_krish,ram_krish,Major,Closed,Fixed,21/Jan/12 06:42,20/Nov/15 11:54
Bug,HBASE-5267,12539549,Add a configuration to disable the slab cache by default,"From what I commented at the tail of HBASE-4027:

{quote}
I changed the release note, the patch doesn't have a ""hbase.offheapcachesize"" configuration and it's enabled as soon as you set -XX:MaxDirectMemorySize (which is actually a big problem when you consider this: http://hbase.apache.org/book.html#trouble.client.oome.directmemory.leak). 
{quote}

We need to add hbase.offheapcachesize and set it to false by default.

Marking as a blocker for 0.92.1 and assigning to Li Pi at Todd's request.",li,jdcryans,Blocker,Closed,Fixed,24/Jan/12 00:57,12/Oct/12 05:35
Bug,HBASE-5269,12539608,IllegalMonitorStateException while retryin HLog split in 0.90 branch.,"As part of HBASE-5137 fix this bug is introduced.  The splitLogLock is released in the finally block inside the do-while loop. So when the loop executes second time the unlock of the splitLogLock throws Illegal Monitor Exception. 

",ram_krish,ram_krish,Major,Closed,Fixed,24/Jan/12 13:42,20/Nov/15 11:53
Bug,HBASE-5271,12539630,Result.getValue and Result.getColumnLatest return the wrong column.,"In the following example result.getValue returns the wrong column

KeyValue kv = new KeyValue(Bytes.toBytes(""r""), Bytes.toBytes(""24""), Bytes.toBytes(""2""), Bytes.toBytes(7L));
Result result = new Result(new KeyValue[] { kv });
System.out.println(Bytes.toLong(result.getValue(Bytes.toBytes(""2""), Bytes.toBytes(""2"")))); //prints 7.
",ghais,ghais,Major,Closed,Fixed,24/Jan/12 16:31,12/Oct/12 05:35
Bug,HBASE-5278,12539818,"HBase shell script refers to removed ""migrate"" functionality","$ hbase migrate
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/util/Migrate
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.util.Migrate
at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.apache.hadoop.hbase.util.Migrate. Program will exit.


The 'hbase' shell script has docs referring to a 'migrate' command which no longer exists.",smanek,smanek,Trivial,Closed,Fixed,25/Jan/12 19:42,20/Dec/12 06:05
Bug,HBASE-5279,12539824,NPE in Master after upgrading to 0.92.0,"I have upgraded my environment from 0.90.4 to 0.92.0

after the table migration I get the following error in the master (permanent)

{noformat}
2012-01-25 18:23:48,648 FATAL master-namenode,60000,1327512209588 org.apache.hadoop.hbase.master.HMaster - Unhandled exception. Starting shutdown.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:2190)
        at org.apache.hadoop.hbase.master.AssignmentManager.joinCluster(AssignmentManager.java:323)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:501)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:326)
        at java.lang.Thread.run(Thread.java:662)
2012-01-25 18:23:48,650 INFO namenode,60000,1327512209588 org.apache.hadoop.hbase.master.HMaster - Aborting
{noformat}

I think that's because I had a hard crash in the cluster a while ago - and the following WARN since then

{noformat}
2012-01-25 21:20:47,121 WARN namenode,60000,1327513078123-CatalogJanitor org.apache.hadoop.hbase.master.CatalogJanitor - REGIONINFO_QUALIFIER is empty in keyvalues={emails,,xxx./info:server/1314336400471/Put/vlen=38, emails,,1314189353300.xxx./info:serverstartcode/1314336400471/Put/vlen=8}
{noformat}

my patch was simple to go around the NPE (as the other code around the lines)
but I don't know if that's correct
",tobiasherbert,tobiasherbert,Critical,Closed,Fixed,25/Jan/12 20:34,20/Nov/15 11:54
Bug,HBASE-5282,12539894,Possible file handle leak with truncated HLog file.,"When debugging hbck, found that the code responsible for this exception can leak open file handles.

{code}
12/01/15 05:58:11 INFO regionserver.HRegion: Replaying edits from hdfs://haus01.
sf.cloudera.com:56020/hbase-jon/test5/98a1e7255731aae44b3836641840113e/recovered
.edits/0000000000003211315; minSequenceid=3214658
12/01/15 05:58:11 ERROR handler.OpenRegionHandler: Failed open of region=test5,8
\x90\x00\x00\x00\x00\x00\x00/000005_0,1326597390073.98a1e7255731aae44b3836641840
113e.
java.io.EOFException
        at java.io.DataInputStream.readByte(DataInputStream.java:250)
        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:299)
        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:320)
        at org.apache.hadoop.io.Text.readString(Text.java:400)
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1486)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1437)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1424)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1419)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:57)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:158)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:572)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1940)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEditsIfAny(HRegion.java:1896)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:366)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2661)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2647)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:312)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:99)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{code}",jmhsieh,jmhsieh,Major,Closed,Fixed,26/Jan/12 13:05,12/Oct/12 05:35
Bug,HBASE-5283,12539934,Request counters may become negative for heavily loaded regions,"Requests counter showing negative count, example under 'Requests' column: -645470239
{code}
Name	Region Server	Start Key	End Key	Requests
usertable,user2037516127892189021,1326756873774.16833e4566d1daef109b8fdcd1f4b5a6. 	xxx.com:60030 	user2037516127892189021 	user2296868939942738705 	-645470239
{code}
RegionLoad.readRequestsCount and RegionLoad.writeRequestsCount are of int type. Our Ops has been running lots of heavy load operation. RegionLoad.getRequestsCount() overflows int.MAX_VALUE. It is set to D986E7E1. In table.jsp, RegionLoad.getRequestsCount() is assigned to long type. D986E7E1 is converted to long FFFFFFFFD986E7E1 which is -645470239 in decimal.

Suggested fix is to make readRequestsCount and writeRequestsCount long type. ",mubarakseyed,zhihyu@ebaysf.com,Major,Closed,Fixed,26/Jan/12 17:31,12/Oct/12 05:35
Bug,HBASE-5286,12539952,bin/hbase's logic of adding Hadoop jar files to the classpath is fragile when presented with split packaged Hadoop 0.23 installation,"Here's the bit from bin/hbase that might need TLC now that Hadoop can be spotted in the wild in split-package configuration:

{noformat}
#If avail, add Hadoop to the CLASSPATH and to the JAVA_LIBRARY_PATH
if [ ! -z $HADOOP_HOME ]; then
  HADOOPCPPATH=""""
  if [ -z $HADOOP_CONF_DIR ]; then
    HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" ""${HADOOP_HOME}/conf"")
  else
    HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" ""${HADOOP_CONF_DIR}"")
  fi
  if [ ""`echo ${HADOOP_HOME}/hadoop-core*.jar`"" != ""${HADOOP_HOME}/hadoop-core*.jar"" ] ; then
    HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" `ls ${HADOOP_HOME}/hadoop-core*.jar | head -1`)
  else
    HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" `ls ${HADOOP_HOME}/hadoop-common*.jar | head -1`)
    HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" `ls ${HADOOP_HOME}/hadoop-hdfs*.jar | head -1`)
    HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" `ls ${HADOOP_HOME}/hadoop-mapred*.jar | head -1`)
  fi
{noformat}

There's a couple of issues with the above code:
   0. HADOOP_HOME is now deprecated in Hadoop 0.23
   1. the list of jar files added to the class-path should be revised
   2. we need to figure out a more robust way to get the jar files that are needed to the classpath (things like hadoop-mapred*.jar tend to match src/test jars as well)

Better yet, it would be useful to look into whether we can transition HBase's bin/hbase onto using bin/hadoop as a launcher script instead of direct JAVA invocations (Pig, Hive, Sqoop and Mahout already do that)",rvs,rvs,Major,Closed,Fixed,26/Jan/12 18:51,12/Oct/12 05:34
Bug,HBASE-5287,12539956,[89-fb] hbck can go into an infinite loop,"HBaseFsckRepair.prompt() should check for -1 return value from System.in.read()

Only affects 0.89 release.",,khemani,Major,Closed,Fixed,26/Jan/12 19:50,12/Jun/22 20:11
Bug,HBASE-5288,12539960,Security source code dirs missing from 0.92.0 release tarballs.,"The release tarballs have a compiled version of the hbase jars and the security tarball seems to have the compiled security bits.  However, the source code and resources for security implementation are missing from the release tarballs in both distributions.  They should be included in both.",jmhsieh,jmhsieh,Blocker,Closed,Fixed,26/Jan/12 20:14,12/Oct/12 05:34
Bug,HBASE-5290,12539973,[FindBugs] Synchronization on boxed primitive,"This bug is reported by the findBugs tool, which is a static analysis tool.

Bug: Synchronization on Integer in org.apache.hadoop.hbase.regionserver.compactions.CompactSelection.emptyFileList()
The code synchronizes on a boxed primitive constant, such as an Integer.
{code}
private static Integer count = 0;
...
  synchronized(count) {
     count++;
     }
...
{code}
Since Integer objects can be cached and shared, this code could be synchronizing on the same object as other, unrelated code, leading to unresponsiveness and possible deadlock

See CERT CON08-J. Do not synchronize on objects that may be reused for more information.

Confidence: Normal, Rank: Troubling (14)
Pattern: DL_SYNCHRONIZATION_ON_BOXED_PRIMITIVE 
Type: DL, Category: MT_CORRECTNESS (Multithreaded correctness)",xodarap,liyin,Minor,Closed,Fixed,26/Jan/12 22:43,12/Oct/12 05:35
Bug,HBASE-5292,12539986,getsize per-CF metric incorrectly counts compaction related reads as well ,"The per-CF ""getsize"" metric's intent was to track bytes returned (to HBase clients) per-CF. [Note: We already have metrics to track # of HFileBlock's read for compaction vs. non-compaction cases -- e.g., compactionblockreadcnt vs. fsblockreadcnt.]

Currently, the ""getsize"" metric gets updated for both client initiated Get/Scan operations as well for compaction related reads. The metric is updated in StoreScanner.java:next() when the Scan query matcher returns an INCLUDE* code via a:

 HRegion.incrNumericMetric(this.metricNameGetsize, copyKv.getLength());

We should not do the above in case of compactions.
",,kannanm,Major,Closed,Fixed,27/Jan/12 01:17,07/Apr/13 04:33
Bug,HBASE-5307,12540560,Unable to gracefully decommission a node because of script error,"Unable to gracefully decommission a node because NameError occurred in region_mover.rb

{code}
$ bin/graceful_stop.sh ip-10-160-226-84.us-west-1.compute.internal
...
NameError: no constructorfor arguments (org.jruby.RubyString) on Java::OrgApacheHadoopHbase::HServerAddress
  available overloads:
    (org.apache.hadoop.hbase.HServerAddress)
    (java.net.InetSocketAddress)
     getRegions at /usr/local/hbase/current/bin/region_mover.rb:254
  unloadRegions at /usr/local/hbase/current/bin/region_mover.rb:314
         (root) at /usr/local/hbase/current/bin/region_mover.rb:430
Unloaded ip-10-160-226-84.us-west-1.compute.internal region(s)
ip-10-160-226-84.us-west-1.compute.internal: stopping regionserver..
{code}

The reason is the region_mover.rb calls wrong HBase APIs to try to establish a connection to the region server.",,uprush,Major,Closed,Fixed,31/Jan/12 14:48,20/Nov/15 11:53
Bug,HBASE-5308,12540600,Retry of distributed log splitting will fail on ./logs/rs-splitting directories,,,khemani,Major,Closed,Fixed,31/Jan/12 19:28,12/Jun/22 20:15
Bug,HBASE-5317,12540848,Fix TestHFileOutputFormat to work against hadoop 0.23,"Running
mvn -Dhadoop.profile=23 test -P localTests -Dtest=org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat

yields this on 0.92:
Failed tests:   testColumnFamilyCompression(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat): HFile for column family info-A not found

Tests in error: 
  test_TIMERANGE(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat): /home/gchanan/workspace/apache92/target/test-data/276cbd0c-c771-4f81-9ba8-c464c9dd7486/test_TIMERANGE_present/_temporary/0/_temporary/_attempt_200707121733_0001_m_000000_0 (Is a directory)
  testMRIncrementalLoad(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat): TestTable
  testMRIncrementalLoadWithSplit(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat): TestTable

It looks like on trunk, this also results in an error:
  testExcludeMinorCompaction(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat): TestTable

I have a patch that fixes testColumnFamilyCompression and test_TIMERANGE, but haven't fixed the other 3 yet.
",gchanan,gchanan,Major,Closed,Fixed,02/Feb/12 00:11,12/Oct/12 05:35
Bug,HBASE-5321,12540894,this.allRegionServersOffline  not set to false after one RS comes online and assignment is done in 0.90.,"In HBASE-5160 we do not wait for TM to assign the regions after the first RS comes online.
After doing this the variable this.allRegionServersOffline needs to be reset which is not done in 0.90.
",ram_krish,ram_krish,Major,Closed,Fixed,02/Feb/12 08:35,20/Nov/15 11:55
Bug,HBASE-5327,12541018,Print a message when an invalid hbase.rootdir is passed,"As seen on the mailing list: http://comments.gmane.org/gmane.comp.java.hadoop.hbase.user/24124

If hbase.rootdir doesn't specify a folder on hdfs we crash while opening a path to .oldlogs:

{noformat}
2012-02-02 23:07:26,292 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: hdfs://sv4r11s38:9100.oldlogs
        at org.apache.hadoop.fs.Path.initialize(Path.java:148)
        at org.apache.hadoop.fs.Path.<init>(Path.java:71)
        at org.apache.hadoop.fs.Path.<init>(Path.java:50)
        at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:112)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:448)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:326)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: hdfs://sv4r11s38:9100.oldlogs
        at java.net.URI.checkPath(URI.java:1787)
        at java.net.URI.<init>(URI.java:735)
        at org.apache.hadoop.fs.Path.initialize(Path.java:145)
        ... 6 more
{noformat}

It could also crash anywhere else, this just happens to be the first place we use hbase.rootdir. We need to verify that it's an actual folder.",jxiang,jdcryans,Major,Closed,Fixed,02/Feb/12 23:16,12/Oct/12 05:34
Bug,HBASE-5329,12541042,"addRowLock() may allocate duplicate lock id, causing the client to be blocked","{code}
protected long addRowLock(Integer r, HRegion region) throws LeaseStillHeldException
{
	long lockId = -1L;
	lockId = rand.nextLong();               //!!!may generate duplicated id，bug?
	String lockName = String.valueOf(lockId);
	rowlocks.put(lockName, r);
	this.leases.createLease(lockName, new RowLockListener(lockName, region));
	return lockId;
}
{code}

In addRowLock(),rand may generate duplicated lock id, it may cause regionserver throw exception(Leases$LeaseStillHeldException).The client will be blocked until old rowlock is released.

{code}
2012-02-03 15:21:50,084 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error obtaining row lock (fsOk: true)
org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException
        at org.apache.hadoop.hbase.regionserver.Leases.createLease(Leases.java:150)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.addRowLock(HRegionServer.java:1986)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.lockRow(HRegionServer.java:1963)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
{code}",ivarley,liaoxiangui,Minor,Closed,Fixed,03/Feb/12 07:49,23/Sep/13 18:31
Bug,HBASE-5344,12541552,[89-fb] Scan unassigned region directory on master failover,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",mikhail,mikhail,Major,Closed,Fixed,07/Feb/12 02:49,12/Jun/22 20:18
Bug,HBASE-5345,12541641,CheckAndPut doesn't work when value is empty byte[],"When a value contains an empty byte[] and then a checkAndPut is performed with an empty byte[] , the operation will fail.

For example:
Put put = new Put(row1);
put.add(fam1, qf1, new byte[0]);
table.put(put);

put = new Put(row1);
put.add(fam1, qf1, val1);
table.checkAndPut(row1, fam1, qf1, new byte[0], put); ---> false

I think this is related to HBASE-3793 and HBASE-3468.

Note that you will also get into this situation when first putting a null value ( put.add(fam1,qf1,null) ), as this value will then be regarded and returned as an empty byte[] upon a get.
",evertot,evertot,Major,Closed,Fixed,07/Feb/12 15:59,12/Oct/12 05:35
Bug,HBASE-5348,12541703,Constraint configuration loaded with bloat,"Constraints load the configuration but don't load the 'correct' configuration, but instead instantiate the default configuration (via new Configuration). It should just be Configuration(false)",jesse_yates,jesse_yates,Minor,Closed,Fixed,07/Feb/12 22:21,12/Oct/12 05:34
Bug,HBASE-5350,12541711,Fix jamon generated package names,"Previously, jamon was creating the template files in ""org.apache.hbase"", but it should be ""org.apache.hadoop.hbase"", so it's in line with rest of source files.",jesse_yates,jesse_yates,Major,Closed,Fixed,07/Feb/12 23:06,12/Oct/12 05:34
Bug,HBASE-5351,12541717,hbase completebulkload to a new table fails in a race,"I have a test that tests vanilla use of importtsv with importtsv.bulk.output option followed by completebulkload to a new table.

This sometimes fails as follows:
11/12/19 15:02:39 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: ml_items_copy, row=ml_items_copy,,99999999999999
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:157)
at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:52)
at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:127)
at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:127)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:103)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:875)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:929)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:817)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:781)
at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:247)
at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:211)
at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:171)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.createTable(LoadIncrementalHFiles.java:673)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.run(LoadIncrementalHFiles.java:697)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.main(LoadIncrementalHFiles.java:707)

The race appears to be calling HbAdmin.createTableAsync(htd, keys) and then creating an HTable object before that call has actually completed.
The following change to /src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java appears to fix the problem, but I have not been able to reproduce the race reliably, in order to write a test.

{code}
-    HTable table = new HTable(this.cfg, tableName);
-
-    HConnection conn = table.getConnection();
     int ctr = 0;
-    while (!conn.isTableAvailable(table.getTableName()) && (ctr<TABLE_CREATE_MA
+    while (!this.hbAdmin.isTableAvailable(tableName) && (ctr<TABLE_CREATE_MAX_R
{code}",gchanan,gchanan,Major,Closed,Fixed,08/Feb/12 00:14,12/Oct/12 05:35
Bug,HBASE-5356,12541885,region_mover.rb can hang if table region it belongs to is deleted.,"I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).

Here's the start of the relevent stack dump
{code}
12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\
Verify_1328735001040,yC^P\xD7\x945\xD4,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\
9)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\
)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\
java:1018)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)
        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)
        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)
        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)
        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)
        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)
        at org.jruby.runtime.Block.call(Block.java:89)
        at org.jruby.RubyProc.call(RubyProc.java:268)
        at org.jruby.RubyProc.call(RubyProc.java:228)
        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)
        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)
        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)
        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)
        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)
        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)
        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)
        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)
        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)
        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)
        at org.jruby.Ruby.runScript(Ruby.java:670)
        at org.jruby.Ruby.runNormally(Ruby.java:574)
        at org.jruby.Ruby.runFromMain(Ruby.java:423)
        at org.jruby.Main.doRunFromMain(Main.java:278)
        at org.jruby.Main.internalRun(Main.java:198)
        at org.jruby.Main.run(Main.java:164)
        at org.jruby.Main.run(Main.java:148)
        at org.jruby.Main.main(Main.java:128)
{code}",jxiang,jmhsieh,Minor,Closed,Fixed,08/Feb/12 22:12,16/Jun/15 23:39
Bug,HBASE-5359,12541900,Alter in the shell can be too quick and return before the table is altered,"This seems to be a recent change in behavior but I'm still not sure where it's coming from.

The shell is able to call HMaster.getAlterStatus before the TableEventHandler is able call AM.setRegionsToReopen so that the returned status shows no pending regions. It means that the alter seems ""instantaneous"" although it's far from completed.",nspiegelberg,jdcryans,Major,Closed,Fixed,08/Feb/12 23:31,23/Sep/13 18:30
Bug,HBASE-5364,12541997,Fix source files missing licenses in 0.92 and trunk,"running 'mvn rat:check' shows that a few files have snuck in that do not have proper apache licenses.  Ideally we should fix these before we cut another release/release candidate.

This is a blocker for 0.94, and probably should be for the other branches as well.
",eclark,jmhsieh,Blocker,Closed,Fixed,09/Feb/12 16:46,12/Oct/12 05:35
Bug,HBASE-5377,12542072,Fix licenses on the 0.90 branch.,There are a handful of empty files and several files missing apache licenses on the 0.90 branch.  This patch will fixes all of them and in conjunction with HBASE-5363 will allow it to pass RAT tests.,jmhsieh,jmhsieh,Major,Closed,Fixed,10/Feb/12 04:47,20/Nov/15 11:52
Bug,HBASE-5379,12542164,"Backport HBASE-4287 to 0.90 - If region opening fails, try to transition region back to ""offline"" in ZK",This issue is needed in 0.90 also.   Else if region assignment fails then need to wait for 30 minutes.,ram_krish,ram_krish,Major,Closed,Fixed,10/Feb/12 17:56,20/Nov/15 11:56
Bug,HBASE-5384,12542205,Up heap used by hadoopqa,,stack,stack,Major,Closed,Fixed,10/Feb/12 23:27,12/Oct/12 05:34
Bug,HBASE-5387,12542215,Reuse compression streams in HFileBlock.Writer,"We need to to reuse compression streams in HFileBlock.Writer instead of allocating them every time. The motivation is that when using Java's built-in implementation of Gzip, we allocate a new GZIPOutputStream object and an associated native data structure every time we create a compression stream. The native data structure is only deallocated in the finalizer. This is one suspected cause of recent TestHFileBlock failures on Hadoop QA: https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/.
",mikhail,mikhail,Critical,Closed,Fixed,11/Feb/12 02:45,12/Oct/12 05:35
Bug,HBASE-5397,12542518,[book] zookeeper quorum mistake,"In Chapter 2, section 2.5 ""ZooKeeper"" under ""How many ZooKeepers should I run?"" there is the sentence
*There can be no quorum if the number of members is an even number.*

This is not true. In ZooKeeper, an even number of peers is supported, but it is normally not used because an even sized ensemble requires, proportionally, more peers to form a quorum than an odd sized ensemble requires. For example, an ensemble with 4 peers requires 3 to form a quorum, while an ensemble with 5 also requires 3 to form a quorum. Thus, an ensemble of 5 allows 2 peers to fail, and thus is more fault tolerant than the ensemble of 4, which allows only 1 down peer.",dmeil,andremedeiros,Minor,Closed,Fixed,14/Feb/12 12:36,12/Jun/22 20:19
Bug,HBASE-5398,12542579,HBase shell disable_all/enable_all/drop_all promp wrong tables for confirmation,"When using hbase shell to disable_all/enable_all/drop_all tables, the tables prompted for confirmation are wrong.

For example, disable_all 'test*'
will ask form confirmation to diable tables like:

mytest1
test123

Fortunately, these tables will not be disabled actually since Java pattern doesn't match this way.",jxiang,jxiang,Major,Closed,Fixed,14/Feb/12 18:35,12/Oct/12 05:34
Bug,HBASE-5401,12542685,PerformanceEvaluation generates 10x the number of expected mappers,"With a command line like 'hbase org.apache.hadoop.hbase.PerformanceEvaluation randomWrite 10' there are 100 mappers spawned, rather than the expected 10.  The culprit appears to be the outer loop in writeInputFile which sets up 10 splits for every ""asked-for client"".  I think the fix is just to remove that outer loop.",easyliangjob,oliver@mineallmeyn.com,Major,Closed,Fixed,15/Feb/12 14:22,12/Jun/22 20:19
Bug,HBASE-5405,12542735,index.xml - correcting BigTable paper description,"""Bigtable: A Distributed Storage System for Structured"" should be ""Bigtable: A Distributed Storage System for Structured Data"" (the ""data"" was missing)

",dmeil,dmeil,Trivial,Closed,Fixed,15/Feb/12 19:18,12/Jun/22 20:20
Bug,HBASE-5412,12542823,"HBase book, section 2.6.4, has deficient list of client dependencies","The current text in section 2.6.4 of the HBase book says this about client dependencies:

Minimally, a client of HBase needs the hbase, hadoop, log4j, commons-logging, commons-lang, and ZooKeeper jars in its CLASSPATH connecting to a cluster.

I tried that, and got an exception due to a class not being found.  I fixed that by searching for that class in the jars in lib/, and tried again.  Got an exception, due to a different class not found.  I iterated until it worked.  When I was done, I found myself using the following JARs:

commons-configuration-1.6.jar  hadoop-core-1.0.0.jar  slf4j-api-1.5.8.jar
commons-lang-2.5.jar           hbase-0.92.0.jar       slf4j-log4j12-1.5.8.jar
commons-logging-1.1.1.jar      log4j-1.2.16.jar       zookeeper-3.4.2.jar
",ivarley,mspreitz,Minor,Closed,Fixed,16/Feb/12 05:17,23/Sep/13 18:30
Bug,HBASE-5415,12542915,FSTableDescriptors should handle random folders in hbase.root.dir better,"I faked an upgrade on a test cluster using our dev data so I had to distcp the data between the two clusters, but after starting up and doing the migration and whatnot the web UI didn't show any table. The reason was in the master's log:

{quote}
org.apache.hadoop.hbase.TableExistsException: No descriptor for _distcp_logs_e0ehek
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:164)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:182)
        at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1554)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1326)
{quote}

I don't think we need to show a full stack (just a WARN maybe), this shouldn't kill the request (still see tables in the web UI), and why is that a TableExistsException?",jdcryans,jdcryans,Critical,Closed,Fixed,16/Feb/12 20:02,12/Oct/12 05:35
Bug,HBASE-5420,12542962,TestImportTsv does not shut down MR Cluster correctly (fails against 0.23 hadoop),"Test calls startMiniMapReduceCluster() but never calls shutdownMiniMapReduceCluster().

This causes failures with -Dhadoop.profile=23 when both testMROnTable and testMROnTableWithCustomMapper are run, because the cluster cannot start up properly for the second test.",gchanan,gchanan,Major,Closed,Fixed,16/Feb/12 22:38,12/Oct/12 05:34
Bug,HBASE-5422,12542990,StartupBulkAssigner would cause a lot of timeout on RIT when assigning large numbers of regions (timeout = 3 mins),"In our produce environment
We find a lot of timeout on RIT when cluster up, there are about 7w regions in the cluster( 25 regionservers ).

First, we could see the following log:(See the region 33cf229845b1009aa8a3f7b0f85c9bd0)
master's log
2012-02-13 18:07:41,409 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x348f4a94723da5 Async create of unassigned node for 33cf229845b1009aa8a3f7b0f85c9bd0 with OFFLINE state 
2012-02-13 18:07:42,560 DEBUG org.apache.hadoop.hbase.master.AssignmentManager$CreateUnassignedAsyncCallback: rs=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=OFFLINE, ts=1329127661409, server=r03f11025.yh.aliyun.com,60020,1329127549907 
2012-02-13 18:07:42,996 DEBUG org.apache.hadoop.hbase.master.AssignmentManager$ExistsUnassignedAsyncCallback: rs=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=OFFLINE, ts=1329127661409 
2012-02-13 18:10:48,072 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=PENDING_OPEN, ts=1329127662996
2012-02-13 18:10:48,072 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 
2012-02-13 18:11:16,744 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=r03f11025.yh.aliyun.com,60020,1329127549907, region=33cf229845b1009aa8a3f7b0f85c9bd0 
2012-02-13 18:38:07,310 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 33cf229845b1009aa8a3f7b0f85c9bd0; deleting unassigned node 
2012-02-13 18:38:07,310 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x348f4a94723da5 Deleting existing unassigned node for 33cf229845b1009aa8a3f7b0f85c9bd0 that is in expected state RS_ZK_REGION_OPENED 
2012-02-13 18:38:07,314 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x348f4a94723da5 Successfully deleted unassigned node for region 33cf229845b1009aa8a3f7b0f85c9bd0 in expected state RS_ZK_REGION_OPENED 
2012-02-13 18:38:07,573 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. on r03f11025.yh.aliyun.com,60020,1329127549907 
2012-02-13 18:50:54,428 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. so generated a random one; hri=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0., src=, dest=r01b05043.yh.aliyun.com,60020,1329127549041; 29 (online=29, exclude=null) available servers 
2012-02-13 18:50:54,428 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. to r01b05043.yh.aliyun.com,60020,1329127549041 
2012-02-13 19:31:50,514 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=PENDING_OPEN, ts=1329132528086 
2012-02-13 19:31:50,514 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 



Regionserver's log
2012-02-13 18:07:43,537 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 
2012-02-13 18:11:16,560 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 





Through the RS's log, we could find it is larger than 3mins from receive openRegion request to start processing openRegion, causing timeout on RIT in master for the region.

Let's see the code of StartupBulkAssigner, we could find regionPlans are not added when assigning regions, therefore, when one region opened, it will not updateTimers of other regions whose destination is the same.",zjushch,zjushch,Major,Closed,Fixed,17/Feb/12 03:33,20/Nov/15 11:54
Bug,HBASE-5423,12542992,Regionserver may block forever on waitOnAllRegionsToClose when aborting,"If closeRegion throws any exception (It would be caused by FS ) when RS is aborting, 
RS will block forever on waitOnAllRegionsToClose().",zjushch,zjushch,Major,Closed,Fixed,17/Feb/12 05:50,12/Oct/12 05:34
Bug,HBASE-5425,12543002, Punt on the timeout doesn't work in BulkEnabler#waitUntilDone (master's EnableTableHandler),"please take a look at the code below in EnableTableHandler(hbase master):

{code:title=EnableTableHandler.java|borderStyle=solid}

    protected boolean waitUntilDone(long timeout)
    throws InterruptedException {
    
      .....
      int lastNumberOfRegions = this.countOfRegionsInTable;

      while (!server.isStopped() && remaining > 0) {
        Thread.sleep(waitingTimeForEvents);
        regions = assignmentManager.getRegionsOfTable(tableName);
        if (isDone(regions)) break;

        // Punt on the timeout as long we make progress
        if (regions.size() > lastNumberOfRegions) {
          lastNumberOfRegions = regions.size();
          timeout += waitingTimeForEvents;
        }
        remaining = timeout - (System.currentTimeMillis() - startTime);
    ....
    }
    private boolean isDone(final List<HRegionInfo> regions) {
      return regions != null && regions.size() >= this.countOfRegionsInTable;
    }

{code} 

We can easily find out if we let lastNumberOfRegions = this.countOfRegionsInTable , the function of punt on timeout code will never be executed. I think initlize lastNumberOfRegions = 0 can make it work.
",terry_zhang,terry_zhang,Major,Closed,Fixed,17/Feb/12 06:55,12/Oct/12 05:34
Bug,HBASE-5430,12543124,Fix licenses in 0.92.1 -- RAT plugin won't pass,Use the -Drelease profile to see we are missing 30 or so license.  Fix.,stack,stack,Blocker,Closed,Fixed,18/Feb/12 01:08,20/Nov/15 11:52
Bug,HBASE-5437,12543598,HRegionThriftServer does not start because of a bug in HbaseHandlerMetricsProxy,"3.facebook.com,60020,1329865516120: Initialization of RS failed.  Hence aborting RS.
java.lang.ClassCastException: $Proxy9 cannot be cast to org.apache.hadoop.hbase.thrift.generated.Hbase$Iface
at org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.newInstance(HbaseHandlerMetricsProxy.java:47)
at org.apache.hadoop.hbase.thrift.ThriftServerRunner.<init>(ThriftServerRunner.java:239)
at org.apache.hadoop.hbase.regionserver.HRegionThriftServer.<init>(HRegionThriftServer.java:74)
at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeThreads(HRegionServer.java:646)
at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:546)
at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:658)
at java.lang.Thread.run(Thread.java:662)
2012-02-21 15:05:18,749 FATAL org.apache.hadoop.h
",schen,schen,Major,Closed,Fixed,22/Feb/12 00:12,12/Oct/12 05:35
Bug,HBASE-5441,12543609,HRegionThriftServer may not start because of a race-condition,"This happens because the master is not started when ThriftServerRunner tries to create an HBaseAdmin.

org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1333)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:899)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:303)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:280)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:332)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:649)
        at org.apache.hadoop.hbase.client.HBaseAdmin.<init>(HBaseAdmin.java:108)
        at org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.<init>(ThriftServerRunner.java:516)
        at org.apache.hadoop.hbase.regionserver.HRegionThriftServer$HBaseHandlerRegion.<init>(HRegionThriftServer.java:104)
        at org.apache.hadoop.hbase.regionserver.HRegionThriftServer.<init>(HRegionThriftServer.java:74)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeThreads(HRegionServer.java:646)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:546)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:658)
        at java.lang.Thread.run(Thread.java:662)
2012-02-21 16:38:18,223 INFO org.apache.hadoop.hba",schen,schen,Minor,Closed,Fixed,22/Feb/12 02:30,23/Sep/13 18:31
Bug,HBASE-5458,12543750,Thread safety issues with Compression.Algorithm.GZ and CompressionTest,"I've seen some occasional NullPointerExceptions in ZlibFactory.isNativeZlibLoaded(conf) during region server startups and the completebulkload process.  This is being caused by a null configuration getting passed to the isNativeZlibLoaded method.  I think this happens when 2 or more threads call the CompressionTest.testCompression method at once.  If the GZ algorithm has not been tested yet both threads could continue on and attempt to load the compressor.  For GZ the getCodec method is not thread safe which could lead to one thread getting a reference to a GzipCodec that has a null configuration.
{code}
current:
      DefaultCodec getCodec(Configuration conf) {
        if (codec == null) {
          codec = new GzipCodec();
          codec.setConf(new Configuration(conf));
        }

        return codec;
      }
{code}
one possible fix would be something like this:
{code}
      DefaultCodec getCodec(Configuration conf) {
        if (codec == null) {
          GzipCodec gzip = new GzipCodec();
          gzip.setConf(new Configuration(conf));
          codec = gzip;
        }

        return codec;
      }
{code}
But that may not be totally safe without some synchronization.  An upstream fix in CompressionTest could also prevent multi thread access to GZ.getCodec(conf)

exceptions:
12/02/21 16:11:56 ERROR handler.OpenRegionHandler: Failed open of region=all-monthly,,1326263896983.bf574519a95263ec23a2bad9f5b8cbf4.
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:89)
        at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:2670)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2659)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2647)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:312)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:99)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded(ZlibFactory.java:63)
        at org.apache.hadoop.io.compress.GzipCodec.getCompressorType(GzipCodec.java:166)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:100)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:112)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:236)
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:84)
        ... 9 more

Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:89)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.readTrailer(HFile.java:890)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.loadFileInfo(HFile.java:819)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.groupOrSplit(LoadIncrementalHFiles.java:405)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$2.call(LoadIncrementalHFiles.java:323)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$2.call(LoadIncrementalHFiles.java:321)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded(ZlibFactory.java:63)
        at org.apache.hadoop.io.compress.GzipCodec.getCompressorType(GzipCodec.java:166)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:100)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:112)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:236)
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:84)
        ... 10 more
",eclark,dmcintosh,Minor,Closed,Fixed,22/Feb/12 22:11,26/Feb/13 08:27
Bug,HBASE-5466,12543902,Opening a table also opens the metatable and never closes it.,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class,

When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed.

This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling
HConnectionManager.deleteConnection(config, true);	


",ashleyt,ashleyt,Major,Closed,Fixed,23/Feb/12 21:03,12/Oct/12 05:35
Bug,HBASE-5470,12543926,Make DataBlockEncodingTool work correctly with no native compression codecs loaded,"DataBlockEncodingTool was fixed as part of porting data block encoding (HBASE-4218) to 89-fb (https://reviews.facebook.net/rHBASEEIGHTNINEFBBRANCH1245291, https://reviews.facebook.net/D1659). The bug appeared when using GZ as baseline compression codec but not loading native Hadoop libraries, in which case the compressor instance would be null. The purpose of this JIRA is to bring the trunk version of DataBlockEncodingTool to parity with the 89-fb version, and further improvements to the tool will be made separately.
",mikhail,mikhail,Minor,Closed,Fixed,23/Feb/12 23:30,12/Oct/12 05:35
Bug,HBASE-5472,12543959,LoadIncrementalHFiles loops forever if the target table misses a CF,"I have some HFiles for two column families 'y','z', but I specified a target table that only has CF 'y'.
I see the following repeated forever.
...
12/02/23 22:57:37 WARN mapreduce.LoadIncrementalHFiles: Attempt to bulk load region containing  into table z with files [family:y path:hdfs://bunnypig:9000/bulk/z2/y/bd6f1c3cc8b443fc9e9e5fddcdaa3b09, family:z path:hdfs://bunnypig:9000/bulk/z2/z/38f12fdbb7de40e8bf0e6489ef34365d] failed.  This is recoverable and they will be retried.
12/02/23 22:57:37 DEBUG client.MetaScanner: Scanning .META. starting at row=z,,00000000000000 for max=2147483647 rows using org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7b7a4989
12/02/23 22:57:37 INFO mapreduce.LoadIncrementalHFiles: Split occured while grouping HFiles, retry attempt 1596 with 2 files remaining to group or split
12/02/23 22:57:37 INFO mapreduce.LoadIncrementalHFiles: Trying to load hfile=hdfs://bunnypig:9000/bulk/z2/y/bd6f1c3cc8b443fc9e9e5fddcdaa3b09 first=r last=r
12/02/23 22:57:37 INFO mapreduce.LoadIncrementalHFiles: Trying to load hfile=hdfs://bunnypig:9000/bulk/z2/z/38f12fdbb7de40e8bf0e6489ef34365d first=r last=r
12/02/23 22:57:37 DEBUG mapreduce.LoadIncrementalHFiles: Going to connect to server region=z,,1330066309814.d5fa76a38c9565f614755e34eacf8316., hostname=localhost, port=60020 for row 
...",liyu,larsh,Minor,Closed,Fixed,24/Feb/12 07:02,23/Sep/13 19:08
Bug,HBASE-5473,12544068,Metrics does not push pread time,The RegionServerMetrics is not pushing the pread times to the MetricsRecord,dhruba,dhruba,Minor,Closed,Fixed,24/Feb/12 22:55,12/Oct/12 05:35
Bug,HBASE-5477,12544097,Cannot build RPM for hbase-0.92.0,"Steps to reproduce:

{code}
tar xzvf hbase-0.92.0.tar.gz
cd hbase-0.92.0
mvn -Dmaven.test.skip.exec=true -P rpm install
{code}

Failure output and patch will be attached.",,b1c1l1,Major,Closed,Fixed,25/Feb/12 03:47,12/Oct/12 05:35
Bug,HBASE-5478,12544122,[0.92] TestMetaReaderEditor intermittently hangs,"From 0.92 build #304:
{code}
Running org.apache.hadoop.hbase.catalog.TestMetaReaderEditor
Running org.apache.hadoop.hbase.catalog.TestCatalogTrackerOnCluster
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 149.104 sec
{code}",apurtell,zhihyu@ebaysf.com,Major,Closed,Fixed,25/Feb/12 15:11,13/Jun/22 15:32
Bug,HBASE-5480,12544186,Fixups to MultithreadedTableMapper for Hadoop 0.23.2+,"There are two issues:

- StatusReporter has a new method getProgress()

- Mapper and reducer context objects can no longer be directly instantiated.

See attached patch. I'm not thrilled with the added reflection but it was the minimally intrusive change.

Raised the priority to critical because compilation fails.",apurtell,apurtell,Critical,Closed,Fixed,26/Feb/12 18:34,18/Sep/13 22:18
Bug,HBASE-5481,12544218,Uncaught UnknownHostException prevents HBase from starting,"If a host gets decommissioned and its hostname no longer resolves, and it was previously hosting ROOT or META, HBase won't be able to start up.  This easily happens when moving across networks (e.g. developing HBase on a laptop), but can also happen during cluster-wide maintenances where HBase is shut down, then one or more nodes get decommissioned such that their hostnames no longer resolve.

{code}
2012-02-26 20:05:48,339 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to nowwhat.tsunanet.net,54092,1330315542087
[...]
2012-02-26 20:05:48,456 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined -ROOT-,,0.70236052; next sequenceid=268
2012-02-26 20:05:48,456 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Attempting to transition node 70236052/-ROOT- from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-02-26 20:05:48,458 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-02-26 20:05:48,459 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=nowwhat.tsunanet.net,54092,1330315542087, region=70236052/-ROOT-
2012-02-26 20:05:48,459 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Post open deploy tasks for region=-ROOT-,,0.70236052, daughter=false
2012-02-26 20:05:48,460 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Setting ROOT region location in ZooKeeper as nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,466 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Done with post open deploy task for region=-ROOT-,,0.70236052, daughter=false
2012-02-26 20:05:48,466 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Attempting to transition node 70236052/-ROOT- from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: region transitioned to opened in zookeeper: {NAME => '-ROOT-,,0', STARTKEY => '', ENDKEY => '', ENCODED => 70236052,}, server: nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opened -ROOT-,,0.70236052 on server:nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=nowwhat.tsunanet.net,54092,1330315542087, region=70236052/-ROOT-
2012-02-26 20:05:48,470 INFO org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for -ROOT-,,0.70236052 from nowwhat.tsunanet.net,54092,1330315542087; deleting unassigned node
2012-02-26 20:05:48,470 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:54081-0x135bcfbb0580000 Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED
2012-02-26 20:05:48,472 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: The znode of region -ROOT-,,0.70236052 has been deleted.
2012-02-26 20:05:48,472 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:54081-0x135bcfbb0580000 Successfully deleted unassigned node for region 70236052 in expected state RS_ZK_REGION_OPENED
2012-02-26 20:05:48,472 INFO org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the region -ROOT-,,0.70236052 that was online on nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,473 INFO org.apache.hadoop.hbase.master.HMaster: -ROOT- assigned=1, rit=false, location=nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,486 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@16d0a6a3; serverName=nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,488 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@16d0a6a3; serverName=nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,620 FATAL org.apache.hadoop.hbase.master.HMaster: Master server abort: loaded coprocessors are: []
2012-02-26 20:05:48,621 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.net.UnknownHostException: unknown host: h-27-1.sfo.stumble.net
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.<init>(HBaseClient.java:227)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1016)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:878)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
        at $Proxy12.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:303)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:280)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:332)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:236)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1278)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1235)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1222)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:564)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:422)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:478)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnection(CatalogTracker.java:503)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:674)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:575)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:491)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:326)
        at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:218)
        at java.lang.Thread.run(Thread.java:680)
2012-02-26 20:05:48,626 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",tsuna,tsuna,Major,Closed,Fixed,27/Feb/12 04:19,12/Oct/12 05:35
Bug,HBASE-5482,12544231,"In 0.90, balancer algo leading to same region balanced twice and picking same region with Src and Destination as same RS.","There are possibility of 2 problems
-> When we populate regionsToMove while iterating the serverinfo in descending manner there is a chance that the same region can be added twice.
Because in the first loop we do a randomization of the regions.
Where as when we get we have neededRegions!= 0 we just get the region in the index and add it again . This may lead to have same region in the regionsToMove list.
-> Another problem is 
when the problem in the first point happens then there is a chance that
the regionToMove can have the same src and destination and the same region can be picked every 5 mins.
{code}
for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
        serversByLoad.descendingMap().entrySet()) {
        BalanceInfo balanceInfo = serverBalanceInfo.get(server.getKey());
        int idx =
          balanceInfo == null ? 0 : balanceInfo.getNextRegionForUnload();
        if (idx >= server.getValue().size()) break;
        HRegionInfo region = server.getValue().get(idx);
        if (region.isMetaRegion()) continue; // Don't move meta regions.
        regionsToMove.add(new RegionPlan(region, server.getKey(), null));
        if(--neededRegions == 0) {
          // No more regions needed, done shedding
          break;
        }
      }
{code}
If i have meta and root in the top two loaded region server(totally 3 RS), we just skip the regions in those region server and populate the region from the least loaded RS.
Then in the next loop we iterate from the least loaded server and populate the destination as also the same server.
This is leading to a condition where every 5 min balancing happens and also the server is same for src and dest.",ram_krish,ram_krish,Major,Closed,Fixed,27/Feb/12 08:39,13/Jun/22 15:33
Bug,HBASE-5484,12544290,Spelling mistake in error message in HMasterCommandLine,"If hadoop-zookeeper-server is installed and started, starting hbase-master in standalone mode will display this error message which has some typos.

$ sudo /etc/init.d/hadoop-hbase-master start
Starting Hadoop HBase master daemon: starting master, logging to /usr/lib/hbase/logs/hbase-hbase-master/cloudera-vm.out
Couldnt start ZK at requested address of 2181, instead got: 2182.  Aborting. Why? Because clients (eg shell) wont be able to find this ZK quorum
hbase-master.",dsw,dsw,Trivial,Closed,Fixed,27/Feb/12 18:44,05/Aug/14 20:11
Bug,HBASE-5485,12544355,LogCleaner refers to non-existant SnapshotLogCleaner,"LogCleaner code refers to SnapshotLogCleaner, but no such class exists.  Perhaps it refers to work done in HBASE-50 that was never checked in.",gchanan,gchanan,Minor,Closed,Fixed,28/Feb/12 02:27,12/Oct/12 05:34
Bug,HBASE-5486,12544451,Warn message in HTable: Stringify the byte[],"The warn message in the method getStartEndKeys() in HTable can be improved by stringifying the byte array for Regions.Qualifier

Currently, a sample message is like :
12/01/17 16:36:34 WARN client.HTable: Null [B@552c8fa8 cell in keyvalues={test5,\xC9\xA2\x00\x00\x00\x00\x00\x00/000000_0,1326642537734.dbc62b2765529a9ad2ddcf8eb58cb2dc./info:server/1326750341579/Put/vlen=28, test5,\xC9\xA2\x00\x00\x00\x00\x00\x00/000000_0,1326642537734.dbc62b2765529a9ad2ddcf8eb58cb2dc./info:serverstartcode/1326750341579/Put/vlen=8}

",v.himanshu,v.himanshu,Trivial,Closed,Fixed,28/Feb/12 17:07,05/Aug/14 20:11
Bug,HBASE-5488,12544557,OfflineMetaRepair doesn't support hadoop 0.20's fs.default.name property,"I want to use ""OfflineMetaRepair"" tools and found onbody fix this bugs. I will make a patch.

> 12/01/05 23:23:30 ERROR util.HBaseFsck: Bailed out due to:
> java.lang.IllegalArgumentException: Wrong FS: hdfs:// 
> us01-ciqps1-name01.carrieriq.com:9000/hbase/M2M-INTEGRATION-MM_TION-13
> 25190318714/0003d2ede27668737e192d8430dbe5d0/.regioninfo,
> expected: file:///
>        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:352)
>        at
> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:47)
>        at
> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:368)
>        at
> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
>        at
> org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:126)
>        at
> org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:284)
>        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:398)
>        at
> org.apache.hadoop.hbase.util.HBaseFsck.loadMetaEntry(HBaseFsck.java:256)
>        at
> org.apache.hadoop.hbase.util.HBaseFsck.loadTableInfo(HBaseFsck.java:284)
>        at
> org.apache.hadoop.hbase.util.HBaseFsck.rebuildMeta(HBaseFsck.java:402)
>        at
> org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair.main(OfflineMetaRe
",sunnygao,sunnygao,Minor,Closed,Fixed,29/Feb/12 03:39,26/Feb/13 08:13
Bug,HBASE-5490,12544611,Move the enum RS_ZK_REGION_FAILED_OPEN to the last of the enum list in 0.90 EventHandler,"The new state that was added  RS_ZK_REGION_FAILED_OPEN was failing the rolling restart.

So move the new enum to the end of the list.",ram_krish,ram_krish,Major,Closed,Fixed,29/Feb/12 13:29,20/Nov/15 11:52
Bug,HBASE-5499,12544710,dev-support/test-patch.sh does not have execute perms,"When I checkout a tree from trunk, I notice that dev-support/test-patch.sh does not come with execute permissions enabled by default, while the rest of the scripts in dev-support do have +x set.",dsw,dsw,Trivial,Closed,Fixed,01/Mar/12 01:23,26/Feb/13 08:12
Bug,HBASE-5502,12544766,region_mover.rb fails to load regions back to original server for regions only containing empty tables.,"The region_mover loadRegion function incorrectly uses 'isSuccessfulScan':

{noformat} 
  for r in regions
    exists = false
    begin
      exists = isSuccessfulScan(admin, r)
    rescue org.apache.hadoop.hbase.NotServingRegionException => e
      $LOG.info(""Failed scan of "" + e.message)
    end
{noformat} 

isSuccessfulScan throws an exception when it fails rather than returning status.

As a result empty regions don't get restored - this is the case in a fresh install (which is how I discovered this) with no user table.

Modifying the code to set exists IF isSuccessfulScan does not throw an exception worked for me:

{noformat}
  for r in regions
    exists = false
    begin
      isSuccessfulScan(admin, r)
      exists = true
    rescue org.apache.hadoop.hbase.NotServingRegionException => e
      $LOG.info(""Failed scan of "" + e.message)
    end
{noformat}



",,javacruft,Minor,Closed,Fixed,01/Mar/12 11:40,12/Oct/12 05:35
Bug,HBASE-5507,12544883,ThriftServerRunner.HbaseHandler.getRegionInfo() and getTableRegions() do not use ByteBuffer correctly,"We observed that when with ""framed transport"" option. The thrift call ThriftServerRunner.HbaseHandler.getRegionInfo() receives corrupted parameter (some garbage string attached to the beginning). This may be a thrift bug requires further investigation.",schen,schen,Major,Closed,Fixed,02/Mar/12 04:26,12/Oct/12 05:35
Bug,HBASE-5514,12545020,Compile against hadoop 0.24-SNAPSHOT,"Need to compile hbase against the latest hadoop trunk which just had NN HA merged in. 

1) add a hadoop 0.24 profile

2) HBASE-5480

3) HADOOP-8124 removed deprecated Syncable.sync(). It brings compile errors for hbase against hadoop trunk(0.24). TestHLogSplit and TestHLog still call the deprecated sync(). Need to replace it with hflush() so the compilation can pass. ",mingjielai,mingjielai,Minor,Closed,Fixed,03/Mar/12 01:29,12/Oct/12 05:34
Bug,HBASE-5522,12545244,hbase 0.92 test artifacts are missing from Maven central,"Could someone with enough karma, please, publish the test artifacts for 0.92.0?",stack,rvs,Major,Closed,Fixed,05/Mar/12 22:38,12/Oct/12 05:35
Bug,HBASE-5524,12545248,Add a couple of more filters to our rat exclusion set,Build up on jenkins is failing because I just enabled the rat/license check as part of our build.  We're failing because CP is writing test data into top-level at ./test.,stack,stack,Major,Closed,Fixed,05/Mar/12 23:00,12/Oct/12 05:35
Bug,HBASE-5529,12545284,MR test failures becuase MALLOC_ARENA_MAX is not set,"When running unit tests on CentOS 6 I get a bunch of unit test failures in mapreduce-related tests due to:
2012-03-03 00:14:18,776 WARN  [Container Monitor]
monitor.ContainersMonitorImpl$MonitoringThread(436): Container
[pid=21446,containerID=container_1330762435849_0002_01_000001] is
running beyond virtual memory limits. Current usage: 223.1mb of 2.0gb
physical memory used; 6.9gb of 4.2gb virtual memory used. Killing
container.

Note: this also came up in the mapreduce project. See: https://issues.apache.org/jira/browse/MAPREDUCE-3933

Patch coming shortly",gchanan,gchanan,Minor,Closed,Fixed,06/Mar/12 06:55,26/Feb/13 08:12
Bug,HBASE-5531,12545293,Maven hadoop profile (version 23) needs to be updated with latest 23 snapshot,"Current profile is still pointing to 0.23.1-SNAPSHOT. 
This is failing to build as 23.1 is already released and snapshot is not available anymore.
We can update this to 0.23.2-SNAPSHOT.",lakshman,lakshman,Major,Closed,Fixed,06/Mar/12 08:22,26/Feb/13 08:12
Bug,HBASE-5535,12545431,Make the functions in task monitor synchronized,"There are some potential race condition in the task monitor. So update the functions in task monitor to be synchronized.

The example of the problem caused by the race condition:
ERROR org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Cache flush failed for region 
java.lang.IndexOutOfBoundsException: Index: 1745, Size: 1744
at java.util.ArrayList.add(ArrayList.java:367)
at java.util.SubList.add(AbstractList.java:633)
at java.util.SubList.add(AbstractList.java:633)
at java.util.SubList.add(AbstractList.java:633)
at java.util.SubList.add(AbstractList.java:633)
at java.util.SubList.add(AbstractList.java:633)
at java.util.AbstractList.add(AbstractList.java:91)
at org.apache.hadoop.hbase.monitoring.TaskMonitor.createStatus(TaskMonitor.java:74)
at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1139)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:260)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:234)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:146)",liyin,liyin,Major,Closed,Fixed,07/Mar/12 00:50,12/Oct/12 05:35
Bug,HBASE-5537,12545538,MXBean shouldn't have a dependence on InterfaceStability until 0.96,"HBASE-5325 has a dependence on InterfaceStability.Evolving in 0.92 and it shouldn't have it until 0.96. One problem it currently causes is that 0.92 can't be compiled against CDH3u3.

Assigning to Stack.",stack,jdcryans,Major,Closed,Fixed,07/Mar/12 18:18,12/Oct/12 05:35
Bug,HBASE-5545,12545758,region can't be opened for a long time. Because the creating File failed.,"Scenario:
------------
1. File is created 
2. But while writing data, all datanodes might have crashed. So writing data will fail.
3. Now even if close is called in finally block, close also will fail and throw the Exception because writing data failed.
4. After this if RS try to create the same file again, then AlreadyBeingCreatedException will come.

Suggestion to handle this scenario.
---------------------------
1. Check for the existence of the file, if exists delete the file and create new file. 

Here delete call for the file will not check whether the file is open or closed.

Overwrite Option:
----------------
1. Overwrite option will be applicable if you are trying to overwrite a closed file.
2. If the file is not closed, then even with overwrite option Same AlreadyBeingCreatedException will be thrown.
This is the expected behaviour to avoid the Multiple clients writing to same file.


Region server logs:

org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /hbase/test1/12c01902324218d14b17a5880f24f64b/.tmp/.regioninfo for DFSClient_hb_rs_158-1-131-48,20020,1331107668635_1331107669061_-252463556_25 on client 158.1.132.19 because current leaseholder is trying to recreate file.
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1570)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1440)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1382)
at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:658)
at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:547)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1137)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1133)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1131)

at org.apache.hadoop.ipc.Client.call(Client.java:961)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:245)
at $Proxy6.create(Unknown Source)
at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at $Proxy6.create(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3643)
at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:778)
at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:364)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:630)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:611)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:518)
at org.apache.hadoop.hbase.regionserver.HRegion.checkRegioninfoOnFilesystem(HRegion.java:424)
at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:340)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2672)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2658)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:330)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:116)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:158)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
[2012-03-07 20:51:45,858] [WARN ] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-23] [com.huawei.isap.ump.ha.client.RPCRetryAndSwitchInvoker 131] Retrying the method call: public abstract void org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,boolean,boolean,short,long) throws java.io.IOException with arguments of length: 7. The exisiting ActiveServerConnection is:
ActiveServerConnectionInfo:
Metadata:158-1-131-48/158.1.132.19:9000
Version:145720623220907

[2012-03-07 20:51:45,872] [DEBUG] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.zookeeper.ZKAssign 849] regionserver:20020-0x135ec32b39e0002-0x135ec32b39e0002 Successfully transitioned node 91bf3e6f8adb2e4b335f061036353126 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[2012-03-07 20:51:45,873] [DEBUG] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.regionserver.HRegion 2649] Opening region: REGION => {NAME => 'test1,00088613810,1331112369360.91bf3e6f8adb2e4b335f061036353126.', STARTKEY => '00088613810', ENDKEY => '00088613815', ENCODED => 91bf3e6f8adb2e4b335f061036353126, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'value', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'GZ', TTL => '86400', BLOCKSIZE => '655360', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
[2012-03-07 20:51:45,873] [DEBUG] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.regionserver.HRegion 316] Instantiated test1,00088613810,1331112369360.91bf3e6f8adb2e4b335f061036353126.
[2012-03-07 20:51:45,874] [ERROR] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [ 

",ram_krish,sunnygao,Major,Closed,Fixed,09/Mar/12 02:17,12/Oct/12 05:35
Bug,HBASE-5546,12545762,Master assigns region in the original region server when opening region failed  ,"Master assigns region in the original region server when RS_ZK_REGION_FAILED_OPEN envent was coming.
Maybe we should choose other region server.

[2012-03-07 10:14:21,750] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:14:31,826] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:14:41,903] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:14:51,975] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:15:02,056] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:15:12,167] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:15:22,231] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:15:32,303] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:15:42,375] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:15:52,447] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:16:02,528] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:16:12,600] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053
[2012-03-07 10:16:22,676] [DEBUG] [main-EventThread] [org.apache.hadoop.hbase.master.AssignmentManager 553] Handling transition=RS_ZK_REGION_FAILED_OPEN, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053

",ashutosh_jindal,sunnygao,Minor,Closed,Fixed,09/Mar/12 02:42,26/Feb/13 08:16
Bug,HBASE-5549,12545833,Master can fail if ZooKeeper session expires,"There is a retry mechanism in RecoverableZooKeeper, but when the session expires, the whole ZooKeeperWatcher is recreated, hence the retry mechanism does not work in this case. This is why a sleep is needed in TestZooKeeper#testMasterSessionExpired: we need to wait for ZooKeeperWatcher to be recreated before using the connection.

This can happen in real life, it can happen when:
- master & zookeeper starts
- zookeeper connection is cut
- master enters the retry loop
- in the meantime the session expires
- the network comes back, the session is recreated
- the retries continues, but on the wrong object, hence fails.
",nkeywal,nkeywal,Minor,Closed,Fixed,09/Mar/12 12:19,07/Apr/13 04:34
Bug,HBASE-5552,12545865,Clean up our jmx view; its a bit of a mess,Fix before we release 0.92.1,stack,stack,Blocker,Closed,Fixed,09/Mar/12 15:42,12/Oct/12 05:34
Bug,HBASE-5553,12545874,Revisit our jmx view,"See HBASE-5552 for some notes.  As is, its hard to make sense of what we are publishing and its not amenable to new beans showing up -- it'll be hard to fit them in to give a nice logical jmx view.  Fix.

Entertain doing this at the singularity.",eclark,stack,Major,Closed,Fixed,09/Mar/12 16:57,23/Sep/13 18:30
Bug,HBASE-5557,12545920,[89-fb] Fix incorrect reader/writer thread interaction in HBaseTest,"In the HBaseTest load test we have a condition when the writer has not written any keys but the reader might attempt to read key 0, resulting in a failure. This bug is specific to 89-fb because it has been fixed while open-sourcing HBaseTest as LoadTestTool, and those improvements still have not been back-ported to 89-fb. Doing a temporary fix now and we will get to the back-port later. 

12/03/09 14:12:52 INFO utils.MultiThreadedReader: Key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 ERROR utils.MultiThreadedReader: No data returned, tried to get actions for key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 INFO utils.MultiThreadedReader: Key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 INFO utils.MultiThreadedReader: Key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 ERROR utils.MultiThreadedReader: No data returned, tried to get actions for key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 ERROR utils.MultiThreadedReader: No data returned, tried to get actions for key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 INFO utils.MultiThreadedReader: Key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 ERROR utils.MultiThreadedReader: No data returned, tried to get actions for key = cfcd208495d565ef66e7dff9f98764da:0
12/03/09 14:12:52 ERROR utils.MultiThreadedReader: Aborting run -- found more than three errors
",mikhail,mikhail,Minor,Closed,Fixed,09/Mar/12 22:21,13/Jun/22 15:36
Bug,HBASE-5562,12545982,test-patch.sh reports a javadoc warning when there are no new javadoc warnings,"test-patch.sh will report new javadoc warnings when there are not any.  e.g.

    -1 javadoc.  The javadoc tool appears to have generated -123 warning messages.  
",dsw,dsw,Trivial,Closed,Fixed,11/Mar/12 02:44,26/Feb/13 08:12
Bug,HBASE-5563,12546056,HRegionInfo#compareTo should compare regionId as well,"In the one region multi assigned case,  we could find that two regions have the same table name, same startKey, same endKey, and different regionId, so these two regions are same in TreeMap but different in HashMap.",zjushch,zjushch,Major,Closed,Fixed,12/Mar/12 05:36,26/Feb/13 08:12
Bug,HBASE-5564,12546108,Bulkload is discarding duplicate records,"Duplicate records are getting discarded when duplicate records exists in same input file and more specifically if they exists in same split.
Duplicate records are considered if the records are from diffrent different splits.

Version under test: HBase 0.92",lakshman,lakshman,Major,Closed,Fixed,12/Mar/12 15:15,23/Sep/13 18:30
Bug,HBASE-5566,12546149,[89-fb] Region server can get stuck in getMaster on master failover,"This is specific to the 89-fb master. We have a retry loop in HRegionServer.getMaster where we do not read the location of the master from ZK, so a region server can get stuck there on master failover. We need to add a unit test to reliably catch this, and fix the bug.
",mikhail,khemani,Major,Closed,Fixed,12/Mar/12 20:26,13/Jun/22 15:37
Bug,HBASE-5567,12546158,test-patch.sh has logic error in findbugs check,Similar bug to HBASE-5562.  The wrong operator is used in the check against $OK_FINDBUGS_WARNINGS.,dsw,dsw,Trivial,Closed,Fixed,12/Mar/12 21:45,05/Aug/14 20:11
Bug,HBASE-5568,12546183,Multi concurrent flushcache() for one region could cause data loss,"We could call HRegion#flushcache() concurrently now through HRegionServer#splitRegion or HRegionServer#flushRegion by HBaseAdmin.
However, we find if HRegion#internalFlushcache() is called concurrently by multi thread, HRegion.memstoreSize will be calculated wrong.
At the end of HRegion#internalFlushcache(), we will do this.addAndGetGlobalMemstoreSize(-flushsize), but the flushsize may not the actual memsize which flushed to hdfs. It cause HRegion.memstoreSize is negative and prevent next flush if we close this region.

Logs in RS for region e9d827913a056e696c39bc569ea3

2012-03-11 16:31:36,690 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest1,,1331454657410.e9d827913a056e696c39bc569ea3
f99f., current region memstore size 128.0m
2012-03-11 16:31:37,999 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf1/8162481165586107427, entries=153106, sequenceid=619316544, memsize=59.6m, filesize=31.2m
2012-03-11 16:31:38,830 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest1,,1331454657410.e9d827913a056e696c39bc569ea3
f99f., current region memstore size 134.8m
2012-03-11 16:31:39,458 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf2/3425971951499794221, entries=230183, sequenceid=619316544, memsize=68.5m, filesize=26.6m
2012-03-11 16:31:39,459 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~128.1m for region writetest1,,1331454657410.e9d827913a
056e696c39bc569ea3f99f. in 2769ms, sequenceid=619316544, compaction requested=false
2012-03-11 16:31:39,459 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest1,,1331454657410.e9d827913a056e696c39bc569ea3
f99f., current region memstore size 6.8m
2012-03-11 16:31:39,529 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf1/1811012969998104626, entries=8002, sequenceid=619332759, memsize=3.1m, filesize=1.6m
2012-03-11 16:31:39,640 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf2/770333473623552048, entries=12231, sequenceid=619332759, memsize=3.6m, filesize=1.4m
2012-03-11 16:31:39,641 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~134.8m for region writetest1,,1331454657410.e9d827913a
056e696c39bc569ea3f99f. in 811ms, sequenceid=619332759, compaction requested=true
2012-03-11 16:31:39,707 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf1/5656568849587368557, entries=119, sequenceid=619332979, memsize=47.4k, filesize=25.6k
2012-03-11 16:31:39,775 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf2/794343845650987521, entries=157, sequenceid=619332979, memsize=47.8k, filesize=19.3k
2012-03-11 16:31:39,777 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~6.8m for region writetest1,,1331454657410.e9d827913a05
6e696c39bc569ea3f99f. in 318ms, sequenceid=619332979, compaction requested=true",zjushch,zjushch,Major,Closed,Fixed,13/Mar/12 04:03,26/Feb/13 08:12
Bug,HBASE-5569,12546184,Do not collect deleted KVs when they are still in use by a scanner.,"I noticed this because TestAtomicOperation.testMultiRowMutationMultiThreads fails rarely.
The solution is similar to HBASE-2856, where expired KVs are not collected when in use by a scanner.

---
What I pieced together so far is that it is the *scanning* side that has problems sometimes.

Every time I see a assertion failure in the log I see this before:
{quote}
2012-03-12 21:48:49,523 DEBUG [Thread-211] regionserver.StoreScanner(499): Storescanner.peek() is changed where before = rowB/colfamily11:qual1/75366/Put/vlen=6,and after = rowB/colfamily11:qual1/75203/DeleteColumn/vlen=0
{quote}
The order of if the Put and Delete is sometimes reversed.

The test threads should always see exactly one KV, if the ""before"" was the Put the thread see 0 KVs, if the ""before"" was the Delete the threads see 2 KVs.

This debug message comes from StoreScanner to checkReseek. It seems we still some consistency issue with scanning sometimes :(
",larsh,larsh,Major,Closed,Fixed,13/Mar/12 04:40,26/Feb/13 08:12
Bug,HBASE-5570,12546201,Compression tool section is referring to wrong link in HBase Book.,"http://hbase.apache.org/book/ops_mgt.html#compression.tool
Above section is refering to itself (recursive) in HBase book.
This needs to be corrected.",dmeil,lakshman,Trivial,Closed,Fixed,13/Mar/12 08:50,13/Jun/22 15:37
Bug,HBASE-5574,12546268,DEFAULT_MAX_FILE_SIZE defaults to a negative value,"HBASE-4365 changed the value of DEFAULT_MAX_FILE_SIZE from 256MB to 10G.  Here is the line of code:

public static final long DEFAULT_MAX_FILE_SIZE = 10 * 1024 * 1024 * 1024;

The problem is that java evaluates the constant as an int which wraps and gets assigned to a long.  I verified this with a test.  The quick fix is to change the end to 1024L;",mdrzal,mdrzal,Major,Closed,Fixed,13/Mar/12 17:57,12/Oct/12 05:34
Bug,HBASE-5579,12546383,A Delete Version could mask other values,A Delete Version operation mask values that have version = 0. The problem happens at ScanDeleteTracker.,dferro,dferro,Major,Closed,Fixed,14/Mar/12 12:51,26/Feb/13 08:13
Bug,HBASE-5581,12546445,Creating a table with invalid syntax does not give an error message when it fails,"Creating a table with invalid syntax does not give an error message when it fails. In this case, it doesn't actually create the CF requested, but doesn't give any indication to the user that it failed.

create 'test', {NAME => 'test', VERSIONS => 1, BLOCKCACHE => true, NUMREGIONS => 20, SPLITALGO => ""HexStringSplit"", COMPRESSION => 'LZO', BLOOMFILTER => 'ROW'}
0 row(s) in 3.0930 seconds

hbase(main):002:0> describe 'test'
DESCRIPTION                                                                     ENABLED                                    
 {NAME => 'test', FAMILIES => []}                                               true                                       
1 row(s) in 0.1430 seconds
----

Putting {NUMREGIONS => 20, SPLITALGO => ""HexStringSplit""} into a separate stanza works fine, so the feature is fine. 

create 'test', {NAME => 'test', VERSIONS => 1, BLOCKCACHE => true, COMPRESSION => 'LZO', BLOOMFILTER => 'ROW'}, {NUMREGIONS => 20, SPLITALGO => ""HexStringSplit""}
0 row(s) in 2.7860 seconds

hbase(main):002:0> describe 'test'
DESCRIPTION                                                                     ENABLED                                    
 {NAME => 'test', FAMILIES => [{NAME => 'test', DATA_BLOCK_ENCODING => 'NONE',  true                                       
 BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', COMPRESSION => 'LZO', VERSIONS                                            
  => '1', TTL => '2147483647', BLOCKSIZE => '65536', BLOOMFILTER_ERRORRATE => '                                            
 0.01', ENCODE_ON_DISK => 'true', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}          
----

We should throw an error if we can't create the CF so it's clear to the user.",,binujohn,Minor,Closed,Fixed,14/Mar/12 18:37,26/Feb/13 08:12
Bug,HBASE-5586,12546669,[replication] NPE in ReplicationSource when creating a stream to an inexistent cluster,"This is from 0.92.1-ish:

{noformat}
2012-03-15 09:52:16,589 ERROR
org.apache.hadoop.hbase.replication.regionserver.ReplicationSource:
Unexpected exception in ReplicationSource, currentPath=null
java.lang.NullPointerException
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.chooseSinks(ReplicationSource.java:223)
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.connectToPeers(ReplicationSource.java:442)
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:246)
{noformat}

I wanted to add a replication stream to a cluster that wasn't existing yet so that the logs would be buffered until then. This should just be treated as if there was no region servers.",jdcryans,jdcryans,Major,Closed,Fixed,15/Mar/12 21:55,26/Feb/13 08:12
Bug,HBASE-5593,12546809,Reverse DNS resolution in regionServerStartup() does not strip trailing dot,HBASE-4109 covered the removal of trailing dots in PTR records from reverse DNS lookups.  We seem to have missed a case in HMaster#regionServerStartup().,dsw,dsw,Major,Closed,Fixed,16/Mar/12 15:53,13/Jun/22 15:37
Bug,HBASE-5594,12546821,Unable to stop a master that's waiting on -ROOT- during initialization,"We just had a case where the master (that was just restarted) was having a hard time assigning -ROOT- (all the PRI handlers were full already) so we tried to shutdown the cluster and even though all the RS closed down properly the master kept running being blocked on:

{noformat}
""master-sv4r20s12,10302,1331916142866"" prio=10 tid=0x00007f3708008800 nid=0x4b20 in Object.wait() [0x00007f370d1d0000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000006030be3f8> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:131)
	- locked <0x00000006030be3f8> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:104)
	- locked <0x00000006030be3f8> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:313)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:571)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:501)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:336)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

I haven't checked the 0.90 code, we got this on 0.92.1",,jdcryans,Major,Closed,Fixed,16/Mar/12 16:56,23/Sep/13 18:31
Bug,HBASE-5595,12546836,Fix NoSuchMethodException in 0.92 when running on local filesystem,"Fix this ugly exception that shows when running 0.92.1 when on local filesystem:

{code}
2012-03-16 10:54:48,351 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: getNumCurrentReplicas--HDFS-826 not available; hdfs_out=org.apache.hadoop.fs.FSDataOutputStream@301abf87
java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()
        at java.lang.Class.getDeclaredMethod(Class.java:1937)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.getGetNumCurrentReplicas(HLog.java:425)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.<init>(HLog.java:408)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.<init>(HLog.java:331)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateHLog(HRegionServer.java:1229)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.setupWALAndReplication(HRegionServer.java:1218)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.handleReportForDutyResponse(HRegionServer.java:937)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:648)
        at java.lang.Thread.run(Thread.java:680)
{code}

",umamaheswararao,stack,Critical,Closed,Fixed,16/Mar/12 17:59,20/Nov/15 11:53
Bug,HBASE-5596,12546859,Few minor bugs from HBASE-5209,"A few leftover bugs from HBASE-5209.  Comments are documented here:

https://reviews.apache.org/r/3892/",dsw,dsw,Minor,Closed,Fixed,16/Mar/12 22:57,02/May/13 02:29
Bug,HBASE-5597,12546865,Findbugs check in test-patch.sh always fails,"""Fix"" is to bump up OK findbugs count in test-patch.properties.  Agreed to on the list.",dsw,dsw,Major,Closed,Fixed,17/Mar/12 00:01,12/Oct/12 05:35
Bug,HBASE-5598,12546936,Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file,"There are many findbugs errors reporting by HbaseQA. HBASE-5597 is going to up the OK count.
This may lead to other issues when we re-factor the code, if we induce new valid ones and remove invalid bugs also can not be reported by QA.

So, I would propose to add the exclude filter file for findbugs(for the invalid bugs). If we find any valid ones, we can fix under this JIRA.
",umamaheswararao,umamaheswararao,Critical,Closed,Fixed,18/Mar/12 01:46,23/Sep/13 18:31
Bug,HBASE-5603,12547153,rolling-restart.sh script hangs when attempting to detect expiration of /hbase/master znode.,"Due to bugfix ZOOKEEPER-1059 (ZK 3.4.0+), the rolling-restart.sh script will hang when attempting to make sure the /hbase/master znode is deleted.

Here's the code
{code}
# make sure the master znode has been deleted before continuing
    zparent=`$bin/hbase org.apache.hadoop.hbase.util.HBaseConfTool zookeeper.znode.parent`
    if [ ""$zparent"" == ""null"" ]; then zparent=""/hbase""; fi
    zmaster=`$bin/hbase org.apache.hadoop.hbase.util.HBaseConfTool zookeeper.znode.master`
    if [ ""$zmaster"" == ""null"" ]; then zmaster=""master""; fi
    zmaster=$zparent/$zmaster
    echo -n ""Waiting for Master ZNode ${zmaster} to expire""
    while bin/hbase zkcli stat $zmaster >/dev/null 2>&1; do
      echo -n "".""
      sleep 1
    done
    echo #force a newline
{code}

Prior to ZOOKEEPER-1059, stat on a null znode would NPE and cause zkcli to exit with retcode 1.  Afterwards, the null is caught, zkcli will exit with 0 in the case where the znode is present and in the case where it does not exist.
",jmhsieh,jmhsieh,Blocker,Closed,Fixed,20/Mar/12 07:53,18/Sep/13 22:23
Bug,HBASE-5605,12547263,compression does not work in Store.java trunk,,he yongqiang,he yongqiang,Major,Closed,Fixed,20/Mar/12 19:21,13/Jun/22 15:38
Bug,HBASE-5606,12547330,SplitLogManger async delete node hangs log splitting when ZK connection is lost ,"1. One rs died, the servershutdownhandler found it out and started the distributed log splitting;
2. All tasks are failed due to ZK connection lost, so the all the tasks were deleted asynchronously;
3. Servershutdownhandler retried the log splitting;
4. The asynchronously deletion in step 2 finally happened for new task
5. This made the SplitLogManger in hanging state.

This leads to .META. region not assigened for long time


{noformat}
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(55413,79):2012-03-14 19:28:47,932 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: put up splitlog task at znode /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(89303,79):2012-03-14 19:34:32,387 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: put up splitlog task at znode /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
{noformat}

{noformat}
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(80417,99):2012-03-14 19:34:31,196 DEBUG org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(89456,99):2012-03-14 19:34:32,497 DEBUG org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
{noformat}
",khemani,gopinathan.av,Critical,Closed,Fixed,21/Mar/12 04:27,18/Sep/13 22:18
Bug,HBASE-5611,12547436,Replayed edits from regions that failed to open during recovery aren't removed from the global MemStore size,"This bug is rather easy to get if the {{TimeoutMonitor}} is on, else I think it's still possible to hit it if a region fails to open for more obscure reasons like HDFS errors.

Consider a region that just went through distributed splitting and that's now being opened by a new RS. The first thing it does is to read the recovery files and put the edits in the {{MemStores}}. If this process takes a long time, the master will move that region away. At that point the edits are still accounted for in the global {{MemStore}} size but they are dropped when the {{HRegion}} gets cleaned up. It's completely invisible until the {{MemStoreFlusher}} needs to force flush a region and that none of them have edits:

{noformat}
2012-03-21 00:33:39,303 DEBUG org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=5.9g
2012-03-21 00:33:39,303 ERROR org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Cache flusher failed for entry null
java.lang.IllegalStateException
        at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:199)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:223)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

The {{null}} here is a region. In my case I had so many edits in the {{MemStore}} during recovery that I'm over the low barrier although in fact I'm at 0. It happened yesterday and it still printing this out.

To fix this we need to be able to decrease the global {{MemStore}} size when the region can't open.",jeason,jdcryans,Critical,Closed,Fixed,21/Mar/12 18:38,18/Sep/13 22:13
Bug,HBASE-5613,12547461,ThriftServer getTableRegions does not return serverName and port,,schen,schen,Minor,Closed,Fixed,21/Mar/12 21:53,26/Feb/13 08:12
Bug,HBASE-5615,12547557,the master never does balance because of balancing the parent region,"the master never do balance becauseof when master do rebuildUserRegions()，it will add the parent region into  AssignmentManager#servers,
if balancer let the parent region to move,the parent will in RIT forever.thus balance will never be executed.",xufeng,xufeng,Critical,Closed,Fixed,22/Mar/12 07:52,13/Jun/22 15:38
Bug,HBASE-5623,12547728,Race condition when rolling the HLog and hlogFlush,"When doing a ycsb test with a large number of handlers (regionserver.handler.count=60), I get the following exceptions:

{code}
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1099)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.getLength(SequenceFileLogWriter.java:314)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1291)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1388)
	at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:920)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:152)
	at $Proxy1.multi(Unknown Source)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1691)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1689)
	at org.apache.hadoop.hbase.client.ServerCallable.withoutRetries(ServerCallable.java:214)
{code}

and 
{code}
	java.lang.NullPointerException
		at org.apache.hadoop.io.SequenceFile$Writer.checkAndWriteSync(SequenceFile.java:1026)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1068)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1035)
		at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.append(SequenceFileLogWriter.java:279)
		at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.hlogFlush(HLog.java:1237)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1271)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1391)
		at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
		at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
		at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
		at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
		at java.lang.reflect.Method.invoke(Method.java:597)
		at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
		at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)
{code}

It seems the root cause of the issue is that we open a new log writer and close the old one at HLog#rollWriter() holding the updateLock, but the other threads doing syncer() calls
{code} 
logSyncerThread.hlogFlush(this.writer);
{code}
without holding the updateLock. LogSyncer only synchronizes against concurrent appends and flush(), but not on the passed writer, which can be closed already by rollWriter(). In this case, since SequenceFile#Writer.close() sets it's out field as null, we get the NPE. 
",enis,enis,Critical,Closed,Fixed,23/Mar/12 01:22,12/Oct/12 05:35
Bug,HBASE-5624,12547739,"Aborting regionserver when splitting region, may cause daughter region not assigned by ServerShutdownHandler.","If one region is splitting when regionserver is stoping.
The following code may executed in SplitTransaction#openDaughters.
{code}
// TODO: Is this check needed here?
    if (stopped || stopping) {
      // add 2nd daughter first (see HBASE-4335)
      MetaEditor.addDaughter(server.getCatalogTracker(),
          b.getRegionInfo(), null);
      MetaEditor.addDaughter(server.getCatalogTracker(),
          a.getRegionInfo(), null);
      LOG.info(""Not opening daughters "" +
          b.getRegionInfo().getRegionNameAsString() +
          "" and "" +
          a.getRegionInfo().getRegionNameAsString() +
          "" because stopping="" + stopping + "", stopped="" + stopped);
    } 
{code}

So, for the two daughter regions, their location are both null in .META.

When ServerShutdownHandler process the dead server, it will not assign these two daughter regions since their location(info:server) are null in .META. by MetaReader.getServerUserRegions().
",zjushch,zjushch,Major,Closed,Fixed,23/Mar/12 05:02,12/Oct/12 05:35
Bug,HBASE-5633,12547940,NPE reading ZK config in HBase,"If zoo.cfg contains server.* (""server.0=server0:2888:3888\n"") and cluster.distributed property (in hbase-site.xml) is empty we get an NPE in parseZooCfg().

The easy way to reproduce the bug is running org.apache.hbase.zookeeper.TestHQuorumPeer with hbase-site.xml containing:
{code}
<property>
  <name>hbase.cluster.distributed</name>
  <value></value>
</property>
{code}",mbertozzi,mbertozzi,Minor,Closed,Fixed,24/Mar/12 05:58,12/Oct/12 05:34
Bug,HBASE-5635,12548086,"If getTaskList() returns null, splitlogWorker would go down and it won't serve any requests","During the hlog split operation if all the zookeepers are down ,then the paths will be returned as null and the splitworker thread wil be exited
Now this regionserver wil not be able to acquire any other tasks since the splitworker thread is exited
Please find the attached code for more details
{code}
private List<String> getTaskList() {
    for (int i = 0; i < zkretries; i++) {
      try {
        return (ZKUtil.listChildrenAndWatchForNewChildren(this.watcher,
            this.watcher.splitLogZNode));
      } catch (KeeperException e) {
        LOG.warn(""Could not get children of znode "" +
            this.watcher.splitLogZNode, e);
        try {
          Thread.sleep(1000);
        } catch (InterruptedException e1) {
          LOG.warn(""Interrupted while trying to get task list ..."", e1);
          Thread.currentThread().interrupt();
          return null;
        }
      }
    }
{code}

in the org.apache.hadoop.hbase.regionserver.SplitLogWorker 


 

",chinnalalam,kristamswathi,Major,Closed,Fixed,26/Mar/12 09:56,26/Feb/13 17:02
Bug,HBASE-5636,12548140,TestTableMapReduce doesn't work properly.,"No map function is called because there are no test data put before test starts.

The following three tests are in the same situation:
- org.apache.hadoop.hbase.mapred.TestTableMapReduce
- org.apache.hadoop.hbase.mapreduce.TestTableMapReduce
- org.apache.hadoop.hbase.mapreduce.TestMulitthreadedTableMapper
",ueshin,ueshin,Major,Closed,Fixed,26/Mar/12 16:33,26/Feb/13 08:12
Bug,HBASE-5637,12548144,Fix failing 0.90 TestHMsg testcase introduced by HBASE-5563.,"After committing HBASE-5128 and HBASE-5563 to the 0.90 branch, Ted noticed that TestHMsg#getList started to fail consistently.  This updates the test to deal with the updated equality semantics in HBASE-5563.  This fixes that.",jmhsieh,jmhsieh,Major,Closed,Fixed,26/Mar/12 17:07,13/Jun/22 15:39
Bug,HBASE-5639,12548202,The logic used in waiting for region servers during startup is broken,"See the tail of HBASE-4993, which I'll report here:

Me:
{quote}
I think a bug was introduced here. Here's the new waiting logic in waitForRegionServers:

the 'hbase.master.wait.on.regionservers.mintostart' is reached AND
   there have been no new region server in for
      'hbase.master.wait.on.regionservers.interval' time

And the code that verifies that:

!(lastCountChange+interval > now && count >= minToStart)
{quote}

Nic:
{quote}
It seems that changing the code to

(count < minToStart ||
lastCountChange+interval > now)

would make the code works as documented.
If you have 0 region servers that checked in and you are under the interval, you wait: (true or true) = true.
If you have 0 region servers but you are above the interval, you wait: (true or false) = true.
If you have 1 or more region servers that checked in and you are under the interval, you wait: (false or true) = true.
{quote}",jdcryans,jdcryans,Blocker,Closed,Fixed,26/Mar/12 23:11,12/Oct/12 05:35
Bug,HBASE-5656,12548342,LoadIncrementalHFiles createTable should detect and set compression algorithm,"LoadIncrementalHFiles doesn't set compression when creating the the table.

This can be detected from the files within each family dir. ",clehene,clehene,Major,Closed,Fixed,27/Mar/12 16:15,26/Feb/13 17:02
Bug,HBASE-5658,12548366,pseduo-distributed.xml - first link pointing to wrong site,"In the web-page 'pseudo-distributed.xml' the first link ""Distributed Operation: Pseudo- and Fully-distributed modes"" is linking to the Javadoc when it should be linking to the Reference Guide in the config section.

Thanks to Dave Wang for pointing this out.",dmeil,dmeil,Minor,Closed,Fixed,27/Mar/12 19:45,13/Jun/22 15:40
Bug,HBASE-5663,12548484,MultithreadedTableMapper doesn't work.,"MapReduce job using MultithreadedTableMapper goes down throwing the following Exception:

{noformat}
java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.mapreduce.Mapper$Context.<init>(org.apache.hadoop.conf.Configuration, org.apache.hadoop.mapred.TaskAttemptID, org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader, org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordWriter, org.apache.hadoop.hbase.mapreduce.TableOutputCommitter, org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter, org.apache.hadoop.hbase.mapreduce.TableSplit)
	at org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$MapRunner.<init>(MultithreadedTableMapper.java:260)
	at org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.run(MultithreadedTableMapper.java:133)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.mapreduce.Mapper$Context.<init>(org.apache.hadoop.conf.Configuration, org.apache.hadoop.mapred.TaskAttemptID, org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader, org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordWriter, org.apache.hadoop.hbase.mapreduce.TableOutputCommitter, org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter, org.apache.hadoop.hbase.mapreduce.TableSplit)
	at java.lang.Class.getConstructor0(Class.java:2706)
	at java.lang.Class.getConstructor(Class.java:1657)
	at org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$MapRunner.<init>(MultithreadedTableMapper.java:241)
	... 8 more
{noformat}

This occured when the tasks are creating MapRunner threads.
",ueshin,ueshin,Major,Closed,Fixed,28/Mar/12 13:07,26/Feb/13 08:12
Bug,HBASE-5665,12548534,Repeated split causes HRegionServer failures and breaks table ,"Repeated splits on large tables (2 consecutive would suffice) will essentially ""break"" the table (and the cluster), unrecoverable.
The regionserver doing the split dies and the master will get into an infinite loop trying to assign regions that seem to have the files missing from HDFS.

The table can be disabled once. upon trying to re-enable it, it will remain in an intermediary state forever.

I was able to reproduce this on a smaller table consistently.

{code}
hbase(main):030:0> (0..10000).each{|x| put 't1', ""#{x}"", 'f1:t', 'dd'}
hbase(main):030:0> (0..1000).each{|x| split 't1', ""#{x*10}""}
{code}

Running overlapping splits in parallel (e.g. ""#{x*10+1}"", ""#{x*10+2}""... ) will reproduce the issue almost instantly and consistently. 

{code}
2012-03-28 10:57:16,320 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Offlined parent region t1,,1332957435767.2fb0473f4e71339e88dab0ee0d4dffa1. in META
2012-03-28 10:57:16,321 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Split requested for t1,5,1332957435767.648d30de55a5cec6fc2f56dcb3c7eee1..  compaction_queue=(0:1), split_queue=10
2012-03-28 10:57:16,343 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of t1,,1332957435767.2fb0473f4e71339e88dab0ee0d4dffa1.; Failed ld2,60020,1332957343833-daughterOpener=2469c5650ea2aeed631eb85d3cdc3124
java.io.IOException: Failed ld2,60020,1332957343833-daughterOpener=2469c5650ea2aeed631eb85d3cdc3124
        at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughters(SplitTransaction.java:363)
        at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:451)
        at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:67)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: File does not exist: /hbase/t1/589c44cabba419c6ad8c9b427e5894e3.2fb0473f4e71339e88dab0ee0d4dffa1/f1/d62a852c25ad44e09518e102ca557237
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1822)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1813)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:544)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:187)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:456)
        at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:341)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.<init>(StoreFile.java:1008)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader.<init>(HalfStoreFileReader.java:65)
        at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:467)
        at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:548)
        at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:284)
        at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:221)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:2511)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:450)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3229)
        at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughterRegion(SplitTransaction.java:504)
        at org.apache.hadoop.hbase.regionserver.SplitTransaction$DaughterOpener.run(SplitTransaction.java:484)
        ... 1 more
2012-03-28 10:57:16,345 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server ld2,60020,1332957343833: Abort; we got an error after point-of-no-return
{code}

http://hastebin.com/diqinibajo.avrasm

later edit:

(I'm using the last 4 characters from each string)
Region 94e3 has storefile 7237
Region 94e3 gets splited in daughters a: ffa1 and b: eee1
Daughter region ffa1 get's splitted in daughters a: 3124 and b: dc77
ffa1 has a reference: 7237.94e3 for it's store file
when ffa1 gets splited it will create another reference: 7237.94e3.ffa1
when SplitTransaction will execute() it will try to open that (openDaughters above) and it will match it from left to right [storefile].[region] 
{code}
""^([0-9a-f]+)(?:\\.(.+))?$""
{code}
and will attempt to go to /hbase/t1/[region] which resolves to 
/hbase/t1/94e3.ffa1/f1/7237 - which obviously doesn't exist and will fail. 

This seems like a design problem: we should either stop from splitting if the path is reference or be able to recursively resolve reference paths (e.g. parse right to left 7237.94e3.ffa1 -> [7237.94e3].ffa1 -> open /hbase/t1/ffa1/f1/7237.94e3 -> [7237].94e3 -> open /hbase/t1/94e3/7237)

",clehene,clehene,Blocker,Closed,Fixed,28/Mar/12 18:47,12/Oct/12 05:34
Bug,HBASE-5669,12548577,AggregationClient fails validation for open stoprow scan,AggregationClient.validateParameters throws an exception when the Scan has a valid startrow but an unset endrow.,mubarakseyed,brian.andrew.rogers,Minor,Closed,Fixed,28/Mar/12 23:32,26/Feb/13 17:02
Bug,HBASE-5672,12548605,TestLruBlockCache#testBackgroundEvictionThread fails occasionally,"We find TestLruBlockCache#testBackgroundEvictionThread fails occasionally.

I think it's a problem of the test case.
Because runEviction() only do evictionThread.evict():
{code}
public void evict() {
      synchronized(this) {
        this.notify(); // FindBugs NN_NAKED_NOTIFY
      }
    }
{code}
However when we call evictionThread.evict(), the evictionThread may haven't been in run() in the TestLruBlockCache#testBackgroundEvictionThread.

If we run the test many times, we could find failture easily.",zjushch,zjushch,Major,Closed,Fixed,29/Mar/12 03:18,04/Jun/13 00:01
Bug,HBASE-5680,12548802,Improve compatibility warning about HBase with Hadoop 0.23.x,"Hmaster is not able to start because of the following error
Please find the following error 
----------------------------
2012-03-30 11:12:19,487 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/protocol/FSConstants$SafeModeAction
	at org.apache.hadoop.hbase.util.FSUtils.waitOnSafeMode(FSUtils.java:524)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:324)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:127)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:112)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:496)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:363)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.protocol.FSConstants$SafeModeAction
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 7 more
There is a change in the FSConstants
",jmhsieh,kristamswathi,Major,Closed,Fixed,30/Mar/12 09:42,09/May/13 05:03
Bug,HBASE-5689,12548934,Skipping RecoveredEdits may cause data loss,"Let's see the following scenario:
1.Region is on the server A
2.put KV(r1->v1) to the region
3.move region from server A to server B
4.put KV(r2->v2) to the region
5.move region from server B to server A
6.put KV(r3->v3) to the region
7.kill -9 server B and start it
8.kill -9 server A and start it 
9.scan the region, we could only get two KV(r1->v1,r2->v2), the third KV(r3->v3) is lost.

Let's analyse the upper scenario from the code:
1.the edit logs of KV(r1->v1) and KV(r3->v3) are both recorded in the same hlog file on server A.
2.when we split server B's hlog file in the process of ServerShutdownHandler, we create one RecoveredEdits file f1 for the region.
2.when we split server A's hlog file in the process of ServerShutdownHandler, we create another RecoveredEdits file f2 for the region.
3.however, RecoveredEdits file f2 will be skiped when initializing region
HRegion#replayRecoveredEditsIfAny
{code}
 for (Path edits: files) {
      if (edits == null || !this.fs.exists(edits)) {
        LOG.warn(""Null or non-existent edits file: "" + edits);
        continue;
      }
      if (isZeroLengthThenDelete(this.fs, edits)) continue;

      if (checkSafeToSkip) {
        Path higher = files.higher(edits);
        long maxSeqId = Long.MAX_VALUE;
        if (higher != null) {
          // Edit file name pattern, HLog.EDITFILES_NAME_PATTERN: ""-?[0-9]+""
          String fileName = higher.getName();
          maxSeqId = Math.abs(Long.parseLong(fileName));
        }
        if (maxSeqId <= minSeqId) {
          String msg = ""Maximum possible sequenceid for this log is "" + maxSeqId
              + "", skipped the whole file, path="" + edits;
          LOG.debug(msg);
          continue;
        } else {
          checkSafeToSkip = false;
        }
      }
{code}

 
",zjushch,zjushch,Critical,Closed,Fixed,31/Mar/12 04:57,12/Oct/12 05:35
Bug,HBASE-5690,12548938,compression does not work in Store.java of 0.94,"HBASE-5442 The store.createWriterInTmp method missing ""compression""",zhh,zhh,Critical,Closed,Fixed,31/Mar/12 08:08,12/Oct/12 05:35
Bug,HBASE-5694,12549023,getRowsWithColumnsTs() in Thrift service handles timestamps incorrectly,"The getRowsWithColumnsTs() method in the Thrift interface only applies the timestamp if columns are explicitly specified. However, this method also allows for columns to be unspecified (this is even used internally to implement e.g. getRows()). The cause of the bug is a minor scoping issue: the time range is set inside a wrong if statement.",,uws,Major,Closed,Fixed,01/Apr/12 21:45,25/Feb/13 10:11
Bug,HBASE-5700,12549236,[89-fb] Fix TestMiniClusterLoad* test failures,Porting TestMiniClusterLoad* tests to 89-fb in HBASE-5679 uncovered certain problems with mini-cluster setup in 89-fb that need to be fixed.,mikhail,mikhail,Minor,Closed,Fixed,02/Apr/12 21:43,13/Jun/22 16:02
Bug,HBASE-5701,12549237,Put RegionServerDynamicStatistics under RegionServer in MBean hierarchy rather than have it as a peer.,,stack,stack,Major,Closed,Fixed,02/Apr/12 21:45,12/Oct/12 05:34
Bug,HBASE-5704,12549346,HBASE-4398 mistakenly rolled back on trunk,,stack,stack,Major,Closed,Fixed,03/Apr/12 14:55,23/Sep/13 18:31
Bug,HBASE-5708,12549415,[89-fb] Make MiniMapRedCluster directory a subdirectory of target/test,Some map-reduce-based tests are failing when executed concurrently in 89-fb because mini-map-reduce cluster uses /tmp/hadoop-<username> for temporary data.,,mikhail,Minor,Closed,Fixed,03/Apr/12 22:05,13/Jun/22 16:04
Bug,HBASE-5717,12549559,Scanner metrics are only reported if you get to the end of a scanner,"When you turn on Scanner Metrics, the metrics are currently only made available if you run over all records available in the scanner. If you stop iterating before the end, the values are never flushed into the metrics object (in the Scan attribute).

Will supply a patch with fix and test.",ivarley,ivarley,Minor,Closed,Fixed,04/Apr/12 19:50,26/Feb/13 08:12
Bug,HBASE-5720,12549604,HFileDataBlockEncoderImpl uses wrong header size when reading HFiles with no checksums,"When reading a .92 HFile without checksums, encoding it, and storing in the block cache, the HFileDataBlockEncoderImpl always allocates a dummy header appropriate for checksums even though there are none.  This corrupts the byte[].

Attaching a patch that allocates a DUMMY_HEADER_NO_CHECKSUM in that case which I think is the desired behavior.",mcorgan,mcorgan,Blocker,Closed,Fixed,05/Apr/12 02:14,12/Oct/12 05:34
Bug,HBASE-5722,12549652,NPE in ZKUtil#getChildDataAndWatchForNewChildren when ZK not available or NW down.,"{code}
List<String> nodes =
      ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);
    List<NodeAndData> newNodes = new ArrayList<NodeAndData>();
    for (String node: nodes) {
      String nodePath = ZKUtil.joinZNode(baseNode, node);
      byte [] data = ZKUtil.getDataAndWatch(zkw, nodePath);
      newNodes.add(new NodeAndData(nodePath, data));
    }
{code}

The above code can throw NPE when listChildrenAndWatchForNewChildren returns null.",umamaheswararao,umamaheswararao,Major,Closed,Fixed,05/Apr/12 09:31,26/Feb/13 08:15
Bug,HBASE-5724,12549678,Row cache of KeyValue should be cleared in readFields().,"KeyValue does not clear its row cache in reading new values (readFields()).
Therefore, If a KeyValue (kv) which caches its row bytes reads another KeyValue instance, kv.getRow() returns a wrong value. ",tzenmyo,tzenmyo,Major,Closed,Fixed,05/Apr/12 11:35,12/Oct/12 05:34
Bug,HBASE-5726,12549725,TestSplitTransactionOnCluster occasionally failing,"When I ran TestSplitTransactionOnCluster, some times tests are failing.

{quote}
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.getAndCheckSingleTableRegion(TestSplitTransactionOnCluster.java:89)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.testShutdownFixupWhenDaughterHasSplit(TestSplitTransactionOnCluster.java:298)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
{quote}

Seems like test is flaky, random other cases also fails.",zhihyu@ebaysf.com,umamaheswararao,Critical,Closed,Fixed,05/Apr/12 14:16,23/Sep/13 18:31
Bug,HBASE-5727,12549740,secure hbase build broke because of 'HBASE-5451 Switch RPC call envelope/headers to PBs',"If you build with the security profile -- i.e. add '-P security' on the command line -- you'll see that the secure build is broke since we messed in rpc.

Assigning Deveraj to take a look.   If you can't work on this now DD, just give it back to me and I'll have a go at it.  Thanks.",ddas,stack,Blocker,Closed,Fixed,05/Apr/12 15:31,23/Sep/13 18:30
Bug,HBASE-5729,12549752,Jenkins build failing; failsafe NPE'ing,"Builds up on jenkins have been failing over the last few days.  Looking at it w/ nkeyway, its kinda odd.  I ran exact command locally as did N and it works fine.  I removed all of my repo and still works.  N looked at surefire source.  Its the includes that is coming back empty causing the NPE we see up on jenkins.  Extra odd is that it does not seem like it a checkin of ours that brought this on.  See here where its 'working' on 0.94 branch: https://builds.apache.org/view/G-L/view/HBase/job/HBase-0.94/76/  Then a little later Ted triggers a build w/ no changes made: https://builds.apache.org/view/G-L/view/HBase/job/HBase-0.94/77/console

Its failing running the integration test phase.  Let me mess around and try and get it going again.",stack,stack,Blocker,Closed,Fixed,05/Apr/12 17:06,13/Jun/22 16:04
Bug,HBASE-5730,12549758,[89-fb] Make HRegionThriftServer's thread pool bounded,This JIRA is for a quick fix in 89-fb to reuse TBoundedThreadPoolServer in HRegionThriftServer. We will address whatever problems HRegionThriftServer still has in trunk in HBASE-5703.,mikhail,mikhail,Major,Closed,Fixed,05/Apr/12 17:48,13/Jun/22 16:16
Bug,HBASE-5733,12549796,AssignmentManager#processDeadServersAndRegionsInTransition can fail with NPE.,"Found while going through the code...
AssignmentManager#processDeadServersAndRegionsInTransition can fail with NPE as this is directly iterating the nodes from listChildrenAndWatchForNewChildren with-out checking for null.

Here also we need to handle with  null  check like other places.",umamaheswararao,umamaheswararao,Major,Closed,Fixed,05/Apr/12 21:40,26/Feb/13 08:16
Bug,HBASE-5736,12549834,ThriftServerRunner.HbaseHandler.mutateRow() does not use ByteBuffer correctly,"We have fixed similar bug in
https://issues.apache.org/jira/browse/HBASE-5507

It uses ByteBuffer.array() to read the ByteBuffer.
This will ignore the offset return the whole underlying byte array.
The bug can be triggered by using framed Transport thrift servers.
",schen,schen,Major,Closed,Fixed,06/Apr/12 02:29,26/Feb/13 08:12
Bug,HBASE-5740,12549923,Compaction interruption may be due to balacing,"Currently, the log shows 

Aborting compaction of store LOG in region .... because user requested stop.

But it is actually because of balancing.

Currently, there is no way to figure out who closed the region.  So it is better to change the message to say it is because of user, or balancing.",jxiang,jxiang,Trivial,Closed,Fixed,06/Apr/12 19:22,23/Sep/13 18:45
Bug,HBASE-5741,12549929,ImportTsv does not check for table existence ,"The usage statement for the ""importtsv"" command to hbase claims this:

""Note: if you do not use this option, then the target table must already exist in HBase"" (in reference to the ""importtsv.bulk.output"" command-line option)

The truth is, the table must exist no matter what, importtsv cannot and will not create it for you.

This is the case because the createSubmittableJob method of ImportTsv does not even attempt to check if the table exists already, much less create it:

(From org.apache.hadoop.hbase.mapreduce.ImportTsv.java)

305 HTable table = new HTable(conf, tableName);

The HTable method signature in use there assumes the table exists and runs a meta scan on it:

(From org.apache.hadoop.hbase.client.HTable.java)

142 * Creates an object to access a HBase table.
...
151 public HTable(Configuration conf, final String tableName)

What we should do inside of createSubmittableJob is something similar to what the ""completebulkloads"" command would do:

(Taken from org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java)

690 boolean tableExists = this.doesTableExist(tableName);
691 if (!tableExists) this.createTable(tableName,dirPath);

Currently the docs are misleading, the table in fact must exist prior to running importtsv. We should check if it exists rather than assume it's already there and throw the below exception:

12/03/14 17:15:42 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table: 
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: myTable2, row=myTable2,,99999999999999
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
...
",v.himanshu,heathcd,Major,Closed,Fixed,06/Apr/12 20:11,26/Feb/13 08:16
Bug,HBASE-5743,12549934,Support GIT patches,Need to import HADOOP-7384 into our version of Hadoop QA to allow test-patch to be more flexible about patch format,nspiegelberg,nspiegelberg,Major,Closed,Fixed,06/Apr/12 20:41,02/May/13 02:29
Bug,HBASE-5744,12550042,Thrift server metrics should be long instead of int,"As we measure our Thrift call latencies in nanoseconds, we need to make latencies long instead of int everywhere.",mikhail,mikhail,Minor,Closed,Fixed,06/Apr/12 23:50,13/Jun/22 16:16
Bug,HBASE-5749,12550250,"""HBASE-5335 Dynamic Schema Config"" broke build, missing change to HRegion heapsize",,stack,stack,Major,Closed,Fixed,09/Apr/12 18:01,23/Sep/13 18:30
Bug,HBASE-5755,12550295,Region sever looking for master forever with cached stale data.,"When the master address tracker doesn't have the master address ZK data, or the cached data is wrong, region server should not use the cached data.
It should pull the data from ZK directly again.",jxiang,jxiang,Major,Closed,Fixed,09/Apr/12 23:41,23/Sep/13 18:31
Bug,HBASE-5757,12550348,TableInputFormat should handle as many errors as possible,"Prior to HBASE-4196 there was different handling of IOExceptions thrown from scanner in mapred and mapreduce API. The patch to HBASE-4196 unified this handling so that if exception is caught a reconnect is attempted (without bothering the mapred client). After that, HBASE-4269 changed this behavior back, but in both mapred and mapreduce APIs. The question is, is there any reason not to handle all errors that the input format can handle? In other words, why not try to reissue the request after *any* IOException? I see the following disadvantages of current approach
 * the client may see exceptions like LeaseException and ScannerTimeoutException if he fails to process all fetched data in timeout
 * to avoid ScannerTimeoutException the client must raise hbase.regionserver.lease.period
 * timeouts for tasks is aready configured in mapred.task.timeout, so this seems to me a bit redundant, because typically one needs to update both these parameters
 * I don't see any possibility to get rid of LeaseException (this is configured on server side)

I think all of these issues would be gone, if the DoNotRetryIOException would not be rethrown. -On the other hand, handling errors in InputFormat has disadvantage, that it may hide from the user some inefficiency. Eg. if I have very big scanner.caching, and I manage to process only a few rows in timeout, I will end up with single row being fetched many times (and will not be explicitly notified about this). Could we solve this problem by adding some counter to the InputFormat?-",je.ik,je.ik,Major,Closed,Fixed,10/Apr/12 09:59,26/Feb/13 08:16
Bug,HBASE-5759,12550405,HBaseClient throws NullPointerException when EOFException should be used.,"When a RPC data input stream is closed, protobuf doesn't raise an EOFException, it returns a null RpcResponse object.

We need to check if the response is null before trying to access it.",jxiang,jxiang,Trivial,Closed,Fixed,10/Apr/12 16:47,23/Sep/13 18:45
Bug,HBASE-5763,12550468,Fix random failures in TestFSErrorsExposed,,mikhail,mikhail,Minor,Closed,Fixed,11/Apr/12 03:56,13/Jun/22 16:03
Bug,HBASE-5772,12550663,Unable to open the few links in http://hbase.apache.org/,"Few links in http://hbase.apache.org/ is not working. 
For example, Ref Guide (multi-page) will actually link to http://hbase.apache.org/book/book.html and if I try to open this, Page not found error is coming.
If I add /book in the url, like http://hbase.apache.org/book/book/book.html, it is taking me to the Apache HBase Reference Guide 



I think the folder structure has been changed.

",stack,kiran_bc,Major,Closed,Fixed,12/Apr/12 11:29,23/Sep/13 18:30
Bug,HBASE-5773,12550665,HtablePool constructor not reading config files in certain cases,"Creating a HtablePool can issue two behaviour depanding on the constructor called. 

Case 1: loads the configs from hbase-site
  public HTablePool() {
    this(HBaseConfiguration.create(), Integer.MAX_VALUE);
  }

Calling this with null values for Configuration: 
public HTablePool(final Configuration config, final int maxSize) {
    this(config, maxSize, null, null);
  }

will issue:

 public HTablePool(final Configuration config, final int maxSize,
      final HTableInterfaceFactory tableFactory, PoolType poolType) {
    // Make a new configuration instance so I can safely cleanup when
    // done with the pool.
    this.config = config == null ? new Configuration() : config;

which does not read the hbase-site config files as HBaseConfiguration.create() does. 

I've tracked this problem to all versions of hbase. 
",ieugen,ieugen,Minor,Closed,Fixed,12/Apr/12 11:49,26/Feb/13 17:02
Bug,HBASE-5780,12550770,Fix race in HBase regionserver startup vs ZK SASL authentication,"Secure RegionServers sometimes fail to start with the following backtrace:

2012-03-22 17:20:16,737 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server centos60-20.ent.cloudera.com,60020,1332462015929: Unexpected exception during initialization, aborting
org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hbase/shutdown
at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1131)
at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:295)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataInternal(ZKUtil.java:518)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:494)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:77)
at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:569)
at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:532)
at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:634)
at java.lang.Thread.run(Thread.java:662)",smanek,smanek,Major,Closed,Fixed,13/Apr/12 02:18,26/Feb/13 17:02
Bug,HBASE-5781,12550776,Zookeeper session got closed while trying to assign the region to RS using hbck -fix,"After running the hbck in the cluster ,it is found that one region is not assigned
So the hbck -fix is used to fix this 
But the assignment didnt happen since the zookeeper session is closed
Please find the attached trace for more details
-----------------------------------------
Trying to fix unassigned region...
12/04/03 11:02:57 INFO util.HBaseFsckRepair: Region still in transition, waiting for it to become assigned: {NAME => 'ufdr,002300,1333379123498.00871fbd7583512e12c4eb38e900be8d.', STARTKEY => '002300', ENDKEY => '002311', ENCODED => 00871fbd7583512e12c4eb38e900be8d,}
12/04/03 11:02:58 INFO client.HConnectionManager$HConnectionImplementation: Closed zookeeper sessionid=0x236738a2630000a
12/04/03 11:02:58 INFO zookeeper.ZooKeeper: Session: 0x236738a2630000a closed
ERROR: Region { meta => ufdr,010444,1333379123857.01594219211d0035b9586f98954462e1., hdfs => hdfs://10.18.40.25:9000/hbase/ufdr/01594219211d0035b9586f98954462e1, deployed => } not deployed on any region server.
Trying to fix unassigned region...
12/04/03 11:02:58 INFO zookeeper.ClientCnxn: EventThread shut down
12/04/03 11:02:58 WARN zookeeper.ZKUtil: hconnection-0x236738a2630000a Unable to set watcher on znode (/hbase)
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase
at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1021)
at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:150)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:263)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.checkIfBaseNodeAvailable(ZooKeeperNodeTracker.java:208)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.checkIfBaseNodeAvailable(HConnectionManager.java:695)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:626)
at org.apache.hadoop.hbase.client.HBaseAdmin.getMaster(HBaseAdmin.java:211)
at org.apache.hadoop.hbase.client.HBaseAdmin.assign(HBaseAdmin.java:1325)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.forceOfflineInZK(HBaseFsckRepair.java:109)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.fixUnassigned(HBaseFsckRepair.java:92)
at org.apache.hadoop.hbase.util.HBaseFsck.tryAssignmentRepair(HBaseFsck.java:1235)
at org.apache.hadoop.hbase.util.HBaseFsck.checkRegionConsistency(HBaseFsck.java:1351)
at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixConsistency(HBaseFsck.java:1114)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:356)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:375)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:2894)
12/04/03 11:02:58 ERROR zookeeper.ZooKeeperWatcher: hconnection-0x236738a2630000a Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase
at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1021)
at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:150)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:263)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.checkIfBaseNodeAvailable(ZooKeeperNodeTracker.java:208)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.checkIfBaseNodeAvailable(HConnectionManager.java:695)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:626)
at org.apache.hadoop.hbase.client.HBaseAdmin.getMaster(HBaseAdmin.java:211)
at org.apache.hadoop.hbase.client.HBaseAdmin.assign(HBaseAdmin.java:1325)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.forceOfflineInZK(HBaseFsckRepair.java:109)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.fixUnassigned(HBaseFsckRepair.java:92)
at org.apache.hadoop.hbase.util.HBaseFsck.tryAssignmentRepair(HBaseFsck.java:1235)
at org.apache.hadoop.hbase.util.HBaseFsck.checkRegionConsistency(HBaseFsck.java:1351)
at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixConsistency(HBaseFsck.java:1114)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:356)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:375)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:2894)
12/04/03 11:02:58 INFO client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, trying to reconnect.
12/04/03 11:02:58 INFO client.HConnectionManager$HConnectionImplementation: Trying to reconnect to zookeeper
12/04/03 11:02:58 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=10.18.40.21:2181,10.18.40.25:2181,10.18.40.93:2181 sessionTimeout=60000 watcher=hconnection
12/04/03 11:02:58 INFO zookeeper.ClientCnxn: Opening socket connection to server /10.18.40.93:2181
12/04/03 11:02:58 INFO zookeeper.RecoverableZooKeeper: The identifier of this process is 18333@HOST-10-18-40-93
12/04/03 11:02:58 WARN client.ZooKeeperSaslClient: SecurityException: java.lang.SecurityException: Unable to locate a login configuration occurred when trying to find JAAS configuration.
12/04/03 11:02:58 INFO client.ZooKeeperSaslClient: Client will not SASL-authenticate because the default JAAS configuration section 'Client' could not be found. If you are not using SASL, you may ignore this. On the other hand, if you expected SASL to work, please fix your JAAS configuration.
12/04/03 11:02:58 INFO zookeeper.ClientCnxn: Socket connection established to HOST-10-18-40-93/10.18.40.93:2181, initiating session
12/04/03 11:02:58 INFO zookeeper.ClientCnxn: Session establishment complete on server HOST-10-18-40-93/10.18.40.93:2181, sessionid = 0x3367392d5140018, negotiated timeout = 40000
12/04/03 11:02:58 INFO client.HConnectionManager$HConnectionImplementation: Reconnected successfully. This disconnect could have been caused by a network partition or a long-running GC pause, either way it's recommended that you verify your environment.
Exception in thread ""main"" org.apache.hadoop.hbase.MasterNotRunningException
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:686)
at org.apache.hadoop.hbase.client.HBaseAdmin.getMaster(HBaseAdmin.java:211)
at org.apache.hadoop.hbase.client.HBaseAdmin.assign(HBaseAdmin.java:1325)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.forceOfflineInZK(HBaseFsckRepair.java:109)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.fixUnassigned(HBaseFsckRepair.java:92)
at org.apache.hadoop.hbase.util.HBaseFsck.tryAssignmentRepair(HBaseFsck.java:1235)
at org.apache.hadoop.hbase.util.HBaseFsck.checkRegionConsistency(HBaseFsck.java:1351)
at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixConsistency(HBaseFsck.java:1114)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:356)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:375)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:2894)
Please find the attached file for more details..


 

",jmhsieh,kristamswathi,Critical,Closed,Fixed,13/Apr/12 06:00,18/Sep/13 22:17
Bug,HBASE-5782,12550831,Edits can be appended out of seqid order since HBASE-4487,"Create a table with 1000 splits, after the region assignemnt, kill the regionserver wich contains META table.

Here few regions are missing after the log splitting and region assigment. HBCK report shows multiple region holes are got created.

Same scenario was verified mulitple times in 0.92.1, no issues.


",larsh,gopinathan.av,Blocker,Closed,Fixed,13/Apr/12 14:31,02/May/13 02:29
Bug,HBASE-5787,12550892,Table owner can't disable/delete his/her own table,"An user with CREATE privileges can create a table, but can not disable it, because disable operation require ADMIN privileges. Also if a table is already disabled, anyone can remove it.
{code}
public void preDeleteTable(ObserverContext<MasterCoprocessorEnvironment> c,
    byte[] tableName) throws IOException {
  requirePermission(Permission.Action.CREATE);
}

public void preDisableTable(ObserverContext<MasterCoprocessorEnvironment> c,
    byte[] tableName) throws IOException {
  /* TODO: Allow for users with global CREATE permission and the table owner */
  requirePermission(Permission.Action.ADMIN);
}
{code}",mbertozzi,mbertozzi,Minor,Closed,Fixed,13/Apr/12 22:47,26/Feb/13 17:02
Bug,HBASE-5794,12550959,Jenkins builds timing out; undo setting hbase.client.retries.number to 100,,stack,stack,Major,Closed,Fixed,14/Apr/12 22:00,23/Sep/13 18:30
Bug,HBASE-5795,12550961,HServerLoad$RegionLoad breaks 0.92<->0.94 compatibility,"This commit broke our 0.92/0.94 compatibility:

{code}
------------------------------------------------------------------------
r1136686 | stack | 2011-06-16 14:18:08 -0700 (Thu, 16 Jun 2011) | 1 line

HBASE-3927 display total uncompressed byte size of a region in web UI
{code}

I just tried the new RC for 0.94.  I brought up a 0.94 master on a 0.92 cluster and rather than just digest version 1 of the HServerLoad, I get this:

{code}
2012-04-14 22:47:59,752 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.4.14.38
java.io.IOException: Error in readFields
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:684)
        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1269)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1184)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:722)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:513)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:488)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: A record version mismatch occured. Expecting v2, found v1
        at org.apache.hadoop.io.VersionedWritable.readFields(VersionedWritable.java:46)
        at org.apache.hadoop.hbase.HServerLoad$RegionLoad.readFields(HServerLoad.java:379)
        at org.apache.hadoop.hbase.HServerLoad.readFields(HServerLoad.java:686)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:681)
        ... 9 more
{code}",zhihyu@ebaysf.com,stack,Major,Closed,Fixed,14/Apr/12 22:58,26/Feb/13 08:12
Bug,HBASE-5799,12551092,[89-fb] Multiget API may return incomplete resutls,"There is a serious bug in the multiget which will cause the multiget function only returns part of the results.

In the process function: 
The initial region is set before sorting the input list.
So after the input list has been sorted, the initial region may no longer be the correct region for the first row in the sorted list.

So the first row in the sorted list may be sent to the wrong region server which has no result for this row.",liyin,liyin,Major,Closed,Fixed,16/Apr/12 17:16,13/Jun/22 16:21
Bug,HBASE-5800,12551101,Birds of a feather link on web page doesn't work.,just missing the http://,eclark,eclark,Major,Closed,Fixed,16/Apr/12 17:42,23/Sep/13 18:30
Bug,HBASE-5806,12551203,Handle split region related failures on master restart and RS restart,"This issue is raised to solve issues that comes out of partial region split happened and the region node in the ZK which is in RS_ZK_REGION_SPLITTING and RS_ZK_REGION_SPLIT is not yet processed.
This also tries to address HBASE-5615.",chinnalalam,ram_krish,Major,Closed,Fixed,17/Apr/12 11:40,26/Feb/13 08:16
Bug,HBASE-5811,12551289,TestLoadAndSwitchEncodeOnDisk fails sometimes,"Looks like its dependent on isTableEnabled actually returning true when the table is enabled only, isTableEnabled looks like its set whenever any region from a table is enabled which is not the semantic I remember it always having.  This needs fixing.  Meantime the test TestLoadAndSwitchEncodeOnDisk will fail for me sometimes.",stack,stack,Major,Closed,Fixed,17/Apr/12 20:06,23/Sep/13 18:30
Bug,HBASE-5817,12551353,Fix uncategorized tests,Some tests are not categorized.  They are not run if they are not categorized. I found the set of six or seven tests by running nkeywal's little ./dev-support/hbasetests.sh  tool.  This looks useful.,stack,stack,Major,Closed,Fixed,18/Apr/12 05:30,23/Sep/13 18:30
Bug,HBASE-5819,12551442,SplitLogs function could leak resources,You would need to be unlucky and with a system in a bad shape but we have no reason to keep this in production code.,nkeywal,nkeywal,Trivial,Closed,Fixed,18/Apr/12 10:55,23/Sep/13 18:45
Bug,HBASE-5821,12551473,Incorrect handling of null value in Coprocessor aggregation function min(),"Both in AggregateImplementation and AggregationClient, the evaluation of the current minimum value is like:
min = (min == null || ci.compare(result, min) < 0) ? result : min;

The LongColumnInterpreter takes null value is treated as the least value, while the above expression takes min as the greater value when it is null. Thus, the real minimum value gets discarded if a null value comes later.
max() could also be wrong if a different ColumnInterpreter other than LongColumnInterpreter treats null value differently (as the greatest).",maryannxue,maryannxue,Major,Closed,Fixed,18/Apr/12 14:21,20/Nov/15 11:53
Bug,HBASE-5824,12551525,HRegion.incrementColumnValue is not used in trunk,"on 0.94 a call to client.HTable#incrementColumnValue will cause HRegion#incrementColumnValue.  On trunk all calls to HTable.incrementColumnValue got to HRegion#increment.

My guess is that HTable#incrementColumnValue and HTable#increment serialize to the same thing over the wire so that the remote HRegionServer no longer knows which htable method was called.

To repro I checked out trunk and put a break point in HRegion#incrementColumnValue and then ran TestFromClientSide.  The breakpoint wasn't hit.",jxiang,eclark,Major,Closed,Fixed,18/Apr/12 20:42,23/Sep/13 18:30
Bug,HBASE-5825,12551534,TestHLog not running any tests; fix,,stack,stack,Major,Closed,Fixed,18/Apr/12 21:19,12/Oct/12 05:34
Bug,HBASE-5829,12551588,"Inconsistency between the ""regions"" map and the ""servers"" map in AssignmentManager","There are occurrences in AM where this.servers is not kept consistent with this.regions. This might cause balancer to offline a region from the RS that already returned NotServingRegionException at a previous offline attempt.

In AssignmentManager.unassign(HRegionInfo, boolean)
    try {
      // TODO: We should consider making this look more like it does for the
      // region open where we catch all throwables and never abort
      if (serverManager.sendRegionClose(server, state.getRegion(),
        versionOfClosingNode)) {
        LOG.debug(""Sent CLOSE to "" + server + "" for region "" +
          region.getRegionNameAsString());
        return;
      }
      // This never happens. Currently regionserver close always return true.
      LOG.warn(""Server "" + server + "" region CLOSE RPC returned false for "" +
        region.getRegionNameAsString());
    } catch (NotServingRegionException nsre) {
      LOG.info(""Server "" + server + "" returned "" + nsre + "" for "" +
        region.getRegionNameAsString());
      // Presume that master has stale data.  Presume remote side just split.
      // Presume that the split message when it comes in will fix up the master's
      // in memory cluster state.
    } catch (Throwable t) {
      if (t instanceof RemoteException) {
        t = ((RemoteException)t).unwrapRemoteException();
        if (t instanceof NotServingRegionException) {
          if (checkIfRegionBelongsToDisabling(region)) {
            // Remove from the regionsinTransition map
            LOG.info(""While trying to recover the table ""
                + region.getTableNameAsString()
                + "" to DISABLED state the region "" + region
                + "" was offlined but the table was in DISABLING state"");
            synchronized (this.regionsInTransition) {
              this.regionsInTransition.remove(region.getEncodedName());
            }
            // Remove from the regionsMap
            synchronized (this.regions) {
              this.regions.remove(region);
            }
            deleteClosingOrClosedNode(region);
          }
        }
        // RS is already processing this region, only need to update the timestamp
        if (t instanceof RegionAlreadyInTransitionException) {
          LOG.debug(""update "" + state + "" the timestamp."");
          state.update(state.getState());
        }
      }

In AssignmentManager.assign(HRegionInfo, RegionState, boolean, boolean, boolean)
          synchronized (this.regions) {
            this.regions.put(plan.getRegionInfo(), plan.getDestination());
          }
",maryannxue,maryannxue,Major,Closed,Fixed,19/Apr/12 08:34,23/Sep/13 18:31
Bug,HBASE-5830,12551613,Cleanup SequenceFileLogWriter to use syncFs api from SequenceFile#Writer directly in trunk.,,umamaheswararao,umamaheswararao,Major,Closed,Fixed,19/Apr/12 12:40,23/Sep/13 18:31
Bug,HBASE-5833,12551669,0.92 build has been failing pretty consistently on TestMasterFailover....,Trunk seems fine but 0.92 fails on this test pretty regularly.  Running it local it seems to hang for me.,stack,stack,Major,Closed,Fixed,19/Apr/12 19:58,12/Oct/12 05:35
Bug,HBASE-5835,12551694,[hbck] Catch and handle NotServingRegionException when close region attempt fails,"Currently, if hbck attempts to close a region and catches a NotServerRegionException, hbck may hang outputting a stack trace.  Since the goal is to close the region at a particular server, and since it is not serving the region, the region is closed, and we should just warn and eat this exception.

{code}
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.NotServingRegionException: Received close for <regionid> but we are not serving it
at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2162)
at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)

at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
at $Proxy5.closeRegion(Unknown Source)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.closeRegionSilentlyAndWait(HBaseFsckRepair.java:165)
at org.apache.hadoop.hbase.util.HBaseFsck.closeRegion(HBaseFsck.java:1185)
at org.apache.hadoop.hbase.util.HBaseFsck.checkRegionConsistency(HBaseFsck.java:1302)
at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixConsistency(HBaseFsck.java:1065)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:351)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:370)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3001)
{code}",xieliang007,jmhsieh,Major,Closed,Fixed,19/Apr/12 22:47,31/Dec/14 19:19
Bug,HBASE-5837,12551697,hbase shell deleteall to .META. allows insertion of malformed rowkey,"When using the hbase shell to manipulate meta entries, one is allowed to 'delete' malformed rows (entries with less than 2 ascii 44 ',' chars).  When this happens HBase servers may go down and the cluster will not be restartable without manual intervention.  

The delete results in a durable malformed rowkey in .META.'s memstore, .META.'s HLog, and eventually .META.'s HFiles.  Subsequent scans to meta (such as when a HMaster starts) fail in the scanner because the comparator fails.  In the case of an HMaster startup, it causes an abort that kills the HMaster process.


{code}
12/04/18 22:07:34 FATAL master.HMaster: Unhandled exception. Starting shutdown.
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:990)
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:979)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1894)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1834)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
Caused by: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.KeyValue.getRequiredDelimiterInReverse(KeyValue.java:1300)
at org.apache.hadoop.hbase.KeyValue$MetaKeyComparator.compareRows(KeyValue.java:1846)
at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:130)
at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:257)
at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:114)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:2435)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2391)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2408)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1870)
... 6 more

at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
at $Proxy9.next(Unknown Source)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:264)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:237)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScanOfResults(MetaReader.java:220)
at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:1580)
at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:221)
at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:422)
at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:295)
12/04/18 22:07:34 INFO master.HMaster: Aborting 
{code}",rickysaltzer,jmhsieh,Major,Closed,Fixed,19/Apr/12 23:08,23/Sep/13 18:31
Bug,HBASE-5840,12551733,"Open Region FAILED_OPEN doesn't clear the TaskMonitor Status, keeps showing the old status","TaskMonitor Status will not be cleared in case Regions FAILED_OPEN. This will keeps showing old status.

This will miss leads the user.",rajesh23,gopinathan.av,Major,Closed,Fixed,20/Apr/12 06:59,26/Feb/13 08:15
Bug,HBASE-5848,12551951,Create table with EMPTY_START_ROW passed as splitKey causes the HMaster to abort,"A coworker of mine just had this scenario. It does not make sense the EMPTY_START_ROW as splitKey (since the region with the empty start key is implicit), but it should not cause the HMaster to abort.
The abort happens because it tries to bulk assign the same region twice and then runs into race conditions with ZK.

The same would (presumably) happen when two identical split keys are passed, but the client blocks that. The simplest solution here is to also block passed null or EMPTY_START_ROW as split key by the client.",ram_krish,larsh,Minor,Closed,Fixed,21/Apr/12 00:06,26/Feb/13 08:12
Bug,HBASE-5849,12551961,"On first cluster startup, RS aborts if root znode is not available","When launching a fresh new cluster, the master has to be started first, which might create race conditions for starting master and rs at the same time. 

Master startup code is smt like this: 
 - establish zk connection
 - create root znodes in zk (/hbase)
 - create ephemeral node for master /hbase/master, 

 Region server start up code is smt like this: 
 - establish zk connection
 - check whether the root znode (/hbase) is there. If not, shutdown. 
 - wait for the master to create znodes /hbase/master

So, the problem is on the very first launch of the cluster, RS aborts to start since /hbase znode might not have been created yet (only the master creates it if needed). Since /hbase/ is not deleted on cluster shutdown, on subsequent cluster starts, it does not matter which order the servers are started. So this affects only first launchs. ",enis,enis,Major,Closed,Fixed,21/Apr/12 01:43,12/Oct/12 05:35
Bug,HBASE-5850,12551971,Refuse operations from Admin before master is initialized - fix for all branches.,"This issue is needed in 0.90 0.92 also.
And update the hbase-5454 patch that add the checkInitialized() into HMaster#createTable().",xufeng,xufeng,Major,Closed,Fixed,21/Apr/12 05:32,12/Oct/12 05:35
Bug,HBASE-5857,12552107,RIT map in RS not getting cleared while region opening,"While opening the region in RS after adding the region to regionsInTransitionInRS if tableDescriptors.get() throws exception the region wont be cleared from regionsInTransitionInRS. So next time if it tries to open the region in the same RS it will throw the RegionAlreadyInTransitionException.

if swap the below statement this issue wont come.
{code}
this.regionsInTransitionInRS.putIfAbsent(region.getEncodedNameAsBytes(),true);
HTableDescriptor htd = this.tableDescriptors.get(region.getTableName());
{code}",chinnalalam,chinnalalam,Major,Closed,Fixed,23/Apr/12 10:18,26/Feb/13 17:02
Bug,HBASE-5861,12552170,Hadoop 23 compilation broken due to tests introduced in HBASE-5604,"When attempting to compile HBase 0.94rc1 against hadoop 23, I got this set of compilation error messages:

{code}
jon@swoop:~/proj/hbase-0.94$ mvn clean test -Dhadoop.profile=23 -DskipTests
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 18.926s
[INFO] Finished at: Mon Apr 23 10:38:47 PDT 2012
[INFO] Final Memory: 55M/555M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure: Compilation failure:
[ERROR] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java:[147,46] org.apache.hadoop.mapreduce.JobContext is abstract; cannot be instantiated
[ERROR] 
[ERROR] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java:[153,29] org.apache.hadoop.mapreduce.JobContext is abstract; cannot be instantiated
[ERROR] 
[ERROR] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java:[194,46] org.apache.hadoop.mapreduce.JobContext is abstract; cannot be instantiated
[ERROR] 
[ERROR] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java:[206,29] org.apache.hadoop.mapreduce.JobContext is abstract; cannot be instantiated
[ERROR] 
[ERROR] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java:[213,29] org.apache.hadoop.mapreduce.JobContext is abstract; cannot be instantiated
[ERROR] 
[ERROR] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java:[226,29] org.apache.hadoop.mapreduce.TaskAttemptContext is abstract; cannot be instantiated
[ERROR] -> [Help 1]
{code}

Upon further investigation this issue is due to code introduced in HBASE-5064 and is also present in trunk.  ",jmhsieh,jmhsieh,Blocker,Closed,Fixed,23/Apr/12 17:43,18/Sep/13 22:09
Bug,HBASE-5864,12552287,Error while reading from hfile in 0.94,"Got the following stacktrace during region split.

{noformat}
2012-04-24 16:05:42,168 WARN org.apache.hadoop.hbase.regionserver.Store: Failed getting store size for value
java.io.IOException: Requested block is out of range: 2906737606134037404, lastDataBlockOffset: 84764558
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:278)
	at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.midkey(HFileBlockIndex.java:285)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.midkey(HFileReaderV2.java:402)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.midkey(StoreFile.java:1638)
	at org.apache.hadoop.hbase.regionserver.Store.getSplitPoint(Store.java:1943)
	at org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.getSplitPoint(RegionSplitPolicy.java:77)
	at org.apache.hadoop.hbase.regionserver.HRegion.checkSplit(HRegion.java:4921)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.splitRegion(HRegionServer.java:2901)
{noformat}",ram_krish,gopinathan.av,Blocker,Closed,Fixed,24/Apr/12 11:49,18/Sep/13 20:26
Bug,HBASE-5865,12552352,test-util.sh broken with unittest updates,"Since the default maven test is meant to be run on the server, this test script always fails. Needs to take into account the location of where the script is being run as well as some debugging options for future fixes.",jesse_yates,jesse_yates,Minor,Closed,Fixed,24/Apr/12 17:50,26/Feb/13 17:02
Bug,HBASE-5866,12552357,Canary in tool package but says its in tools.,,stack,stack,Major,Closed,Fixed,24/Apr/12 17:59,26/Feb/13 08:12
Bug,HBASE-5870,12552454,Hadoop 23 compilation broken because JobTrackerRunner#getJobTracker() method is not found,"After HBASE-5861 on 0.94 we are left with this issue on trunk.

{code}
$ mvn clean test -PlocalTests -DskipTests -Dhadoop.profile=23
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure
[ERROR] /home/jon/proj/hbase-svn/hbase/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:[1333,35] cannot find symbol
[ERROR] symbol  : method getJobTracker()
[ERROR] location: class org.apache.hadoop.mapred.MiniMRCluster.JobTrackerRunner
[ERROR] -> [Help 1]
{code}",zhihyu@ebaysf.com,jmhsieh,Blocker,Closed,Fixed,25/Apr/12 00:13,23/Sep/13 18:31
Bug,HBASE-5871,12552456,"Usability regression, we don't parse compression algos anymore","It seems that string with 0.92.0 we can't create tables in the shell by specifying ""lzo"" anymore. I remember we used to do better parsing than that, but right now if you follow the wiki doing this:

bq. create 'mytable', {NAME=>'colfam:', COMPRESSION=>'lzo'}

You'll get:

bq. ERROR: java.lang.IllegalArgumentException: No enum const class org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.lzo

Bad for usability.",larsh,jdcryans,Critical,Closed,Fixed,25/Apr/12 00:29,18/Sep/13 21:26
Bug,HBASE-5873,12552533,TimeOut Monitor thread should be started after atleast one region server registers.,"Currently timeout monitor thread is started even before the region server has registered with the master.
In timeout monitor we depend on the region server to be online 
{code}
boolean allRSsOffline = this.serverManager.getOnlineServersList().
        isEmpty();
{code}

Now when the master starts up it sees there are no online servers and hence sets 
allRSsOffline to true.
{code}
setAllRegionServersOffline(allRSsOffline);
{code}
So this.allRegionServersOffline is also true.
By this time an RS has come up,
Now timeout comes up again (after 10secs) in the next cycle he sees allRSsOffline  as false.
Hence 
{code}
else if (this.allRegionServersOffline && !allRSsOffline) {
            // if some RSs just came back online, we can start the
            // the assignment right away
            actOnTimeOut(regionState);
{code}
This condition makes him to take action based on timeout.
Because of this even if one Region assignment of ROOT is going on, this piece of code triggers another assignment and thus we get RegionAlreadyinTransition Exception. Later we need to wait for 30 mins for assigning ROOT itself.",rajesh23,ram_krish,Minor,Closed,Fixed,25/Apr/12 08:17,26/Feb/13 08:12
Bug,HBASE-5874,12552537,"When 'fs.default.name' not configured, the hbck tool and Merge tool throw IllegalArgumentException.","The HBase do not configure the 'fs.default.name' attribute, the hbck tool and Merge tool throw IllegalArgumentException.
the hbck tool and Merge tool, we should add 'fs.default.name' attriubte to the code.

hbck exception:
Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: hdfs://160.176.0.101:9000/hbase/.META./1028785192/.regioninfo, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:412)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:59)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:382)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:285)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:128)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:301)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:489)
	at org.apache.hadoop.hbase.util.HBaseFsck.loadHdfsRegioninfo(HBaseFsck.java:565)
	at org.apache.hadoop.hbase.util.HBaseFsck.loadHdfsRegionInfos(HBaseFsck.java:596)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:332)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:360)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:2907)
    
Merge exception:  
[2012-05-05 10:48:24,830] [ERROR] [main] [org.apache.hadoop.hbase.util.Merge 381] exiting due to error
java.lang.IllegalArgumentException: Wrong FS: hdfs://160.176.0.101:9000/hbase/.META./1028785192/.regioninfo, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:412)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:59)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:382)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:285)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:823)
	at org.apache.hadoop.hbase.regionserver.HRegion.checkRegioninfoOnFilesystem(HRegion.java:415)
	at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:340)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2679)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2665)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2634)
	at org.apache.hadoop.hbase.util.MetaUtils.openMetaRegion(MetaUtils.java:276)
	at org.apache.hadoop.hbase.util.MetaUtils.scanMetaRegion(MetaUtils.java:261)
	at org.apache.hadoop.hbase.util.Merge.run(Merge.java:115)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.Merge.main(Merge.java:379)",fulin,fulin,Major,Closed,Fixed,25/Apr/12 09:27,28/Feb/13 22:49
Bug,HBASE-5875,12552546,Process RIT and Master restart may remove an online server considering it as a dead server,"If on master restart it finds the ROOT/META to be in RIT state, master tries to assign the ROOT region through ProcessRIT.

Master will trigger the assignment and next will try to verify the Root Region Location.
Root region location verification is done seeing if the RS has the region in its online list.
If the master triggered assignment has not yet been completed in RS then the verify root region location will fail.
Because it failed 
{code}
splitLogAndExpireIfOnline(currentRootServer);
{code}
we do split log and also remove the server from online server list. Ideally here there is nothing to do in splitlog as no region server was restarted.

So master, though the server is online, master just invalidates the region server.
In a special case, if i have only one RS then my cluster will become non operative.
",ram_krish,ram_krish,Major,Closed,Fixed,25/Apr/12 11:06,26/Feb/13 08:15
Bug,HBASE-5876,12552707,TestImportExport has been failing against hadoop 0.23 profile,TestImportExport has been failing against hadoop 0.23 profile,jmhsieh,zhihyu@ebaysf.com,Major,Closed,Fixed,25/Apr/12 19:03,26/Feb/13 08:16
Bug,HBASE-5878,12552712,Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2.,"SequencFileLogReader: 

Currently Hbase using getFileLength api from DFSInputStream class by reflection. DFSInputStream is not exposed as public. So, this may change in future. Now HDFS exposed HdfsDataInputStream as public API.
We can make use of it, when we are not able to find the getFileLength api from DFSInputStream as a else condition. So, that we will not have any sudden surprise like we are facing today.

Also,  it is just logging one warn message and proceeding if it throws any exception while getting the length. I think we can re-throw the exception because there is no point in continuing with dataloss.


{code}
long adjust = 0;

          try {
            Field fIn = FilterInputStream.class.getDeclaredField(""in"");
            fIn.setAccessible(true);
            Object realIn = fIn.get(this.in);
            // In hadoop 0.22, DFSInputStream is a standalone class.  Before this,
            // it was an inner class of DFSClient.
            if (realIn.getClass().getName().endsWith(""DFSInputStream"")) {
              Method getFileLength = realIn.getClass().
                getDeclaredMethod(""getFileLength"", new Class<?> []{});
              getFileLength.setAccessible(true);
              long realLength = ((Long)getFileLength.
                invoke(realIn, new Object []{})).longValue();
              assert(realLength >= this.length);
              adjust = realLength - this.length;
            } else {
              LOG.info(""Input stream class: "" + realIn.getClass().getName() +
                  "", not adjusting length"");
            }
          } catch(Exception e) {
            SequenceFileLogReader.LOG.warn(
              ""Error while trying to get accurate file length.  "" +
              ""Truncation / data loss may occur if RegionServers die."", e);
          }

          return adjust + super.getPos();
{code}
",ashish singhi,umamaheswararao,Major,Closed,Fixed,25/Apr/12 19:20,31/Aug/15 22:39
Bug,HBASE-5883,12552830,Backup master is going down due to connection refused exception,"The active master node network was down for some time (This node contains Master,DN,ZK,RS). Here backup node got 
notification, and started to became active. Immedietly backup node got aborted with the below exception.

{noformat}
2012-04-09 10:42:24,270 INFO org.apache.hadoop.hbase.master.SplitLogManager: finished splitting (more than or equal to) 861248320 bytes in 4 log files in [hdfs://192.168.47.205:9000/hbase/.logs/HOST-192-168-47-202,60020,1333715537172-splitting] in 26374ms
2012-04-09 10:42:24,316 FATAL org.apache.hadoop.hbase.master.HMaster: Master server abort: loaded coprocessors are: []
2012-04-09 10:42:24,333 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.io.IOException: java.net.ConnectException: Connection refused
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:375)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1045)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:897)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
	at $Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:303)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:280)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:332)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:236)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1276)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1233)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1220)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:569)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getRootServerConnection(CatalogTracker.java:369)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRootServerConnection(CatalogTracker.java:353)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:660)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:616)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:540)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:363)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:488)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupConnection(HBaseClient.java:328)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:362)
	... 20 more
2012-04-09 10:42:24,336 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2012-04-09 10:42:24,336 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
{noformat}",jeason,gopinathan.av,Major,Closed,Fixed,26/Apr/12 10:34,26/Feb/13 16:56
Bug,HBASE-5884,12552881,MapReduce package info has broken link to bulk-loads,"Bulk Loads link goes to an old link, which we have dropped recently.",jesse_yates,jesse_yates,Trivial,Closed,Fixed,26/Apr/12 17:16,26/Feb/13 17:02
Bug,HBASE-5885,12552887,Invalid HFile block magic on Local file System,"ERROR: java.lang.RuntimeException: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=7, exceptions:
Thu Apr 26 11:19:18 PDT 2012, org.apache.hadoop.hbase.client.ScannerCallable@190a621a, java.io.IOException: java.io.IOException: Could not iterate StoreFileScanner[HFileScanner for reader reader=file:/tmp/hbase-eclark/hbase/TestTable/e2d1c846363c75262cbfd85ea278b342/info/bae2681d63734066957b58fe791a0268, compression=none, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false], firstKey=0000000001/info:data/1335463981520/Put, lastKey=0002588100/info:data/1335463902296/Put, avgKeyLen=30, avgValueLen=1000, entries=1215085, length=1264354417, cur=0000000248/info:data/1335463994457/Put/vlen=1000/ts=0]
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.next(StoreFileScanner.java:135)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:95)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:368)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:127)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3323)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3279)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3296)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2393)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1376)
Caused by: java.io.IOException: Invalid HFile block magic: \xEC\xD5\x9D\xB4\xC2bfo
	at org.apache.hadoop.hbase.io.hfile.BlockType.parse(BlockType.java:153)
	at org.apache.hadoop.hbase.io.hfile.BlockType.read(BlockType.java:164)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock.<init>(HFileBlock.java:254)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1779)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1637)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:327)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.readNextDataBlock(HFileReaderV2.java:555)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.next(HFileReaderV2.java:651)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.next(StoreFileScanner.java:130)
	... 12 more

Thu Apr 26 11:19:19 PDT 2012, org.apache.hadoop.hbase.client.ScannerCallable@190a621a, java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:1132)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:1121)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2420)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1376)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.position(Buffer.java:216)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.next(HFileReaderV2.java:630)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.next(StoreFileScanner.java:130)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:95)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:406)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:127)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3323)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3279)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3296)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2393)
	... 5 more



On latest 0.94 branch I spun up a new standalone hbase.  Then I started a performance evaluation run hbase/bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred randomWrite 10
after that completed I tried a scan of TestTable.  The scan got the above error.",eclark,eclark,Blocker,Closed,Fixed,26/Apr/12 18:26,18/Sep/13 22:11
Bug,HBASE-5893,12553211,Allow spaces in coprocessor conf (aka trim() className),"This is annoying especially for coprocessors where you've long class name.
but maybe is a bug of Configuration.getStrings() that doesn't trim each string.

When you've comma separated values like in the coprocessors case, you've to pack together your values without spaces (""v1,v2,v3,..."") otherwise the coprocessor is not loaded because the class name with spaces is not found.
{code}
<property>
   <name>hbase.coprocessor.master.classes</name>
   <value>
    org.apache.hadoop.hbase.security.token.TokenProvider,
    org.apache.hadoop.hbase.security.access.AccessController
   </value>
</property>
{code}",mbertozzi,mbertozzi,Minor,Closed,Fixed,27/Apr/12 21:56,12/Oct/12 05:35
Bug,HBASE-5894,12553240,Table deletion failed but HBaseAdmin#deletetable reports it as success,"Reproduce this issue by following steps:
For reproduce it I add this code in DeleteTableHandler#handleTableOperation():
{noformat}
      LOG.debug(""Deleting region "" + region.getRegionNameAsString() +
        "" from META and FS"");
	  +if (true) {
	  +  throw new IOException(""ERROR"");
	  +}
      // Remove region from META
      MetaEditor.deleteRegion(this.server.getCatalogTracker(), region);
{noformat}


step1:create a table and disable it.
step2:delete it by HBaseAdmin#deleteTable() API.

result:after lone time, The log say the Table has been deleted, but in fact if we do ""list"" in shell,the table also exists.",xufeng,xufeng,Minor,Closed,Fixed,28/Apr/12 08:01,26/Feb/13 08:16
Bug,HBASE-5897,12553263,prePut coprocessor hook causing substantial CPU usage,"I was running an insert workload against trunk under oprofile and saw that a significant portion of CPU usage was going to calling the ""prePut"" coprocessor hook inside doMiniBatchPut, even though I don't have any coprocessors installed. I ran a million-row insert and collected CPU time spent in the RS after commenting out the preput hook, and found CPU usage reduced by 33%.",tlipcon,tlipcon,Critical,Closed,Fixed,29/Apr/12 02:24,18/Sep/13 22:15
Bug,HBASE-5900,12553308,HRegion#FIXED_OVERHEAD is miscalculated in 94.,"After apply the patch of HBASE-5611, and tested on a 32-bit machine. This problem was triggered.
Before this patch, TestHeapSize is passed by pure coincidence in 94.
{noformat}
  public static final long FIXED_OVERHEAD = ClassSize.align(
      ClassSize.OBJECT +
      ClassSize.ARRAY +
      30  * ClassSize.REFERENCE + Bytes.SIZEOF_INT +
      (6 * Bytes.SIZEOF_LONG) +
      Bytes.SIZEOF_BOOLEAN);
{noformat}

Actually, there are 31 REFERENCEs and 5 LONGs in HRegion.





",jeason,jeason,Major,Closed,Fixed,30/Apr/12 06:50,12/Oct/12 05:35
Bug,HBASE-5902,12553341,Some scripts are not executable,"-rw-rw-r--  graceful_stop.sh
-rw-rw-r--  hbase-config.sh
-rw-rw-r--  local-master-backup.sh
-rw-rw-r--  local-regionservers.sh
",nkeywal,nkeywal,Trivial,Closed,Fixed,30/Apr/12 11:03,26/Feb/13 08:16
Bug,HBASE-5904,12553377,is_enabled from shell returns differently from pre- and post- HBASE-5155,"If I launch an hbase shell that uses HBase and ZooKeeper without HBASE-5155, against HBase servers with HBASE-5155, then is_enabled for a table always returns false even if the table is considered enabled by the servers from the logs.  If I then do the same thing but with an HBase shell and ZooKeeper with HBASE-5155, then is_enabled returns as expected.

If I launch an HBase shell that uses HBase and ZooKeeper without HBASE-5155, against HBase servers also without HBASE-5155, then is_enabled works as you'd expect.  But if I then do the same thing but with an HBase shell and ZooKeeper with HBASE-5155, then is_enabled returns false even though the table is considered enabled by the servers from the logs.

Additionally, if I then try to enable the table from the HBASE-5155-containing shell, it hangs because the ZooKeeper code waits for the ZNode to be updated with ""ENABLED"" in the data field, but what actually happens is that the ZNode gets deleted since the servers are running without HBASE-5155.

I think the culprit is that the indication of how a table is considered enabled inside ZooKeeper has changed with HBASE-5155.  Before HBASE-5155, a table was considered enabled if the ZNode for it did not exist.  After HBASE-5155, a table is considered enabled if the ZNode for it exists and has ""ENABLED"" in its data.  I think the current code is incompatible when running clients and servers where one side has HBASE-5155 and the other side does not.",dsw,dsw,Blocker,Closed,Fixed,30/Apr/12 15:45,13/Jun/22 16:16
Bug,HBASE-5906,12553401,TestChangingEncoding failing sporadically in 0.94 build,The test passes locally for me and Elliott but takes a long time to run.  Timeout is only two minutes for the test though.,stack,stack,Major,Closed,Fixed,30/Apr/12 18:04,12/Jul/13 21:45
Bug,HBASE-5908,12553430,TestHLogSplit.testTralingGarbageCorruptionFileSkipErrorsPasses should not use append to corrupt the HLog,"TestHLogSplit.testTralingGarbageCorruptionFileSkipErrorsPasses fails against a version of hadoop with https://issues.apache.org/jira/browse/HADOOP-8230

The failure:
""java.io.IOException: Append is not supported. Please see the dfs.support.append configuration parameter.""

Instead of using append, we can probably just:
- copy over the contents to a new file
- append the garbage to the new file
- copy back to the old file",gchanan,gchanan,Minor,Closed,Fixed,30/Apr/12 22:13,26/Feb/13 08:13
Bug,HBASE-5909,12553449,SlabStats should be a daemon thread,"I had a hanging JVM on shutdown caused by:

{noformat}
""Slab Statistics #0"" prio=5 tid=7fc0238bc800 nid=0x10dadf000 waiting on condition [10dade000]
   java.lang.Thread.State: TIMED_WAITING (parking)
{noformat}",larsh,jdcryans,Major,Closed,Fixed,01/May/12 04:24,26/Feb/13 08:15
Bug,HBASE-5916,12553597,RS restart just before master intialization we make the cluster non operative,"Consider a case where my master is getting restarted.  RS that was alive when the master restart started, gets restarted before the master initializes the ServerShutDownHandler.
{code}
serverShutdownHandlerEnabled = true;
{code}

In this case when the RS tries to register with the master, the master will try to expire the server but the server cannot be expired as still the serverShutdownHandler is not enabled.

This case may happen when i have only one RS gets restarted or all the RS gets restarted at the same time.(before assignRootandMeta).
{code}
LOG.info(message);
      if (existingServer.getStartcode() < serverName.getStartcode()) {
        LOG.info(""Triggering server recovery; existingServer "" +
          existingServer + "" looks stale, new server:"" + serverName);
        expireServer(existingServer);
      }
{code}
If another RS is brought up then the cluster comes back to normalcy.

May be a very corner case.



",ram_krish,ram_krish,Critical,Closed,Fixed,02/May/12 09:22,18/Sep/13 20:47
Bug,HBASE-5918,12553615,Master will block forever at startup if root server dies between assigning root and assigning meta,"When master is initializing, if root server died between assign root and assign meta, master will block at 
HMaster#assignRootAndMeta:{code}assignmentManager.assignMeta();
this.catalogTracker.waitForMeta();{code}
because ServerShutdownHandler is disabled,

So we should enable ServerShutdownHandler after called assignmentManager.assignMeta();",zjushch,zjushch,Major,Closed,Fixed,02/May/12 12:26,26/Feb/13 08:16
Bug,HBASE-5919,12553653,Add fixes for Ted's review comments from HBASE-5869,I missed addressing a few of Ted's comments on the end of my navigating HBASE-5869 commit.  Fix here.  Make it a blocker.,yuzhihong@gmail.com,stack,Blocker,Closed,Fixed,02/May/12 18:07,13/Jun/22 16:20
Bug,HBASE-5920,12553663,New Compactions Logic can silently prevent user-initiated compactions from occurring,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",dwollen,dwollen,Minor,Closed,Fixed,02/May/12 20:14,13/Jun/22 16:20
Bug,HBASE-5922,12553678,HalfStoreFileReader seekBefore causes StackOverflowError,"Calling HRegionServer.getClosestRowBefore() can cause a stack overflow if the underlying store file is a reference and the row key is in the bottom.

java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:990)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:978)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1651)
        at sun.reflect.GeneratedMethodAccessor174.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:147)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:149)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:149)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:149)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:149)",nputnam,nputnam,Critical,Closed,Fixed,02/May/12 22:48,26/Feb/13 08:16
Bug,HBASE-5925,12553762,Issue with only using the old config param hbase.hstore.compactionThreshold but not the corresponding new one,"One observation while going through the code:-

In MemStoreFlusher constructor
{code}
this.blockingStoreFilesNumber =
      conf.getInt(""hbase.hstore.blockingStoreFiles"", 7);
    if (this.blockingStoreFilesNumber == -1) {
      this.blockingStoreFilesNumber = 1 +
        conf.getInt(""hbase.hstore.compactionThreshold"", 3);
    }
{code}
Here as per the code if hbase.hstore.blockingStoreFiles is configured as -1, we are making this value to be 1+ min files to compact

But here we read the old config item only!

Here also we need to read the new config 1st and if not there then the old one.. Is this a miss?

Like
conf.getInt(""hbase.hstore.compaction.min"",
         conf.getInt(""hbase.hstore.compactionThreshold"", 3))",anoopsamjohn,anoopsamjohn,Minor,Closed,Fixed,03/May/12 14:12,23/Sep/13 18:30
Bug,HBASE-5927,12553790,SSH and DisableTableHandler happening together does not clear the znode of the region and RIT map.,"A possible exception: If the related regionserver was just killed(But HMaster has not perceived that), then we will get a local exception ""Connection reset by peer"". If this region belongs to a disabling table. what will happen?

ServerShutdownHandler will remove this region from AM#regions. So this region is still existing in RIT. TimeoutMonitor will take care of it after it got timeout. Then invoke unassign again. Since this region has been removed from AM#regions, it will return directly due to the below code:
{code}
    synchronized (this.regions) {
      // Check if this region is currently assigned
      if (!regions.containsKey(region)) {
        LOG.debug(""Attempted to unassign region "" +
          region.getRegionNameAsString() + "" but it is not "" +
          ""currently assigned anywhere"");
        return;
      }
    }
{code}
Then it leads to an end-less loop.
",rajesh23,jeason,Major,Closed,Fixed,03/May/12 15:58,26/Feb/13 08:15
Bug,HBASE-5928,12553804,Hbck shouldn't npe when there are no tables.,"hbase fsck errors out when there are no tables.

Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.HBaseFsck.reportTablesInFlux(HBaseFsck.java:560)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:346)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:382)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3120)",eclark,eclark,Minor,Closed,Fixed,03/May/12 18:08,26/Feb/13 08:15
Bug,HBASE-5931,12553821,HBase security profile doesn't compile ,The compilation is broken.,jxiang,jxiang,Major,Closed,Fixed,03/May/12 19:55,23/Sep/13 18:31
Bug,HBASE-5942,12553992,HConnnectionManager.getRegionServerWithRetries doesn't call afterCall properly,"HConnnectionManager.getRegionServerWithRetries:
{code}
          return callable.call();
        } catch (Throwable t) {
          callable.shouldRetry(t);
{code}
shouldRetry relies on the proper startTime and endTime to calculate the timeout value. However, callable.afterCall() is called in finally block. Thus callable.callTimeout will be set to negative value in callable.shouldRetry().
",ram_krish,zhihyu@ebaysf.com,Major,Closed,Fixed,04/May/12 20:25,20/Nov/15 11:55
Bug,HBASE-5946,12554040,Thrift Filter Language documentation is inconsistent,"Syntax: SingleColumnValueFilter(<compare operator>, '<comparator>', '<family>', '<qualifier>), as described here: http://hbase.apache.org/book/thrift.html is not correct.
The correct syntax is: SingleColumnValueFilter('<family>', '<qualifier>', <compare operator>, '<comparator>')
Also, <comparator> parameter must always contain the comparator, e.g. binary: or binaryprefix: etc. Without it (except PrefixFilter and maybe some other filters) TSocket class throws TTransportException: TSocket read 0 bytes. 
All examples in section 9.3.1.9. Individual Filter Syntax are written without comparator.

There also a typo: 
in section 9.3.1.9.12 - Family Filter, syntax and example described for QualifierFilter",eljefe6a,starlng,Minor,Closed,Fixed,05/May/12 13:32,23/Sep/13 19:08
Bug,HBASE-5952,12554221,Sync hardcoded default flush size and max file size with hbase-default.xml,"Since the hardcoded default flush size is 64MB, but the default in hbase-default.xml is 128MB, if the client does set it to 64MB,
the actual flush size will be 128MB instead, due to the way HRegion get the flush size. We can change how HRegion get the flush size,
but it is clean and simple to sync up the defaults.",jxiang,jxiang,Minor,Closed,Fixed,07/May/12 20:14,20/Nov/15 11:55
Bug,HBASE-5955,12554276,Guava 11 drops MapEvictionListener and Hadoop 2.0.0-alpha requires it,"Hadoop 2.0.0-alpha depends on Guava 11.0.2. Updating HBase dependencies to match produces the following compilation errors:

{code}
[ERROR] SingleSizeCache.java:[41,32] cannot find symbol
[ERROR] symbol  : class MapEvictionListener
[ERROR] location: package com.google.common.collect
[ERROR] 
[ERROR] SingleSizeCache.java:[94,4] cannot find symbol
[ERROR] symbol  : class MapEvictionListener
[ERROR] location: class org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache
[ERROR] 
[ERROR] SingleSizeCache.java:[94,69] cannot find symbol
[ERROR] symbol  : class MapEvictionListener
[ERROR] location: class org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache
{code}
",larsh,apurtell,Major,Closed,Fixed,08/May/12 04:52,12/Oct/12 05:36
Bug,HBASE-5957,12554336,Mismatch with config param name in xml and code,"In hbase-default.xml there listed one param ""io.storefile.bloom.cacheonwrite""
{code}
<property>
  <name>io.storefile.bloom.cacheonwrite</name>
  <value>false</value>
  <description>
	  Enables cache-on-write for inline blocks of a compound Bloom filter.
  </description>
</property>
{code}

No place in the code this config is getting used.
Instead in CacheConfig there is a config param name 
{code}
/**
   * Configuration key to cache compound bloom filter blocks on write.
   */
  public static final String CACHE_BLOOM_BLOCKS_ON_WRITE_KEY =
      ""hfile.block.bloom.cacheonwrite"";
{code}

Seems issue with entry in the xml file.
We can correct the xml with the config name as hfile.block.bloom.cacheonwrite ?",anoopsamjohn,anoopsamjohn,Major,Closed,Fixed,08/May/12 15:26,13/Jun/22 16:33
Bug,HBASE-5962,12554371,interop issue: RowMutations should be added at the end in HbaseObjectWriteable class,"In HbaseObjectWriteable.java new classes should be added to the end; else, old clients will not be able to talk to new HBase servers. This is causing issues in test cluster with the following stack trace:

2012-05-08 11:24:32,416 ERROR org.apache.hadoop.hbase.io.HbaseObjectWritable: Can't find class
java.lang.ClassNotFoundException: 
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:247)
at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:792)
at org.apache.hadoop.hbase.io.HbaseObjectWritable.getClassByName(HbaseObjectWritable.java:552)
at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:520)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invocation.readFields(HBaseRPC.java:136)
at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:953)
at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:895)
at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:471)
at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:371)
2012-05-08 11:24:33,766 ERROR org.apache.hadoop.hbase.io.HbaseObjectWritable: Can't find class",,kannanm,Major,Closed,Fixed,08/May/12 19:18,23/Sep/13 18:31
Bug,HBASE-5963,12554376,ClassCastException: FileSystem$Cache$ClientFinalizer cannot be cast to Thread,"{code}
12/05/08 19:49:26 INFO regionserver.HRegionServer: STOPPED: Failed initialization
Exception in thread ""main"" java.lang.ClassCastException: org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer cannot be cast to java.lang.Thread
	at org.apache.hadoop.hbase.regionserver.ShutdownHook.suppressHdfsShutdownHook(ShutdownHook.java:181)
	at org.apache.hadoop.hbase.regionserver.ShutdownHook.install(ShutdownHook.java:82)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3601)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3585)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3645)
{code}
",zhihyu@ebaysf.com,apurtell,Major,Closed,Fixed,08/May/12 19:56,26/Feb/13 08:15
Bug,HBASE-5964,12554400,"HFileSystem: ""No FileSystem for scheme: hdfs""","I've been seeing this with Hadoop 2.0.0-alpha-SNAPSHOT and HBase 0.94.0-SNAPSHOT:

{noformat}
2012-05-08 15:18:00,692 FATAL [RegionServer:0;acer.localdomain,48307,1336515479011] regionserver.HRegionServer(1674): ABORTING region server acer.localdomain,48307,1336515479011: Unhandled exception: No FileSystem for scheme: hdfs
java.io.IOException: No FileSystem for scheme: hdfs
	at org.apache.hadoop.hbase.fs.HFileSystem.newInstanceFileSystem(HFileSystem.java:146)
	at org.apache.hadoop.hbase.fs.HFileSystem.<init>(HFileSystem.java:75)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.handleReportForDutyResponse(HRegionServer.java:973)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.handleReportForDutyResponse(MiniHBaseCluster.java:110)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:671)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.runRegionServer(MiniHBaseCluster.java:136)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.access$000(MiniHBaseCluster.java:89)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer$1.run(MiniHBaseCluster.java:120)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:357)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.hadoop.hbase.util.Methods.call(Methods.java:37)
	at org.apache.hadoop.hbase.security.User.call(User.java:586)
	at org.apache.hadoop.hbase.security.User.access$700(User.java:50)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:426)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:118)
	at java.lang.Thread.run(Thread.java:679)
{noformat}

Not sure precisely when it started. First I thought it might be due to HBASE-5955 but I cherry picked that change over from trunk. Then I got HBASE-5963 out of the way.  ",apurtell,apurtell,Major,Closed,Fixed,08/May/12 22:26,26/Feb/13 08:16
Bug,HBASE-5966,12554413,MapReduce based tests broken on Hadoop 2.0.0-alpha,"Some fairly recent change in Hadoop 2.0.0-alpha has broken our MapReduce test rigging. Below is a representative error, can be easily reproduced with:

{noformat}
mvn -PlocalTests -Psecurity \
  -Dhadoop.profile=23 -Dhadoop.version=2.0.0-SNAPSHOT \
  clean test \
  -Dtest=org.apache.hadoop.hbase.mapreduce.TestTableMapReduce
{noformat}

And the result:

{noformat}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.hbase.mapreduce.TestTableMapReduce
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 54.292 sec <<< FAILURE!

-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.mapreduce.TestTableMapReduce
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 54.292 sec <<< FAILURE!
testMultiRegionTable(org.apache.hadoop.hbase.mapreduce.TestTableMapReduce)  Time elapsed: 21.935 sec  <<< ERROR!
java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:135)
	at org.apache.hadoop.yarn.api.impl.pb.client.ClientRMProtocolPBClientImpl.getNewApplication(ClientRMProtocolPBClientImpl.java:134)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:183)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:216)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:339)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1226)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1223)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1223)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1244)
	at org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.runTestOnTable(TestTableMapReduce.java:151)
	at org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.testMultiRegionTable(TestTableMapReduce.java:129)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
	at org.junit.rules.RunRules.evaluate(RunRules.java:18)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110)
	at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175)
	at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:81)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)
Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From acer.localdomain/192.168.122.1 to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:188)
	at $Proxy89.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ClientRMProtocolPBClientImpl.getNewApplication(ClientRMProtocolPBClientImpl.java:132)
	... 45 more
Caused by: java.net.ConnectException: Call From acer.localdomain/192.168.122.1 to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:725)
	at org.apache.hadoop.ipc.Client.call(Client.java:1160)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:185)
	... 47 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:522)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:487)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:469)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:563)
	at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:212)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1266)
	at org.apache.hadoop.ipc.Client.call(Client.java:1136)
	... 48 more
{noformat}
",jxiang,apurtell,Major,Closed,Fixed,09/May/12 01:04,03/Apr/13 05:55
Bug,HBASE-5967,12554419,OpenDataException because HBaseProtos.ServerLoad cannot be converted to an open data type,"I saw this error in the master log:

Caused by: java.lang.IllegalArgumentException: Method org.apache.hadoop.hbase.master.MXBean.getRegionServers has parameter or return type that cannot be translated into an open type
    at com.sun.jmx.mbeanserver.ConvertingMethod.from(ConvertingMethod.java:32)
    at com.sun.jmx.mbeanserver.MXBeanIntrospector.mFrom(MXBeanIntrospector.java:63)
    at com.sun.jmx.mbeanserver.MXBeanIntrospector.mFrom(MXBeanIntrospector.java:33)
    at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:118)
    at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:99)
    ... 14 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: java.util.Map<java.lang.String, org.apache.hadoop.hbase.ServerLoad>
    at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.jav

",gchanan,jxiang,Minor,Closed,Fixed,09/May/12 02:29,23/Sep/13 18:31
Bug,HBASE-5974,12554557,Scanner retry behavior with RPC timeout on next() seems incorrect,"I'm seeing the following behavior:
- set RPC timeout to a short value
- call next() for some batch of rows, big enough so the client times out before the result is returned
- the HConnectionManager stuff will retry the next() call to the same server. At this point, one of two things can happen: 1) the previous next() call will still be processing, in which case you get a LeaseException, because it was removed from the map during the processing, or 2) the next() call will succeed but skip the prior batch of rows.",anoopsamjohn,tlipcon,Critical,Closed,Fixed,09/May/12 20:25,19/Sep/14 20:12
Bug,HBASE-5975,12554564,Failed suppression of fs shutdown hook with Hadoop 2.0.0,"Unit test failed with error:  Failed suppression of fs shutdown hook

ShutdownHookManager.deleteShutdownHook failed to delete the hook since it should be runnable instead of a thread for HADOOP 2.0.0.
For other HADOOP version, runnable should work too since thread implements runnable.",jxiang,jxiang,Major,Closed,Fixed,09/May/12 20:44,26/Feb/13 08:16
Bug,HBASE-5986,12554839,Clients can see holes in the META table when regions are being split,"We found this issue when running large scale ingestion tests for HBASE-5754. The problem is that the .META. table updates are not atomic while splitting a region. In SplitTransaction, there is a time lap between the marking the parent offline, and adding of daughters to the META table. This can result in clients using MetaScanner, of HTable.getStartEndKeys (used by the TableInputFormat) missing regions which are made just offline, but the daughters are not added yet. 

This is also related to HBASE-4335. ",enis,enis,Major,Closed,Fixed,10/May/12 23:47,26/Feb/13 08:15
Bug,HBASE-5990,12554936,TestHCM failed with Hadoop 2.0.0,"Running org.apache.hadoop.hbase.client.TestHCM
Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 11.742 sec <<< FAILURE!

Failed tests:   testRegionCaching(org.apache.hadoop.hbase.client.TestHCM)
",jxiang,jxiang,Major,Closed,Fixed,11/May/12 17:00,23/Sep/13 18:30
Bug,HBASE-5992,12554964,Generalization of region move implementation + manage draining servers in bulk assign,"The region move implementation now has now a similar behavior whatever the destination server is specified or not. This allows:
 - to benefit from the improvement in HBASE-5877
 - as a side effect to have the coprocessors calls when the destination server is not specified
 
This includes various fixes around draining servers. Draining servers were not excluded during a bulk assign. This is now fixed.",nkeywal,nkeywal,Minor,Closed,Fixed,11/May/12 19:49,23/Sep/13 18:30
Bug,HBASE-5997,12555096,Fix concerns raised in HBASE-5922 related to HalfStoreFileReader,"Pls refer to the comment
https://issues.apache.org/jira/browse/HBASE-5922?focusedCommentId=13269346&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13269346.
Raised this issue to solve that comment. Just incase we don't forget it.",anoopsamjohn,ram_krish,Major,Closed,Fixed,14/May/12 05:19,07/Apr/13 04:34
Bug,HBASE-6002,12555842,Possible chance of resource leak in HlogSplitter,In HLogSplitter.splitLogFileToTemp-Reader(in) is not closed and in finally block in loop while closing the writers(wap.w) if any exception comes other writers won't close.,chinnalalam,chinnalalam,Major,Closed,Fixed,15/May/12 17:52,26/Feb/13 08:15
Bug,HBASE-6003,12555896,[refguide] performance.xml - fixed 2 links to Case Studies that were ??? ,"There were 2 links in the performance chapter that were ??? links that should have been going to Case Studies.

Corrected them so that they weren't dead links.",dmeil,dmeil,Minor,Closed,Fixed,15/May/12 21:09,13/Jun/22 16:40
Bug,HBASE-6007,12555929,Make getTableRegions return an empty list if the table does not exist,"Making the getTableRegions Thrift API method handle TableNotFoundException and return an empty list in that case. Without this the behavior is dependent on whether an HTable object is present in the thread-local cache in case a table was deleted.
",mikhail,mikhail,Minor,Closed,Fixed,15/May/12 23:30,13/Jun/22 16:39
Bug,HBASE-6011,12555944,Unable to start master in local mode,"Got this trying to launch head of 0.94 branch in local mode from the build tree but it happens with trunk and 0.92 too:

{noformat}
12/05/15 19:35:45 ERROR master.HMasterCommandLine: Failed to start master
java.lang.ClassCastException: org.apache.hadoop.hbase.master.HMaster cannot be cast to org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster
	at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:142)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:103)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
	at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1761)
{noformat}
",apurtell,apurtell,Major,Closed,Fixed,16/May/12 02:43,26/Feb/13 08:16
Bug,HBASE-6012,12555948,Handling RegionOpeningState for bulk assign,"Since HBASE-5914, we using bulk assign for SSH

But in the bulk assign case if we get an ALREADY_OPENED case there is no one to clear the znode created by bulk assign. 


Another thing, when RS opening a list of regions, if one region is already in transition, it will throw RegionAlreadyInTransitionException and stop opening other regions.",zjushch,zjushch,Major,Closed,Fixed,16/May/12 03:07,15/Oct/13 04:46
Bug,HBASE-6016,12556000,ServerShutdownHandler#processDeadRegion could return false for disabling table regions,"{code}
   * @return Returns true if specified region should be assigned, false if not.
   * @throws IOException
   */
  public static boolean processDeadRegion(HRegionInfo hri, Result result,
      AssignmentManager assignmentManager, CatalogTracker catalogTracker)
{code}

For the disabling region, I think we needn't assign it , and processDeadRegion could return false.",zjushch,zjushch,Major,Closed,Fixed,16/May/12 09:51,12/Oct/12 05:36
Bug,HBASE-6018,12556051,hbck fails with a RejectedExecutionException when >50 regions present,"On a long running job 0.94.0rc3 cluster, we get to a point where hbck consistently encounters this error and fails:

{code}
Exception in thread ""main"" java.util.concurrent.RejectedExecutionException
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
	at org.apache.hadoop.hbase.util.HBaseFsck.loadHdfsRegionInfos(HBaseFsck.java:633)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:354)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:382)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3120)
{code}
",jmhsieh,jmhsieh,Major,Closed,Fixed,16/May/12 15:01,26/Feb/13 08:16
Bug,HBASE-6021,12556104,NullPointerException when running LoadTestTool without specifying compression type,"If you don't specify a compression type on the LoadTestTool command line then this happens:

{noformat}
12/05/16 18:41:23 ERROR util.AbstractHBaseTool: Error running command-line tool
java.lang.NullPointerException
	at org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType(HColumnDescriptor.java:535)
	at org.apache.hadoop.hbase.HBaseTestingUtility.createPreSplitLoadTestTable(HBaseTestingUtility.java:1885)
	at org.apache.hadoop.hbase.util.LoadTestTool.doWork(LoadTestTool.java:297)
	at org.apache.hadoop.hbase.util.AbstractHBaseTool.run(AbstractHBaseTool.java:103)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.AbstractHBaseTool.doStaticMain(AbstractHBaseTool.java:173)
	at org.apache.hadoop.hbase.util.LoadTestTool.main(LoadTestTool.java:341)
{noformat}

This should be handled better.",apurtell,apurtell,Minor,Closed,Fixed,16/May/12 18:44,26/Feb/13 08:15
Bug,HBASE-6027,12556163,Update the reference guide to reflect the changes in the security profile,"The refguide needs to be updated to reflect the fact that there is no security profile anymore, etc. [Follow up to HBASE-5732]",ddas,ddas,Major,Closed,Fixed,17/May/12 01:11,23/Sep/13 18:30
Bug,HBASE-6028,12556167,Implement a cancel for in-progress compactions,"Depending on current server load, it can be extremely expensive to run periodic minor / major compactions.  It would be helpful to have a feature where a user could use the shell or a client tool to explicitly cancel an in-progress compactions.  This would allow a system to recover when too many regions became eligible for compactions at once",mogoel,dwollen,Minor,Closed,Fixed,17/May/12 01:59,13/Jun/22 16:31
Bug,HBASE-6029,12556169,HBCK doesn't recover Balance switch if exception occurs in onlineHbck(),,maryannxue,maryannxue,Major,Closed,Fixed,17/May/12 02:47,26/Feb/13 08:16
Bug,HBASE-6031,12556238,RegionServer does not go down while aborting,"Following is the thread dump.
{code}
""1997531088@qtp-716941846-5"" prio=10 tid=0x00007f7c5820c800 nid=0xe1b in Object.wait() [0x00007f7c56ae8000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockWritable(SelectChannelEndPoint.java:279)
	- locked <0x00007f7cfe0616d0> (a org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:545)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:639)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:109)
	- locked <0x00007f7cfe74d758> (a org.mortbay.util.ByteArrayOutputStream2)
	at org.mortbay.jetty.AbstractGenerator$OutputWriter.write(AbstractGenerator.java:904)
	at java.io.Writer.write(Writer.java:96)
	- locked <0x00007f7cfca02fc0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at java.io.PrintWriter.write(PrintWriter.java:361)
	- locked <0x00007f7cfca02fc0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at org.jamon.escaping.HtmlEscaping.write(HtmlEscaping.java:43)
	at org.jamon.escaping.AbstractCharacterEscaping.write(AbstractCharacterEscaping.java:35)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:222)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:180)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:171)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

""1374615312@qtp-716941846-3"" prio=10 tid=0x00007f7c58214800 nid=0xc42 in Object.wait() [0x00007f7c55bd9000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockWritable(SelectChannelEndPoint.java:279)
	- locked <0x00007f7cfdbb6cc8> (a org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:545)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:639)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:109)
	- locked <0x00007f7cfe7dbda0> (a org.mortbay.util.ByteArrayOutputStream2)
	at org.mortbay.jetty.AbstractGenerator$OutputWriter.write(AbstractGenerator.java:904)
	at java.io.Writer.write(Writer.java:96)
	- locked <0x00007f7cfe30ebe0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at java.io.PrintWriter.write(PrintWriter.java:361)
	- locked <0x00007f7cfe30ebe0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at org.jamon.escaping.HtmlEscaping.write(HtmlEscaping.java:43)
	at org.jamon.escaping.AbstractCharacterEscaping.write(AbstractCharacterEscaping.java:35)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:222)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:180)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:171)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

""615294984@qtp-716941846-2"" prio=10 tid=0x00007f7c58086000 nid=0x907 in Object.wait() [0x00007f7c57ffd000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockWritable(SelectChannelEndPoint.java:279)
	- locked <0x00007f7d6383cd40> (a org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:545)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:639)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:109)
	- locked <0x00007f7d116bbf90> (a org.mortbay.util.ByteArrayOutputStream2)
	at org.mortbay.jetty.AbstractGenerator$OutputWriter.write(AbstractGenerator.java:904)
	at java.io.Writer.write(Writer.java:96)
	- locked <0x00007f7d54b19ea0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at java.io.PrintWriter.write(PrintWriter.java:361)
	- locked <0x00007f7d54b19ea0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at org.jamon.escaping.HtmlEscaping.write(HtmlEscaping.java:43)
	at org.jamon.escaping.AbstractCharacterEscaping.write(AbstractCharacterEscaping.java:35)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:222)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:180)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:171)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

""regionserver60020.decayingSampleTick.1"" daemon prio=10 tid=0x0000000040b2b800 nid=0x6f1a waiting on condition [0x00007f7c697a5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f7c9eb40cb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
	at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)

""LeaseRenewer:root@10.18.40.217:9000"" daemon prio=10 tid=0x00007f7c6c770000 nid=0x6f18 waiting on condition [0x00007f7c699a7000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:432)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:299)
	at java.lang.Thread.run(Thread.java:619)
{code}",,ram_krish,Major,Closed,Fixed,17/May/12 13:42,23/Sep/13 18:31
Bug,HBASE-6035,12556272,"[0.89-fb] region_status.rb does not count total regions correctly, when favornodes are used","bin/hbase-jruby region_status.rb prints some weired numbers

1047/2 .. the denominator is 2, which should have been the total number of regions in the region.

It turns out that this was because we were using both:

scan.setFilter(FirstKeyOnlyFilter.new)
and
scan.addColumn INFO, REGION_INFO

it worked fine when there was no column before info:regioninfo.

But, after adding info:favourednodes we were no longer getting info:regioninfo.

Removing the scan.setFilter fixes the problem.",amitanand,amitanand,Major,Closed,Fixed,17/May/12 18:49,13/Jun/22 16:31
Bug,HBASE-6041,12556304,NullPointerException prevents the master from starting up,"This is 0.90 only.

2012-05-04 14:27:57,913 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:731)
	at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:215)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:419)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:293)
2012-05-04 14:27:57,914 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2012-05-04 14:27:57,915 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 1433",jxiang,jxiang,Major,Closed,Fixed,17/May/12 20:59,13/Jun/22 16:30
Bug,HBASE-6046,12556392,Master retry on ZK session expiry causes inconsistent region assignments.,"1> ZK Session timeout in the hmaster leads to bulk assignment though all the RSs are online.
2> While doing bulk assignment, if the master again goes down & restart(or backup comes up) all the node created in the ZK will now be tried to reassign to the new RSs. This is leading to double assignment.

we had 2800 regions, among this 1900 region got double assignment, taking the region count to 4700. ",ram_krish,gopinathan.av,Major,Closed,Fixed,18/May/12 13:19,26/Feb/13 08:16
Bug,HBASE-6047,12556396,Put.has() can't determine result correctly,"the public method 'has(byte [] family, byte [] qualifier)' internally invoked the private method 'has(byte [] family, byte [] qualifier, long ts, byte [] value, boolean ignoreTS, boolean ignoreValue)' with 'value=new byte[0], ignoreTS=true, ignoreValue=true', but there's a logical error in the body, it'll enter the block
{code}
else if (ignoreValue) {
      for (KeyValue kv: list) {
        if (Arrays.equals(kv.getFamily(), family) && Arrays.equals(kv.getQualifier(), qualifier)
            && kv.getTimestamp() == ts) {
          return true;
        }
      }
    }
{code}
the expression 'kv.getTimestamp() == ts' in the if conditions should only exist when 'ignoreTS=false', otherwise, the following code will return false!
{code}
Put put = new Put(Bytes.toBytes(""row-01""));
put.add(Bytes.toBytes(""family-01""), Bytes.toBytes(""qualifier-01""),
				1234567L, Bytes.toBytes(""value-01""));
System.out.println(put.has(Bytes.toBytes(""family-01""),
				Bytes.toBytes(""qualifier-01"")));
{code}",posix4e,aaronwq,Major,Closed,Fixed,18/May/12 13:59,26/Feb/13 08:16
Bug,HBASE-6049,12556404,"Serializing ""List"" containing null elements will cause NullPointerException in HbaseObjectWritable.writeObject()","An error case could be in Coprocessor AggregationClient, the median() function handles an empty region and returns a List Object with the first element as a Null value. NPE occurs in the RPC response stage and the response never gets sent.",maryannxue,maryannxue,Major,Closed,Fixed,18/May/12 14:58,26/Feb/13 08:15
Bug,HBASE-6050,12556411,"HLogSplitter renaming recovered.edits and CJ removing the parent directory race, making the HBCK think cluster is inconsistent.","The scenario is like this
-> A region is getting splitted.
-> The master is still not processed the split .
-> Region server goes down.
-> Split log manager starts splitting the logs and creates the recovered.edits in the splitlog path.
-> CJ starts and deletes the entry from META and also just completes the deletion of the region dir.
-> in hlogSplitter on final step we rename the recovered.edits to come under the regiondir.
There if the regiondir doesnot exist we tend to create and then add the recovered.edits.

Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo.
Ideally cluster is fine but we it is misleading.
{code}
        } else {
          Path dstdir = dst.getParent();
          if (!fs.exists(dstdir)) {
            if (!fs.mkdirs(dstdir)) LOG.warn(""mkdir failed on "" + dstdir);
          }
        }
        fs.rename(src, dst);
        LOG.debug("" moved "" + src + "" => "" + dst);
      } else {
        LOG.debug(""Could not move recovered edits from "" + src +
            "" as it doesn't exist"");
      }
    }
    archiveLogs(null, corruptedLogs, processedLogs,
        oldLogDir, fs, conf);
{code}",ram_krish,ram_krish,Major,Closed,Fixed,18/May/12 15:41,26/Feb/13 08:16
Bug,HBASE-6054,12556460,0.92 failing because of missing commons-io after upgrade to hadoop 1.0.3.,See this note: http://search-hadoop.com/m/0UrOr19BG8v1/test+failure+after+upgrading+to+hadoop+1.0.3+Was%253A+ClassNotFoundException%253A+org.apache.commons.io.FileUtils&subj=test+failure+after+upgrading+to+hadoop+1+0+3+Was+ClassNotFoundException+org+apache+commons+io+FileUtils,stack,stack,Major,Closed,Fixed,18/May/12 21:41,20/Nov/15 11:53
Bug,HBASE-6056,12556506,Restore hbase-default version check,Was removed by mistake.,stack,stack,Major,Closed,Fixed,19/May/12 05:15,12/Oct/12 05:36
Bug,HBASE-6059,12556656,Replaying recovered edits would make deleted data exist again,"When we replay recovered edits, we used the minSeqId of Store, It may cause deleted data appeared again.

Let's see how it happens. Suppose the region with two families(cf1,cf2)

1.put one data to the region (put r1,cf1:q1,v1)

2.move the region from server A to server B.

3.delete the data put by step 1(delete r1)

4.flush this region.

5.make major compaction for this region

6.move the region from server B to server A.

7.Abort server A

8.After the region is online, we could get the deleted data(r1,cf1:q1,v1)
(When we replay recovered edits, we used the minSeqId of Store, because cf2 has no store files, so its seqId is 0, so the edit log of put data will be replayed to the region)


",zjushch,zjushch,Major,Closed,Fixed,21/May/12 08:23,23/Sep/13 18:30
Bug,HBASE-6060,12556729,Regions's in OPENING state from failed regionservers takes a long time to recover,"we have seen a pattern in tests, that the regions are stuck in OPENING state for a very long time when the region server who is opening the region fails. My understanding of the process: 
 
 - master calls rs to open the region. If rs is offline, a new plan is generated (a new rs is chosen). RegionState is set to PENDING_OPEN (only in master memory, zk still shows OFFLINE). See HRegionServer.openRegion(), HMaster.assign()
 - RegionServer, starts opening a region, changes the state in znode. But that znode is not ephemeral. (see ZkAssign)
 - Rs transitions zk node from OFFLINE to OPENING. See OpenRegionHandler.process()
 - rs then opens the region, and changes znode from OPENING to OPENED
 - when rs is killed between OPENING and OPENED states, then zk shows OPENING state, and the master just waits for rs to change the region state, but since rs is down, that wont happen. 
 - There is a AssignmentManager.TimeoutMonitor, which does exactly guard against these kind of conditions. It periodically checks (every 10 sec by default) the regions in transition to see whether they timedout (hbase.master.assignment.timeoutmonitor.timeout). Default timeout is 30 min, which explains what you and I are seeing. 
 - ServerShutdownHandler in Master does not reassign regions in OPENING state, although it handles other states. 

Lowering that threshold from the configuration is one option, but still I think we can do better. 

Will investigate more. ",jxiang,enis,Major,Closed,Fixed,21/May/12 18:53,10/Jan/15 01:38
Bug,HBASE-6063,12556781,Replication related failures on trunk after HBASE-5453,"HBASE-5453 added this line:
{code}
return ClusterId.parseFrom(data).toString();
{code}

in function:
public static String readClusterIdZNode(ZooKeeperWatcher watcher)

but this is not implemented, so you get log messages like:
2012-05-21 16:46:31,256 ERROR [RegionServer:0;cloudera-vm,60456,1337643971995-EventThread] zookeeper.ClientCnxn$EventThread(523): Error while calling watcher 
java.lang.IllegalArgumentException: Invalid UUID string: org.apache.hadoop.hbase.ClusterId@5563d208
	at java.util.UUID.fromString(UUID.java:204)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.init(ReplicationSource.java:192)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getReplicationSource(ReplicationSourceManager.java:328)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.addSource(ReplicationSourceManager.java:206)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$PeersWatcher.nodeChildrenChanged(ReplicationSourceManager.java:505)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:300)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497)
2012-05-21 16:46:31,256 ERROR [RegionServer:0;cloudera-vm,50926,1337643981835-EventThread] zookeeper.ClientCnxn$EventThread(523): Error while calling watcher 

and replication fails because the ClusterId does not match what is expected.  Patch coming soon.",gchanan,gchanan,Major,Closed,Fixed,22/May/12 00:01,23/Sep/13 18:31
Bug,HBASE-6065,12556798,"Log for flush would append a non-sequential edit in the hlog, leading to possible data loss","After completing flush region, we will append a log edit in the hlog file through HLog#completeCacheFlush.

{code}
public void completeCacheFlush(final byte [] encodedRegionName,
      final byte [] tableName, final long logSeqId, final boolean isMetaRegion)
{
...
HLogKey key = makeKey(encodedRegionName, tableName, logSeqId,
            System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
...
}
{code}



when we make the hlog key, we use the seqId from the parameter, and it is generated by HLog#startCacheFlush,
Here, we may append a lower seq id edit than the last edit in the hlog file.

If it is the last edit log in the file, it may cause data loss.
because 
{code}

HRegion#replayRecoveredEditsIfAny{
...
maxSeqId = Math.abs(Long.parseLong(fileName));
      if (maxSeqId <= minSeqId) {
        String msg = ""Maximum sequenceid for this log is "" + maxSeqId
            + "" and minimum sequenceid for the region is "" + minSeqId
            + "", skipped the whole file, path="" + edits;
        LOG.debug(msg);
        continue;
      }
...
}
{code}

We may skip the splitted log file, because we use the lase edit's seq id as its file name, and consider this seqId as the max seq id in this log file.",zjushch,zjushch,Critical,Closed,Fixed,22/May/12 03:29,26/Feb/13 08:16
Bug,HBASE-6068,12556956,Secure HBase cluster : Client not able to call some admin APIs,"In case of secure cluster, we allow the HBase clients to read the zk nodes by providing the global read permissions to all for certain nodes. These nodes are the master address znode, root server znode and the clusterId znode. In ZKUtil.createACL() , we can see these node names are specially handled.
But there are some other client side admin APIs which makes a read call into the zookeeper from the client. This include the isTableEnabled() call (May be some other. I have seen this).  Here the client directly reads a node in the zookeeper ( node created for this table ) and the data is matched to know whether this is enabled or not.
Now in secure cluster case any client can read zookeeper nodes which it needs for its normal operation like the master address and root server address.  But what if the client calls this API? [isTableEnaled () ].",mbertozzi,anoopsamjohn,Major,Closed,Fixed,23/May/12 04:25,12/Oct/12 05:36
Bug,HBASE-6069,12556992,TableInputFormatBase#createRecordReader() doesn't initialize TableRecordReader which causes NPE,"While running Hive(0.9.0) query over HBase(0.94.0) with hive-hbase-handler, there always throws a Null Pointer Exception on Scanner object. Since the TableInputFormatBase#createRecordReader() missed the initialization of TableRecordReader object. The scanner will be null in that case. This issue causes Hive query fails.",grace.huang,grace.huang,Critical,Closed,Fixed,23/May/12 12:26,17/Feb/14 01:23
Bug,HBASE-6070,12557000,AM.nodeDeleted and SSH races creating problems for regions under SPLIT,"We tried to address the problems in Master restart and RS restart while SPLIT region is in progress as part of HBASE-5806.
While doing some more we found still there is one race condition.
-> Split has just started and the znode is in RS_SPLIT state.
-> RS goes down.
-> First call back for SSH comes.
-> As part of the fix for HBASE-5806 SSH knows that some region is in RIT.
-> But now nodeDeleted event comes for the SPLIt node and there we try to delete the RIT.
-> After this we try to see in the SSH whether any node is in RIT.  As we dont find the region in RIT the region is never assigned.

When we fixed HBASE-5806 step 6 happened first and then step 5 happened.  So we missed it.  Now we found that. Will come up with a patch shortly.",ram_krish,ram_krish,Major,Closed,Fixed,23/May/12 13:17,26/Feb/13 08:16
Bug,HBASE-6076,12557073,Improve h.r.global.memstore.upper(lower)Limit description,"hbase.regionserver.global.memstore.upper(lower)Limit settings documentation (hbase-default.xml) may be misleading. It mentions that:
* flushes are forced *and updates are blocked* when memstore size reaches hbase.regionserver.global.memstore.upperLimit. In this case flushes are forced and updates are blocked until memstore size is less than hbase.regionserver.global.memstore.lowerLimit.

But it doesn't mention this:
* flushes are forced when memstore size hits hbase.regionserver.global.memstore.lowerLimit",alexb,alexb,Trivial,Closed,Fixed,23/May/12 21:21,13/Jun/22 16:42
Bug,HBASE-6084,12557110,Server Load does not display correctly on the ui,The ui uses the toString method and toString does not implement it any more.,eclark,eclark,Major,Closed,Fixed,24/May/12 00:17,23/Sep/13 18:30
Bug,HBASE-6087,12557129,Add hbase-common module,"Add an hbase-common module so common/utility classes can be pulled up out of hbase-server. This is _not_ the moving of classes, just the general project setup.",jesse_yates,jesse_yates,Major,Closed,Fixed,24/May/12 05:48,15/Oct/13 04:46
Bug,HBASE-6088,12557439, Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node,"Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node

{noformat}
2012-05-24 01:45:41,363 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26668ms for sessionid 0x1377a75f41d0012, closing socket connection and attempting reconnect
2012-05-24 01:45:41,464 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/unassigned/bd1079bf948c672e493432020dc0e144
{noformat}

{noformat}
2012-05-24 01:45:43,300 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: cleanupCurrentWriter  waiting for transactions to get synced  total 189377 synced till here 189365
2012-05-24 01:45:48,474 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.; Failed setting SPLITTING znode on ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.
java.io.IOException: Failed setting SPLITTING znode on ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:242)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:450)
	at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:67)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/unassigned/bd1079bf948c672e493432020dc0e144
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1246)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:321)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:659)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:811)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:747)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.transitionNodeSplitting(SplitTransaction.java:919)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createNodeSplitting(SplitTransaction.java:869)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:239)
	... 5 more
2012-05-24 01:45:48,476 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Successful rollback of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.
{noformat}


{noformat}
2012-05-24 01:47:28,141 ERROR org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/unassigned/bd1079bf948c672e493432020dc0e144 already exists and this is not a retry
2012-05-24 01:47:28,142 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.; Failed create of ephemeral /hbase/unassigned/bd1079bf948c672e493432020dc0e144
java.io.IOException: Failed create of ephemeral /hbase/unassigned/bd1079bf948c672e493432020dc0e144
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createNodeSplitting(SplitTransaction.java:865)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:239)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:450)
	at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:67)
{noformat}

Due to the above exception, region splitting was failing contineously more than 5hrs",rajesh23,gopinathan.av,Major,Closed,Fixed,24/May/12 13:32,26/Feb/13 08:15
Bug,HBASE-6089,12557530,SSH and AM.joinCluster causes Concurrent Modification exception.,AM.regions map is parallely accessed in SSH and Master initialization leading to ConcurrentModificationException.,rajesh23,ram_krish,Major,Closed,Fixed,24/May/12 15:54,26/Feb/13 08:16
Bug,HBASE-6095,12558137,ActiveMasterManager NullPointerException,"It is for 0.94 and 0.92.  Trunk doesn't have the issue.

{code}
      byte [] bytes =
        ZKUtil.getDataAndWatch(watcher, watcher.masterAddressZNode);
      // TODO: redo this to make it atomic (only added for tests)
      ServerName master = ServerName.parseVersionedServerName(bytes);
{code}

bytes could be null.",jxiang,jxiang,Minor,Closed,Fixed,25/May/12 16:33,12/Oct/12 05:36
Bug,HBASE-6097,12558149,TestHRegion.testBatchPut is flaky on 0.92,"If I run this test in a loop, I get failures like the following:

Error Message:
expected:<1> but was:<2>

Stack Trace:
junit.framework.AssertionFailedError: expected:<1> but was:<2>
at junit.framework.Assert.fail(Assert.java:50)
at junit.framework.Assert.failNotEquals(Assert.java:287)
at junit.framework.Assert.assertEquals(Assert.java:67)
at junit.framework.Assert.assertEquals(Assert.java:134)
at junit.framework.Assert.assertEquals(Assert.java:140)
at org.apache.hadoop.hbase.regionserver.TestHRegion.testBatchPut(TestHRegion.java:536)
",gchanan,gchanan,Major,Closed,Fixed,25/May/12 17:48,20/Nov/15 11:55
Bug,HBASE-6107,12558194,Distributed log splitting hangs even there is no task under /hbase/splitlog,"Sometimes, master web UI shows the distributed log splitting is going on, waiting for one last task to be done.  However, in ZK, there is no task under /hbase/splitlog at all.",jxiang,jxiang,Major,Closed,Fixed,25/May/12 20:59,23/Sep/13 18:30
Bug,HBASE-6108,12558204,Use HRegion.closeHRegion instead of HRegion.close() and HRegion.getLog().close(),"There are a bunch of places in the code like this:
region.close();
region.getLog().closeAndDelete();

Instead of the better:
HRegion.closeHRegion(region);

We should change these for a few reasons:
1) If we ever need to change the implementation, it's easier to change in one place
2) closeHRegion properly checks for nulls.  There are a few places where this could make a difference, for example in TestOpenedRegionHandler.java it's possible that an exception can be thrown before ""region"" is assigned and thus region.close() could throw an NPE.  closeHRegion avoids this issue.",gchanan,gchanan,Minor,Closed,Fixed,25/May/12 22:04,23/Sep/13 18:31
Bug,HBASE-6110,12558262,Fix TestInfoServers,"With the recent port to modules, we broke a couple of tests, including this one. The fix needs to ensure that the webapp still works from the in-situ and packaged running of HBase.",jesse_yates,jesse_yates,Major,Closed,Fixed,27/May/12 01:49,13/Jun/22 16:42
Bug,HBASE-6112,12558278,Fix hadoop-2.0 build,Some of the pom definitions are broken for the Hadoop 2.0 build. Its breaking the build for that version,jesse_yates,jesse_yates,Major,Closed,Fixed,27/May/12 16:31,23/Sep/13 18:30
Bug,HBASE-6113,12558282,[eclipse] Fix eclipse import of hbase-assembly null pointer,"occasionally, eclipse will throw a null pointer when attempting to import all the modules via m2eclipse.",jesse_yates,jesse_yates,Major,Closed,Fixed,27/May/12 18:47,23/Sep/13 18:31
Bug,HBASE-6115,12558308,NullPointerException is thrown when root and meta table regions are assigning to another RS.,"Lets suppose we have two region servers RS1 and RS2.
If region server (RS1) having root and meta regions went down, master will assign them to another region server RS2. At that time recieved NullPointerException.
{code}
2012-05-04 20:19:52,912 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@25de152f; serverName=
2012-05-04 20:19:52,914 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@25de152f; serverName=
2012-05-04 20:19:52,916 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=1028785192
java.lang.NullPointerException
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1483)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1367)
at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:945)
at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:801)
at org.apache.hadoop.hbase.client.HTable.put(HTable.java:776)
at org.apache.hadoop.hbase.catalog.MetaEditor.put(MetaEditor.java:98)
at org.apache.hadoop.hbase.catalog.MetaEditor.putToCatalogTable(MetaEditor.java:88)
at org.apache.hadoop.hbase.catalog.MetaEditor.updateLocation(MetaEditor.java:259)
at org.apache.hadoop.hbase.catalog.MetaEditor.updateMetaLocation(MetaEditor.java:221)
at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1625)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:241)
2012-05-04 20:19:52,920 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing .META.,,1.1028785192: disabling compactions & flushes
2012-05-04 20:19:52,920 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region .META.,,1.1028785192

{code}",ram_krish,rajesh23,Minor,Closed,Fixed,28/May/12 07:01,12/Oct/12 05:36
Bug,HBASE-6117,12558324,Revisit default condition added to Switch cases in Trunk,We found that in some cases the default case in switch block was just throwing illegalArg Exception. There are cases where we may get some other state for which we should not throw IllegalArgException.,ram_krish,ram_krish,Major,Closed,Fixed,28/May/12 10:06,23/Sep/13 18:30
Bug,HBASE-6119,12558352,Region server logs its own address at the end of getMaster(),"I saw the following in region server log where a.ebay.com is region server itself:
{code}
2012-05-28 08:56:35,315 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Connected to master at a.ebay.com/10.115.13.20:60020
{code}
We should be logging the address of master",zhihyu@ebaysf.com,zhihyu@ebaysf.com,Minor,Closed,Fixed,28/May/12 16:10,23/Sep/13 18:31
Bug,HBASE-6122,12558459,Backup master does not become Active master after ZK exception,"-> Active master gets ZK expiry exception.
-> Backup master becomes active.
-> The previous active master retries and becomes the back up master.
Now when the new active master goes down and the current back up master comes up, it goes down again with the zk expiry exception it got in the first step.

{code}
if (abortNow(msg, t)) {
      if (t != null) LOG.fatal(msg, t);
      else LOG.fatal(msg);
      this.abort = true;
      stop(""Aborting"");
    }
{code}
In ActiveMasterManager.blockUntilBecomingActiveMaster we try to wait till the back up master becomes active. 
{code}
    synchronized (this.clusterHasActiveMaster) {
      while (this.clusterHasActiveMaster.get() && !this.master.isStopped()) {
        try {
          this.clusterHasActiveMaster.wait();
        } catch (InterruptedException e) {
          // We expect to be interrupted when a master dies, will fall out if so
          LOG.debug(""Interrupted waiting for master to die"", e);
        }
      }
      if (!clusterStatusTracker.isClusterUp()) {
        this.master.stop(""Cluster went down before this master became active"");
      }
      if (this.master.isStopped()) {
        return cleanSetOfActiveMaster;
      }
      // Try to become active master again now that there is no active master
      blockUntilBecomingActiveMaster(startupStatus,clusterStatusTracker);
    }
    return cleanSetOfActiveMaster;
{code}
When the back up master (it is in back up mode as he got ZK exception), once again tries to come to active we don't get the return value that comes out from 
{code}
// Try to become active master again now that there is no active master
      blockUntilBecomingActiveMaster(startupStatus,clusterStatusTracker);
{code}
We tend to return the 'cleanSetOfActiveMaster' which was previously false.
Now because of this instead of again becoming active the back up master goes down in the abort() code.  Thanks to Gopi,my colleague for reporting this issue.",ram_krish,ram_krish,Major,Closed,Fixed,29/May/12 17:08,12/Oct/12 05:36
Bug,HBASE-6123,12558466,dev-support/test-patch.sh should compile against hadoop 2.0.0-alpha instead of hadoop 0.23,"test-patch.sh currently does this:
{code}
  $MVN clean test -DskipTests -Dhadoop.profile=23 -D${PROJECT_NAME}PatchProcess > $PATCH_DIR/trunk23JavacWarnings.txt 2>&1
{code}
we should compile against hadoop 2.0.0-alpha",zhihyu@ebaysf.com,zhihyu@ebaysf.com,Major,Closed,Fixed,29/May/12 17:59,13/Jun/22 16:45
Bug,HBASE-6126,12558493,Fix broke TestLocalHBaseCluster in 0.92/0.94,Related to HBASE-6100,stack,stack,Major,Closed,Fixed,29/May/12 20:49,12/Oct/12 05:36
Bug,HBASE-6132,12558520,ColumnCountGetFilter & PageFilter not working with FilterList,"Thanks to Anoop and Ramkrishna, here's what we found with FilterList

If I use FilterList to include ColumnCountGetFilter among other filters, the returning Result has no keyvalues.

This problem seems to occur when specified column count is less then actual number of existing columns.

Also same problem arises with PageFilter

Following is the code of the problem:

{code}
Configuration conf = HBaseConfiguration.create();
HTable table = new HTable(conf, ""test"");
Get get = new Get(Bytes.toBytes(""test00001""));
FilterList filterList = new FilterList();
filterList.addFilter(new ColumnCountGetFilter(100));           
get.setFilter(filterList);
Result r = table.get(get);
System.out.println(r.size()); // prints zero
{code}",anoopsamjohn,benkimkimben,Major,Closed,Fixed,30/May/12 01:09,23/Mar/13 04:53
Bug,HBASE-6133,12558536,TestRestartCluster failing in 0.92,This test failed a few times just now on 0.92 branch.  Seems pretty basic failure.  The master is not up yet and its throwing PleaseHoldException.,stack,stack,Major,Closed,Fixed,30/May/12 04:35,12/Oct/12 05:36
Bug,HBASE-6138,12558720,HadoopQA not running findbugs [Trunk],"HadoopQA shows like
 -1 findbugs.  The patch appears to cause Findbugs (version 1.3.9) to fail.
But not able to see any reports link

When I checked the console output for the build I can see
{code}
[INFO] --- findbugs-maven-plugin:2.4.0:findbugs (default-cli) @ hbase-common ---
[INFO] Fork Value is true
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] HBase ............................................. SUCCESS [1.890s]
[INFO] HBase - Common .................................... FAILURE [2.238s]
[INFO] HBase - Server .................................... SKIPPED
[INFO] HBase - Assembly .................................. SKIPPED
[INFO] HBase - Site ...................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 4.856s
[INFO] Finished at: Thu May 31 03:35:35 UTC 2012
[INFO] Final Memory: 23M/154M
[INFO] ------------------------------------------------------------------------
[ERROR] Could not find resource '${parent.basedir}/dev-support/findbugs-exclude.xml'. -> [Help 1]
[ERROR] 
{code}
Because of this error Findbugs is getting run!

",jesse_yates,anoopsamjohn,Major,Closed,Fixed,31/May/12 04:34,23/Sep/13 18:31
Bug,HBASE-6141,12558844,InterfaceAudience breaks 0.94 on older versions of hadoop,,eclark,eclark,Major,Closed,Fixed,31/May/12 19:06,12/Oct/12 05:36
Bug,HBASE-6146,12558921,Disabling of Catalog tables should not be allowed,"HBaseAdmin#disableTable() when called with META or ROOT table, it will pass the disable instruction to Master and table is actually getting disabled. Later this API call will fail as there is a call to HBaseAdmin#isTableDisabled() which is having a check like isLegalTableName(tableName).So this call makes the catalog table to be in disabled state.

We can have same kind of isLegalTableName(tableName) checks in disableTable() and enableTable() APIs",anoopsamjohn,anoopsamjohn,Major,Closed,Fixed,01/Jun/12 06:56,26/Feb/13 08:16
Bug,HBASE-6149,12559006,Fix TestFSUtils creating dirs under top level dir,,stack,stack,Major,Closed,Fixed,01/Jun/12 20:42,23/Sep/13 18:30
Bug,HBASE-6150,12559016,Remove empty files causing rat check fail,Set of empty files found by Jesse.,stack,stack,Major,Closed,Fixed,01/Jun/12 22:03,23/Sep/13 18:31
Bug,HBASE-6156,12559157,Improve multiop performances in HTable#flushCommits,"This code:

{noformat}
  @Override
  public void flushCommits() throws IOException {
    try {
      Object[] results = new Object[writeBuffer.size()];
      try {
        this.connection.processBatch(writeBuffer, tableName, pool, results);
      } catch (InterruptedException e) {
        throw new IOException(e);
      } finally {
        // mutate list so that it is empty for complete success, or contains
        // only failed records results are returned in the same order as the
        // requests in list walk the list backwards, so we can remove from list
        // without impacting the indexes of earlier members
        for (int i = results.length - 1; i>=0; i--) {
          if (results[i] instanceof Result) {
            // successful Puts are removed from the list here.
            writeBuffer.remove(i);
          }
        }
      }
    } finally {
      if (clearBufferOnFail) {
        writeBuffer.clear();
        currentWriteBufferSize = 0;
      } else {
        // the write buffer was adjusted by processBatchOfPuts
        currentWriteBufferSize = 0;
        for (Put aPut : writeBuffer) {
          currentWriteBufferSize += aPut.heapSize();
        }
      }
    }
  }
{noformat}

Can be improved by:
- not iterating on the list if clearBufferOnFail is set
- not iterating the the list of there are no error
- iterating on the list only once instead of two when we really have to.",nkeywal,nkeywal,Minor,Closed,Fixed,04/Jun/12 10:05,23/Sep/13 18:31
Bug,HBASE-6158,12559239,Data loss if the words 'merges' or 'splits' are used as Column Family name,"If a table is creates with either 'merges' or 'splits' as one of the Column Family name it can never be flushed to the disk even though the table creation (and data population) succeeds.

The reason for this is that these two are used as temporary directory names inside the region folder or merge and splits respectively and hence conflicts with the directories created for CF with same name.

A simple fix would be to uses "".merges' and "".splits"" as the working folder (patch attached). This will also be consistent with other work folder names. An alternate fix would be to declare these words (and other similar) as reserve words and throw exception when they are used. However, I do find the alternate approach as unnecessarily restrictive.",adityakishore,adityakishore,Major,Closed,Fixed,04/Jun/12 18:11,12/Oct/12 05:36
Bug,HBASE-6160,12559264,META entries from daughters can be deleted before parent entries,"HBASE-5986 fixed and issue, where the client sees the META entry for the parent, but not the children. However, after the fix, we have seen the following issue in tests: 

Region A is split to -> B, C
Region B is split to -> D, E

After some time, META entry for B is deleted since it is not needed anymore, but META entry for Region A stays in META (C still refers it). In this case, the client throws RegionOfflineException for B. ",enis,enis,Major,Closed,Fixed,04/Jun/12 20:21,12/Oct/12 05:36
Bug,HBASE-6164,12559428,Correct the bug in block encoding usage in bulkload,"Address the issue raised under HBASE-6040
https://issues.apache.org/jira/browse/HBASE-6040?focusedCommentId=13289334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13289334",anoopsamjohn,anoopsamjohn,Major,Closed,Fixed,05/Jun/12 18:34,26/Feb/13 08:16
Bug,HBASE-6165,12559443,Replication can overrun .META. scans on cluster re-start,When restarting a large set of regions on a reasonably small cluster the replication from another cluster tied up every xceiver meaning nothing could be onlined.,v.himanshu,eclark,Major,Closed,Fixed,05/Jun/12 21:15,07/Apr/13 04:35
Bug,HBASE-6167,12559458,Fix xinclude for docs broke when we multi-moduled,,stack,stack,Major,Closed,Fixed,05/Jun/12 22:20,23/Sep/13 18:30
Bug,HBASE-6175,12559584,TestFSUtils flaky on hdfs getFileStatus method,"This is a simplified version of a TestFSUtils issue: a sleep and the test works 100% of the time. No sleep and it becomes flaky. Root cause unknown. While the issue appears on the tests, the root cause could be an issue on real production system as well.

{noformat}
@Test
 public void testFSUTils() throws Exception {
   final String hosts[] = {""host1"", ""host2"", ""host3"", ""host4""};
   Path testFile = new Path(""/test1.txt"");

   HBaseTestingUtility htu = new HBaseTestingUtility();

   try {
     htu.startMiniDFSCluster(hosts).waitActive();
     FileSystem fs = htu.getDFSCluster().getFileSystem();

     for (int i = 0; i < 100; ++i) {
       FSDataOutputStream out = fs.create(testFile);
       byte[] data = new byte[1];
       out.write(data, 0, 1);
       out.close();

       // Put a sleep here to make me work
       //Thread.sleep(2000);

       FileStatus status = fs.getFileStatus(testFile);
       HDFSBlocksDistribution blocksDistribution =
         FSUtils.computeHDFSBlocksDistribution(fs, status, 0, status.getLen());
       assertEquals(""Wrong number of hosts distributing blocks. at
iteration ""+i, 3,
         blocksDistribution.getTopHosts().size());

       fs.delete(testFile, true);
     }
   } finally {
     htu.shutdownMiniDFSCluster();
   }
 }
{noformat}",nkeywal,nkeywal,Trivial,Closed,Fixed,06/Jun/12 18:17,26/Feb/13 08:22
Bug,HBASE-6176,12559598,Take down the stargate reset doc or add pointer to manual documentation,Its confusing folks having it still up at http://wiki.apache.org/hadoop/Hbase/Stargate (Just tripped over a confused individual in irc).,,stack,Major,Closed,Fixed,06/Jun/12 21:01,13/Jun/22 16:48
Bug,HBASE-6178,12559602,LoadTest tool no longer packaged after the modularization,,jesse_yates,eclark,Major,Closed,Fixed,06/Jun/12 21:24,23/Sep/13 18:30
Bug,HBASE-6179,12559609,Fix stylesheet broke since multimodule and address feedback gotten in new comment system,,stack,stack,Major,Closed,Fixed,06/Jun/12 21:54,23/Sep/13 18:30
Bug,HBASE-6185,12559642,Update javadoc for ConstantSizeRegionSplitPolicy class,"When using hbase0.94.0 we met a strange problem.
We config the 'hbase.hregion.max.filesize' to 100Gb (The recommed value to act as auto-split turn off). 
{code:xml}
<property>
  <name>hbase.hregion.max.filesize</name>
  <value>107374182400</value>
</property>
{code}
Then we keep putting datas into a table.
But when the data size far more less than 100Gb(about 500~600 uncompressed datas), the table auto splte to 2 regions...

I change the log4j config to DEBUG, and saw logs below:
{code}
2012-06-07 10:30:52,161 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~128.0m/134221272, currentsize=1.5m/1617744 for region FileStructIndex,,1339032525500.7b229abcd0785408251a579e9bdf49c8. in 3201ms, sequenceid=176387980, compaction requested=false
2012-06-07 10:30:52,161 DEBUG org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy: ShouldSplit because info size=138657416, sizeToCheck=134217728, regionsWithCommonTable=1
2012-06-07 10:30:52,161 DEBUG   org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy: ShouldSplit because info size=138657416, sizeToCheck=134217728, regionsWithCommonTable=1
2012-06-07 10:30:52,240 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Split requested for FileStructIndex,,1339032525500.7b229abcd0785408251a579e9bdf49c8..  compaction_queue=(0:0), split_queue=0
2012-06-07 10:30:52,265 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region FileStructIndex,,1339032525500.7b229abcd0785408251a579e9bdf49c8.
2012-06-07 10:30:52,265 DEBUG org.apache.hadoop.hbase.regionserver.SplitTransaction: regionserver:60020-0x137c4929efe0001 Creating ephemeral node for 7b229abcd0785408251a579e9bdf49c8 in SPLITTING state
2012-06-07 10:30:52,368 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x137c4929efe0001 Attempting to transition node 7b229abcd0785408251a579e9bdf49c8 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
2012-06-07 10:30:52,382 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x137c4929efe0001 Successfully transitioned node 7b229abcd0785408251a579e9bdf49c8 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
2012-06-07 10:30:52,410 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing FileStructIndex,,1339032525500.7b229abcd0785408251a579e9bdf49c8.: disabling compactions & flushes
2012-06-07 10:30:52,410 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; FileStructIndex,,1339032525500.7b229abcd0785408251a579e9bdf49c8. is closing
2012-06-07 10:30:52,411 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; FileStructIndex,,1339032525500.7b229abcd0785408251a579e9bdf49c8. is closing
{code}

{color:red}IncreasingToUpperBoundRegionSplitPolicy: ShouldSplit because info size=138657416, sizeToCheck=134217728{color}
I did not config splitPolicy for hbase, so it means *IncreasingToUpperBoundRegionSplitPolicy is the default splitPolicy of 0.94.0*

After add
{code:xml}
<property>
    <name>hbase.regionserver.region.split.policy</name>
    <value>org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy</value>
</property>
{code}
autosplit did not happen again and everything goes well.


But we can still see javadoc on ConstantSizeRegionSplitPolicy, it says 'This is the default split policy'. Or even in the http://hbase.apache.org/book/regions.arch.html 9.7.4.1. Custom Split Policies, 'default split policy: ConstantSizeRegionSplitPolicy.'.

Those may mistaken us that if we set hbase.hregion.max.filesize to 100Gb, than the auto-split can be almost shutdown.
You may change those docs, and What more, in many scenerys, we actually need to control split manually（As you know when spliting the table are offline, reads and writes will fail） 

 
",,nneverwei,Major,Closed,Fixed,07/Jun/12 05:59,12/Oct/12 05:36
Bug,HBASE-6195,12560111,Increment data will be lost when the memstore is flushed,"There are two problems in increment() now:
First:
I see that the timestamp(the variable now) in HRegion's Increment() is generated before got the rowLock, so when there are multi-thread increment the same row, although it generate earlier, it may got the lock later. Because increment just store one version, so till now, the result will still be right.

When the region is flushing, these increment will read the kv from snapshot and memstore with whose timestamp is larger, and write it back to memstore. If the snapshot's timestamp larger than the memstore, the increment will got the old data and then do the increment, it's wrong.

Secondly:
Also there is a risk in increment. Because it writes the memstore first and then HLog, so if it writes HLog failed, the client will also read the incremented value.",xingshi,shixing,Major,Closed,Fixed,11/Jun/12 03:00,04/Dec/13 17:47
Bug,HBASE-6197,12560183,HRegion's append operation may lose data,"Like the HBASE-6195, when flushing the append thread will read out the old value for the larger timestamp in snapshot and smaller timestamp in memstore.

We Should make the first-in-thread generates the smaller timestamp.",xingshi,shixing,Major,Closed,Fixed,11/Jun/12 15:23,23/Sep/13 18:30
Bug,HBASE-6200,12560247,KeyComparator.compareWithoutRow can be wrong when families have the same prefix,"As reported by Desert Rose on IRC and on the ML, {{Result}} has a weird behavior when some families share the same prefix. He posted a link to his code to show how it fails, http://pastebin.com/7TBA1XGh

Basically {{KeyComparator.compareWithoutRow}} doesn't differentiate families and qualifiers so ""f:a"" is said to be bigger than ""f1:"", which is false. Then what happens is that the KVs are returned in the right order from the RS but then doing {{Result.binarySearch}} it uses {{KeyComparator.compareWithoutRow}} which has a different sorting so the end result is undetermined.

I added some debug and I can see that the data is returned in the right order but {{Arrays.binarySearch}} returned the wrong KV, which is then verified agains the passed family and qualifier which fails so null is returned.

I don't know how frequent it is for users to have families with the same prefix, but those that do have that and that use those families at the same time will have big correctness issues. This is why I mark this as a blocker.",jeason,jdcryans,Blocker,Closed,Fixed,11/Jun/12 21:37,18/Sep/13 20:00
Bug,HBASE-6201,12560281,HBase integration/system tests,"Integration and general system tests have been discussed previously, and the conclusion is that we need to unify how we do ""release candidate"" testing (HBASE-6091).

In this issue, I would like to discuss and agree on a general plan, and open subtickets for execution so that we can carry out most of the tests in HBASE-6091 automatically. 

Initially, here is what I have in mind: 

1. Create hbase-it (or hbase-tests) containing forward port of HBASE-4454 (without any tests). This will allow integration test to be run with
 {code}
  mvn verify
 {code}

2. Add ability to run all integration/system tests on a given cluster. Smt like: 
 {code}
  mvn verify -Dconf=/etc/hbase/conf/
 {code}
should run the test suite on the given cluster. (Right now we can launch some of the tests (TestAcidGuarantees) from command line). Most of the system tests will be client side, and interface with the cluster through public APIs. We need a tool on top of MiniHBaseCluster or improve HBaseTestingUtility, so that tests can interface with the mini cluster or the actual cluster uniformly.

3. Port candidate unit tests to the integration tests module. Some of the candidates are: 
 - TestAcidGuarantees / TestAtomicOperation
 - TestRegionBalancing (HBASE-6053)
 - TestFullLogReconstruction
 - TestMasterFailover
 - TestImportExport
 - TestMultiVersions / TestKeepDeletes
 - TestFromClientSide
 - TestShell and src/test/ruby
 - TestRollingRestart
 - Test**OnCluster
 - Balancer tests

These tests should continue to be run as unit tests w/o any change in semantics. However, given an actual cluster, they should use that, instead of spinning a mini cluster.  

4. Add more tests, especially, long running ingestion tests (goraci, BigTop's TestLoadAndVerify, LoadTestTool), and chaos monkey style fault tests. 

All suggestions welcome. ",enis,enis,Major,Closed,Fixed,12/Jun/12 02:33,20/Nov/15 11:53
Bug,HBASE-6210,12560682,Backport HBASE-6197 to 0.94,Backport HBASE-6197 'HRegion's append operation may lose data' and the accompanying HBASE-6195 to 0.94 and 0.92,ram_krish,stack,Blocker,Closed,Fixed,14/Jun/12 20:28,12/Oct/12 05:36
Bug,HBASE-6211,12560692,Put latencies in jmx,,eclark,eclark,Major,Closed,Fixed,14/Jun/12 21:54,07/Apr/13 04:35
Bug,HBASE-6219,12560844,New UI elements may request external resources,"After HBASE-6135, the UI may, depending on browser version, attempt to pull in external resources, e.g. from MasterStatusTmpl.jamon:

{code}
    <!--[if lt IE 9]>
      <script src=""http://html5shim.googlecode.com/svn/trunk/html5.js""></script>
    <![endif]-->
{code}

This won't work if the UI is viewed in a restricted environment. Also, pulling external resources from Googlecode / SVN like this seems not a good practice, those can change at any given third party checkin.

Rather, we should pull in any resources needed into our /static/ ?",eclark,apurtell,Blocker,Closed,Fixed,15/Jun/12 22:50,13/Jun/22 16:50
Bug,HBASE-6220,12560849,PersistentMetricsTimeVaryingRate gets used for non-time-based metrics,"PersistentMetricsTimeVaryingRate gets used for metrics that are not time-based, leading to confusing names such as ""avg_time"" for compaction size, etc.  You hav to read the code in order to understand that this is actually referring to bytes, not seconds.",paulcavallaro,dsw,Minor,Closed,Fixed,15/Jun/12 23:59,13/Jun/22 16:44
Bug,HBASE-6227,12593995,SSH and cluster startup  causes data loss,"In AssignmentManager#processDeadServersAndRegionsInTransition, if servershutdownhandler is processing and master consider it cluster startup, master will assign all user regions, however, servershutdownhandler has not completed splitting logs.

Let me describe it in more detail.

Suppose there are two regionservers A1 and B1, A1 carried META and ROOT

1.master restart and completed assignRootAndMeta

2.A1 and B1 are both restarted, new regionservers are A2 and B2

3.SSH which processed for A1 completed assigning ROOT and META

4.master do rebuilding user regions and no regions added to master's region list

5.master consider it as a cluster startup, and assign all user regions

6.SSH which processed for B1 start to split B1's logs

7.All regions' data carried by B1 would loss.",zjushch,zjushch,Major,Closed,Fixed,18/Jun/12 03:47,26/Feb/13 08:16
Bug,HBASE-6229,12594927,AM.assign() should not set table state to ENABLED directly.,In case of assign from EnableTableHandler table state is ENABLING. Any how EnableTableHandler will set ENABLED after assigning all the the table regions. If we try to set to ENABLED directly then client api may think ENABLE table is completed. When we have a case like all the regions are added directly into META and we call assignRegion then we need to make the table ENABLED.  Hence in such case the table will not be in ENABLING or ENABLED state.,rajesh23,rajesh23,Major,Closed,Fixed,18/Jun/12 11:58,26/Feb/13 08:16
Bug,HBASE-6236,12595041,Offline meta repair fails if the HBase base mount point is on a different cluster/volume than its parent in a ViewFS or similar FS,"While building the .META. and \-ROOT\- from FS data alone (HBASE-4377), hbck tries to move the existing .META. and \-ROOT\- directories to a backup folder.

This backup folder is created at the same level as the base HBase folder (e.g. /hbase-xxxxxx if the base HBase folder is '/hbase').

In a federated HDFS like ViewFS and other similar FS implementations, it is not possible to rename files/directories across namespace volumes (ViewFS guide section 3.5) and as a result hbck crashes.

A solution to this problem is to create the backup directory under the folder where HBase base folder has been mounted. This ensures that source and destination of rename operation are on the same namespace volume.

Patch for 0.94 and trunk is attached for review. The patch modifies the location of the backup directory from '/hbase-xxxxxxx' to '/hbase/.hbcktmp-xxxxxxx'",adityakishore,adityakishore,Major,Closed,Fixed,19/Jun/12 01:56,26/Feb/13 08:16
Bug,HBASE-6237,12595043,Fix race on ACL table creation in TestTablePermissions,,apurtell,apurtell,Major,Closed,Fixed,19/Jun/12 02:28,26/Feb/13 08:16
Bug,HBASE-6239,12595151,[replication] ReplicationSink uses the ts of the first KV for the other KVs in the same row,"ReplicationSink assumes that all the KVs for the same row inside a WALEdit will have the same timestamp, which is not necessarily the case.

This only affects 0.90 and 0.92 since HBASE-5203 fixes it in 0.94",jdcryans,jdcryans,Critical,Closed,Fixed,19/Jun/12 19:13,20/Nov/15 11:53
Bug,HBASE-6240,12595185,Race in HCM.getMaster stalls clients,"I found this issue trying to run YCSB on 0.94, I don't think it exists on any other branch. I believe that this was introduced in HBASE-5058 ""Allow HBaseAdmin to use an existing connection"".

The issue is that in HCM.getMaster it does this recipe:

 # Check if the master is null and runs (if so, return)
 # Grab a lock on masterLock
 # nullify this.master
 # try to get a new master

The issue happens at 3, it should re-run 1 since while you're waiting on the lock someone else could have already fixed it for you. What happens right now is that the threads are all able to set the master to null before others are able to get out of getMaster and it's a complete mess.

Figuring it out took me some time because it doesn't manifest itself right away, silent retries are done in the background. Basically the first clue was this:

{noformat}
Error doing get: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=10, exceptions:
Tue Jun 19 23:40:46 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:47 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:48 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:49 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:51 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:53 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:57 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:41:01 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:41:09 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:41:25 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
{noformat}

This was caused by the little dance up in HBaseAdmin where it deletes ""stale"" connections... which are not stale at all.",ram_krish,jdcryans,Critical,Closed,Fixed,19/Jun/12 23:43,12/Oct/12 05:36
Bug,HBASE-6246,12595314,Admin.move without specifying destination does not go through AccessController,"{code}
    if (destServerName == null || destServerName.length == 0) {
      LOG.info(""Passed destination servername is null/empty so "" +
        ""choosing a server at random"");
      this.assignmentManager.clearRegionPlan(hri);
      // Unassign will reassign it elsewhere choosing random server.
      this.assignmentManager.unassign(hri);
{code}
I think we should go through security to see if there is sufficient permissions to do this operation?",rajesh23,ram_krish,Major,Closed,Fixed,20/Jun/12 18:05,12/Oct/12 05:36
Bug,HBASE-6248,12595337,"Jetty init may fail if directory name contains ""master""","InfoServer.getWebAppsPath() unsafely assumes that any instance of the string ""master"" in the full path to hbase-webapps can be truncated. This breaks in the case where hbase is installed in a directory such as ""/my/hbasemaster/"".

The result is that Jetty initialization will fail since the master determines an incorrect path to hbase-webapps. The master still runs but the web UI returns 503.

I have a patch for this problem that I'll upload soon.",dave_revell,dave_revell,Minor,Closed,Fixed,20/Jun/12 20:41,26/Feb/13 08:16
Bug,HBASE-6263,12595649,Use default mode for HBase Thrift gateway if not specified,"The Thrift gateway should start with a default mode if one is not selected. Currently, instead we see:

{noformat}
Exception in thread ""main"" java.lang.AssertionError: Exactly one option out of [-hsha, -nonblocking, -threadpool, -threadedselector] has to be specified
	at org.apache.hadoop.hbase.thrift.ThriftServerRunner$ImplType.setServerImpl(ThriftServerRunner.java:201)
	at org.apache.hadoop.hbase.thrift.ThriftServer.processOptions(ThriftServer.java:169)
	at org.apache.hadoop.hbase.thrift.ThriftServer.doMain(ThriftServer.java:85)
	at org.apache.hadoop.hbase.thrift.ThriftServer.main(ThriftServer.java:192)
{noformat}

See also BIGTOP-648. ",apurtell,apurtell,Minor,Closed,Fixed,22/Jun/12 23:33,05/Aug/14 20:11
Bug,HBASE-6265,12595773,Calling getTimestamp() on a KV in cp.prePut() causes KV not to be flushed,"There is an issue when you call getTimestamp() on any KV handed into a Coprocessor's prePut(). It initializes the internal ""timestampCache"" variable. 

When you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updateLatestStamp() call). 

The TimeRangeTracker then calls getTimestamp() later on to see if it has to include the KV, but instead of getting the proper time it sees the cached timestamp from the prePut() call.",larsgeorge,larsgeorge,Critical,Closed,Fixed,25/Jun/12 11:16,07/Feb/14 23:12
Bug,HBASE-6268,12595850,Can't enable a table on a 0.94 cluster from a 0.92 client,"In 0.92 we know a table's enabled by doing this in HCM.isEnabledTable:

bq. return getTableState(zkw, tableName) == null;

In 0.94 we do:

bq. return getTableState(zkw, tableName) == TableState.ENABLED;

So what happens is that the the 0.92 client will hang forever since the znode contains ENABLED instead of being absent.",yuzhihong@gmail.com,jdcryans,Major,Closed,Fixed,26/Jun/12 00:07,24/Oct/12 05:40
Bug,HBASE-6269,12595882,Lazyseek should use the maxSequenseId StoreFile's KeyValue as the latest KeyValue,"When I fix the bug HBASE-6195, there is happened to find sometimes the test case will fail, https://builds.apache.org/job/HBase-0.94/259/.

If there are two Put/Increment with same row, family, qualifier, timestamp and different memstoreTS, after each Put/Increment, we do a memstore flush. So there will be two StoreFile with same KeyValue(except memstoreTS and SequenceId).

When I got the row, I always got the old records, the test case like this:
{code}
  public void testPutWithMemStoreFlush() throws Exception {
    Configuration conf = HBaseConfiguration.create();
    String method = ""testPutWithMemStoreFlush"";
    byte[] tableName = Bytes.toBytes(method);
    byte[] family = Bytes.toBytes(""family"");;
    byte[] qualifier = Bytes.toBytes(""qualifier"");
    byte[] row = Bytes.toBytes(""putRow"");
    byte[] value = null;
    this.region = initHRegion(tableName, method, conf, family);
    Put put = null;
    Get get = null;
    List<KeyValue> kvs = null;
    Result res = null;
    
    put = new Put(row);
    value = Bytes.toBytes(""value0"");
    put.add(family, qualifier, 1234567l, value);
    region.put(put);
    System.out.print(""get value before flush after put value0 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }

    region.flushcache();
    
    System.out.print(""get value after flush after put value0 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    
    put = new Put(row);
    value = Bytes.toBytes(""value1"");
    put.add(family, qualifier, 1234567l, value);
    region.put(put);
    System.out.print(""get value before flush after put value1 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    region.flushcache();
    System.out.print(""get value after flush after put value1 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    
    put = new Put(row);
    value = Bytes.toBytes(""value2"");
    put.add(family, qualifier, 1234567l, value);
    region.put(put);
    System.out.print(""get value before flush after put value2 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    region.flushcache();
    System.out.print(""get value after flush after put value2 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    } 
  }
{code}
and the result print as followed:
{code}
get value before flush after put value0 : value0
get value after flush after put value0 : value0
get value before flush after put value1 : value1
get value after flush after put value1 : value0
get value before flush after put value2 : value2
get value after flush after put value2 : value0
{code}

I analyze the code for StoreFileScanner with lazy seek, the StoreFileScanners are sorted by SequenceId, so the latest StoreFile is on the top KeyValueHeap, and the KeyValue for latest StoreFile will comapre to the second latest StoreFile, but the second latest StoreFile generated the fake row for same row, family, qualifier excepts the timestamp( maximum), memstoreTS(0). And the latest KeyValue recognized as not latest than the second latest.",xingshi,xingshi,Major,Closed,Fixed,26/Jun/12 09:21,26/Feb/13 08:16
Bug,HBASE-6272,12595937,In-memory region state is inconsistent,"AssignmentManger stores region state related information in several places: regionsInTransition, regions (region info to server name map), and servers (server name to region info set map).  However the access to these places is not coordinated properly.  It leads to inconsistent in-memory region state information.  Sometimes, some region could even be offline, and not in transition.",jxiang,jxiang,Major,Closed,Fixed,26/Jun/12 16:56,23/Sep/13 18:31
Bug,HBASE-6276,12595991,TestClassLoading is racy,,,apurtell,Minor,Closed,Fixed,26/Jun/12 21:59,13/Jun/22 16:46
Bug,HBASE-6281,12596072,Assignment need not be called for disabling table regions during clean cluster start up.,"Currently during clean cluster start up if there are tables in DISABLING state, we do bulk assignment through assignAllUserRegions() and after region is OPENED in RS, master checks if the table is in DISBALING/DISABLED state (in Am.regionOnline) and again calls unassign.  This roundtrip can be avoided even before calling assignment.
This JIRA is to address the above scenario.",rajesh23,rajesh23,Major,Closed,Fixed,27/Jun/12 11:36,26/Feb/13 08:16
Bug,HBASE-6284,12596165,Introduce HRegion#doMiniBatchMutation(),"From Anoop under thread 'Can there be a doMiniBatchDelete in HRegion':

The HTable#delete(List<Delete>) groups the Deletes for the same RS and make one n/w call only. But within the RS, there will be N number of delete calls on the region one by one. This will include N number of HLog write and sync. If this also can be grouped can we get better performance for the multi row delete.

I have made the new miniBatchDelete () and made the HTable#delete(List<Delete>) to call this new batch delete.
Just tested initially with the one node cluster.  In that itself I am getting a performance boost which is very much promising.
Only one CF and qualifier.
10K total rows delete with a batch of 100 deletes. Only deletes happening on the table from one thread.
With the new way the net time taken is reduced by more than 1/10
Will test in a 4 node cluster also. I think it will worth doing this change.",anoopsamjohn,zhihyu@ebaysf.com,Major,Closed,Fixed,27/Jun/12 21:12,26/Feb/13 16:56
Bug,HBASE-6285,12596166,HBase master should log INFO message when it attempts to assign a region,"With the default logging level (INFO), it is very difficult to diagnose a large HBase cluster that is having problems assigning regions because the HBase master logs a DEBUG message when it instructs a region-server to assign a region.

You actually have to crawl EVERY HBase region-server log to find out which node received the request for a particular region. Further, lets say the HBase master sends the request and something goes wrong, we might not even get a message in the region-server log.",adityakishore,adityakishore,Minor,Closed,Fixed,27/Jun/12 21:23,23/Sep/13 18:31
Bug,HBASE-6293,12596380,HMaster does not go down while splitting logs even if explicit shutdown is called.,"When master starts up and tries to do splitlog, in case of any error we try to do that infinitely in a loop until it succeeds.
But now if we get a shutdown call, inside SplitLogManager
{code}
          if (stopper.isStopped()) {
            LOG.warn(""Stopped while waiting for log splits to be completed"");
            return;
          }
{code}
Here we know that the master has stopped.  As the task may not be completed now
{code}
 if (batch.done != batch.installed) {
      batch.isDead = true;
      tot_mgr_log_split_batch_err.incrementAndGet();
      LOG.warn(""error while splitting logs in "" + logDirs +
      "" installed = "" + batch.installed + "" but only "" + batch.done + "" done"");
      throw new IOException(""error or interrupt while splitting logs in ""
          + logDirs + "" Task = "" + batch);
    }
{code} 
we throw an exception.  In MasterFileSystem.splitLogAfterStartup() we don't check if the master is stopped and we try continously. 
",larsh,rajesh23,Major,Closed,Fixed,29/Jun/12 11:35,26/Feb/13 08:16
Bug,HBASE-6297,12596513,Backport HBASE-6195 to 0.92,Apply HBASE-6195 to 0.92,ram_krish,ram_krish,Major,Closed,Fixed,30/Jun/12 17:41,20/Nov/15 11:53
Bug,HBASE-6299,12596526,RS starting region open while failing ack to HMaster.sendRegionOpen() causes inconsistency in HMaster's region state and a series of successive problems,"1. HMaster tries to assign a region to an RS.
2. HMaster creates a RegionState for this region and puts it into regionsInTransition.
3. In the first assign attempt, HMaster calls RS.openRegion(). The RS receives the open region request and starts to proceed, with success eventually. However, due to network problems, HMaster fails to receive the response for the openRegion() call, and the call times out.
4. HMaster attemps to assign for a second time, choosing another RS. 
5. But since the HMaster's OpenedRegionHandler has been triggered by the region open of the previous RS, and the RegionState has already been removed from regionsInTransition, HMaster finds invalid and ignores the unassigned ZK node ""RS_ZK_REGION_OPENING"" updated by the second attempt.
6. The unassigned ZK node stays and a later unassign fails coz RS_ZK_REGION_CLOSING cannot be created.
{code}
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.; plan=hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-004,60020,1340890123243, dest=swbss-hadoop-006,60020,1340890678078
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to swbss-hadoop-006,60020,1340890678078
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=swbss-hadoop-002:60000, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:28,882 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,291 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. from serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=518945, regions=575, usedHeap=15282, maxHeap=31301); deleting unassigned node
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Deleting existing unassigned node for b713fd655fa02395496c5a6e39ddf568 that is in expected state RS_ZK_REGION_OPENED
2012-06-29 07:06:32,301 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Successfully deleted unassigned node for region b713fd655fa02395496c5a6e39ddf568 in expected state RS_ZK_REGION_OPENED
2012-06-29 07:06:32,301 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: The master has opened the region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. that was online on serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=518945, regions=575, usedHeap=15282, maxHeap=31301)
2012-06-29 07:07:41,140 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=575, usedHeap=0, maxHeap=0), trying to assign elsewhere instead; retry=0
java.net.SocketTimeoutException: Call to /172.16.0.6:60020 failed on socket timeout exception: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.16.0.2:53765 remote=/172.16.0.6:60020]
        at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:805)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:778)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:283)
        at $Proxy8.openRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:573)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1127)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:912)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:892)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:92)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.16.0.2:53765 remote=/172.16.0.6:60020]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        at java.io.FilterInputStream.read(FilterInputStream.java:116)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:301)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:541)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:479)
2012-06-29 07:07:41,142 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. so generated a random one; hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=, dest=swbss-hadoop-164,60020,1340888346294; 15 (online=15, exclude=serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=575, usedHeap=0, maxHeap=0)) available servers
2012-06-29 07:07:41,142 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Creating (or updating) unassigned node for b713fd655fa02395496c5a6e39ddf568 with OFFLINE state
2012-06-29 07:07:41,145 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.; plan=hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=, dest=swbss-hadoop-164,60020,1340888346294
2012-06-29 07:07:41,145 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to swbss-hadoop-164,60020,1340888346294
2012-06-29 07:07:41,149 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,150 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,296 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,296 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,302 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,302 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENED for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:08:38,872 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-006,60020,1340890678078, dest=swbss-hadoop-008,60020,1340891085175
2012-06-29 07:08:38,872 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. (offlining)
2012-06-29 07:08:47,875 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
...
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:47,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:47,682 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
{code}

",maryannxue,maryannxue,Critical,Closed,Fixed,01/Jul/12 05:07,07/Apr/13 04:36
Bug,HBASE-6303,12596664,HCD.setCompressionType should use Enum support for storing compression types as strings,Let's not require an update to HCD every time the HFile compression enum is changed.,apurtell,gopinathan.av,Minor,Closed,Fixed,02/Jul/12 15:04,26/Feb/13 08:15
Bug,HBASE-6306,12596832,TestFSUtils fails against hadoop 2.0,"trunk: mvn clean test -Dhadoop.profile=2.0 -Dtest=TestFSUtils

{code}
java.io.FileNotFoundException: File /home/jon/proj/hbase-trunk/hbase-server/target/test-data/02beb8c8-06c1-47ea-829b-6e7ce0570cf8/hbase.version does not exist
        at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:315)
        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)
        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1319)
        at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:557)
        at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:213)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:270)
        at org.apache.hadoop.hbase.util.TestFSUtils.testVersion(TestFSUtils.java:58)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...     
{code}

",jmhsieh,jmhsieh,Major,Closed,Fixed,02/Jul/12 22:16,23/Sep/13 18:30
Bug,HBASE-6311,12596931,Data error after majorCompaction caused by keeping MVCC for opened scanners,"It is a big problem we found in 0.94, and you could reproduce the problem in Trunk using the test case I uploaded.

When we do compaction, we will use region.getSmallestReadPoint() to keep MVCC for opened scanners;
However,It will make data mistake after majorCompaction because we will skip delete type KV but keep the put type kv in the compacted storefile.

The following is the reason from code:
In StoreFileScanner, enforceMVCC is false when compaction, so we could read the delete type KV,
However, we will skip this delete type KV in ScanQueryMatcher because following code

{code}
if (kv.isDelete())
{
...
 if (includeDeleteMarker
            && kv.getMemstoreTS() <= maxReadPointToTrackVersions) {
          System.out.println(""add deletes,maxReadPointToTrackVersions=""
              + maxReadPointToTrackVersions);
          this.deletes.add(bytes, offset, qualLength, timestamp, type);
        }
...
}
{code}

Here maxReadPointToTrackVersions = region.getSmallestReadPoint();
and kv.getMemstoreTS() > maxReadPointToTrackVersions 
So we won't add this to DeleteTracker.

Why test case passed if remove the line MultiVersionConsistencyControl.setThreadReadPoint(smallestReadPoint);

Because in the StoreFileScanner#skipKVsNewerThanReadpoint
{code}
if (cur.getMemstoreTS() <= readPoint) {
      cur.setMemstoreTS(0);
    }
{code}
So if we remove the line MultiVersionConsistencyControl.setThreadReadPoint(smallestReadPoint);
Here readPoint is LONG.MAX_VALUE, we will set memStore ts as 0, so we will add it to DeleteTracker in ScanQueryMatcher 


Solution:
We use smallestReadPoint of region when compaction to keep MVCC for OPENED scanner, So we should retain delete type kv in output in the case(Already deleted KV is retained in output to make old opened scanner could read this KV) even if it is a majorcompaction.",zjushch,zjushch,Blocker,Closed,Fixed,03/Jul/12 03:30,18/Sep/13 22:07
Bug,HBASE-6313,12597031,Client hangs because the client is not notified ,"If the call first remove from the calls, when some exception happened in reading from the DataInputStream, the call will not be notified, cause the client hangs.
",binlijin,binlijin,Major,Closed,Fixed,03/Jul/12 08:51,02/Jun/13 02:44
Bug,HBASE-6316,12597246,Confirm can upgrade to 0.96 from 0.94 by just stopping and restarting,"Over in HBASE-6294, LarsH says you have to currently clear zk to get a 0.96 to start over data written by a 0.94.  Need to fix it so don't have to do this -- that zk state left over gets auto-migrated.",stack,stack,Blocker,Closed,Fixed,03/Jul/12 14:07,23/Sep/13 18:30
Bug,HBASE-6317,12597250,Master clean start up and Partially enabled tables make region assignment inconsistent.,"If we have a  table in partially enabled state (ENABLING) then on HMaster restart we treat it as a clean cluster start up and do a bulk assign.  Currently in 0.94 bulk assign will not handle ALREADY_OPENED scenarios and it leads to region assignment problems.  Analysing more on this we found that we have better way to handle these scenarios.
{code}
if (false == checkIfRegionBelongsToDisabled(regionInfo)
            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {
          synchronized (this.regions) {
            regions.put(regionInfo, regionLocation);
            addToServers(regionLocation, regionInfo);
          }
{code}
We dont add to regions map so that enable table handler can handle it.  But as nothing is added to regions map we think it as a clean cluster start up.
Will come up with a patch tomorrow.",rajesh23,ram_krish,Major,Closed,Fixed,03/Jul/12 18:08,26/Feb/13 08:23
Bug,HBASE-6318,12597337,SplitLogWorker exited due to ConcurrentModificationException,"In playing with 0.96 code on a live cluster, found this issue:

2012-07-03 12:13:32,572 ERROR org.apache.hadoop.hbase.regionserver.SplitLogWorker: unexpected error
java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.closeLogWriters(HLogSplitter.java:1330)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWritingAndClose(HLogSplitter.java:1221)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:441)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:369)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:113)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:276)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:197)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:164)
        at java.lang.Thread.run(Thread.java:662)
2012-07-03 12:13:32,575 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: SplitLogWorker ****.cloudera.com,57020,1341335300238 exiting",jxiang,jxiang,Major,Closed,Fixed,03/Jul/12 20:42,23/Sep/13 18:30
Bug,HBASE-6319,12597340,ReplicationSource can call terminate on itself and deadlock,In a few places in the ReplicationSource code calls terminate on itself which is a problem since in terminate() we wait on that thread to die.,jdcryans,jdcryans,Major,Closed,Fixed,03/Jul/12 20:58,12/Oct/12 05:36
Bug,HBASE-6321,12597342,ReplicationSource dies reading the peer's id,"This is what I saw:

{noformat}
2012-07-01 05:04:01,638 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Closing source 8 because an error occurred: Could not read peer's cluster id
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /va1-backup/hbaseid
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1021)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:154)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:259)
        at org.apache.hadoop.hbase.zookeeper.ClusterId.readClusterIdZNode(ClusterId.java:61)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:253)
{noformat}

The session should just be reopened.",jdcryans,jdcryans,Major,Closed,Fixed,03/Jul/12 21:00,07/Apr/13 04:37
Bug,HBASE-6325,12597368,[replication] Race in ReplicationSourceManager.init can initiate a failover even if the node is alive,"Yet another bug found during the leap second madness, it's possible to miss the registration of new region servers so that in ReplicationSourceManager.init we start the failover of a live and replicating region server. I don't think there's data loss but the RS that's being failed over will die on:

{noformat}
2012-07-01 06:25:15,604 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server sv4r23s48,10304,1341112194623: Writing replication status
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/replication/rs/sv4r23s48,10304,1341112194623/4/sv4r23s48%2C10304%2C1341112194623.1341112195369
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1246)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:372)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:655)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:697)
        at org.apache.hadoop.hbase.replication.ReplicationZookeeper.writeReplicationStatus(ReplicationZookeeper.java:470)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.logPositionAndCleanOldLogs(ReplicationSourceManager.java:154)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:607)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:368)
{noformat}

It seems to me that just refreshing {{otherRegionServers}} after getting the list of {{currentReplicators}} would be enough to fix this.",jdcryans,jdcryans,Major,Closed,Fixed,03/Jul/12 23:29,26/Feb/13 16:56
Bug,HBASE-6326,12597372,Avoid nested retry loops in HConnectionManager,"While testing client timeouts when the HBase is not available we found that even with aggressive settings, it takes the client 10 minutes or more to finally receive an exception.
Part of this is due to nested nested retry loops in locateRegion.

locateRegion will first try to locate the table in meta (which is retried), then it will try to locate the meta table is root (which is also retried).
So for each retry of the meta lookup we retry the root lookup as well.

I have have that avoids locateRegion retrying if it is called from code that already has a retry loop.",larsh,larsh,Critical,Closed,Fixed,03/Jul/12 23:53,26/Feb/13 08:15
Bug,HBASE-6327,12597438,HLog can be null when create table,"As HBASE-4010 discussed, the HLog can be null.

We have meet createTable failed because the no use hlog.

When createHReagion, the HLog.LogSyncer is run sync(), in under layer it call the DFSClient.DFSOutputStream.sync(). 

Then the hlog.closeAndDelete() was called，firstly the HLog.close() will interrupt the LogSyncer, and interrupt DFSClient.DFSOutputStream.sync().The DFSClient.DFSOutputStream will store the exception and throw it when we called DFSClient.close(). 

The HLog.close() call the writer.close()/DFSClient.close() after interrupt the LogSyncer. And there is no catch exception for the close().

So the Master throw exception to the client. There is no need to throw this exception, further， the hlog is no use.

Our cluster is 0.90, the logs is attached, after ""closing hlog writer"", there is no log for the createTable().

The trunk and 0.92, 0.94, we used just one hlog, and if the exception happends, the client will got createTable failed, but indeed ,we expect all the regions for the table can also be assigned.

I will give the patch for this later.",xingshi,xingshi,Major,Closed,Fixed,04/Jul/12 10:49,26/Feb/13 08:22
Bug,HBASE-6328,12597472,FSHDFSUtils#recoverFileLease tries to rethrow InterruptedException but actually shallows it,"Coding error is:

{noformat}
      try {
        Thread.sleep(1000);
      } catch (InterruptedException ex) {
        new InterruptedIOException().initCause(ex);
      }
{noformat}

The exception is created but not thrown...",nkeywal,nkeywal,Minor,Closed,Fixed,04/Jul/12 15:11,26/Feb/13 08:16
Bug,HBASE-6329,12597520,Stopping META regionserver when splitting region could cause daughter region to be assigned twice,"We found this issue in 0.94, first let me describe the case：
Stop META rs when split is in progress

1.Stopping META rs(Server A).
2.The main thread of rs close ZK and delete ephemeral node of the rs.
3.SplitTransaction is retring MetaEditor.addDaughter
4.Master's ServerShutdownHandler process the above dead META server
5.Master fixup daughter and assign the daughter
6.The daughter is opened on another server(Server B)
7.Server A's splitTransaction successfully add the daughter to .META. with serverName=Server A
8.Now, in the .META., daughter's region location is Server A but it is onlined on Server B
9.Restart Master, and master will assign the daughter again.


Attaching the logs, daughter region 80f999ea84cb259e20e9a228546f6c8a

Master log:
2012-07-04 13:45:56,493 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for dw93.kgb.sqa.cm4,60020,1341378224464
2012-07-04 13:45:58,983 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Fixup; missing daughter writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. 
2012-07-04 13:45:58,985 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added daughter writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a., serverName=null 
2012-07-04 13:45:58,988 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. to dw88.kgb.sqa.cm4,60020,1341379188777 
2012-07-04 13:46:00,201 INFO org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. that was online on dw88.kgb.sqa.cm4,60020,1341379188777 

Master log after restart:
2012-07-04 14:27:05,824 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x136187d60e34644 Creating (or updating) unassigned node for 80f999ea84cb259e20e9a228546f6c8a with OFFLINE state 
2012-07-04 14:27:05,851 INFO org.apache.hadoop.hbase.master.AssignmentManager: Processing region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. in state M_ZK_REGION_OFFLINE 
2012-07-04 14:27:05,854 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. to dw93.kgb.sqa.cm4,60020,1341380812020 
2012-07-04 14:27:06,051 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=dw93.kgb.sqa.cm4,60020,1341380812020, region=80f999ea84cb259e20e9a228546f6c8a 



Regionserver(META rs) log:
2012-07-04 13:45:56,491 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: stopping server dw93.kgb.sqa.cm4,60020,1341378224464; zookeeper connection c
losed.
2012-07-04 13:46:11,951 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added daughter writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a., serverName=dw93.kgb.sqa.cm4,60020,1341378224464 
2012-07-04 13:46:11,952 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Done with post open deploy task for region=writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a., daughter=true 


",zjushch,zjushch,Major,Closed,Fixed,05/Jul/12 03:51,26/Feb/13 08:16
Bug,HBASE-6336,12597696,Split point should not be equal to start row or end row,"Should we allow split point equal with region's start row or end row?
{code}
// if the midkey is the same as the first and last keys, then we cannot
        // (ever) split this region.
        if (this.comparator.compareRows(mk, firstKey) == 0 &&
            this.comparator.compareRows(mk, lastKey) == 0) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(""cannot split because midkey is the same as first or "" +
              ""last row"");
          }
{code}

Here, I think it is a mistake.",zjushch,zjushch,Major,Closed,Fixed,06/Jul/12 05:58,23/Sep/13 18:30
Bug,HBASE-6337,12597697,[MTTR] Remove renaming tmp log file in SplitLogManager ,"As HBASE-6309 mentioned, we also encounter problem of distributed-log-splitting take much more time than matser-local-log-splitting because lots of SplitLogManager 's renaming operations when finishing task.

Could we try to remove renaming tmp log file in SplitLogManager through splitting log to regions' recover.edits directory directly as the same as the master-local-log-splitting.
",zjushch,zjushch,Major,Closed,Fixed,06/Jul/12 06:06,26/Feb/13 16:56
Bug,HBASE-6340,12597811,HBase RPC should allow protocol extension with common interfaces.,"HBase RPC fails if MyProtocol extends an interface, which is not a VersionedProtocol even if MyProtocol also directly extends VersionedProtocol. The reason is that rpc Invocation uses Method.getDeclaringClass(), which returns the interface class rather than the class of MyProtocol.",shv,shv,Major,Closed,Fixed,06/Jul/12 18:04,24/Oct/12 05:40
Bug,HBASE-6347,12597857,-ROOT- and .META. are stale in table.jsp if they moved,"table.jsp does not use a lookup method on {{CatalogTracker}} that does not force a refresh of the cache, thus it can get a stale location if -ROOT- or .META. moved and the master hasn't tried to access them yet.

Should just be a matter of using waitForRoot/Meta.",larsh,jdcryans,Major,Closed,Fixed,07/Jul/12 00:03,05/Aug/14 20:11
Bug,HBASE-6356,12598038,printStackTrace in FSUtils,"This is bad...
{noformat}
    public boolean accept(Path p) {
      boolean isValid = false;
      try {
        if (HConstants.HBASE_NON_USER_TABLE_DIRS.contains(p.toString())) {
          isValid = false;
        } else {
            isValid = this.fs.getFileStatus(p).isDir();
        }
      } catch (IOException e) {
        e.printStackTrace();          <================ 
      }
      return isValid;
    }
  }
{noformat}",gustavoanatoly,nkeywal,Trivial,Closed,Fixed,09/Jul/12 14:38,13/Jun/22 16:49
Bug,HBASE-6357,12598078,Failed distributed log splitting stuck on master web UI,Failed distributed log splitting MonitoredTask is stuck on the master web UI since it is not aborted.,jxiang,jxiang,Major,Closed,Fixed,09/Jul/12 19:34,28/Feb/13 22:49
Bug,HBASE-6359,12598102,KeyValue may return incorrect values after readFields(),"When the same KeyValue object is used multiple times for deserialization using readFields, some methods may return incorrect values. Here is a sequence of operations that will reproduce the problem:

 # A KeyValue is created whose key has length 10. The private field keyLength is initialized to 0.
 # KeyValue.getKeyLength() is called. This reads the key length 10 from the backing array and caches it in keyLength.
 # KeyValue.readFields() is called to deserialize a new value. The keyLength field is not cleared and keeps its value of 10, even though this value is probably incorrect.
 # If getKeyLength() is called, the value 10 will be returned.

For example, in a reducer with Iterable<KeyValue>, all values after the first one from the iterable are likely to return incorrect values from getKeyLength().

The solution is to clear all memoized values in KeyValue.readFields(). I'll write a patch for this soon.",dave_revell,dave_revell,Major,Closed,Fixed,09/Jul/12 22:00,05/Aug/14 20:11
Bug,HBASE-6364,12598225,Powering down the server host holding the .META. table causes HBase Client to take excessively long to recover and connect to reassigned .META. table,"When a server host with a Region Server holding the .META. table is powered down on a live cluster, while the HBase cluster itself detects and reassigns the .META. table, connected HBase Client's take an excessively long time to detect this and re-discover the reassigned .META. 

Workaround: Decrease the ipc.socket.timeout on HBase Client side to a low  value (default is 20s leading to 35 minute recovery time; we were able to get acceptable results with 100ms getting a 3 minute recovery) 

This was found during some hardware failure testing scenarios. 

Test Case:
1) Apply load via client app on HBase cluster for several minutes
2) Power down the region server holding the .META. server (i.e. power off ... and keep it off)
3) Measure how long it takes for cluster to reassign META table and for client threads to re-lookup and re-orient to the lesser cluster (minus the RS and DN on that host).

Observation:
1) Client threads spike up to maxThreads size ... and take over 35 mins to recover (i.e. for the thread count to go back to normal) - no client calls are serviced - they just back up on a synchronized method (see #2 below)

2) All the client app threads queue up behind the oahh.ipc.HBaseClient#setupIOStreams method http://tinyurl.com/7js53dj

After taking several thread dumps we found that the thread within this synchronized method was blocked on  NetUtils.connect(this.socket, remoteId.getAddress(), getSocketTimeout(conf));

The client thread that gets the synchronized lock would try to connect to the dead RS (till socket times out after 20s), retries, and then the next thread gets in and so forth in a serial manner.

Workaround:
-------------------
Default ipc.socket.timeout is set to 20s. We dropped this to a low number (1000 ms,  100 ms, etc) on the client side hbase-site.xml. With this setting, the client threads recovered in a couple of minutes by failing fast and re-discovering the .META. table on a reassigned RS.

Assumption: This ipc.socket.timeout is only ever used during the initial ""HConnection"" setup via the NetUtils.connect and should only ever be used when connectivity to a region server is lost and needs to be re-established. i.e it does not affect the normal ""RPC"" actiivity as this is just the connect timeout.
During RS GC periods, any _new_ clients trying to connect will fail and will require .META. table re-lookups.

This above timeout workaround is only for the HBase client side.",nkeywal,svarma,Major,Closed,Fixed,10/Jul/12 17:06,30/May/13 09:33
Bug,HBASE-6368,12598260,Upgrade Guava for critical performance bug fix,"The bug is http://code.google.com/p/guava-libraries/issues/detail?id=1055

See discussion under 'Upgrade to Guava 12.0.1: Performance bug in CacheBuilder/LoadingCache fixed!'",zhihyu@ebaysf.com,zhihyu@ebaysf.com,Critical,Closed,Fixed,10/Jul/12 19:58,23/Sep/13 18:31
Bug,HBASE-6369,12598309,HTable is not closed in AggregationClient,"In AggregationClient, HTable instance is not closed.",binlijin,binlijin,Major,Closed,Fixed,11/Jul/12 01:59,26/Feb/13 16:56
Bug,HBASE-6375,12598471,Master may be using a stale list of region servers for creating assignment plan during startup,"While investigating an Out of Memory issue, I had an interesting observation where the master tries to assign all regions to a single region server even though 7 other had already registered with it.

As the cluster had MSLAB enabled, this resulted in OOM on the RS when it tired to open all of them.

*From master's log (edited for brevity):*
{quote}
55,468 Waiting on regionserver(s) to checkin
56,968 Waiting on regionserver(s) to checkin
58,468 Waiting on regionserver(s) to checkin
59,968 Waiting on regionserver(s) to checkin
01,242 Registering server=srv109.datacenter,60020,1338673920529,regionCount=0,userLoad=false
01,469 Waiting on regionserver(s) count to settle; currently=1
02,969 Finished waiting for regionserver count to settle; count=1,sleptFor=46500
02,969 Exiting wait on regionserver(s) to checkin; count=1, stopped=false,count of regions out on cluster=0
03,010 Processing region \-ROOT\-,,0.70236052 in state M_ZK_REGION_OFFLINE
03,220 \-ROOT\- assigned=0, rit=true, location=srv109.datacenter:60020
03,221 Processing region .META.,,1.1028785192 in state M_ZK_REGION_OFFLINE
03,336 Detected completed assignment of META, notifying catalog tracker
03,350 .META. assigned=0, rit=true, location=srv109.datacenter:60020
03,350 Master startup proceeding: cluster startup
04,006 Registering server=srv111.datacenter,60020,1338673923399,regionCount=0,userLoad=false
04,012 Registering server=srv113.datacenter,60020,1338673923532,regionCount=0,userLoad=false
04,269 Registering server=srv115.datacenter,60020,1338673923471,regionCount=0,userLoad=false
04,363 Registering server=srv117.datacenter,60020,1338673923928,regionCount=0,userLoad=false
04,599 Registering server=srv127.datacenter,60020,1338673924067,regionCount=0,userLoad=false
04,606 Registering server=srv119.datacenter,60020,1338673923953,regionCount=0,userLoad=false
04,804 Registering server=srv129.datacenter,60020,1338673924339,regionCount=0,userLoad=false
05,126 Bulk assigning 1252 region(s) across 1 server(s), retainAssignment=true
05,546 hd109.datacenter,60020,1338673920529 unassigned znodes=207 of
{quote}

*A peek at AssignmentManager code offer some explanation:*
{code}
  public void assignAllUserRegions() throws IOException, InterruptedException {
    // Get all available servers
    List<HServerInfo> servers = serverManager.getOnlineServersList();

    // Scan META for all user regions, skipping any disabled tables
    Map<HRegionInfo,HServerAddress> allRegions =
      MetaReader.fullScan(catalogTracker, this.zkTable.getDisabledTables(), true);
    if (allRegions == null || allRegions.isEmpty()) return;

    // Determine what type of assignment to do on startup
    boolean retainAssignment = master.getConfiguration().
      getBoolean(""hbase.master.startup.retainassign"", true);

    Map<HServerInfo, List<HRegionInfo>> bulkPlan = null;
    if (retainAssignment) {
      // Reuse existing assignment info
      bulkPlan = LoadBalancer.retainAssignment(allRegions, servers);
    } else {
      // assign regions in round-robin fashion
      bulkPlan = LoadBalancer.roundRobinAssignment(new ArrayList<HRegionInfo>(allRegions.keySet()), servers);
    }
    LOG.info(""Bulk assigning "" + allRegions.size() + "" region(s) across "" +
      servers.size() + "" server(s), retainAssignment="" + retainAssignment);
    ...
{code}

In the function assignAllUserRegions(), listed above, AM fetches the server list from ServerManager long before it actually use it to create assignment plan.

In between these, it performs a full scan of META to create an assignment map of regions. So even if additional RSes have registered in the meantime (as happened in this case), AM still has the old list of just one server.

This code snippet is from 0.90.6 but the same issue exists in 0.92, 0.94 and trunk. Since MSLAB is enabled by default in 0.92 onwards, any large cluster can hit this issue upon cluster start-up when the following sequence holds true.

# Master start long before the RSes (by default this long ~= 4.5 seconds)
# All the RSes start togather but one wins the race of registering with Master by few seconds.

I am attaching a patch for the trunk which moves the code which fetches the RS list form the beginning of the function to where it is first use.

Apart from this change, one other HBase setting that now becomes important is ""hbase.master.wait.on.regionservers.mintostart"" due to MSLAB being enabled by default.

In large clusters which keeps it enabled now must modify ""hbase.master.wait.on.regionservers.mintostart"" to a suitable number than the default of 1 to ensure that the master waits for a quorum of RSes which are sufficient to open all the regions among themselves. I'll create a separate JIRA for the documentation change.",adityakishore,adityakishore,Major,Closed,Fixed,11/Jul/12 23:16,26/Feb/13 08:15
Bug,HBASE-6377,12598486,HBASE-5533 metrics miss all operations submitted via MultiAction,"A client application (LoadTestTool) calls put() on HTables. Internally to the HBase client those puts are batched into MultiActions. The total number of put operations shown in the RegionServer's put metrics histogram never increases from 0 even though millions of such operations are made. Needless to say the latency for those operations are not measured either. The value of HBASE-5533 metrics are suspect given the client will batch put and delete ops like this.

I had a fix in progress but HBASE-6284 messed it up. Before, MultiAction processing in HRegionServer would distingush between puts and deletes and dispatch them separately. It was easy to account for the time for them. Now both puts and deletes are submitted in batch together as mutations.",apurtell,apurtell,Major,Closed,Fixed,12/Jul/12 02:19,29/Jul/14 03:07
Bug,HBASE-6378,12598491,the javadoc of  setEnabledTable maybe not describe accurately ,"  /**
   * Sets the ENABLED state in the cache and deletes the zookeeper node. Fails
   * silently if the node is not in enabled in zookeeper
   * 
   * @param tableName
   * @throws KeeperException
   */
  public void setEnabledTable(final String tableName) throws KeeperException {
    setTableState(tableName, TableState.ENABLED);
  }

When setEnabledTable occours ,It will update the cache and the zookeeper node,rather than to delete the zk node.
",dsw,zhou wenjian,Major,Closed,Fixed,12/Jul/12 03:37,24/Oct/12 05:40
Bug,HBASE-6380,12598538,bulkload should update the store.storeSize,"After bulkloading some HFiles into the Table, we found the force-split didn't work because of the MidKey == NULL. Only if we re-booted the HBase service, the force-split can work normally. ",grace.huang,grace.huang,Critical,Closed,Fixed,12/Jul/12 11:55,26/Feb/13 08:15
Bug,HBASE-6381,12598584,AssignmentManager should use the same logic for clean startup and failover,"Currently AssignmentManager handles clean startup and failover very differently.
Different logic is mingled together so it is hard to find out which is for which.

We should clean it up and share the same logic so that AssignmentManager handles
both cases the same way.  This way, the code will much easier to understand and
maintain.",jxiang,jxiang,Major,Closed,Fixed,12/Jul/12 17:26,23/Sep/13 18:30
Bug,HBASE-6385,12598630,[0.90 branch] Backport HBASE-4195 to 0.90,We are seeing some HBASE-4195 failures on 0.90.  Backport this fix.,gchanan,gchanan,Major,Closed,Fixed,12/Jul/12 21:34,13/Jun/22 17:03
Bug,HBASE-6388,12598651,Avoid potential data loss if the flush fails during regionserver shutdown,"During a controlled shutdown, Regionserver deletes HLogs even if HRegion.close() fails. We should not be doing this.

",amitanand,amitanand,Critical,Closed,Fixed,13/Jul/12 00:33,13/Jun/22 17:03
Bug,HBASE-6389,12598653,Modify the conditions to ensure that Master waits for sufficient number of Region Servers before starting region assignments,"Continuing from HBASE-6375.

It seems I was mistaken in my assumption that changing the value of ""hbase.master.wait.on.regionservers.mintostart"" to a sufficient number (from default of 1) can help prevent assignment of all regions to one (or a small number of) region server(s).

While this was the case in 0.90.x and 0.92.x, the behavior has changed in 0.94.0 onwards to address HBASE-4993.

From 0.94.0 onwards, Master will proceed immediately after the timeout has lapsed, even if ""hbase.master.wait.on.regionservers.mintostart"" has not reached.

Reading the current conditions of waitForRegionServers() clarifies it

{code:title=ServerManager.java (trunk rev:1360470)}
....
581	  /**
582	   * Wait for the region servers to report in.
583	   * We will wait until one of this condition is met:
584	   *  - the master is stopped
585	   *  - the 'hbase.master.wait.on.regionservers.timeout' is reached
586	   *  - the 'hbase.master.wait.on.regionservers.maxtostart' number of
587	   *    region servers is reached
588	   *  - the 'hbase.master.wait.on.regionservers.mintostart' is reached AND
589	   *   there have been no new region server in for
590	   *      'hbase.master.wait.on.regionservers.interval' time
591	   *
592	   * @throws InterruptedException
593	   */
594	  public void waitForRegionServers(MonitoredTask status)
595	  throws InterruptedException {
....
....
612	    while (
613	      !this.master.isStopped() &&
614	        slept < timeout &&
615	        count < maxToStart &&
616	        (lastCountChange+interval > now || count < minToStart)
617	      ){
....
{code}

So with the current conditions, the wait will end as soon as timeout is reached even lesser number of RS have checked-in with the Master and the master will proceed with the region assignment among these RSes alone.

As mentioned in -[HBASE-4993|https://issues.apache.org/jira/browse/HBASE-4993?focusedCommentId=13237196#comment-13237196]-, and I concur, this could have disastrous effect in large cluster especially now that MSLAB is turned on.

To enforce the required quorum as specified by ""hbase.master.wait.on.regionservers.mintostart"" irrespective of timeout, these conditions need to be modified as following

{code:title=ServerManager.java}
..
  /**
   * Wait for the region servers to report in.
   * We will wait until one of this condition is met:
   *  - the master is stopped
   *  - the 'hbase.master.wait.on.regionservers.maxtostart' number of
   *    region servers is reached
   *  - the 'hbase.master.wait.on.regionservers.mintostart' is reached AND
   *   there have been no new region server in for
   *      'hbase.master.wait.on.regionservers.interval' time AND
   *   the 'hbase.master.wait.on.regionservers.timeout' is reached
   *
   * @throws InterruptedException
   */
  public void waitForRegionServers(MonitoredTask status)
..
..
    int minToStart = this.master.getConfiguration().
    getInt(""hbase.master.wait.on.regionservers.mintostart"", 1);
    int maxToStart = this.master.getConfiguration().
    getInt(""hbase.master.wait.on.regionservers.maxtostart"", Integer.MAX_VALUE);
    if (maxToStart < minToStart) {
      maxToStart = minToStart;
    }
..
..
    while (
      !this.master.isStopped() &&
        count < maxToStart &&
        (lastCountChange+interval > now || timeout > slept || count < minToStart)
      ){
..
{code}",adityakishore,adityakishore,Critical,Closed,Fixed,13/Jul/12 01:05,26/Feb/13 08:20
Bug,HBASE-6392,12598717,UnknownRegionException blocks hbck from sideline big overlap regions,"Before sidelining a big overlap region, hbck tries to close it and offline it at first.  However, sometimes, it throws NotServingRegion or UnknownRegionException.
It could be because the region is not open/assigned at all, or some other issue.
We should figure out why and fix it.

By the way, it's better to print out in the log the command line to bulk load back sidelined regions, if any. ",jxiang,jxiang,Major,Closed,Fixed,13/Jul/12 16:58,26/Feb/13 08:15
Bug,HBASE-6394,12598773,verifyrep MR job map tasks throws NullPointerException ,"{noformat}
2012-07-02 16:23:34,871 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
2012-07-02 16:23:34,876 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.NullPointerException
	at org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier.cleanup(VerifyReplication.java:140)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:645)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)
2012-07-02 16:23:34,882 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task
{noformat}
",jxiang,jxiang,Minor,Closed,Fixed,13/Jul/12 23:46,26/Feb/13 08:16
Bug,HBASE-6397,12599030,[hbck] print out bulk load commands for sidelined regions if necessary,"It's better to print out in the log the command line to bulk load back sidelined regions, if any.

Separate it out from HBASE-6392 since it is a different issue.",jxiang,jxiang,Trivial,Closed,Fixed,16/Jul/12 20:08,26/Feb/13 08:16
Bug,HBASE-6406,12599175,TestReplicationPeer.testResetZooKeeperSession and TestZooKeeper.testClientSessionExpired fail frequently,"Looking back through the 0.94 test runs these two tests accounted for 11 of 34 failed tests.
They should be fixed or (temporarily) disabled.",larsh,larsh,Major,Closed,Fixed,17/Jul/12 21:53,15/Nov/17 15:15
Bug,HBASE-6420,12599302,Gracefully shutdown logsyncer,"Currently, in closing a HLog, logSyncerThread is interrupted. logSyncer could be in the middle to sync the writer.  We should avoid interrupting the sync.",jxiang,jxiang,Major,Closed,Fixed,18/Jul/12 17:30,26/Feb/13 08:16
Bug,HBASE-6421,12599307,[pom] add jettison and fix netty specification,"Currently, jettison isn't required for testing hbase-server, but TestSchemaConfigured requires it, causing the compile phase (at least on my MBP) to fail. Further, in cleaning up the poms, netty should be declared in the parent hbase/pom.xml and then inherited in the subclass.",jesse_yates,jesse_yates,Major,Closed,Fixed,18/Jul/12 17:51,13/Jun/22 16:54
Bug,HBASE-6423,12599338,Writes should not block reads on blocking updates to memstores,"We have a big data use case where we turn off WAL and have a ton of reads and writes. We found that:

1. flushing a memstore takes a while (GZIP compression)
2. incoming writes cause the new memstore to grow in an unbounded fashion
3. this triggers blocking memstore updates
4. in turn, this causes all the RPC handler threads to block on writes to that memstore
5. we are not able to read during this time as RPC handlers are blocked

At a higher level, we should not hold up the RPC threads while blocking updates, and we should build in some sort of rate control.",jxiang,karthik.ranga,Major,Closed,Fixed,18/Jul/12 20:42,02/May/13 02:30
Bug,HBASE-6424,12599380,TestReplication frequently hangs,TestReplication frequently hangs. Separated out from HBASE-6406.,,apurtell,Major,Closed,Fixed,18/Jul/12 23:37,13/Jun/22 16:54
Bug,HBASE-6426,12599391,Add Hadoop 2.0.x profile to 0.92+,0.96 already has a Hadoop-2.0 build profile. Let's add this to 0.92 and 0.94 as well.,larsh,larsh,Major,Closed,Fixed,19/Jul/12 01:43,12/Oct/12 05:36
Bug,HBASE-6429,12599414,Filter with filterRow() returning true is incompatible with scan with limit,"Currently if we scan with bot limit and a Filter with filterRow(List<KeyValue>) implemented, an  IncompatibleFilterException will be thrown. The same exception should also be thrown if the filer has its filterRow() implemented.

",grace.huang,jason.dai,Major,Closed,Fixed,19/Jul/12 07:23,23/Sep/13 18:31
Bug,HBASE-6431,12599555,Some FilterList Constructors break addFilter,"Some of the constructors for FilterList set the internal list of filters to list types which don't support the add operation. As a result 

FilterList(final List<Filter> rowFilters)
FilterList(final Filter... rowFilters)
FilterList(final Operator operator, final List<Filter> rowFilters)
FilterList(final Operator operator, final Filter... rowFilters)

may init private List<Filter> filters = new ArrayList<Filter>(); incorrectly.",posix4e,posix4e,Minor,Closed,Fixed,20/Jul/12 00:10,06/Apr/13 07:06
Bug,HBASE-6432,12599576,HRegionServer doesn't properly set clusterId in conf,"ClusterId is normally set into the passed conf during instantiation of an HTable class. In the case of a HRegionServer this is bypassed and set to ""default"" since getMaster() since it uses HBaseRPC to create the proxy directly and bypasses the class which retrieves and sets the correct clusterId. 

This becomes a problem with clients (ie within a coprocessor) using delegation tokens for authentication. Since the token's service will be the correct clusterId and while the TokenSelector is looking for one with service ""default"".",toffer,toffer,Major,Closed,Fixed,20/Jul/12 04:57,07/Apr/13 04:38
Bug,HBASE-6437,12599718,Avoid admin.balance during master initialize,In HBASE-5850 many of the admin operations have been blocked till the master initializes.  But the balancer is not.  So this JIRA is to extend the PleaseHoldException in case of admin.balance() call before master is initialized.,rajesh23,ram_krish,Major,Closed,Fixed,21/Jul/12 11:52,07/Apr/13 04:39
Bug,HBASE-6438,12599720,RegionAlreadyInTransitionException needs to give more info to avoid assignment inconsistencies,"Seeing some of the recent issues in region assignment, RegionAlreadyInTransitionException is one reason after which the region assignment may or may not happen(in the sense we need to wait for the TM to assign).
In HBASE-6317 we got one problem due to RegionAlreadyInTransitionException on master restart.
Consider the following case, due to some reason like master restart or external assign call, we try to assign a region that is already getting opened in a RS.
Now the next call to assign has already changed the state of the znode and so the current assign that is going on the RS is affected and it fails.  The second assignment that started also fails getting RAITE exception.  Finally both assignments not carrying on.  Idea is to find whether any such RAITE exception can be retried or not.
Here again we have following cases like where
-> The znode is yet to transitioned from OFFLINE to OPENING in RS
-> RS may be in the step of openRegion.
-> RS may be trying to transition OPENING to OPENED.
-> RS is yet to add to online regions in the RS side.

Here in openRegion() and updateMeta() any failures we are moving the znode to FAILED_OPEN.  So in these cases getting an RAITE should be ok.  But in other cases the assignment is stopped.
The idea is to just add the current state of the region assignment in the RIT map in the RS side and using that info we can determine whether the assignment can be retried or not on getting an RAITE.

Considering the current work going on in AM, pls do share if this is needed atleast in the 0.92/0.94 versions?  ",rajesh23,ram_krish,Major,Closed,Fixed,21/Jul/12 12:05,07/Apr/13 04:39
Bug,HBASE-6439,12599763,Ignore .archive directory as a table,"From a recent test run:
{quote}
2012-07-22 02:27:30,699 WARN  [IPC Server handler 0 on 47087] util.FSTableDescriptors(168): The following folder is in HBase's root directory and doesn't contain a table descriptor, do consider deleting it: .archive
{quote}

With the addition of HBASE-5547, table-level folders are no-longer all table folders. FSTableDescriptors needs to then have a 'gold-list' that we can update with directories that aren't tables so we don't have this kind of thing showing up in the logs.

Currently, we have the following block:
{quote}
    invocations++;
    if (HTableDescriptor.ROOT_TABLEDESC.getNameAsString().equals(tablename)) {
      cachehits++;
      return HTableDescriptor.ROOT_TABLEDESC;
    }
    if (HTableDescriptor.META_TABLEDESC.getNameAsString().equals(tablename)) {
      cachehits++;
      return HTableDescriptor.META_TABLEDESC;
    }
{quote}

to handle special cases, but that's a bit clunky and not clean in terms of table-level directories that need to be ignored.",jesse_yates,jesse_yates,Major,Closed,Fixed,22/Jul/12 16:18,05/Aug/14 20:12
Bug,HBASE-6440,12599774,SplitLogManager - log the exception when failed to finish split log file,"We should log the exception itself too:

{noformat}
        try {
          HLogSplitter.moveRecoveredEditsFromTemp(tmpname, logfile, conf);
        } catch (IOException e) {
          LOG.warn(""Could not finish splitting of log file "" + logfile);
          return Status.ERR;
        }
{noformat}",jxiang,jxiang,Major,Closed,Fixed,22/Jul/12 18:51,26/Feb/13 08:16
Bug,HBASE-6441,12599779,MasterFS doesn't set scheme for internal FileSystem,"FSUtils.getRootDir() just takes a configuration object, which is used to:
1) Get the name of the root directory
2) Create a filesystem (based on the configured scheme)
3) Qualify the root onto the filesystem

However, the FileSystem from the master filesystem won't generate the correctly qualified root directory under hadoop-2.0 (though it works fine on hadoop-1.0). Seems to be an issue with the configuration parameters.",jesse_yates,jesse_yates,Major,Closed,Fixed,22/Jul/12 20:44,23/Sep/13 18:30
Bug,HBASE-6445,12599963,rat check fails if hs_err_pid26514.log dropped in tests,Let test fail because jvm crashed rather than because of rat license complaint,stack,stack,Major,Closed,Fixed,24/Jul/12 08:47,26/Feb/13 08:16
Bug,HBASE-6447,12600026,Common TestZooKeeper failures on jenkins: testMasterSessionExpired and testCreateSilentIsReallySilent,"Studying 0.94 failures of late, in the last 15 or so builds, TestZooKeeper has failed at least three times.  Running it local, it fails 50% of the time: once with testCreateSilentIsReallySilent and then less frequently with testMasterExpiration.",stack,stack,Major,Closed,Fixed,24/Jul/12 15:37,24/Oct/12 05:40
Bug,HBASE-6448,12600040,Add RecoverableZooKeeper#setACL which handles transient zookeeper exception,"In HBASE-6447, Stack added retry logic for calling ZooKeeper#setACL.

The retry logic should be encapsulated in a new method: RecoverableZooKeeper#setACL",,zhihyu@ebaysf.com,Major,Closed,Fixed,24/Jul/12 17:15,13/Jun/22 18:54
Bug,HBASE-6450,12600067,HBase startup should be with MALLOC_MAX_ARENA set,I think we should do the same as what HADOOP-7154 has done in terms of starting up daemons with MALLOC_MAX_ARENA set. I recently noticed that there were RS crashes on RHEL6 due to memory (could be avoided by setting MALLOC_MAX_ARENA).,ddas,ddas,Major,Closed,Fixed,24/Jul/12 21:00,24/Oct/12 05:40
Bug,HBASE-6455,12600243,org.apache.hadoop.hbase.PerformanceEvaluation sets the map reduce output path as a child of input path,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.",adityakishore,adityakishore,Minor,Closed,Fixed,26/Jul/12 00:30,23/Sep/13 18:44
Bug,HBASE-6460,12600300,"hbck ""-repairHoles"" usage inconsistent with ""-fixHdfsOrphans""","According to the hbck's help info, shortcut - ""-repairHoles"" will enable ""-fixHdfsOrphans"" as below.
{noformat}
 -repairHoles      Shortcut for -fixAssignments -fixMeta -fixHdfsHoles -fixHdfsOrphans
{noformat}
However, in the implementation, the function ""fsck.setFixHdfsOrphans(false);"" is called in ""-repairHoles"". This is not consistent with the usage information.",grace.huang,grace.huang,Minor,Closed,Fixed,26/Jul/12 12:00,15/Oct/13 04:46
Bug,HBASE-6468,12600519,RowCounter may return incorrect result if column name is specified in command line,"The RowCounter use FirstKeyOnlyFilter regardless of whether or not the
command line argument specified a column family (or family:qualifier).
In case when no qualifier was specified as argument, the scan will
give correct result. However in the other case the scan instance may
have been set with columns other than the very first column in the
row, causing scan to get nothing as the FirstKeyOnlyFilter removes
everything else.

https://issues.apache.org/jira/browse/HBASE-6042 is related. ",shrijeet,shrijeet,Major,Closed,Fixed,27/Jul/12 21:43,27/Mar/15 10:34
Bug,HBASE-6471,12600593,Performance regression caused by HBASE-4054,"The patch in HBASE-4054 switches the PooledHTable to extend HTable as opposed to implement HTableInterface.

Since HTable does not have an empty constructor, the patch added a call to the super() constructor, which though does trigger the ZooKeeper and META scan, causing a considerable delay. 

With multiple threads using the pool in parallel, the first thread is holding up all the subsequent ones, in effect it negates the whole reason we have a HTable pool.

We should complete HBASE-5728, or alternatively add a protected, empty constructor the HTable. I am +1 for the former.",jxiang,larsgeorge,Critical,Closed,Fixed,29/Jul/12 12:48,02/May/13 02:29
Bug,HBASE-6478,12600778,TestClassLoading.testClassLoadingFromLibDirInJar occasionally fails,"When hudson runs for HBASE-6459, it encounters a failed testcase in org.apache.hadoop.hbase.coprocessor.TestClassLoading.testClassLoadingFromLibDirInJar. The link is https://builds.apache.org/job/PreCommit-HBASE-Build/2455/testReport/org.apache.hadoop.hbase.coprocessor/TestClassLoading/testClassLoadingFromLibDirInJar/

I check the log, and find that the function waitTableAvailable will only check the meta table, when rs open the region and update the metalocation in meta, it may not be added to the onlineregions in rs.

for (HRegion region:
        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {

this Loop will ship, and found1 will be false altogether.
that's why the testcase failed.

So maybe we can  hbave some strictly check when table is created
",,zhou wenjian,Major,Closed,Fixed,31/Jul/12 04:42,07/Apr/13 04:41
Bug,HBASE-6479,12600785,HFileReaderV1 caching the same parent META block could cause server abort when splitting,"If the hfile's version is 1 now, when splitting, two daughters would loadBloomfilter concurrently in the open progress. Because their META block is the same one(parent's META block),  the following expection would be thrown when doing HFileReaderV1#getMetaBlock
{code}
java.io.IOException: Failed null-daughterOpener=af73f8c9a9b409531ac211a9a7f92eba
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughters(SplitTransaction.java:367)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:453)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.testWholesomeSplit(TestSplitTransaction.java:225)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.testWholesomeSplitWithHFileV1(TestSplitTransaction.java:203)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
	at org.junit.rules.RunRules.evaluate(RunRules.java:18)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.io.IOException: java.io.IOException: java.lang.RuntimeException: Cached an already cached block
	at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:540)
	at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:463)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3784)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughterRegion(SplitTransaction.java:506)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction$DaughterOpener.run(SplitTransaction.java:486)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: java.lang.RuntimeException: Cached an already cached block
	at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:424)
	at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:271)
	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:2918)
	at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:516)
	at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	... 1 more
Caused by: java.lang.RuntimeException: Cached an already cached block
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(LruBlockCache.java:271)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV1.getMetaBlock(HFileReaderV1.java:258)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV1.getGeneralBloomFilterMetadata(HFileReaderV1.java:689)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.loadBloomfilter(StoreFile.java:1564)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.access$1(StoreFile.java:1558)
	at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:571)
	at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:606)
	at org.apache.hadoop.hbase.regionserver.Store$1.call(Store.java:395)
	at org.apache.hadoop.hbase.regionserver.Store$1.call(Store.java:1)
	... 8 more

{code}

We could reproduce the problem through the attached test patch,

It would happen when cluster upgrading from 0.90.x to 0.94.x or 0.92.x


",zjushch,zjushch,Major,Closed,Fixed,31/Jul/12 06:22,23/Sep/13 18:30
Bug,HBASE-6481,12600904,SkipFilter javadoc is incorrect,"The javadoc for SkipFilter (http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SkipFilter.html) states : 
 
A wrapper filter that filters an entire row if any of the KeyValue checks do not pass.
 
But the example same javadocs gives to support this statement is wrong. The *scan.setFilter(new SkipFilter(new ValueFilter(CompareOp.EQUAL,
     new BinaryComparator(Bytes.toBytes(0))));* , will only emit rows which have all column values zero. In other words it is going to skip all rows for which 
ValueFilter(CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(0))) does not pass , which happen to be all non zero valued cells. 

In the same example a ValueFilter created with CompareOp.NOT_EQUAL will filter out the rows which have a column value zero. ",shrijeet,shrijeet,Minor,Closed,Fixed,31/Jul/12 20:21,05/Aug/14 20:12
Bug,HBASE-6488,12600943,HBase wont run on IPv6 on OSes that use zone-indexes,"In IPv6, an address may have a zone-index, which is specified with a percent, eg: ...%0.  This looks like a format string, and thus in a part of the code which uses the hostname as a prefix to another string which is interpreted with String.format, you end up with an exception:


2012-07-31 18:21:39,848 FATAL org.apache.hadoop.hbase.master.HMaster:
Unhandled exception. Starting shutdown.
java.util.UnknownFormatConversionException: Conversion = '0'
        at java.util.Formatter.checkText(Formatter.java:2503)
        at java.util.Formatter.parse(Formatter.java:2467)
        at java.util.Formatter.format(Formatter.java:2414)
        at java.util.Formatter.format(Formatter.java:2367)
        at java.lang.String.format(String.java:2769)
        at com.google.common.util.concurrent.ThreadFactoryBuilder.setNameFormat(ThreadFactoryBuilder.java:68)
        at org.apache.hadoop.hbase.executor.ExecutorService$Executor.<init>(ExecutorService.java:299)
        at org.apache.hadoop.hbase.executor.ExecutorService.startExecutorService(ExecutorService.java:185)
        at org.apache.hadoop.hbase.executor.ExecutorService.startExecutorService(ExecutorService.java:227)
        at org.apache.hadoop.hbase.master.HMaster.startServiceThreads(HMaster.java:821)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:507)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:344)
        at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:220)
        at java.lang.Thread.run(Thread.java:680)
2012-07-31 18:21:39,908 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
",ryanobjc,ryanobjc,Major,Closed,Fixed,01/Aug/12 01:40,07/Apr/13 04:42
Bug,HBASE-6489,12600955,Incorrect TaskTracker logfile name,"http://hbase.apache.org/book/trouble.log.html
""TaskTracker: $HADOOP_HOME/logs/hadoop-<user>-jobtracker-<hostname>.log""
should be 
""TaskTracker: $HADOOP_HOME/logs/hadoop-<user>-tasktracker-<hostname>.log""",,xieliang007,Minor,Closed,Fixed,01/Aug/12 07:18,23/Sep/13 18:45
Bug,HBASE-6493,12601138,HashSet of byte array is being used in couple of places,"While working on a jira I realized I had made a mistake of making a HashSet of byte array. 
Then out of curiosity I checked if we do same any where else in code base. I came with following files. 

# /src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java:    Set<byte []> qualifiers = new HashSet<byte[]>();
# /src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:        columnFamilies = new HashSet<byte[]>();
# /src/test/java/org/apache/hadoop/hbase/filter/TestFirstKeyValueMatchingQualifiersFilter.java:    Set<byte[]> quals = new HashSet<byte[]>();
# /src/test/java/org/apache/hadoop/hbase/regionserver/metrics/TestSchemaMetrics.java:    Set<byte[]> families = new HashSet<byte[]>();
(1) and (3) are mine and I will fix them (not yet committed). Quoting the exact reference from (2) below :

{code}
 @Override                                                                     
  public GetStoreFileResponse getStoreFile(final RpcController controller,          
      final GetStoreFileRequest request) throws ServiceException {              
    try {                                                                       
      HRegion region = getRegion(request.getRegion());                          
      requestCount.incrementAndGet();                                           
      Set<byte[]> columnFamilies = null;                                        
      if (request.getFamilyCount() == 0) {                                      
        columnFamilies = region.getStores().keySet();                           
      } else {                                                                  
        columnFamilies = new HashSet<byte[]>();                                 
        for (ByteString cf: request.getFamilyList()) {                          
          columnFamilies.add(cf.toByteArray());                                 
        }                                                                       
      }  
{code}",ndimiduk,shrijeet,Minor,Closed,Fixed,01/Aug/12 22:42,23/Sep/13 18:45
Bug,HBASE-6495,12601141,HBaseAdmin shouldn't expect HConnection to be an HConnectionImplementation,"Currently, the HBaseAdmin has a constructor that takes an HConnection, but then immediately casts it to an HConnectionManager.HConnectionImplementation:
{code}
  public HBaseAdmin(HConnection connection)
      throws MasterNotRunningException, ZooKeeperConnectionException {
    this.conf = connection.getConfiguration();

    // We want the real class, without showing it our public interface,
    //  hence the cast.
    this.connection = (HConnectionManager.HConnectionImplementation)connection;
{code}

However, this breaks the explicit contract in the javadocs and makes it basically impossible to mock out the hbaseadmin. 

We need to either make the hbaseadmin use a basic HConnection and optimize for cases where its smarter or bring up the couple of methods in HConnectionManager.HConnectionImplementation to the HConnection interface.",jesse_yates,jesse_yates,Major,Closed,Fixed,02/Aug/12 00:15,23/Sep/13 18:31
Bug,HBASE-6499,12601187,StoreScanner's QueryMatcher not reset on store update,"When underlying store changed (due compact, bulk load, etc), we destroy current KeyValueHeap and recreate it using checkReseek call. Besides heap recreation, it resets underlying QueryMatcher instance.

The problem is that checkReseek not called by seek() and reseek(), only by next(). If someone calls seek() just after store changed, it gets wrong scanner results. Call to reseek may end up with NPE.

AFAIK, current codebase don't call seek and reseek, but it is quite possible in future. Personally, I spent lots of time to find source of wrong scanner results in HBASE-5416.
",shmuma,shmuma,Major,Closed,Fixed,02/Aug/12 09:20,23/Sep/13 18:31
Bug,HBASE-6503,12601261,HBase Shell Documentation For DROP Is Outdated,"HBase Shell help documentation for the drop command says:


""If table has more than one region, run a major compaction on .META.""

According to JD this is old news:

jdcryans: back in the days when hadoop didn't support durability it was possible to lose .META. data so we were force flushing .META. and major compacting it all the time also we used to have consistency issues that major compacting was solving ahhh the good old days",paulcavallaro,paulcavallaro,Trivial,Closed,Fixed,02/Aug/12 19:18,24/Oct/12 05:40
Bug,HBASE-6504,12601277,Adding GC details prevents HBase from starting in non-distributed mode,"The {{conf/hbase-env.sh}} that ships with HBase contains a few commented out examples of variables that could be useful, such as adding {{-XX:+PrintGCDetails -XX:+PrintGCDateStamps}} to {{HBASE_OPTS}}.  This has the annoying side effect that the JVM prints a summary of memory usage when it exits, and it does so on stdout:

{code}
$ ./bin/hbase org.apache.hadoop.hbase.util.HBaseConfTool hbase.cluster.distributed
false
Heap
 par new generation   total 19136K, used 4908K [0x000000073a200000, 0x000000073b6c0000, 0x0000000751860000)
  eden space 17024K,  28% used [0x000000073a200000, 0x000000073a6cb0a8, 0x000000073b2a0000)
  from space 2112K,   0% used [0x000000073b2a0000, 0x000000073b2a0000, 0x000000073b4b0000)
  to   space 2112K,   0% used [0x000000073b4b0000, 0x000000073b4b0000, 0x000000073b6c0000)
 concurrent mark-sweep generation total 63872K, used 0K [0x0000000751860000, 0x00000007556c0000, 0x00000007f5a00000)
 concurrent-mark-sweep perm gen total 21248K, used 6994K [0x00000007f5a00000, 0x00000007f6ec0000, 0x0000000800000000)
$ ./bin/hbase org.apache.hadoop.hbase.util.HBaseConfTool hbase.cluster.distributed >/dev/null
(nothing printed)
{code}

And this confuses {{bin/start-hbase.sh}} when it does
{{distMode=`$bin/hbase --config ""$HBASE_CONF_DIR"" org.apache.hadoop.hbase.util.HBaseConfTool hbase.cluster.distributed`}}, because then the {{distMode}} variable is not just set to {{false}}, it also contains all this JVM spam.

If you don't pay enough attention and realize that 3 processes are getting started (ZK, HM, RS) instead of just one (HM), then you end up with this confusing error message:
{{Could not start ZK at requested port of 2181.  ZK was started at port: 2182.  Aborting as clients (e.g. shell) will not be able to find this ZK quorum.}}, which is even more puzzling because when you run {{netstat}} to see who owns that port, then you won't find any rogue process other than the one you just started.

I'm wondering if the fix is not to just change the {{if [ ""$distMode"" == 'false' ]}} to a {{switch $distMode case (false*)}} type of test, to work around this annoying JVM misfeature that pollutes stdout.",mdrzal,tsuna,Trivial,Closed,Fixed,02/Aug/12 22:21,05/Aug/14 20:11
Bug,HBASE-6506,12601285,Setting CACHE_BLOCKS to false in an hbase shell scan doesn't work,"I was attempting to prevent blocks from being cached by setting CACHE_BLOCKS => false in the hbase shell when doing a scan but I kept seeing tons of evictions when I ran it. After inspecting ""table.rb"" I found this line:

cache = args[""CACHE_BLOCKS""] || true

The problem then is that if CACHE_BLOCKS is false then this expression will always return true. Therefore, it's impossible to turn off block caching. ",stack,jbwyme,Minor,Closed,Fixed,02/Aug/12 23:24,21/Feb/15 23:34
Bug,HBASE-6510,12601435,Fix HConnection typo in TestFromClientSide,"{code}
* API that accept a pre-created HConnction instance
{code}",gchanan,gchanan,Trivial,Closed,Fixed,03/Aug/12 22:25,05/Aug/14 20:11
Bug,HBASE-6512,12601487,Incorrect OfflineMetaRepair log class name,"At the beginning of OfflineMetaRepair.java, we can observe:
""private static final Log LOG = LogFactory.getLog(HBaseFsck.class.getName());""

It would be better change to :
""private static final Log LOG = LogFactory.getLog(OfflineMetaRepair.class.getName());""",xieliang007,xieliang007,Major,Closed,Fixed,05/Aug/12 05:07,07/Apr/13 04:43
Bug,HBASE-6513,12601530,Test errors when building on MacOS,"Results :

Failed tests:   testBackgroundEvictionThread[0](org.apache.hadoop.hbase.io.hfile.TestLruBlockCache): expected:<2> but was:<1>
  testBackgroundEvictionThread[1](org.apache.hadoop.hbase.io.hfile.TestLruBlockCache): expected:<2> but was:<1>
  testSplitCalculatorEq(org.apache.hadoop.hbase.util.TestRegionSplitCalculator): expected:<2> but was:<1>
",larsfrancke,trajano,Major,Closed,Fixed,06/Aug/12 05:14,05/Mar/13 07:52
Bug,HBASE-6514,12601531,unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.MetricsHistogram,"When trying to run a unit test that just starts up and shutdown the server the following errors occur in System.out

01:10:59,874 ERROR MetricsUtil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.MetricsHistogram
01:10:59,874 ERROR MetricsUtil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.MetricsHistogram
01:10:59,875 ERROR MetricsUtil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.MetricsHistogram
01:10:59,875 ERROR MetricsUtil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.MetricsHistogram
",eclark,trajano,Major,Closed,Fixed,06/Aug/12 05:32,07/Apr/13 04:43
Bug,HBASE-6516,12601562,"hbck cannot detect any IOException while "".tableinfo"" file is missing","HBaseFsck checks those missing .tableinfo files in loadHdfsRegionInfos() function. However, no IoException will be catched while .tableinfo is missing, since ""FSTableDescriptors.getTableDescriptor"" doesn't throw any IoException.",grace.huang,grace.huang,Major,Closed,Fixed,06/Aug/12 12:46,07/Apr/13 04:44
Bug,HBASE-6518,12601584,Bytes.toBytesBinary() incorrect trailing backslash escape,"Bytes.toBytesBinary() converts escaped strings to byte arrays. When encountering a '\' character, it looks at the next one to see if it is an 'x', without checking if it exists.",tscurtu,tscurtu,Trivial,Closed,Fixed,06/Aug/12 17:13,23/Sep/13 18:44
Bug,HBASE-6520,12601664,MSLab May cause the Bytes.toLong not work correctly for increment,"When use MemStoreLAB, the KeyValues will share the byte array allocated by the MemStoreLAB, all the KeyValues' ""bytes"" attributes are the same byte array. When use the functions such as Bytes.toLong(byte[] bytes, int offset):
{code}
  public static long toLong(byte[] bytes, int offset) {
    return toLong(bytes, offset, SIZEOF_LONG);
  }

  public static long toLong(byte[] bytes, int offset, final int length) {
    if (length != SIZEOF_LONG || offset + length > bytes.length) {
      throw explainWrongLengthOrOffset(bytes, offset, length, SIZEOF_LONG);
    }
    long l = 0;
    for(int i = offset; i < offset + length; i++) {
      l <<= 8;
      l ^= bytes[i] & 0xFF;
    }
    return l;
  }
{code}
If we do not put a long value to the KeyValue, and read it as a long value in HRegion.increment(),the check 
{code}
offset + length > bytes.length
{code}
will take no effects, because the bytes.length is not equal to keyLength+valueLength, indeed it is MemStoreLAB chunkSize which is default 2048 * 1024.



I will paste the patch later.",xingshi,xingshi,Major,Closed,Fixed,07/Aug/12 04:30,07/Apr/13 04:44
Bug,HBASE-6525,12602091,bin/replication/copy_tables_desc.rb references non-existent class,"$ hbase org.jruby.Main copy_tables_desc.rb
NameError: cannot load Java class org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper
  get_proxy_or_package_under_package at org/jruby/javasupport/JavaUtilities.java:54
                      method_missing at file:/mnt/data/hbase/lib/jruby-complete-1.6.5.jar!/builtin/javasupport/java.rb:51
                              (root) at copy_tables_desc.rb:35

Removing the line that references the non-existent class seems to make the script work without any visible side-effects.",dsw,dsw,Trivial,Closed,Fixed,07/Aug/12 22:24,05/Aug/14 20:11
Bug,HBASE-6528,12602122,Raise the wait time for TestSplitLogWorker#testAcquireTaskAtStartup to reduce the failure probability,"All the code for the TestSplitLogWorker, only the testAcquireTaskAtStartup waits 100ms, other testCase wait 1000ms. The 100ms is short and sometimes causes testAcquireTaskAtStartup failure.",xingshi,xingshi,Major,Closed,Fixed,08/Aug/12 05:09,23/Sep/13 18:31
Bug,HBASE-6529,12602365,"With HFile v2, the region server will always perform an extra copy of source files","With HFile v2 implementation in HBase 0.94 & 0.96, the region server will use HFileSystem as its {color:blue}fs{color}. When it performs bulk load in Store.bulkLoadHFile(), it checks if its {color:blue}fs{color} is the same as {color:blue}srcFs{color}, which however will be DistributedFileSystem. Consequently, it will always perform an extra copy of source files.",grace.huang,jason.dai,Major,Closed,Fixed,09/Aug/12 07:08,07/Apr/13 04:45
Bug,HBASE-6537,12602431,Race between balancer and disable table can lead to inconsistent cluster,"Appear in 94. trunk is ok for the issue
Balancer will collect the regionplans to move(unassign and then assign).
before unassign, disable table appears, 
after close the region in rs, master will delete the znode, romove region from RIT,
and then clean the region from the online regions.

During romoving region from RIT and cleaning out the region from the online regions. 
balancer begins to unassign, it will get a NotServingRegionException and if the table is disabling, it will deal with the state in master and delete the znode . However the table is disabled now, so the RIT and znode will remain. TimeoutMonitor draws a blank on it.

It will hold back enabling the table or balancer unless restart
",zhou wen jian,zhou wen jian,Major,Closed,Fixed,09/Aug/12 09:16,31/Mar/14 10:38
Bug,HBASE-6552,12602644,TestAcidGuarantees system test should flush more aggressively,"HBASE-5887 allowed TestAcidGuarantees to be run as a system test by avoiding the call to util.flush().

It would be better to go through the HBaseAdmin interface to force flushes.  This would unify the code path between the unit test and the system test, as well as forcing more frequent flushes, which have previously been the source of ACID guarantee problems, e.g. HBASE-2856.",gchanan,gchanan,Major,Closed,Fixed,09/Aug/12 20:03,07/Apr/13 04:45
Bug,HBASE-6561,12603148,"Gets/Puts with many columns send the RegionServer into an ""endless"" loop","This came from the mailing this:
We were able to replicate this behavior in a pseudo-distributed hbase
(hbase-0.94.1) environment. We wrote a test program that creates a test
table ""MyTestTable"" and populates it with random rows, then it creates a
row with 60,000 columns and repeatedly updates it. Each column has a 18
byte qualifier and a 50 byte value. In our tests, when we ran the
program, we usually never got beyond 15 updates before it would flush
for a really long time. The rows that are being updated are about 4MB
each (minues any hbase metadata).

It doesn't seem like it's caused by GC. I turned on gc logging, and
didn't see any long pauses. This is the gc log during the flush.
http://pastebin.com/vJKKXDx5

This is the regionserver log with debug on during the same flush
http://pastebin.com/Fh5213mg

This is the test program we wrote.
http://pastebin.com/aZ0k5tx2

You should be able to just compile it, and run it against a running
HBase cluster.
$ java TestTable

Carlos
",larsh,larsh,Major,Closed,Fixed,12/Aug/12 21:26,07/Apr/13 04:46
Bug,HBASE-6562,12603154,Fake KVs are sometimes passed to filters,"In internal tests at Salesforce we found that fake row keys sometimes are passed to filters (Filter.filterRowKey(...) specifically).

The KVs are eventually filtered by the StoreScanner/ScanQueryMatcher, but the row key is passed to filterRowKey in RegionScannImpl *before* that happens.",,larsh,Minor,Closed,Fixed,12/Aug/12 22:53,13/Jun/22 19:07
Bug,HBASE-6564,12603175,HDFS space is not reclaimed when a column family is deleted,"When a column family of a table is deleted, the HDFS space of the column family does not seem to be reclaimed even after a major compaction.",zahoor,zahoor,Minor,Closed,Fixed,13/Aug/12 05:31,23/Sep/13 18:45
Bug,HBASE-6565,12603182,Coprocessor exec result Map is not thread safe,"I develop a coprocessor program ,but found some different results in repeated tests.for example,normally,the result's size is 10.but sometimes it appears 9.
I read the HTable.java code,found a TreeMap(thread-unsafe) be used in multithreading environment.It cause the bug happened",kirayuan,kirayuan,Major,Closed,Fixed,13/Aug/12 06:18,07/Apr/13 04:46
Bug,HBASE-6576,12603359,HBaseAdmin.createTable should wait until the table is enabled,"The function:
{code}
public void createTable(final HTableDescriptor desc, byte [][] splitKeys)
{code}

in HBaseAdmin is synchronous and returns once all the regions of the table are online, but does not wait for the table to be enabled, which is the last step of table creation (see CreateTableHandler).

This is confusing and leads to racy code because users do not realize that this is the case.  For example, I saw the following test failure in 0.92 when I ran:
mvn test -Dtest=org.apache.hadoop.hbase.client.TestAdmin#testEnableDisableAddColumnDeleteColumn 

{code}
Error Message

org.apache.hadoop.hbase.TableNotEnabledException: testMasterAdmin at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75) at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1154) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)

Stacktrace

org.apache.hadoop.hbase.TableNotEnabledException: org.apache.hadoop.hbase.TableNotEnabledException: testMasterAdmin
at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75)
at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1154)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)
{code}

The issue is that code will create and table and immediately disable it in order to do some testing, for example, to test an operation that only works when the table is disabled.  If the table has not been enabled yet, they will get back a TableNotEnabledException.

The specific test above was fixed in HBASE-5206, but other examples exist in the code, for example the following:
{code}
hbase org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat newtable asdf14
{code}

The code in question is:
{code}
byte[] tname = args[1].getBytes();
HTable table = util.createTable(tname, FAMILIES);
HBaseAdmin admin = new HBaseAdmin(conf);
admin.disableTable(tname);
{code}

It would be better if createTable just waited until the table was enabled, or threw a TableNotEnabledException if it exhausted the configured number of retries.",gchanan,gchanan,Major,Closed,Fixed,14/Aug/12 00:01,07/Apr/13 04:47
Bug,HBASE-6579,12603383,Unnecessary KV order check in StoreScanner,"In StoreScanner.next(List<KeyValue>, int, String) I find this code:
{code}
      // Check that the heap gives us KVs in an increasing order.
      if (prevKV != null && comparator != null
          && comparator.compare(prevKV, kv) > 0) {
        throw new IOException(""Key "" + prevKV + "" followed by a "" +
            ""smaller key "" + kv + "" in cf "" + store);
      }
      prevKV = kv;
{code}

So this checks for bugs in the HFiles or the scanner code. It needs to compare each KVs with its predecessor. This seems unnecessary now, I propose that we remove this.",larsh,larsh,Minor,Closed,Fixed,14/Aug/12 05:00,07/Apr/13 04:47
Bug,HBASE-6582,12603498,Code generation does run under m2eclipse,"When going through the instructions on http://hbase.apache.org/book/ides.html, the build will fail in eclipse because none of the sources are generated.  After looking at our pom and reading http://wiki.eclipse.org/M2E_compatible_maven_plugins, we are telling m2eclipse to ignore the avro and jamon plugins.",mdrzal,mdrzal,Minor,Closed,Fixed,14/Aug/12 19:55,23/Sep/13 18:45
Bug,HBASE-6583,12603499,Enhance Hbase load test tool to automatically create column families if not present,"The load test tool currently disables the table and applies any changes to the cf descriptor if any, but does not create the cf if not present.",sershe,karthik.ranga,Major,Closed,Fixed,14/Aug/12 20:03,05/Aug/14 20:11
Bug,HBASE-6584,12603503,TestAssignmentManager#testDisablingTableRegionsAssignmentDuringCleanClusterStartup fails due to port 60000 already in use.,,rajesh23,sameerv,Critical,Closed,Fixed,14/Aug/12 20:39,23/Sep/13 18:31
Bug,HBASE-6587,12603555,Region would be assigned twice in the case of all RS offline,"In the TimeoutMonitor, we would act on time out for the regions if (this.allRegionServersOffline && !noRSAvailable)
The code is as the following:
{code}
 if (regionState.getStamp() + timeout <= now ||
          (this.allRegionServersOffline && !noRSAvailable)) {
          //decide on action upon timeout or, if some RSs just came back online, we can start the
          // the assignment
          actOnTimeOut(regionState);
        }
{code}

But we found it exists a bug that it would act on time out for the region which was assigned just now , and cause assigning the region twice.


Master log for the region 277b9b6df6de2b9be1353b4fa25f4222:
{code}
2012-08-14 20:42:54,367 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Unable to determine a plan to assign .META.,,1.1028785192 state=OFFLINE, ts=1
344948174367, server=null
2012-08-14 20:44:31,640 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for writete
st,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df6de2b9be1353b4fa25f4222. so generated a random one; hri=writetest,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df6de2b9be13
53b4fa25f4222., src=, dest=dw92.kgb.sqa.cm4,60020,1344948267642; 1 (online=1, available=1) available servers
2012-08-14 20:44:31,640 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x438f53bbf9b0acd Creating (or updating) unassigned node for 277b9b6df6de2b9be13
53b4fa25f4222 with OFFLINE state
2012-08-14 20:44:31,643 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df6de2b9be1353b4fa
25f4222. to dw92.kgb.sqa.cm4,60020,1344948267642
2012-08-14 20:44:32,291 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=dw92.kgb.sqa.cm4,60020,1344948267642, 
region=277b9b6df6de2b9be1353b4fa25f4222
// 异常的超时
2012-08-14 20:44:32,518 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: writetest,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df
6de2b9be1353b4fa25f4222. state=OPENING, ts=1344948272279, server=dw92.kgb.sqa.cm4,60020,1344948267642
2012-08-14 20:44:32,518 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPENING for too long, reassigning region=writetest,VHXYHJN0BL48HMR4DI1L,
1344925649429.277b9b6df6de2b9be1353b4fa25f4222.
{code}",zjushch,zjushch,Major,Closed,Fixed,15/Aug/12 06:56,07/Apr/13 04:48
Bug,HBASE-6590,12603606,[0.89-fb] Assign sequence number to bulk loaded data,"Currently bulk loaded files are not assigned a sequence number. Thus, they can only be used to import historical data, dating to the past. There are cases where we want to bulk load ""current data""; but the bulk load mechanism does not support this, as the bulk loaded files are always sorted behind the non-bulkloaded hfiles. Assigning Sequence Id to bulk loaded files should solve this issue.

StoreFiles within a store are sorted based on the sequenceId. SequenceId is a monotonically increasing number that accompanies every edit written to the WAL. For entries that update the same cell, we would like the latter edit to win. This comparision is accomplished using memstoreTS, at the KV level; and sequenceId at the StoreFile level (to order scanners in the KeyValueHeap).

BulkLoaded files are generated outside of HBase/RegionServer, so they do not have a sequenceId written in the file.  This causes HBase to lose track of the point in time, when the BulkLoaded file was imported to HBase. Resulting in a behavior, that **only** supports viewing bulkLoaded files as files back-filling data from the begining of time.

By assigning a sequence number to the file, we can allow the bulk loaded file to fit in where we want. Either at the ""current time"" or the ""begining of time"". The latter is the default, to maintain backward compatibility.

Design approach:
  Store files keep track of the sequence Id in the trailer. Since we do not wish to edit/rewrite the bulk loaded file upon import, we will encode the assigned sequenceId into the fileName. The filename RegEx is updated for this regard. If the sequenceId is encoded in the filename, the sequenceId will be used as the sequenceId for the file. If none is found, the sequenceId will be considered 0 (as per the default, backward-compatible behavior).

  To enable clients to request pre-existing behavior, the command line utility allows for 2 ways to import BulkLoaded Files: to assign or not assign a sequence Number. 
   - If a sequence Number is assigned, the imporeted file will be imported with the ""current sequence Id"".
   - if the sequence Number is not assigned, it will be as if it was backfilling old data, from the begining of time.

Compaction behavior:
  - With the current compaction algorithm, bulk loaded files -- that backfill data, to the begining of time -- can cause a compaction storm, converting every minor compaction to a major compaction. To address this, these files are excluded from minor compaction, based on a config param. (enabled for the messages use case).
   - Since, bulk loaded files that are not back-filling data do not cause this issue, they will not be ignored during minor compactions based on the config parameter. This is also required to ensure that there are no holes in the set of files selected for compaction -- this is necessary to preserve the order of KV's comparision before and after compaction.
",amitanand,amitanand,Minor,Closed,Fixed,15/Aug/12 15:54,13/Jun/22 19:02
Bug,HBASE-6595,12603729,Wrong book author name on Other HBase Resource page,There is a typing miss of the HBase Administration Cookbook's author name on the Other HBase Resource page.,uprush,uprush,Trivial,Closed,Fixed,16/Aug/12 13:51,13/Jun/22 19:02
Bug,HBASE-6596,12603754,Revert HBASE-5022; it undoes HBC.create,See this thread: http://search-hadoop.com/m/WAXXV1OQ1QY/What+is+HBaseConfiguration.create%2528Configuration%2529+good+for%253F&subj=What+is+HBaseConfiguration+create+Configuration+good+for+,stack,stack,Major,Closed,Fixed,16/Aug/12 17:05,24/Oct/12 05:40
Bug,HBASE-6602,12603803,Region Server Dynamic Metrics can cause high cpu usage.,"When regions are getting added and removed lots of cpu time can be used by jmx.  This is caused by sending jmx messages for every new metric that is added or removed.

Seeing jstacks like this:

""RMI TCP Connection(3)-10.4.19.33"" daemon prio=10 tid=0x00007f9d64b1d000 nid=0x353 runnable [0x00007f9d598d6000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.put(HashMap.java:374)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:103)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:75)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttribute(MetricsDynamicMBeanBase.java:133)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
--
	at java.util.TimerThread.run(Timer.java:462)

""Timer thread for monitoring hbase"" daemon prio=10 tid=0x00007f9d648fe000 nid=0x2b5 runnable [0x00007f9d624c7000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.put(HashMap.java:374)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:103)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:75)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicMetrics.setNumericMetric(RegionServerDynamicMetrics.java:105)",eclark,eclark,Critical,Closed,Fixed,16/Aug/12 23:16,24/Oct/12 05:40
Bug,HBASE-6603,12603813,RegionMetricsStorage.incrNumericMetric is called too often,"Running an HBase scan load through the profiler revealed that RegionMetricsStorage.incrNumericMetric is called way too often.

It turns out that we make this call for *each* KV in StoreScanner.next(...).
Incrementing AtomicLong requires expensive memory barriers.

The observation here is that StoreScanner.next(...) can maintain a simple 
long in its internal loop and only update the metric upon exit. Thus the AtomicLong is not updated nearly as often.

That cuts about 10% runtime from scan only load (I'll quantify this better soon).",mycnyc,larsh,Major,Closed,Fixed,17/Aug/12 00:16,07/Apr/13 04:49
Bug,HBASE-6604,12603892,Bump log4j to 1.2.17,"Hadoop bumped to 1.2.17 log4j (HADOOP-8687), we should probably as well.",jmhsieh,jmhsieh,Major,Closed,Fixed,17/Aug/12 17:50,23/Sep/13 18:30
Bug,HBASE-6607,12603925,NullPointerException when accessing master web ui while master is initializing,"Probably I tried to check the master web ui too soon.  I got some internal error page.  In the master log, there is such exception:

{noformat}
2012-08-17 16:06:25,146 ERROR org.mortbay.log: /master-status
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.HMaster.isCatalogJanitorEnabled(HMaster.java:1213)
        at org.apache.hadoop.hbase.master.MasterStatusServlet.doGet(MasterStatusServlet.java:72)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:835)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}",jxiang,jxiang,Trivial,Closed,Fixed,17/Aug/12 23:19,05/Aug/14 20:11
Bug,HBASE-6608,12603939,"Fix for HBASE-6160, META entries from daughters can be deleted before parent entries, shouldn't compare HRegionInfo's","Our nightlies discovered that the patch for HBASE-6160 did not actually fix the issue of ""META entries from daughters can be deleted before parent entries"". Instead of reopening the HBASE-6160, it is cleaner to track it here. 

The original issue is: 
{quote}
HBASE-5986 fixed and issue, where the client sees the META entry for the parent, but not the children. However, after the fix, we have seen the following issue in tests:
Region A is split to -> B, C
Region B is split to -> D, E
After some time, META entry for B is deleted since it is not needed anymore, but META entry for Region A stays in META (C still refers it). In this case, the client throws RegionOfflineException for B.
{quote}

The problem with the fix seems to be that we keep and compare HRegionInfo's in the HashSet at CatalogJanitor.java#scan(), but HRI that are compared are not equal.  
{code}
HashSet<HRegionInfo> parentNotCleaned = new HashSet<HRegionInfo>(); //regions whose parents are still around
      for (Map.Entry<HRegionInfo, Result> e : splitParents.entrySet()) {
        if (!parentNotCleaned.contains(e.getKey()) && cleanParent(e.getKey(), e.getValue())) {
          cleaned++;
        } else {
...
{code}
In the above case, Meta row for region A will contain a serialized version of B that is not offline. However Meta row for region B will contain a serialized version of B that is offline (MetaEditor.offlineParentInMeta() does that). So the deserialized version we put to HashSet and the deserialized version we query contains() from HashSet are different in the offline field, thus HRI.equals() fail. 

",enis,enis,Major,Closed,Fixed,18/Aug/12 01:45,07/Apr/13 04:49
Bug,HBASE-6611,12603982,Forcing region state offline cause double assignment,"In assigning a region, assignment manager forces the region state offline if it is not. This could cause double assignment, for example, if the region is already assigned and in the Open state, you should not just change it's state to Offline, and assign it again.

I think this could be the root cause for all double assignments IF the region state is reliable.

After this loophole is closed, TestHBaseFsck should come up a different way to create some assignment inconsistencies, for example, calling region server to open a region directly. ",jxiang,jxiang,Major,Closed,Fixed,19/Aug/12 16:08,23/Sep/13 18:31
Bug,HBASE-6616,12604083,test failure in TestDelayedRpc#testTooManyDelayedRpcs ,"java.lang.AssertionError
at org.junit.Assert.fail(Assert.java:92)
at org.junit.Assert.assertTrue(Assert.java:43)
at org.junit.Assert.assertTrue(Assert.java:54)
at org.apache.hadoop.hbase.ipc.TestDelayedRpc.testTooManyDelayedRpcs(TestDelayedRpc.java:146)

assertTrue(listAppender.getMessages().isEmpty());

listAppender.getMessages returned something like

[Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting IPC Server listener on 41965, IPC Server Responder: starting, IPC Server listener on 41965: starting, IPC Server handler 0 on 41965: starting]

That comes from

HBaseServer.java, Reader class
LOG.info(""Starting "" + getName());
",mingma,mingma,Major,Closed,Fixed,20/Aug/12 17:24,24/Oct/12 05:40
Bug,HBASE-6621,12604148,Reduce calls to Bytes.toInt,"Bytes.toInt shows up quite often in a profiler run.
It turns out that one source is HFileReaderV2$ScannerV2.getKeyValue().

Notice that we call the KeyValue(byte[], int) constructor, which forces the constructor to determine its size by reading some of the header information and calculate the size. In this case, however, we already know the size (from the call to readKeyValueLen), so we could just use that.

In the extreme case of 10000's of columns this noticeably reduces CPU. ",larsh,larsh,Minor,Closed,Fixed,20/Aug/12 23:49,07/Apr/13 04:50
Bug,HBASE-6622,12604168,TestUpgradeFromHFileV1ToEncoding#testUpgrade fails in trunk,"TestUpgradeFromHFileV1ToEncoding started to fail since build #3242
Build #3246 was more recent one where it failed.
{code}
2012-08-21 00:49:06,536 INFO  [SplitLogWorker-vesta.apache.org,40294,1345510146310] regionserver.SplitLogWorker(135): SplitLogWorker vesta.apache.org,40294,1345510146310 starting
2012-08-21 00:49:06,537 INFO  [RegionServer:0;vesta.apache.org,40294,1345510146310] regionserver.HRegionServer(2431): Registered RegionServer MXBean
2012-08-21 00:49:06,620 WARN  [Master:0;vesta.apache.org,60969,1345510146282] master.AssignmentManager(1606): Failed assignment of -ROOT-,,0.70236052 to vesta.apache.org,40294,1345510146310, trying to assign elsewhere instead; retry=0
org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:187)
	at $Proxy15.openRegion(Unknown Source)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:500)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1587)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1256)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1226)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1221)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignRoot(AssignmentManager.java:2103)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:785)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:665)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:439)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1766)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1187)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:178)
	... 11 more
2012-08-21 00:49:06,621 INFO  [Master:0;vesta.apache.org,60969,1345510146282] master.RegionStates(250): Region {NAME => '-ROOT-,,0', STARTKEY => '', ENDKEY => '', ENCODED => 70236052,} transitioned from {-ROOT-,,0.70236052 state=PENDING_OPEN, ts=1345510146520, server=vesta.apache.org,40294,1345510146310} to {-ROOT-,,0.70236052 state=OFFLINE, ts=1345510146621, server=null}
2012-08-21 00:49:06,621 WARN  [Master:0;vesta.apache.org,60969,1345510146282] master.AssignmentManager(1772): Can't move the region 70236052, there is no destination server available.
2012-08-21 00:49:06,621 WARN  [Master:0;vesta.apache.org,60969,1345510146282] master.AssignmentManager(1618): Unable to find a viable location to assign region -ROOT-,,0.70236052
2012-08-21 00:50:06,406 DEBUG [Master:0;vesta.apache.org,60969,1345510146282.archivedHFileCleaner] cleaner.CleanerChore(145): Checking directory: hdfs://localhost:56237/user/hudson/hbase/.archive/UpgradeTable
{code}
Looks like ROOT region couldn't be assigned.",zhihyu@ebaysf.com,zhihyu@ebaysf.com,Major,Closed,Fixed,21/Aug/12 03:03,23/Sep/13 18:31
Bug,HBASE-6623,12604173,[replication] replication metrics value AgeOfLastShippedOp is not set correctly,"From code below we can see AgeOfLastShippedOp is not set correctly



{code:title=ReplicationSource.java|borderStyle=solid}
// entriesArray init
  public void init(){
    this.entriesArray = new HLog.Entry[this.replicationQueueNbCapacity];
    for (int i = 0; i < this.replicationQueueNbCapacity; i++) {
      this.entriesArray[i] = new HLog.Entry();
    }

}

//set the metrics value should not get the array length

protected void shipEdits() {
...
        this.metrics.setAgeOfLastShippedOp(
            this.entriesArray[this.entriesArray.length-1].getKey().getWriteTime());
...
}

{code} 

",terry_zhang,terry_zhang,Minor,Closed,Fixed,21/Aug/12 05:12,24/Oct/12 05:40
Bug,HBASE-6631,12604296,TestHMasterRPCException in 0.92 failed twice on socket timeout,#502 and #498 0.92 builds have TestHMasterRPCException failing because of socket timeout when servernotrunning is expected.  Socket timeout is 100ms only.,stack,stack,Major,Closed,Fixed,21/Aug/12 22:46,24/Oct/12 05:40
Bug,HBASE-6632,12604325,[0.92 UNIT TESTS] testCreateTableRPCTimeOut sets rpc timeout to 1500ms and leaves it (testHundredsOfTable fails w/ 1500ms timeout),"I see that in 0.92 #502 and #501 that TestAdmin.testHundredsOfTable fails because socket times out after 1500ms.  I see in TestAdmin that before this test runs, testCreateTableRPCTimeOut sets the socket timeout to 1500 and then does not set it back.  Maybe the obnoxious testHundredsOfTable will pass more often if it has the default rpc timeout.",stack,stack,Major,Closed,Fixed,22/Aug/12 05:01,24/Oct/12 05:40
Bug,HBASE-6634,12604444,REST API ScannerModel's protobuf converter code duplicates the setBatch call,There's a dupe call to setBatch when a scanner model object is created for protobuf outputs.,qwertymaniac,qwertymaniac,Trivial,Closed,Fixed,22/Aug/12 14:23,23/Sep/13 18:45
Bug,HBASE-6637,12604530,Move DaemonThreadFactory into Threads and Threads to hbase-common,,jesse_yates,jesse_yates,Minor,Closed,Fixed,22/Aug/12 22:46,23/Sep/13 18:30
Bug,HBASE-6638,12604537,Move DaemonThreadFactory into Threads (0.94),Move DaemonThreadFactory out of HTable and into Threads since its a generally useful class.,jesse_yates,jesse_yates,Major,Closed,Fixed,23/Aug/12 00:26,02/May/13 02:30
Bug,HBASE-6641,12604599,more message with DoNotRetryIOException in client,"when  write a row with wrong or unexist family into a table , we will get message below

org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 2000 actions: DoNotRetryIOException: 2000 times, servers with issues: dw82.kgb.sqa.cm4:64020, at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1591)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1367)
at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:945)
at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:801)
at org.apache.hadoop.hbase.client.HTable.put(HTable.java:784)
at zookeeper.WriteMultiThread.doInsert(WriteToTable.java:101)
at zookeeper.WriteMultiThread.run(WriteToTable.java:80)


it not friendly to the client. Need to show the client more details about the exception.",zhou wen jian,zhou wen jian,Major,Closed,Fixed,23/Aug/12 11:55,24/Oct/12 05:40
Bug,HBASE-6642,12604602,"enable_all,disable_all,drop_all can call ""list"" command with regex directly.","created few tables. then performing disable_all operation in shell prompt.
but it is not performing operation successfully.
{noformat}
hbase(main):043:0> disable_all '*'
table12
zk0113
zk0114

Disable the above 3 tables (y/n)?
y/
3 tables successfully disabled

just it is showing the message but operation is not success.

but the following way only performing successfully


hbase(main):043:0> disable_all '*.*'
table12
zk0113
zk0114

Disable the above 3 tables (y/n)?
y
3 tables successfully disabled
{noformat}
",mbertozzi,sreenivasulureddy,Major,Closed,Fixed,23/Aug/12 12:40,21/Feb/15 23:32
Bug,HBASE-6647,12604691,[performance regression] appendNoSync/HBASE-4528 doesn't take deferred log flush into account,"Since we upgraded to 0.94.1 from 0.92 I saw that our ICVs are about twice as slow as they were. jstack'ing I saw that most of the time we are waiting on sync()... but those tables have deferred log flush turned on so they shouldn't even be calling it.

HTD.isDeferredLogFlush is currently only called in the append() methods which are pretty much not in use anymore.",jdcryans,jdcryans,Major,Closed,Fixed,23/Aug/12 22:32,07/Apr/13 04:51
Bug,HBASE-6648,12604694,[0.92 UNIT TESTS] TestMasterObserver.testRegionTransitionOperations fails occasionally,"I have seen a hudson run failing with TestMasterObserver.testRegionTransitionOperations failing. The logs (http://bit.ly/Q5PiVJ) seemed to indicate that {noformat} HMaster.move {noformat} was called on a region that was under transition, and the master's assignment manager at that time didn't have information about the region.",ddas,ddas,Major,Closed,Fixed,23/Aug/12 22:59,07/Apr/13 04:51
Bug,HBASE-6649,12604703,[0.92 UNIT TESTS] TestReplication.queueFailover occasionally fails [Part-1],"Have seen it twice in the recent past: http://bit.ly/MPCykB & http://bit.ly/O79Dq7 .. 

Looking briefly at the logs hints at a pattern - in both the failed test instances, there was an RS crash while the test was running.",ddas,ddas,Blocker,Closed,Fixed,24/Aug/12 01:34,07/Apr/13 04:52
Bug,HBASE-6653,12604720,Minor type and Incorrect link at WIKI index page,"At ""http://wiki.apache.org/hadoop/Hbase"",

1) ""HBase dist-lists""   should be ""HBase mail-lists""
2) the link for ""Rolling Restart of HBase"" should be changed from ""http://hbase.apache.org/book.html#decommission"" to ""http://hbase.apache.org/book.html#rolling""",,xieliang007,Trivial,Closed,Fixed,24/Aug/12 06:29,13/Jun/22 19:09
Bug,HBASE-6655,12604869,thrift.TestThriftServerCmdLine takes too much time: 733.972 sec,"on trunk, as of today.
3 minutes is a good target.",nkeywal,nkeywal,Major,Closed,Fixed,24/Aug/12 15:59,23/Sep/13 18:31
Bug,HBASE-6658,12604915,Rename WritableByteArrayComparable to something not mentioning Writable,"After HBASE-6477, WritableByteArrayComparable will no longer be Writable, so should be renamed.

Current idea is ByteArrayComparator (since all the derived classes are *Comparator not *Comparable), but I'm open to suggestions.",gchanan,gchanan,Minor,Closed,Fixed,24/Aug/12 19:57,23/Sep/13 18:45
Bug,HBASE-6659,12604918,Port HBASE-6508 Filter out edits at log split time,"HBASE-6508 is for 0.89-fb branch.

This JIRA ports the feature to trunk.",yuzhihong@gmail.com,zhihyu@ebaysf.com,Major,Closed,Fixed,24/Aug/12 20:13,15/Oct/13 04:46
Bug,HBASE-6662,12604938,Region server incorrectly reports its own address as master's address,"Region server incorrectly reports its own address as Master's address while announcing successful connection to Master. 

Example: ine-51 is a RS connecting to master at ine-60
{noformat}
2012-08-22 20:16:02,427 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Attempting connect to Master server at ine-60.rfiserve.net,60000,1345680901361
2012-08-22 20:16:09,501 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Connected to master at ine-51.rfiserve.net/172.22.2.51:60020
{noformat}


Bug is introduced by a typo, the variable *isa* is declared both as a field in class and local variable in the method printing this line. 
{code}
LOG.info(""Connected to master at "" + isa);
{code}

",shrijeet,shrijeet,Minor,Closed,Fixed,24/Aug/12 23:08,24/Oct/12 05:40
Bug,HBASE-6663,12604953,NPE race in HConnection if zookeeper is reset,"On 0.94, HConnectionImplementation has code like this:

{code}
getZooKeeperWatcher();
String tableNameStr = Bytes.toString(tableName);
try {
  if (online) {
    return ZKTable.isEnabledTable(this.zooKeeper, tableNameStr);
  }
  return ZKTable.isDisabledTable(this.zooKeeper, tableNameStr);
{code}

The issue is that between the time that getZooKeeperWatcher is called and this.zooKeeper is used, this.zooKeeper can be set to null by another thread via resetZooKeeperTrackers.

The cleanest solution to me seems to be to cache the value.  I have a patch that does this and a test that fails without the patch and passes with it.

This issue doesn't appear on 0.96 because the zookeeper code has been separated in HBASE-5399.",gchanan,gchanan,Minor,Closed,Fixed,25/Aug/12 01:08,24/Oct/12 05:40
Bug,HBASE-6665,12604956,ROOT region should not be splitted even with META row as explicit split key,"split operation on ROOT table by specifying explicit split key as .META.
closing the ROOT region and taking long time to fail the split before rollback.
I think we can skip split for ROOT table as how we are doing for META region.",rajesh23,sreenivasulureddy,Major,Closed,Fixed,25/Aug/12 05:24,26/Feb/13 08:20
Bug,HBASE-6671,12605110,Kerberos authenticated super user should be able to retrieve proxied delegation tokens,There a services such a oozie which perform actions in behalf of the user using proxy authentication. Retrieving delegation tokens should support this behavior. ,toffer,toffer,Major,Closed,Fixed,27/Aug/12 18:08,07/Apr/13 04:53
Bug,HBASE-6677,12605265,Random ZooKeeper port in test can overrun max port,"{code} 
     while (true) {
        try {
          standaloneServerFactory = new NIOServerCnxnFactory();
          standaloneServerFactory.configure(
            new InetSocketAddress(tentativePort),
            configuration.getInt(HConstants.ZOOKEEPER_MAX_CLIENT_CNXNS,
              1000));
        } catch (BindException e) {
          LOG.debug(""Failed binding ZK Server to client port: "" +
              tentativePort);
          // This port is already in use, try to use another.
          tentativePort++;
          continue;
        }
        break;
      }
{code}

In the case of failure and all the above ports have already been binded, you can extend past the max port.  Need to check against a max value.",xieliang007,gchanan,Trivial,Closed,Fixed,28/Aug/12 17:51,05/Aug/14 20:11
Bug,HBASE-6679,12605299,RegionServer aborts due to race between compaction and split,"In our nightlies, we have seen RS aborts due to compaction and split racing. Original parent file gets deleted after the compaction, and hence, the daughters don't find the parent data file. The RS kills itself when this happens. Will attach a snippet of the relevant RS logs.",ddas,ddas,Major,Closed,Fixed,28/Aug/12 21:16,07/Apr/13 04:53
Bug,HBASE-6682,12605370,Bad characters in logs for server names: SplitLogManager: task following PBUF ,"See how the server name is printed:

2012-08-29 14:28:53,567 INFO org.apache.hadoop.hbase.master.SplitLogManager: task /hbase/splitlog/hdfs%3A%2F%2Flocalhost%3A9000%2Fhbase%2F.logs%2Flocalhost%2C60202%2C1346241077569-splitting%2Flocalhost%252C60202%252C1346241077569.1346241967431 entered state PBUF
	localhost����ޑ�'

",aleksshulman,nkeywal,Minor,Closed,Fixed,29/Aug/12 13:15,23/Sep/13 18:30
Bug,HBASE-6683,12605373,Wrong warning in logs WARN org.apache.hadoop.ipc.HBaseServer: Methods length : 5,"From ProtobufRpcEngine.java
{code}
    static Method getMethod(Class<? extends VersionedProtocol> protocol,
        String methodName) {
      Method method = methodInstances.get(methodName);
      if (method != null) {
        return method;
      }
      Method[] methods = protocol.getMethods();
      LOG.warn(""Methods length : "" + methods.length); <=========
      for (Method m : methods) {
        if (m.getName().equals(methodName)) {
          m.setAccessible(true);
          methodInstances.put(methodName, m);
          return m;
        }
      }
      return null;
    }
{code}",ddas,nkeywal,Critical,Closed,Fixed,29/Aug/12 13:23,23/Sep/13 18:31
Bug,HBASE-6684,12605380,.META. timeout value is incorrect,"We are seeing the timeout value of 2147483647ms which is ~24days.  That seems a little high for not talking to META.
{code}
2012-08-27 21:57:04,572 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Running rollback/cleanup of failed split of table,PGE:3659323005:READ:\x7F\xFF\xFE\xC 
6\xC9\x9DS\x7F,1346030679280.94cf5ab361b0e7d92b0b263ffb995852.; Timed out (2147483647ms) 
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Timed out (2147483647ms) 
at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:390) 
at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:422) 
at org.apache.hadoop.hbase.catalog.MetaEditor.offlineParentInMeta(MetaEditor.java:109) 
at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:290) 
at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:156) 
at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:87) 
2012-08-27 21:57:04,585 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=hdpr002.va.cust.it,60020,1345819108288, load=(requests=102, regions=187, usedHeap=4699, maxHeap=12281): Abort; we got an error after point-of-no-return
{code}",kevin.odell,kevin.odell,Major,Closed,Fixed,29/Aug/12 14:23,23/Sep/13 18:31
Bug,HBASE-6685,12605407,Thrift DemoClient.pl got NullPointerException,"expected error: TSocket: Could not read 4 bytes from localhost:9090

12/06/25 13:48:21 ERROR server.TThreadPoolServer: Error occurred during processing of message. 
java.lang.NullPointerException 
at org.apache.hadoop.hbase.util.Bytes.getBytes(Bytes.java:765) 
at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRowTs(ThriftServer.java:591) 
at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRow(ThriftServer.java:576) 
at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRow.getResult(Hbase.java:3630) 
at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRow.getResult(Hbase.java:3618) 
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32) 
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34) 
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:176) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)",jxiang,jxiang,Minor,Closed,Fixed,29/Aug/12 18:04,07/Apr/13 04:53
Bug,HBASE-6686,12605416,HFile Quarantine fails with missing dirs in hadoop 2.0 ,"Two unit tests fail because listStatus's semantics change between hadoop 1.0 and hadoop 2.0.  (specifically -- hadoop 1.0 returns empty array if used on dir that does not exist, but hadoop 2.0 throws FileNotFoundException).

here's the exception:
{code}
2012-08-28 16:01:19,789 WARN  [Thread-3155] hbck.HFileCorruptionChecker(230): Failed to quaratine an HFile in regiondir hdfs://localhost:38096/user/jenkins/hbase/testQuarantineMissingFamdir/34b2e072b33052bf4875f85513e9c669
java.io.FileNotFoundException: File hdfs://localhost:38096/user/jenkins/hbase/testQuarantineMissingFamdir/34b2e072b33052bf4875f85513e9c669/fam does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:406)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1341)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1381)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.checkColFamDir(HFileCorruptionChecker.java:152)
	at org.apache.hadoop.hbase.util.TestHBaseFsck$2$1.checkColFamDir(TestHBaseFsck.java:1401)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.checkRegionDir(HFileCorruptionChecker.java:185)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker.call(HFileCorruptionChecker.java:267)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker.call(HFileCorruptionChecker.java:258)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}",jmhsieh,jmhsieh,Major,Closed,Fixed,29/Aug/12 19:13,07/Apr/13 04:54
Bug,HBASE-6688,12605437,folder referred by thrift demo app instructions is outdated,"Due to the source tree module change for 0.96, the instructions in the thrift demo example don't match the folder structure any more.

In the instruction, it is referring to:

../../../src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift

it should be

../../hbase-server/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift

",stack,jxiang,Minor,Closed,Fixed,29/Aug/12 21:29,07/Apr/13 04:55
Bug,HBASE-6691,12605446,HFile quarantine fails with missing files in hadoop 2.0,"Trunk/0.96 has a specific issue mentioned in HBASE-6686 when run against hadoop 2.0.   This addresses this problem.

{code}
2012-08-29 12:55:26,031 ERROR [IPC Server handler 0 on 41070] security.UserGroupInformation(1235): PriviledgedActionException as:jon (auth:SIMPLE) cause:java.io.FileNotFoundException: File does not exist: /user/jon/hbase/testQuarantineMissingHFile/4332ea87d02d33e443550537722ff4fc/fam/befbe65ff30e4a46866f04a5671f0e44
2012-08-29 12:55:26,085 WARN  [Thread-2994] hbck.HFileCorruptionChecker(253): Failed to quaratine an HFile in regiondir hdfs://localhost:41070/user/jon/hbase/testQuarantineMissingHFile/4332ea87d02d33e443550537722ff4fc
java.lang.reflect.UndeclaredThrowableException
	at $Proxy23.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:882)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:152)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:119)
	at org.apache.hadoop.hdfs.DFSInputStream.&lt;init&gt;(DFSInputStream.java:112)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:955)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:212)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:75)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:664)
	at org.apache.hadoop.hbase.io.hfile.HFile.createReaderWithEncoding(HFile.java:575)
	at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:605)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.checkHFile(HFileCorruptionChecker.java:94)
	at org.apache.hadoop.hbase.util.TestHBaseFsck$1$1.checkHFile(TestHBaseFsck.java:1401)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.checkColFamDir(HFileCorruptionChecker.java:175)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.checkRegionDir(HFileCorruptionChecker.java:208)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker.call(HFileCorruptionChecker.java:290)
	at org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker.call(HFileCorruptionChecker.java:281)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:261)
	... 27 more
Caused by: java.io.FileNotFoundException: File does not exist: /user/jon/hbase/testQuarantineMissingHFile/4332ea87d02d33e443550537722ff4fc/fam/befbe65ff30e4a46866f04a5671f0e44
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1133)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1095)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1067)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:384)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:165)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42586)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)

	at org.apache.hadoop.ipc.Client.call(Client.java:1161)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)
	at $Proxy17.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
	at $Proxy17.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:149)
	... 31 more
{code}",jmhsieh,jmhsieh,Major,Closed,Fixed,29/Aug/12 22:38,23/Sep/13 18:30
Bug,HBASE-6692,12605449,[shell] Cannot describe '.META.' or '-ROOT-' tables,"This got introduced as a side effect of HBASE-3313. Now it is not possible to describe '.META.' or '\-ROOT\-' tables from HBase shell.

{noformat}
hbase(main):002:0> describe '-ROOT-'
DESCRIPTION                                                                                                                                               ENABLED

ERROR: java.lang.IllegalArgumentException: Illegal first character <45> at 0. User-space table names can only start with 'word characters': i.e. [a-zA-Z_0-9]: -ROOT-

Here is some help for this command:
Describe the named table. For example:
  hbase> describe 't1'
{noformat}
",adityakishore,adityakishore,Major,Closed,Fixed,29/Aug/12 23:41,23/Sep/13 18:30
Bug,HBASE-6693,12605455,Typo in Javadoc of AggregationClient.rowCount(),FirstKEyValueFilter -> FirstKeyValueFilter,jmhsieh,ikeda,Trivial,Closed,Fixed,30/Aug/12 00:46,23/Sep/13 18:44
Bug,HBASE-6697,12605486,regionserver.TestPriorityRpc uses a fixed port (60020),"You can have this if the port is used.
Testable by doing ""nc -l 60020"" before launching the test.
{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.regionserver.TestPriorityRpc
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.286 sec <<< FAILURE!
org.apache.hadoop.hbase.regionserver.TestPriorityRpc  Time elapsed: 0 sec  <<< ERROR!
java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer
        at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2376)
        at org.apache.hadoop.hbase.regionserver.TestPriorityRpc.onetimeSetup(TestPriorityRpc.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110)
        at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175)
        at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:81)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2374)
        ... 22 more
Caused by: java.net.BindException: Problem binding to localhost/127.0.0.1:60020 : Address already in use
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:295)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.<init>(HBaseServer.java:510)
        at org.apache.hadoop.hbase.ipc.HBaseServer.<init>(HBaseServer.java:1922)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:247)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:85)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:57)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getServer(HBaseRPC.java:400)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:516)
        ... 27 more
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:126)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:293)
        ... 34 more
{noformat}",nkeywal,nkeywal,Minor,Closed,Fixed,30/Aug/12 08:07,23/Sep/13 18:45
Bug,HBASE-6700,12605644,[replication] empty znodes created during queue failovers aren't deleted,"Please check code below

{code:title=ReplicationSourceManager.java|borderStyle=solid}
// NodeFailoverWorker class
public void run() {
{
    ...

      LOG.info(""Moving "" + rsZnode + ""'s hlogs to my queue"");
      SortedMap<String, SortedSet<String>> newQueues =
          zkHelper.copyQueuesFromRS(rsZnode);   // Node create here*
      zkHelper.deleteRsQueues(rsZnode); 
      if (newQueues == null || newQueues.size() == 0) {
        return;  
      }
    ...
}


  public void closeRecoveredQueue(ReplicationSourceInterface src) {
    LOG.info(""Done with the recovered queue "" + src.getPeerClusterZnode());
    this.oldsources.remove(src);
    this.zkHelper.deleteSource(src.getPeerClusterZnode(), false);  // Node delete here*
  }
{code} 

So from code we can see if newQueues == null or newQueues.size() == 0, Failover replication Source will never start and the failover zk node will never deleted.


eg below failover node will never be delete:

[zk: 10.232.98.77:2181(CONNECTED) 16] ls /hbase-test3-repl/replication/rs/dw93.kgb.sqa.cm4,60020,1346337383956/1-dw93.kgb.sqa.cm4,60020,1346309263932-dw91.kgb.sqa.cm4,60020,1346307150041-dw89.kgb.sqa.cm4,60020,1346307911711-dw93.kgb.sqa.cm4,60020,1346312019213-dw88.kgb.sqa.cm4,60020,1346311774939-dw89.kgb.sqa.cm4,60020,1346312314229-dw93.kgb.sqa.cm4,60020,1346312524307-dw88.kgb.sqa.cm4,60020,1346313203367-dw89.kgb.sqa.cm4,60020,1346313944402-dw88.kgb.sqa.cm4,60020,1346314214286-dw91.kgb.sqa.cm4,60020,1346315119613-dw93.kgb.sqa.cm4,60020,1346314186436-dw88.kgb.sqa.cm4,60020,1346315594396-dw89.kgb.sqa.cm4,60020,1346315909491-dw92.kgb.sqa.cm4,60020,1346315315634-dw89.kgb.sqa.cm4,60020,1346316742242-dw93.kgb.sqa.cm4,60020,1346317604055-dw92.kgb.sqa.cm4,60020,1346318098972-dw91.kgb.sqa.cm4,60020,1346317855650-dw93.kgb.sqa.cm4,60020,1346318532530-dw92.kgb.sqa.cm4,60020,1346318573238-dw89.kgb.sqa.cm4,60020,1346321299040-dw91.kgb.sqa.cm4,60020,1346321304393-dw92.kgb.sqa.cm4,60020,1346325755894-dw89.kgb.sqa.cm4,60020,1346326520895-dw91.kgb.sqa.cm4,60020,1346328246992-dw92.kgb.sqa.cm4,60020,1346327290653-dw93.kgb.sqa.cm4,60020,1346337303018-dw91.kgb.sqa.cm4,60020,1346337318929
[] // empty node will never be deleted
       

",terry_zhang,terry_zhang,Major,Closed,Fixed,31/Aug/12 06:38,26/Feb/13 08:20
Bug,HBASE-6701,12605742,Revisit thrust of paragraph on splitting,See the thread 'md5 hash key and splits' for the confusion our paragraph on splitting seems to bring on (as well as good input on when manual splitting might be favored).  The user is under the impression that he needs to manually split though his keys have md5 salt.  The paragraph needs to make sure it does not bring on such confusion as it would seem to in this case.,misty,stack,Major,Closed,Fixed,31/Aug/12 15:26,21/Feb/15 23:34
Bug,HBASE-6703,12605761,Increment does not check resources,"The increment code does not check for resource constraints. This could cause problems if it is called frequently. The other modifying code paths like put, delete etc call checkResources(). This is a one-line fix",,rvadali,Major,Closed,Fixed,31/Aug/12 18:03,13/Jun/22 19:12
Bug,HBASE-6707,12605795,TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps,"https://builds.apache.org/job/HBase-TRUNK/3293/

Error Message

Archived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]

Stacktrace

java.lang.AssertionError: Archived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertNull(Assert.java:551)
	at org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables(TestZooKeeperTableArchiveClient.java:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",jesse_yates,sameerv,Critical,Closed,Fixed,31/Aug/12 21:18,23/Sep/13 18:31
Bug,HBASE-6710,12605840,0.92/0.94 compatibility issues due to HBASE-5206,"HBASE-5206 introduces some compatibility issues between {0.94,0.94.1} and
{0.92.0,0.92.1}.  The release notes of HBASE-5155 describes the issue (HBASE-5206 is a backport of HBASE-5155).

I think we can make 0.94.2 compatible with both {0.94.0,0.94.1} and {0.92.0,0.92.1}, although one of those sets will require configuration changes.

The basic problem is that there is a znode for each table ""zookeeper.znode.tableEnableDisable"" that is handled differently.

On 0.92.0 and 0.92.1 the states for this table are:
[ disabled, disabling, enabling ] or deleted if the table is enabled

On 0.94.1 and 0.94.2 the states for this table are:
[ disabled, disabling, enabling, enabled ]

What saves us is that the location of this znode is configurable.  So the basic idea is to have the 0.94.2 master write two different znodes, ""zookeeper.znode.tableEnableDisabled92"" and ""zookeeper.znode.tableEnableDisabled94"" where the 92 node is in 92 format, the 94 node is in 94 format.  And internally, the master would only use the 94 format in order to solve the original bug HBASE-5155 solves.
We can of course make one of these the same default as exists now, so we don't need to make config changes for one of 0.92 or 0.94 clients.  I argue that 0.92 clients shouldn't have to make config changes for the same reason I argued above.  But that is debatable.

Then, I think the only question left is the question of how to bring along the {0.94.0, 0.94.1} crew.  A {0.94.0, 0.94.1} client would work against a 0.94.2 cluster by just configuring ""zookeeper.znode.tableEnableDisable"" in the client to be whatever ""zookeeper.znode.tableEnableDisabled94"" is in the cluster.  A 0.94.2 client would work against both a {0.94.0, 0.94.1} and {0.92.0, 0.92.1} cluster if it had HBASE-6268 applied.  About rolling upgrade from {0.94.0, 0.94.1} to 0.94.2 -- I'd have to think about that.  Do the regionservers ever read the tableEnableDisabled znode?

On the mailing list, Lars H suggested the following:
""The only input I'd have is that format we'll use going forward will not have a version attached to it.
So maybe the 92 version would still be called ""zookeeper.znode.tableEnableDisable"" and the new node could have a different name ""zookeeper.znode.tableEnableDisableNew"" (or something).""",gchanan,gchanan,Critical,Closed,Fixed,01/Sep/12 04:26,05/Jan/13 00:42
Bug,HBASE-6711,12605880,Avoid local results copy in StoreScanner,"In StoreScanner the number of results is limited to avoid OOMs.
However, this is done by first adding the KV into a local ArrayList and then copying the entries in this list to the final result list.

In turns out the this temporary list is only used to keep track of the size of the result set in this loop. Can use a simple int instead.",larsh,larsh,Minor,Closed,Fixed,02/Sep/12 04:51,07/Apr/13 04:56
Bug,HBASE-6713,12605960,Stopping META/ROOT RS may take 50mins when some region is splitting,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed",zjushch,zjushch,Major,Closed,Fixed,03/Sep/12 05:55,07/Apr/13 04:56
Bug,HBASE-6714,12606113,TestMultiSlaveReplication#testMultiSlaveReplication may fail,"java.lang.AssertionError: expected:<1> but was:<0>
at org.junit.Assert.fail(Assert.java:93)
at org.junit.Assert.failNotEquals(Assert.java:647)
at org.junit.Assert.assertEquals(Assert.java:128)
at org.junit.Assert.assertEquals(Assert.java:472)
at org.junit.Assert.assertEquals(Assert.java:456)
at org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.checkRow(TestMultiSlaveReplication.java:203)
at org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.testMultiSlaveReplication(TestMultiSlaveReplication.java:188)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)



TestMultiSlaveReplication->testMultiSlaveReplication failed in our local build citing that ""row"" was not replicated to second peer. This is because after inserting ""row"", log is rolled and we look for ""row2"" in both the clusters and then we check for existence of ""row"" in both clusters. Meanwhile, Replication thread was sleeping for the second cluster and Row ""row2"" is not present in the second cluster from the very beginning. So, the ""row2"" existence check succeeds and control move on to find ""row"" in both clusters where it fails for the second cluster.",v.himanshu,v.himanshu,Minor,Closed,Fixed,04/Sep/12 17:18,07/Apr/13 04:57
Bug,HBASE-6716,12606130,Hadoopqa is hosed,"See this thread on list: http://search-hadoop.com/m/PtDLC19vEd62/%2522Looks+like+HadoopQA+is+hosed%2522&subj=Looks+like+HadoopQA+is+hosed+

Lots of the hadoopqa builds are failing complaining about missing dir.",stack,stack,Major,Closed,Fixed,04/Sep/12 18:43,13/Jun/22 19:21
Bug,HBASE-6724,12606333,Port HBASE-6165 'Replication can overrun .META. scans on cluster re-start' to 0.92,,yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,05/Sep/12 23:57,14/Jun/22 17:51
Bug,HBASE-6727,12606450,[89-fb] allow HBaseServers's callqueue to be better configurable to avoid OOMs,"The callQueue size (where requests get queued up if all handlers are busy) is a LinkedBlockingQueue of size 100 * number_of_handlers. So, with say 300 handler threads, the call queue can have upto 30k entries queued up. If the requests are large enough, this can result in OOM or severe GC pauses.

Ideally, we should allow this param to be separately configurable independent of the numberof handlers; perhaps an even better approach would be to specify a memory size based limit, instead of a number of entries based limit.

[I have not looked at the trunk version for this issue. So it may or may not be relevant there.]",adela,kannanm,Major,Closed,Fixed,06/Sep/12 18:00,14/Jun/22 17:51
Bug,HBASE-6728,12606451,[89-fb] prevent OOM possibility due to per connection responseQueue being unbounded,"The per connection responseQueue is an unbounded queue. The request handler threads today try to send the response in line, but if things start to backup, the response is sent via a per connection responder thread. This intermediate queue, because it has no bounds, can be another source of OOMs.

[Have not looked at this issue in trunk. So it may or may not be applicable there.]",michalgr,kannanm,Major,Closed,Fixed,06/Sep/12 18:02,03/Dec/12 21:47
Bug,HBASE-6733,12606508,[0.92 UNIT TESTS] TestReplication.queueFailover occasionally fails [Part-2],"The failure is in TestReplication.queueFailover (fails due to unreplicated rows). I have come across two problems:
1. The sleepMultiplier is not properly reset when the currentPath is changed (in ReplicationSource.java).
2. ReplicationExecutor sometime removes files to replicate from the queue too early, resulting in corresponding edits missing. Here the problem is due to the fact the log-file length that the replication executor finds is not the most updated one, and hence it doesn't read anything from there, and ultimately, when there is a log roll, the replication-queue gets a new entry, and the executor drops the old entry out of the queue.",ddas,ddas,Major,Closed,Fixed,06/Sep/12 23:50,26/Feb/13 08:20
Bug,HBASE-6734,12606512,Code duplication in LoadIncrementalHFiles,"This was due to the merge of two Jiras:

{code}
      if (queue.isEmpty()) {
        LOG.warn(""Bulk load operation did not find any files to load in "" +
            ""directory "" + hfofDir.toUri() + "".  Does it contain files in "" +
            ""subdirectories that correspond to column family names?"");
        return;
      }

      if (queue.isEmpty()) {
        LOG.warn(""Bulk load operation did not find any files to load in "" +
        ""directory "" + hfofDir.toUri() + "".  Does it contain files in "" +
        ""subdirectories that correspond to column family names?"");
      }
{code}",rding,rding,Minor,Closed,Fixed,07/Sep/12 00:17,07/Apr/13 04:58
Bug,HBASE-6738,12606604,Too aggressive task resubmission from the distributed log manager,"With default settings for ""hbase.splitlog.manager.timeout"" => 25s and ""hbase.splitlog.max.resubmit"" => 3.

On tests mentionned on HBASE-5843, I have variations around this scenario, 0.94 + HDFS 1.0.3:

The regionserver in charge of the split does not answer in less than 25s, so it gets interrupted but actually continues. Sometimes, we go out of the number of retry, sometimes not, sometimes we're out of retry, but the as the interrupts were ignored we finish nicely. In the mean time, the same single task is executed in parallel by multiple nodes, increasing the probability to get into race conditions.

Details:
t0: unplug a box with DN+RS
t + x: other boxes are already connected, to their connection starts to dies. Nevertheless, they don't consider this node as suspect.
t + 180s: zookeeper -> master detects the node as dead. recovery start. It can be less than 180s sometimes it around 150s.
t + 180s: distributed split starts. There is only 1 task, it's immediately acquired by a one RS.
t + 205s: the RS has multiple errors when splitting, because a datanode is missing as well. The master decides to give the task to someone else. But often the task continues in the first RS. Interrupts are often ignored, as it's well stated in the code (""// TODO interrupt often gets swallowed, do what else?"")
{code}
   2012-09-04 18:27:30,404 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
{code}
t + 211s: two regionsservers are processing the same task. They fight for the leases:
{code}
2012-09-04 18:27:32,004 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException:          org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on
   /hbase/TABLE/4d1c1a4695b1df8c58d13382b834332e/recovered.edits/0000000000000000037.temp owned by DFSClient_hb_rs_BOX2,60020,1346775882980 but is accessed by DFSClient_hb_rs_BOX1,60020,1346775719125
{code}
     They can fight like this for many files, until the tasks finally get interrupted or finished.
     The taks on the second box can be cancelled as well. In this case, the task is created again for a new box.
     The master seems to stop after 3 attemps. It can as well renounce to split the files. Sometimes the tasks were not cancelled on the RS side, so the split is finished despites what the master thinks and logs. In this case, the assignement starts. In the other, it's ""we've got a problem"").
{code}
2012-09-04 18:43:52,724 INFO org.apache.hadoop.hbase.master.SplitLogManager: Skipping resubmissions of task /hbase/splitlog/hdfs%3A%2F%2FBOX1%3A9000%2Fhbase%2F.logs%2FBOX0%2C60020%2C1346776587640-splitting%2FBOX0%252C60020%252C1346776587640.1346776587832 because threshold 3 reached     
{code}
t + 300s: split is finished. Assignement starts
t + 330s: assignement is finished, regions are available again.


There are a lot of subcases possible depending on the number of logs files, of region server and so on.

The issues are:
1) it's difficult, especially in HBase but not only, to interrupt a task. The pattern is often
{code}
 void f() throws IOException{
  try {
     // whatever throw InterruptedException
  }catch(InterruptedException){
    throw new InterruptedIOException();
  }
}

 boolean g(){
   int nbRetry= 0;  
   for(;;)
      try{
         f();
         return true;
      }catch(IOException e){
         nbRetry++;
         if ( nbRetry > maxRetry) return false;
      }
   } 
 }
{code}

This tyically shallows the interrupt. There are other variation, but this one seems to be the standard.
Even if we fix this in HBase, we need the other layers to be Interrupteble as well. That's not proven.

2) 25s is very aggressive, considering that we have a default timeout of 180s for zookeeper. In other words, we give 180s to a regionserver before acting, but when it comes to split, it's 25s only. There may be reasons for this, but it seems dangerous, as during a failure the cluster is less available than during normal operations. We could do stuff around this, for example:
=> Obvious option: increase the timeout at each try. Something like *2.
=> Also possible: increase the initial timeout
=> check for an update instead of blindly cancelling + resubmitting.

3) Globally, it seems that this retry mechanism duplicates the failure detection already in place with ZK. Would it not make sense to just hook into this existing detection mechanism, and resubmit a task if and only if we detect that the regionserver in charge died? During a failure scenario we should be much more gentle than during normal operation, not the opposite.
",nkeywal,nkeywal,Critical,Closed,Fixed,07/Sep/12 15:09,05/Nov/14 04:51
Bug,HBASE-6746,12606671,Impacts of HBASE-6435 vs. HDFS 2.0 trunk,"When using the trunk of HDFS branch 2, I had two errors linked to HBASE-6435:
- a missing test to null
- a method removed. 

This fixes it:
- add the test
- make the test case less dependant on HDFS internal.",nkeywal,nkeywal,Major,Closed,Fixed,07/Sep/12 20:43,23/Sep/13 18:31
Bug,HBASE-6748,12606860,Endless recursive of deleteNode happened in SplitLogManager#DeleteAsyncCallback,"You can ealily understand the problem from the below logs:
{code}
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=3
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=2
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=1
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=0
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 393] failed to create task node/hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 353] Error splitting /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775807
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775806
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775805
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775804
[2012-09-01 11:41:02,065] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775803
...................
[2012-09-01 11:41:03,307] [ERROR] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.zookeeper.ClientCnxn 623] Caught unexpected throwable
java.lang.StackOverflowError
{code}",jeffreyz,jeason,Major,Closed,Fixed,10/Sep/12 12:00,23/Mar/13 04:53
Bug,HBASE-6757,12607071,Very inefficient behaviour of scan using FilterList,"The behaviour of scan is very inefficient when using with FilterList.

The FilterList rewrites the return code from NEXT_ROW to SKIP from a filter if Operator.MUST_PASS_ALL is used. 

This happens when using ColumnPrefixFilter. Even though the ColumnPrefixFilter indicates to jump to NEXT_ROW because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the ReturnCode from ColumnPrefixFilter has been rewritten by the FilterList from NEXT_ROW to SKIP. 

This is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name.",larsh,jerrylam,Major,Closed,Fixed,11/Sep/12 14:42,07/Apr/13 04:58
Bug,HBASE-6758,12607103,[replication] The replication-executor should make sure the file that it is replicating is closed before declaring success on that file,"I have seen cases where the replication-executor would lose data to replicate since the file hasn't been closed yet. Upon closing, the new data becomes visible. Before that happens the ZK node shouldn't be deleted in ReplicationSourceManager.logPositionAndCleanOldLogs. Changes need to be made in ReplicationSource.processEndOfFile as well (currentPath related).",ddas,ddas,Critical,Closed,Fixed,11/Sep/12 17:52,23/Sep/13 18:30
Bug,HBASE-6769,12607368,HRS.multi eats NoSuchColumnFamilyException since HBASE-5021,"I think this is a pretty major usability regression, since HBASE-5021 this is what you get in the client when using a wrong family:

{noformat}
2012-09-11 09:45:29,634 WARN org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: DoNotRetryIOException: 1 time, servers with issues: sfor3s44:10304, 
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1601)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1377)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:916)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:772)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:747)
{noformat}

Then you have to log on the server to understand what failed.

Since everything is now a multi call, even single puts in the shell fail like this.

This is present since 0.94.0

Assigning to Elliott because he asked.",eclark,jdcryans,Critical,Closed,Fixed,13/Sep/12 00:04,07/Apr/13 04:59
Bug,HBASE-6778,12607541,Deprecate Chore; its a thread per task when we should have one thread to do all tasks,Should use something like ScheduledThreadPoolExecutor instead (Elliott said this first I think; J-D said something similar just now).,jonathan.lawlor,stack,Major,Closed,Fixed,13/Sep/12 23:23,03/Jun/15 15:29
Bug,HBASE-6779,12607543,Fix issues analysis.apache.org raises about StochasticLoadBalancer,,eclark,eclark,Major,Closed,Fixed,13/Sep/12 23:46,23/Sep/13 18:30
Bug,HBASE-6780,12607548,On the master status page the Number of Requests per second is incorrect for RegionServer's,The number of requests per second is getting divided when it shouldn't be.,eclark,eclark,Major,Closed,Fixed,14/Sep/12 00:11,23/Sep/13 18:30
Bug,HBASE-6782,12607566,HBase shell's 'status 'detailed'' should escape the printed keys,Currently the HBase shell's status command prints unescaped keys on the terminal causing the terminal to print garbage characters. We should escape the printed keys.,viji_r,viji_r,Minor,Closed,Fixed,14/Sep/12 05:25,23/Sep/13 18:30
Bug,HBASE-6784,12607659,TestCoprocessorScanPolicy is sometimes flaky when run locally,"The problem is not seen in jenkins build.  
When we run TestCoprocessorScanPolicy.testBaseCases locally or in our internal jenkins we tend to get random failures.  The reason is the 2 puts that we do here is sometimes getting the same timestamp.  This is leading to improper scan results as the version check tends to skip one of the row seeing the timestamp to be same. Marking this as minor.  As we are trying to solve testcase related failures just raising this incase we need to resolve this also.

For eg,
Both the puts are getting the time
{code}
time 1347635287360
time 1347635287360
{code}
",larsh,ram_krish,Minor,Closed,Fixed,14/Sep/12 15:17,07/Apr/13 04:59
Bug,HBASE-6794,12607745,FilterBase should provide a default implementation of toByteArray,"See HBASE-6657, FilterBase provides stub implementations for other Filter methods, it seems reasonable for it to provide a default implementation for toByteArray, for Filters that don't need special serialization (e.g. ones with no state).",gchanan,gchanan,Minor,Closed,Fixed,15/Sep/12 01:22,23/Sep/13 18:30
Bug,HBASE-6795,12607747,mvn compile fails on a fresh checkout with empty ~/.m2/repo,"I have noticed that mvn compile fails if your ~/m2/repository/ does not contain hbase test jars, however mvn test-compile, mvn install, etc works as expected. 

The patch for HBASE-6706 introduced test-jar dependency from hbase-server and hbase-hadoop1-compat to hbase-hadoop-compat test jar in the test scope. But stupid maven still tries to resolve the test jar when you do maven compile (notice that we are not even in the test scope).

mvn test-compile, etc works b/c the test-jar for hbase-hadoop-compat is build before hbase-hadoop1-compat.

One way to solve this is to push SNAPSHOT test-jars for hbase-hadoop-compat to the snapshot repository, so next time, they are referenced from there.

Other alternative is to move classes under hbase-hadoop{|1|2}-compat/src/test to src/main, and remove the test-jar intra-module dependency. Still, it seems we might need intra-module test-jar dependency in the future. 

Any other suggestions are welcome. ",enis,enis,Critical,Closed,Fixed,15/Sep/12 01:50,23/Sep/13 18:31
Bug,HBASE-6796,12607749,"Backport HBASE-5547, Don't delete HFiles in backup mode.",See HBASE-5547,jesse_yates,larsh,Major,Closed,Fixed,15/Sep/12 04:22,02/May/13 02:30
Bug,HBASE-6797,12607767,TestHFileCleaner#testHFileCleaning sometimes fails in trunk,"In build #3334, I saw:
{code}
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.master.cleaner.TestHFileCleaner.testHFileCleaning(TestHFileCleaner.java:88)
{code}",jesse_yates,yuzhihong@gmail.com,Major,Closed,Fixed,15/Sep/12 15:05,14/Jun/22 17:59
Bug,HBASE-6803,12607973,script hbase should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH,Snappy SO fails to load properly if LD_LIBRARY_PATH does not include the path where snappy SO is.,jxiang,jxiang,Major,Closed,Fixed,17/Sep/12 22:43,07/Apr/13 05:00
Bug,HBASE-6806,12608064,HBASE-4658 breaks backward compatibility / example scripts,HBASE-4658 introduces the new 'attributes' argument as a non optional parameter. This is not backward compatible and also breaks the code in the example section. Resolution: Mark as 'optional',,mr_luk,Major,Closed,Fixed,18/Sep/12 13:55,23/Sep/13 18:30
Bug,HBASE-6811,12608132,TestDrainingServer#testDrainingServerWithAbort sometimes fails in trunk,"TestDrainingServer#testDrainingServerWithAbort failed in trunk build #3348:
{code}
Error Message

Test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24>

Stacktrace

junit.framework.AssertionFailedError: Test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24>
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.Assert.failNotEquals(Assert.java:287)
	at junit.framework.Assert.assertEquals(Assert.java:67)
	at junit.framework.Assert.assertEquals(Assert.java:134)
	at org.apache.hadoop.hbase.TestDrainingServer.testDrainingServerWithAbort(TestDrainingServer.java:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)

Standard Output

Shutting down the Mini HDFS Cluster
Shutting down DataNode 4
Shutting down DataNode 3
Shutting down DataNode 2
Shutting down DataNode 1
Shutting down DataNode 0

Standard Error

2012-09-18 20:18:30,026 INFO  [pool-1-thread-1] hbase.ResourceChecker(144): before TestDrainingServer#testDrainingServerWithAbort: 441 threads, 700 file descriptors 7 connections, 
2012-09-18 20:18:30,044 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeCreated, state=SyncConnected, path=/hbase/balancer
2012-09-18 20:18:30,044 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; PBUF\x08\x00
2012-09-18 20:18:30,045 DEBUG [IPC Server handler 2 on 35050] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; PBUF\x08\x00
2012-09-18 20:18:30,045 INFO  [IPC Server handler 2 on 35050] master.HMaster(1363): BalanceSwitch=false
2012-09-18 20:18:30,047 INFO  [Thread-604] hbase.TestDrainingServer(211): Regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.]
2012-09-18 20:18:30,047 INFO  [Thread-604] hbase.TestDrainingServer(112): Making hemera.apache.org,33334,1347999502311 the draining server; it has 1 online regions
2012-09-18 20:18:30,048 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/draining
2012-09-18 20:18:30,049 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/draining/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,049 INFO  [pool-1-thread-1-EventThread] zookeeper.DrainingServerTracker(83): Draining RS node created, adding to list [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,049 INFO  [Thread-604] hbase.TestDrainingServer(220): The available servers are: [hemera.apache.org,38814,1347999502374, hemera.apache.org,58959,1347999502361, hemera.apache.org,43875,1347999502347, hemera.apache.org,51601,1347999502333]
2012-09-18 20:18:30,049 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,51601,1347999502333: Aborting
2012-09-18 20:18:30,050 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,054 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=3, numberOfStores=3, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=167, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,056 ERROR [IPC Server handler 0 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,51601,1347999502333 reported a fatal error:
ABORTING region server hemera.apache.org,51601,1347999502333: Aborting
2012-09-18 20:18:30,058 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,058 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,43875,1347999502347: Aborting
2012-09-18 20:18:30,058 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,062 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=7, numberOfStores=7, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=167, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,062 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,063 INFO  [SplitLogWorker-hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,063 INFO  [SplitLogWorker-hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,51601,1347999502333 exiting
2012-09-18 20:18:30,063 ERROR [IPC Server handler 1 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,43875,1347999502347 reported a fatal error:
ABORTING region server hemera.apache.org,43875,1347999502347: Aborting
2012-09-18 20:18:30,063 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.compactionChecker] hbase.Chore(81): RegionServer:1;hemera.apache.org,51601,1347999502333.compactionChecker exiting
2012-09-18 20:18:30,063 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,58959,1347999502361: Aborting
2012-09-18 20:18:30,063 DEBUG [pool-1-thread-1.LruBlockCache.EvictionThread] hfile.LruBlockCache(418): Block cache LRU eviction started; Attempting to free -408721.95 KB of total=1.98 MB
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:1;hemera.apache.org,51601,1347999502333.cacheFlusher exiting
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] handler.CloseRegionHandler(124): Processing close of t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(954): Closing t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.: disabling compactions & flushes
2012-09-18 20:18:30,064 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(943): aborting server hemera.apache.org,51601,1347999502333
2012-09-18 20:18:30,064 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] handler.CloseRegionHandler(124): Processing close of t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:2;hemera.apache.org,43875,1347999502347.cacheFlusher exiting
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.compactionChecker] hbase.Chore(81): RegionServer:2;hemera.apache.org,43875,1347999502347.compactionChecker exiting
2012-09-18 20:18:30,063 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,065 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,065 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(954): Closing t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.: disabling compactions & flushes
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.: disabling compactions & flushes
2012-09-18 20:18:30,065 INFO  [SplitLogWorker-hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,067 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(943): aborting server hemera.apache.org,43875,1347999502347
2012-09-18 20:18:30,067 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@11a59ce
2012-09-18 20:18:30,064 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@f7bf2d
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(975): Updates disabled for region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] handler.CloseRegionHandler(124): Processing close of t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,067 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690009
2012-09-18 20:18:30,067 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=4, numberOfStores=4, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=168, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,067 INFO  [SplitLogWorker-hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,43875,1347999502347 exiting
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,069 ERROR [IPC Server handler 3 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,58959,1347999502361 reported a fatal error:
ABORTING region server hemera.apache.org,58959,1347999502361: Aborting
2012-09-18 20:18:30,069 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.: disabling compactions & flushes
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(975): Updates disabled for region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,066 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(124): Processing close of t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,070 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690007
2012-09-18 20:18:30,070 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,070 INFO  [StoreCloserThread-t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,070 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,070 INFO  [StoreCloserThread-t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(1023): Closed t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,068 INFO  [StoreCloserThread-t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] handler.CloseRegionHandler(168): Closed region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,068 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(954): Closing t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.: disabling compactions & flushes
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(1023): Closed t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,071 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(1091): Waiting on 2 regions to close
2012-09-18 20:18:30,071 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(1095): {73c7f7079688e986da1850e53fb74f9d=t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.}
2012-09-18 20:18:30,071 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,070 INFO  [Thread-604] hbase.TestDrainingServer(239): Regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.]
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:3;hemera.apache.org,58959,1347999502361.cacheFlusher exiting
2012-09-18 20:18:30,070 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(954): Closing t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,070 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(1091): Waiting on 8 regions to close
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(124): Processing close of t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(954): Closing t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(124): Processing close of t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,073 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/draining/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(975): Updates disabled for region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,073 INFO  [StoreCloserThread-t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.compactionChecker] hbase.Chore(81): RegionServer:3;hemera.apache.org,58959,1347999502361.compactionChecker exiting
2012-09-18 20:18:30,073 INFO  [StoreCloserThread-t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,073 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,075 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(1023): Closed t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,075 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,072 INFO  [SplitLogWorker-hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,071 INFO  [StoreCloserThread-t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,076 INFO  [SplitLogWorker-hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,58959,1347999502361 exiting
2012-09-18 20:18:30,076 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(1023): Closed t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] handler.CloseRegionHandler(168): Closed region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(975): Updates disabled for region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(168): Closed region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,078 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.: disabling compactions & flushes
2012-09-18 20:18:30,076 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,075 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(168): Closed region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,079 INFO  [StoreCloserThread-t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,073 INFO  [pool-1-thread-1-EventThread] zookeeper.DrainingServerTracker(101): Draining RS node deleted, removing from list [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,084 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(1023): Closed t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,084 INFO  [pool-1-thread-1] hbase.ResourceChecker(144): after TestDrainingServer#testDrainingServerWithAbort: 348 threads (was 441), 584 file descriptors (was 700). 5 connections (was 7), 
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(954): Closing t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.: disabling compactions & flushes
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(975): Updates disabled for region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(943): aborting server hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,086 INFO  [StoreCloserThread-t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,072 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(1095): {c1e5810326004add2ab532ccc9e8c24e=t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e., 6b45157a30794a82fc5cbdb3589032e2=t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2., 95255bd4b3285804a2c818d1bf7f459f=t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f., cb765c4343d7b8c707c88b4d4b79a165=t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165., 78622cfb2af5eca037e2375d7566cac6=t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.}
2012-09-18 20:18:30,086 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(1023): Closed t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(168): Closed region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(124): Processing close of t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,086 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1880b02
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(975): Updates disabled for region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,084 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] handler.CloseRegionHandler(168): Closed region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,084 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/draining
2012-09-18 20:18:30,084 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.: disabling compactions & flushes
2012-09-18 20:18:30,081 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(124): Processing close of t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(954): Closing t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.: disabling compactions & flushes
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(975): Updates disabled for region t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.: disabling compactions & flushes
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,089 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690008
2012-09-18 20:18:30,091 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(1023): Closed t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(168): Closed region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(124): Processing close of t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,089 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(954): Closing t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.: disabling compactions & flushes
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(975): Updates disabled for region t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(954): Closing t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.: disabling compactions & flushes
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(975): Updates disabled for region t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,091 INFO  [StoreCloserThread-t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 INFO  [StoreCloserThread-t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,092 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(1023): Closed t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,091 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,092 INFO  [pool-1-thread-1] hbase.HBaseTestingUtility(747): Shutting down minicluster
2012-09-18 20:18:30,092 DEBUG [pool-1-thread-1] util.JVMClusterUtil(223): Shutting down HBase Cluster
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,093 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(1023): Closed t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,092 INFO  [pool-1-thread-1] master.HMaster(2049): Cluster shutdown requested
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.
2012-09-18 20:18:30,093 INFO  [StoreCloserThread-t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(168): Closed region t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,093 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(1023): Closed t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,092 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.CompactSplitThread(253): Waiting for Split Thread to finish...
2012-09-18 20:18:30,093 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.CompactSplitThread(253): Waiting for Large Compaction Thread to finish...
2012-09-18 20:18:30,091 INFO  [StoreCloserThread-t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,093 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.CompactSplitThread(253): Waiting for Small Compaction Thread to finish...
2012-09-18 20:18:30,094 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311-EventThread] zookeeper.ZooKeeperWatcher(265): regionserver:33334-0x139db080e690002 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/shutdown
2012-09-18 20:18:30,093 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(168): Closed region t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,093 INFO  [hemera.apache.org,35050,1347999502111-CatalogJanitor] hbase.Chore(81): hemera.apache.org,35050,1347999502111-CatalogJanitor exiting
2012-09-18 20:18:30,093 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.: disabling compactions & flushes
2012-09-18 20:18:30,093 INFO  [Master:0;hemera.apache.org,35050,1347999502111] master.ServerManager(398): Waiting on regionserver(s) to go down hemera.apache.org,38814,1347999502374, hemera.apache.org,58959,1347999502361, hemera.apache.org,43875,1347999502347, hemera.apache.org,33334,1347999502311, hemera.apache.org,51601,1347999502333
2012-09-18 20:18:30,093 INFO  [hemera.apache.org,35050,1347999502111-BalancerChore] hbase.Chore(81): hemera.apache.org,35050,1347999502111-BalancerChore exiting
2012-09-18 20:18:30,093 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(168): Closed region t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,094 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.
2012-09-18 20:18:30,094 DEBUG [RS_CLOSE_ROOT-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of -ROOT-,,0.70236052
2012-09-18 20:18:30,094 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(1091): Waiting on 2 regions to close
2012-09-18 20:18:30,094 INFO  [StoreCloserThread-t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,094 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.
2012-09-18 20:18:30,095 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.
2012-09-18 20:18:30,094 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361-EventThread] zookeeper.ZooKeeperWatcher(265): regionserver:58959-0x139db080e690005 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/shutdown
2012-09-18 20:18:30,094 INFO  [pool-1-thread-1] regionserver.HRegionServer(1737): STOPPED: Shutdown requested
2012-09-18 20:18:30,094 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(1023): Closed t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,094 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333-EventThread] zookeeper.ZooKeeperWatcher(265): regionserver:51601-0x139db080e690001 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/shutdown
2012-09-18 20:18:30,093 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374-EventThread] zookeeper.ZooKeeperWatcher(265): regionserver:38814-0x139db080e690003 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/shutdown
2012-09-18 20:18:30,093 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347-EventThread] zookeeper.ZooKeeperWatcher(265): regionserver:43875-0x139db080e690004 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/shutdown
2012-09-18 20:18:30,093 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/shutdown
2012-09-18 20:18:30,095 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361-EventThread] zookeeper.ZKUtil(237): regionserver:58959-0x139db080e690005 /hbase/shutdown does not exist. Watcher is set.
2012-09-18 20:18:30,095 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374-EventThread] zookeeper.ZKUtil(237): regionserver:38814-0x139db080e690003 /hbase/shutdown does not exist. Watcher is set.
2012-09-18 20:18:30,095 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(168): Closed region t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,095 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.
2012-09-18 20:18:30,096 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(237): master:35050-0x139db080e690000 /hbase/shutdown does not exist. Watcher is set.
2012-09-18 20:18:30,095 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.: disabling compactions & flushes
2012-09-18 20:18:30,094 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(1095): {70236052=-ROOT-,,0.70236052}
2012-09-18 20:18:30,094 DEBUG [RS_CLOSE_ROOT-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing -ROOT-,,0.70236052: disabling compactions & flushes
2012-09-18 20:18:30,094 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311-EventThread] zookeeper.ZKUtil(237): regionserver:33334-0x139db080e690002 /hbase/shutdown does not exist. Watcher is set.
2012-09-18 20:18:30,096 DEBUG [RS_CLOSE_ROOT-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region -ROOT-,,0.70236052
2012-09-18 20:18:30,096 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.
2012-09-18 20:18:30,096 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347-EventThread] zookeeper.ZKUtil(237): regionserver:43875-0x139db080e690004 /hbase/shutdown does not exist. Watcher is set.
2012-09-18 20:18:30,095 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,096 INFO  [StoreCloserThread-t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,098 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:0;hemera.apache.org,33334,1347999502311.cacheFlusher exiting
2012-09-18 20:18:30,098 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(1023): Closed t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.
2012-09-18 20:18:30,098 INFO  [StoreCloserThread--ROOT-,,0.70236052-1] regionserver.HStore(635): Closed info
2012-09-18 20:18:30,095 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333-EventThread] zookeeper.ZKUtil(237): regionserver:51601-0x139db080e690001 /hbase/shutdown does not exist. Watcher is set.
2012-09-18 20:18:30,098 INFO  [RS_CLOSE_ROOT-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed -ROOT-,,0.70236052
2012-09-18 20:18:30,098 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(168): Closed region t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.
2012-09-18 20:18:30,098 DEBUG [RS_CLOSE_REGION-hemera.apache.org,33334,1347999502311-0] handler.CloseRegionHandler(124): Processing close of t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.
2012-09-18 20:18:30,098 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311.compactionChecker] hbase.Chore(81): RegionServer:0;hemera.apache.org,33334,1347999502311.compactionChecker exiting
2012-09-18 20:18:30,098 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,098 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.HRegionServer(947): stopping server hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,098 INFO  [SplitLogWorker-hemera.apache.org,33334,1347999502311] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,098 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@5afcb1
2012-09-18 20:18:30,098 DEBUG [RS_CLOSE_REGION-hemera.apache.org,33334,1347999502311-0] regionserver.HRegion(954): Closing t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.: disabling compactions & flushes
2012-09-18 20:18:30,098 DEBUG [RS_CLOSE_ROOT-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region -ROOT-,,0.70236052
2012-09-18 20:18:30,099 DEBUG [RS_CLOSE_REGION-hemera.apache.org,33334,1347999502311-0] regionserver.HRegion(975): Updates disabled for region t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.
2012-09-18 20:18:30,099 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e69000b
2012-09-18 20:18:30,099 INFO  [SplitLogWorker-hemera.apache.org,33334,1347999502311] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,33334,1347999502311 exiting
2012-09-18 20:18:30,099 INFO  [StoreCloserThread-t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,099 INFO  [RS_CLOSE_REGION-hemera.apache.org,33334,1347999502311-0] regionserver.HRegion(1023): Closed t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.
2012-09-18 20:18:30,100 DEBUG [RS_CLOSE_REGION-hemera.apache.org,33334,1347999502311-0] handler.CloseRegionHandler(168): Closed region t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.
2012-09-18 20:18:30,100 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.HRegionServer(966): stopping server hemera.apache.org,33334,1347999502311; all regions closed.
2012-09-18 20:18:30,100 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311.logSyncer] wal.HLog$LogSyncer(1245): RegionServer:0;hemera.apache.org,33334,1347999502311.logSyncer exiting
2012-09-18 20:18:30,100 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311] wal.HLog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,272 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(966): stopping server hemera.apache.org,51601,1347999502333; all regions closed.
2012-09-18 20:18:30,272 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.logSyncer] wal.HLog$LogSyncer(1245): RegionServer:1;hemera.apache.org,51601,1347999502333.logSyncer exiting
2012-09-18 20:18:30,272 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] wal.HLog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,51601,1347999502333
2012-09-18 20:18:30,284 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311.leaseChecker] regionserver.Leases(123): RegionServer:0;hemera.apache.org,33334,1347999502311.leaseChecker closing leases
2012-09-18 20:18:30,284 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311.leaseChecker] regionserver.Leases(130): RegionServer:0;hemera.apache.org,33334,1347999502311.leaseChecker closed leases
2012-09-18 20:18:30,285 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.leaseChecker] regionserver.Leases(123): RegionServer:1;hemera.apache.org,51601,1347999502333.leaseChecker closing leases
2012-09-18 20:18:30,285 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.leaseChecker] regionserver.Leases(123): RegionServer:2;hemera.apache.org,43875,1347999502347.leaseChecker closing leases
2012-09-18 20:18:30,285 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.leaseChecker] regionserver.Leases(130): RegionServer:2;hemera.apache.org,43875,1347999502347.leaseChecker closed leases
2012-09-18 20:18:30,286 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.Leases(123): RegionServer:1;hemera.apache.org,51601,1347999502333 closing leases
2012-09-18 20:18:30,286 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.Leases(130): RegionServer:1;hemera.apache.org,51601,1347999502333 closed leases
2012-09-18 20:18:30,285 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.leaseChecker] regionserver.Leases(130): RegionServer:1;hemera.apache.org,51601,1347999502333.leaseChecker closed leases
2012-09-18 20:18:30,286 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.CompactSplitThread(253): Waiting for Split Thread to finish...
2012-09-18 20:18:30,286 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(966): stopping server hemera.apache.org,43875,1347999502347; all regions closed.
2012-09-18 20:18:30,286 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.CompactSplitThread(253): Waiting for Large Compaction Thread to finish...
2012-09-18 20:18:30,287 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.CompactSplitThread(253): Waiting for Small Compaction Thread to finish...
2012-09-18 20:18:30,287 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.logSyncer] wal.HLog$LogSyncer(1245): RegionServer:2;hemera.apache.org,43875,1347999502347.logSyncer exiting
2012-09-18 20:18:30,287 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] wal.HLog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,43875,1347999502347
2012-09-18 20:18:30,287 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.leaseChecker] regionserver.Leases(123): RegionServer:3;hemera.apache.org,58959,1347999502361.leaseChecker closing leases
2012-09-18 20:18:30,288 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.leaseChecker] regionserver.Leases(130): RegionServer:3;hemera.apache.org,58959,1347999502361.leaseChecker closed leases
2012-09-18 20:18:30,289 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/rs/hemera.apache.org,51601,1347999502333
2012-09-18 20:18:30,289 INFO  [pool-1-thread-1-EventThread] zookeeper.RegionServerTracker(94): RegionServer ephemeral node deleted, processing expiration [hemera.apache.org,51601,1347999502333]
2012-09-18 20:18:30,289 INFO  [pool-1-thread-1-EventThread] master.ServerManager(446): Cluster shutdown set; hemera.apache.org,51601,1347999502333 expired; onlineServers=4
2012-09-18 20:18:30,289 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/rs
2012-09-18 20:18:30,290 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(995): stopping server hemera.apache.org,51601,1347999502333; zookeeper connection closed.
2012-09-18 20:18:30,290 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(998): RegionServer:1;hemera.apache.org,51601,1347999502333 exiting
2012-09-18 20:18:30,290 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@21e115] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@21e115
2012-09-18 20:18:30,291 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374
2012-09-18 20:18:30,291 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,43875,1347999502347
2012-09-18 20:18:30,292 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,293 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,296 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(966): stopping server hemera.apache.org,58959,1347999502361; all regions closed.
2012-09-18 20:18:30,296 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.logSyncer] wal.HLog$LogSyncer(1245): RegionServer:3;hemera.apache.org,58959,1347999502361.logSyncer exiting
2012-09-18 20:18:30,297 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] wal.HLog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,298 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.Leases(123): RegionServer:2;hemera.apache.org,43875,1347999502347 closing leases
2012-09-18 20:18:30,298 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.Leases(130): RegionServer:2;hemera.apache.org,43875,1347999502347 closed leases
2012-09-18 20:18:30,298 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.CompactSplitThread(253): Waiting for Split Thread to finish...
2012-09-18 20:18:30,298 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.CompactSplitThread(253): Waiting for Large Compaction Thread to finish...
2012-09-18 20:18:30,298 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.CompactSplitThread(253): Waiting for Small Compaction Thread to finish...
2012-09-18 20:18:30,300 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/rs/hemera.apache.org,43875,1347999502347
2012-09-18 20:18:30,300 INFO  [pool-1-thread-1-EventThread] zookeeper.RegionServerTracker(94): RegionServer ephemeral node deleted, processing expiration [hemera.apache.org,43875,1347999502347]
2012-09-18 20:18:30,300 INFO  [pool-1-thread-1-EventThread] master.ServerManager(446): Cluster shutdown set; hemera.apache.org,43875,1347999502347 expired; onlineServers=3
2012-09-18 20:18:30,301 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/rs
2012-09-18 20:18:30,301 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(995): stopping server hemera.apache.org,43875,1347999502347; zookeeper connection closed.
2012-09-18 20:18:30,301 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(998): RegionServer:2;hemera.apache.org,43875,1347999502347 exiting
2012-09-18 20:18:30,302 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@147e54e] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@147e54e
2012-09-18 20:18:30,302 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374
2012-09-18 20:18:30,303 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,303 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,307 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.Leases(123): RegionServer:3;hemera.apache.org,58959,1347999502361 closing leases
2012-09-18 20:18:30,308 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.Leases(130): RegionServer:3;hemera.apache.org,58959,1347999502361 closed leases
2012-09-18 20:18:30,309 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/rs/hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,309 INFO  [pool-1-thread-1-EventThread] zookeeper.RegionServerTracker(94): RegionServer ephemeral node deleted, processing expiration [hemera.apache.org,58959,1347999502361]
2012-09-18 20:18:30,309 INFO  [pool-1-thread-1-EventThread] master.ServerManager(446): Cluster shutdown set; hemera.apache.org,58959,1347999502361 expired; onlineServers=2
2012-09-18 20:18:30,309 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/rs
2012-09-18 20:18:30,310 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(995): stopping server hemera.apache.org,58959,1347999502361; zookeeper connection closed.
2012-09-18 20:18:30,310 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(998): RegionServer:3;hemera.apache.org,58959,1347999502361 exiting
2012-09-18 20:18:30,310 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@1c7510d] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@1c7510d
2012-09-18 20:18:30,310 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374
2012-09-18 20:18:30,311 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(869): Closing user regions
2012-09-18 20:18:30,311 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,311 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] handler.CloseRegionHandler(124): Processing close of t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.
2012-09-18 20:18:30,311 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] regionserver.HRegion(954): Closing t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.: disabling compactions & flushes
2012-09-18 20:18:30,311 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(124): Processing close of t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.
2012-09-18 20:18:30,311 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] regionserver.HRegion(975): Updates disabled for region t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.
2012-09-18 20:18:30,312 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(954): Closing t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.: disabling compactions & flushes
2012-09-18 20:18:30,312 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(975): Updates disabled for region t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.
2012-09-18 20:18:30,312 INFO  [StoreCloserThread-t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,311 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] handler.CloseRegionHandler(124): Processing close of t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.
2012-09-18 20:18:30,312 INFO  [StoreCloserThread-t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,312 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] regionserver.HRegion(954): Closing t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.: disabling compactions & flushes
2012-09-18 20:18:30,313 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1023): Closed t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.
2012-09-18 20:18:30,313 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] regionserver.HRegion(975): Updates disabled for region t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.
2012-09-18 20:18:30,313 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(168): Closed region t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.
2012-09-18 20:18:30,312 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] regionserver.HRegion(1023): Closed t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.
2012-09-18 20:18:30,313 INFO  [StoreCloserThread-t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,313 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(124): Processing close of t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.
2012-09-18 20:18:30,313 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] regionserver.HRegion(1023): Closed t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.
2012-09-18 20:18:30,313 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(954): Closing t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.: disabling compactions & flushes
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] handler.CloseRegionHandler(168): Closed region t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(975): Updates disabled for region t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] handler.CloseRegionHandler(124): Processing close of t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.
2012-09-18 20:18:30,313 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] handler.CloseRegionHandler(168): Closed region t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] regionserver.HRegion(954): Closing t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.: disabling compactions & flushes
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] handler.CloseRegionHandler(124): Processing close of t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.
2012-09-18 20:18:30,314 INFO  [StoreCloserThread-t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] regionserver.HRegion(954): Closing t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.: disabling compactions & flushes
2012-09-18 20:18:30,315 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] regionserver.HRegion(975): Updates disabled for region t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.
2012-09-18 20:18:30,314 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] regionserver.HRegion(975): Updates disabled for region t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.
2012-09-18 20:18:30,315 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1023): Closed t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.
2012-09-18 20:18:30,315 INFO  [StoreCloserThread-t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,315 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(168): Closed region t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.
2012-09-18 20:18:30,315 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] regionserver.HRegion(1023): Closed t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.
2012-09-18 20:18:30,316 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-1] handler.CloseRegionHandler(168): Closed region t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.
2012-09-18 20:18:30,315 INFO  [StoreCloserThread-t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,316 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(124): Processing close of t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.
2012-09-18 20:18:30,316 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] regionserver.HRegion(1023): Closed t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.
2012-09-18 20:18:30,316 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(954): Closing t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.: disabling compactions & flushes
2012-09-18 20:18:30,316 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-2] handler.CloseRegionHandler(168): Closed region t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.
2012-09-18 20:18:30,316 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(975): Updates disabled for region t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.
2012-09-18 20:18:30,317 INFO  [StoreCloserThread-t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,317 INFO  [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1023): Closed t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.
2012-09-18 20:18:30,317 DEBUG [RS_CLOSE_REGION-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(168): Closed region t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.
2012-09-18 20:18:30,392 INFO  [hemera.apache.org,35050,1347999502111.splitLogManagerTimeoutMonitor] hbase.Chore(81): hemera.apache.org,35050,1347999502111.splitLogManagerTimeoutMonitor exiting
2012-09-18 20:18:30,507 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311] wal.HLog(975): Moved 1 log files to /user/jenkins/hbase/.oldlogs
2012-09-18 20:18:30,509 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.Leases(123): RegionServer:0;hemera.apache.org,33334,1347999502311 closing leases
2012-09-18 20:18:30,509 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.Leases(130): RegionServer:0;hemera.apache.org,33334,1347999502311 closed leases
2012-09-18 20:18:30,509 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.CompactSplitThread(253): Waiting for Split Thread to finish...
2012-09-18 20:18:30,509 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.CompactSplitThread(253): Waiting for Large Compaction Thread to finish...
2012-09-18 20:18:30,509 DEBUG [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.CompactSplitThread(253): Waiting for Small Compaction Thread to finish...
2012-09-18 20:18:30,510 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/rs/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,511 INFO  [pool-1-thread-1-EventThread] zookeeper.RegionServerTracker(94): RegionServer ephemeral node deleted, processing expiration [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,511 INFO  [pool-1-thread-1-EventThread] master.ServerManager(446): Cluster shutdown set; hemera.apache.org,33334,1347999502311 expired; onlineServers=1
2012-09-18 20:18:30,511 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/rs
2012-09-18 20:18:30,512 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.HRegionServer(995): stopping server hemera.apache.org,33334,1347999502311; zookeeper connection closed.
2012-09-18 20:18:30,512 INFO  [RegionServer:0;hemera.apache.org,33334,1347999502311] regionserver.HRegionServer(998): RegionServer:0;hemera.apache.org,33334,1347999502311 exiting
2012-09-18 20:18:30,512 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@1ee9cc3] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@1ee9cc3
2012-09-18 20:18:30,513 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374
2012-09-18 20:18:30,513 INFO  [pool-1-thread-1] regionserver.HRegionServer(1737): STOPPED: Shutdown requested
2012-09-18 20:18:30,514 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,514 INFO  [SplitLogWorker-hemera.apache.org,38814,1347999502374] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,514 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374.compactionChecker] hbase.Chore(81): RegionServer:4;hemera.apache.org,38814,1347999502374.compactionChecker exiting
2012-09-18 20:18:30,514 INFO  [SplitLogWorker-hemera.apache.org,38814,1347999502374] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,38814,1347999502374 exiting
2012-09-18 20:18:30,514 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(947): stopping server hemera.apache.org,38814,1347999502374
2012-09-18 20:18:30,514 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,514 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:4;hemera.apache.org,38814,1347999502374.cacheFlusher exiting
2012-09-18 20:18:30,514 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@df39bc
2012-09-18 20:18:30,515 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e69000a
2012-09-18 20:18:30,516 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.CompactSplitThread(253): Waiting for Split Thread to finish...
2012-09-18 20:18:30,516 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.CompactSplitThread(253): Waiting for Large Compaction Thread to finish...
2012-09-18 20:18:30,516 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.CompactSplitThread(253): Waiting for Small Compaction Thread to finish...
2012-09-18 20:18:30,517 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(1091): Waiting on 1 regions to close
2012-09-18 20:18:30,517 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(124): Processing close of .META.,,1.1028785192
2012-09-18 20:18:30,517 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(1095): {1028785192=.META.,,1.1028785192}
2012-09-18 20:18:30,517 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(954): Closing .META.,,1.1028785192: disabling compactions & flushes
2012-09-18 20:18:30,518 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(975): Updates disabled for region .META.,,1.1028785192
2012-09-18 20:18:30,518 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1481): Started memstore flush for .META.,,1.1028785192, current region memstore size 20.1k
2012-09-18 20:18:30,518 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1528): Finished snapshotting .META.,,1.1028785192, commencing wait for mvcc, flushsize=20560
2012-09-18 20:18:30,518 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1538): Finished snapshotting, commencing flushing stores
2012-09-18 20:18:30,521 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] util.FSUtils(167): Creating file=hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/.tmp/6c449086188648c08a990537dba22ca4 with permission=rwxrwxrwx
2012-09-18 20:18:30,523 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] hfile.HFileWriterV2(142): Initialized with CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false]
2012-09-18 20:18:30,524 INFO  [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.StoreFile$Writer(1021): Delete Family Bloom filter type for hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/.tmp/6c449086188648c08a990537dba22ca4: CompoundBloomFilterWriter
2012-09-18 20:18:30,939 INFO  [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.StoreFile$Writer(1241): NO General Bloom and NO DeleteFamily was added to HFile (hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/.tmp/6c449086188648c08a990537dba22ca4) 
2012-09-18 20:18:30,939 INFO  [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HStore(767): Flushed , sequenceid=59, memsize=20.1k, into tmp file hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/.tmp/6c449086188648c08a990537dba22ca4
2012-09-18 20:18:30,948 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HStore(792): Renaming flushed file at hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/.tmp/6c449086188648c08a990537dba22ca4 to hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/info/6c449086188648c08a990537dba22ca4
2012-09-18 20:18:30,954 INFO  [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HStore(815): Added hdfs://localhost:52077/user/jenkins/hbase/.META./1028785192/info/6c449086188648c08a990537dba22ca4, entries=89, sequenceid=59, filesize=10.2k
2012-09-18 20:18:30,955 INFO  [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1617): Finished memstore flush of ~20.1k/20560, currentsize=0.0/0 for region .META.,,1.1028785192 in 437ms, sequenceid=59, compaction requested=false
2012-09-18 20:18:30,956 INFO  [StoreCloserThread-.META.,,1.1028785192-1] regionserver.HStore(635): Closed info
2012-09-18 20:18:30,956 INFO  [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] regionserver.HRegion(1023): Closed .META.,,1.1028785192
2012-09-18 20:18:30,956 DEBUG [RS_CLOSE_META-hemera.apache.org,38814,1347999502374-0] handler.CloseRegionHandler(168): Closed region .META.,,1.1028785192
2012-09-18 20:18:31,112 INFO  [Master:0;hemera.apache.org,35050,1347999502111] master.ServerManager(398): Waiting on regionserver(s) to go down hemera.apache.org,38814,1347999502374
2012-09-18 20:18:31,118 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(966): stopping server hemera.apache.org,38814,1347999502374; all regions closed.
2012-09-18 20:18:31,118 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374.logSyncer] wal.HLog$LogSyncer(1245): RegionServer:4;hemera.apache.org,38814,1347999502374.logSyncer exiting
2012-09-18 20:18:31,118 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] wal.HLog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,38814,1347999502374
2012-09-18 20:18:31,124 DEBUG [RegionServer:4;hemera.apache.org,38814,1347999502374] wal.HLog(975): Moved 1 log files to /user/jenkins/hbase/.oldlogs
2012-09-18 20:18:31,125 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.Leases(123): RegionServer:4;hemera.apache.org,38814,1347999502374 closing leases
2012-09-18 20:18:31,125 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.Leases(130): RegionServer:4;hemera.apache.org,38814,1347999502374 closed leases
2012-09-18 20:18:31,126 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/rs/hemera.apache.org,38814,1347999502374
2012-09-18 20:18:31,126 INFO  [pool-1-thread-1-EventThread] zookeeper.RegionServerTracker(94): RegionServer ephemeral node deleted, processing expiration [hemera.apache.org,38814,1347999502374]
2012-09-18 20:18:31,126 INFO  [pool-1-thread-1-EventThread] master.ServerManager(446): Cluster shutdown set; hemera.apache.org,38814,1347999502374 expired; onlineServers=0
2012-09-18 20:18:31,126 DEBUG [Master:0;hemera.apache.org,35050,1347999502111] master.HMaster(1067): Stopping service threads
2012-09-18 20:18:31,126 INFO  [pool-1-thread-1-EventThread] master.HMaster(2049): Cluster shutdown set; onlineServer=0
2012-09-18 20:18:31,127 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(995): stopping server hemera.apache.org,38814,1347999502374; zookeeper connection closed.
2012-09-18 20:18:31,127 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374] regionserver.HRegionServer(998): RegionServer:4;hemera.apache.org,38814,1347999502374 exiting
2012-09-18 20:18:31,127 INFO  [Master:0;hemera.apache.org,35050,1347999502111.oldLogCleaner] hbase.Chore(81): Master:0;hemera.apache.org,35050,1347999502111.oldLogCleaner exiting
2012-09-18 20:18:31,127 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/rs
2012-09-18 20:18:31,127 INFO  [Master:0;hemera.apache.org,35050,1347999502111.archivedHFileCleaner] hbase.Chore(81): Master:0;hemera.apache.org,35050,1347999502111.archivedHFileCleaner exiting
2012-09-18 20:18:31,128 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@194f467] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@194f467
2012-09-18 20:18:31,128 INFO  [pool-1-thread-1] util.JVMClusterUtil(263): Shutdown of 1 master(s) and 5 regionserver(s) complete
2012-09-18 20:18:31,128 INFO  [pool-1-thread-1] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e69000d
2012-09-18 20:18:31,129 DEBUG [Master:0;hemera.apache.org,35050,1347999502111] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 36 byte(s) of data from znode /hbase/master; data=PBUF\x0A\x1E\x0A\x11hemera.ap...
2012-09-18 20:18:31,130 DEBUG [Master:0;hemera.apache.org,35050,1347999502111] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1152c0e
2012-09-18 20:18:31,130 INFO  [hemera.apache.org,35050,1347999502111.timeoutMonitor] hbase.Chore(81): hemera.apache.org,35050,1347999502111.timeoutMonitor exiting
2012-09-18 20:18:31,130 INFO  [hemera.apache.org,35050,1347999502111.timerUpdater] hbase.Chore(81): hemera.apache.org,35050,1347999502111.timerUpdater exiting
2012-09-18 20:18:31,130 INFO  [Master:0;hemera.apache.org,35050,1347999502111] master.HMaster(481): HMaster main thread exiting
2012-09-18 20:18:31,132 INFO  [pool-1-thread-1] zookeeper.MiniZooKeeperCluster(238): Shutdown MiniZK cluster with all ZK servers
2012-09-18 20:18:31,132 INFO  [pool-1-thread-1] log.Slf4jLog(67): Stopped SelectChannelConnector@localhost:0
2012-09-18 20:18:31,231 DEBUG [Master:0;hemera.apache.org,35050,1347999502111-EventThread] zookeeper.ZooKeeperWatcher(265): hconnection 0x1c1eceb-0x139db080e690006 Received ZooKeeper Event, type=None, state=Disconnected, path=null
2012-09-18 20:18:31,231 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): unittest-0x139db080e69000e Received ZooKeeper Event, type=None, state=Disconnected, path=null
2012-09-18 20:18:31,231 DEBUG [Master:0;hemera.apache.org,35050,1347999502111-EventThread] zookeeper.ZooKeeperWatcher(363): hconnection 0x1c1eceb-0x139db080e690006 Received Disconnected from ZooKeeper, ignoring
2012-09-18 20:18:31,231 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(363): unittest-0x139db080e69000e Received Disconnected from ZooKeeper, ignoring
2012-09-18 20:18:31,237 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@1f09a31] datanode.DataXceiverServer(138): DatanodeRegistration(127.0.0.1:34950, storageID=DS-1597293543-140.211.11.27-34950-1347999501880, infoPort=40910, ipcPort=42169):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:159)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:662)

2012-09-18 20:18:31,285 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374.leaseChecker] regionserver.Leases(123): RegionServer:4;hemera.apache.org,38814,1347999502374.leaseChecker closing leases
2012-09-18 20:18:31,285 INFO  [RegionServer:4;hemera.apache.org,38814,1347999502374.leaseChecker] regionserver.Leases(130): RegionServer:4;hemera.apache.org,38814,1347999502374.leaseChecker closed leases
2012-09-18 20:18:32,237 WARN  [pool-1-thread-1] util.MBeans(73): Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-809266774
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-809266774
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2067)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:799)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:566)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:550)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:503)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:752)
	at org.apache.hadoop.hbase.TestDrainingServer.tearDownAfterClass(TestDrainingServer.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2012-09-18 20:18:32,238 WARN  [pool-1-thread-1] datanode.FSDatasetAsyncDiskService(121): AsyncDiskService has already shut down.
2012-09-18 20:18:32,238 INFO  [pool-1-thread-1] log.Slf4jLog(67): Stopped SelectChannelConnector@localhost:0
2012-09-18 20:18:32,341 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2a5ab9] datanode.DataXceiverServer(138): DatanodeRegistration(127.0.0.1:46013, storageID=DS-1826840059-140.211.11.27-46013-1347999501721, infoPort=33986, ipcPort=42754):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:159)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:662)

2012-09-18 20:18:33,341 WARN  [pool-1-thread-1] util.MBeans(73): Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId1641970170
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId1641970170
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2067)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:799)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:566)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:550)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:503)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:752)
	at org.apache.hadoop.hbase.TestDrainingServer.tearDownAfterClass(TestDrainingServer.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2012-09-18 20:18:33,342 WARN  [pool-1-thread-1] datanode.FSDatasetAsyncDiskService(121): AsyncDiskService has already shut down.
2012-09-18 20:18:33,342 INFO  [pool-1-thread-1] log.Slf4jLog(67): Stopped SelectChannelConnector@localhost:0
2012-09-18 20:18:33,444 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@18c458] datanode.DataXceiverServer(138): DatanodeRegistration(127.0.0.1:39857, storageID=DS-635958744-140.211.11.27-39857-1347999501558, infoPort=49006, ipcPort=39278):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:159)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:662)

2012-09-18 20:18:34,444 WARN  [pool-1-thread-1] util.MBeans(73): Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId389747850
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId389747850
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2067)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:799)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:566)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:550)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:503)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:752)
	at org.apache.hadoop.hbase.TestDrainingServer.tearDownAfterClass(TestDrainingServer.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2012-09-18 20:18:34,445 WARN  [pool-1-thread-1] datanode.FSDatasetAsyncDiskService(121): AsyncDiskService has already shut down.
2012-09-18 20:18:34,445 INFO  [pool-1-thread-1] log.Slf4jLog(67): Stopped SelectChannelConnector@localhost:0
2012-09-18 20:18:34,547 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@11c55bb] datanode.DataXceiverServer(138): DatanodeRegistration(127.0.0.1:47094, storageID=DS-1080581881-140.211.11.27-47094-1347999501341, infoPort=51461, ipcPort=36418):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:159)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:662)

2012-09-18 20:18:35,547 WARN  [pool-1-thread-1] util.MBeans(73): Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1857067045
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1857067045
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2067)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:799)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:566)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:550)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:503)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:752)
	at org.apache.hadoop.hbase.TestDrainingServer.tearDownAfterClass(TestDrainingServer.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2012-09-18 20:18:35,548 WARN  [pool-1-thread-1] datanode.FSDatasetAsyncDiskService(121): AsyncDiskService has already shut down.
2012-09-18 20:18:35,548 INFO  [pool-1-thread-1] log.Slf4jLog(67): Stopped SelectChannelConnector@localhost:0
2012-09-18 20:18:35,650 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@b3b6a6] datanode.DataXceiverServer(138): DatanodeRegistration(127.0.0.1:58813, storageID=DS-1129259921-140.211.11.27-58813-1347999501165, infoPort=47167, ipcPort=40188):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:159)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:662)

2012-09-18 20:18:36,179 WARN  [DataNode: [/x1/jenkins/jenkins-slave/workspace/HBase-TRUNK/trunk/hbase-server/target/test-data/d1b07c00-61dd-47e9-ac9b-95a060d6795d/dfscluster_6ef910b6-2888-4dda-84f2-de311d45c6a2/dfs/data/data1,/x1/jenkins/jenkins-slave/workspace/HBase-TRUNK/trunk/hbase-server/target/test-data/d1b07c00-61dd-47e9-ac9b-95a060d6795d/dfscluster_6ef910b6-2888-4dda-84f2-de311d45c6a2/dfs/data/data2]] util.MBeans(73): Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:522)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:737)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1471)
	at java.lang.Thread.run(Thread.java:662)
2012-09-18 20:18:36,650 WARN  [pool-1-thread-1] util.MBeans(73): Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-245364923
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-245364923
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2067)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:799)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:566)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:550)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:503)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:752)
	at org.apache.hadoop.hbase.TestDrainingServer.tearDownAfterClass(TestDrainingServer.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2012-09-18 20:18:36,651 WARN  [pool-1-thread-1] datanode.FSDatasetAsyncDiskService(121): AsyncDiskService has already shut down.
2012-09-18 20:18:36,651 INFO  [pool-1-thread-1] log.Slf4jLog(67): Stopped SelectChannelConnector@localhost:0
2012-09-18 20:18:36,753 WARN  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$ReplicationMonitor@1827d1] namenode.FSNamesystem$ReplicationMonitor(2718): ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
2012-09-18 20:18:36,774 INFO  [pool-1-thread-1] hbase.HBaseTestingUtility(756): Minicluster is down
{code}",nkeywal,yuzhihong@gmail.com,Major,Closed,Fixed,18/Sep/12 20:53,23/Sep/13 18:31
Bug,HBASE-6820,12608161,[WINDOWS] MiniZookeeperCluster should ensure that ZKDatabase is closed upon shutdown(),"MiniZookeeperCluster.shutdown() shuts down the ZookeeperServer and NIOServerCnxnFactory. However, MiniZookeeperCluster uses a deprecated ZookeeperServer constructor, which in turn constructs its own FileTxnSnapLog, and ZKDatabase. Since ZookeeperServer.shutdown() does not close() the ZKDatabase, we have to explicitly close it in MiniZookeeperCluster.shutdown().

Tests effected by this are
{code}
TestSplitLogManager
TestSplitLogWorker
TestOfflineMetaRebuildBase
TestOfflineMetaRebuildHole
TestOfflineMetaRebuildOverlap
{code}

",enis,enis,Major,Closed,Fixed,18/Sep/12 23:49,10/Jan/14 03:59
Bug,HBASE-6821,12608162,[WINDOWS] In TestMetaMigrationConvertingToPB .META. table name causes file system problems on windows,"TestMetaMigrationRemovingHTD untars a cluster dir having a .META. subdirectory. This causes mvn clean to fail.
",enis,enis,Major,Closed,Fixed,18/Sep/12 23:52,23/Sep/13 18:31
Bug,HBASE-6822,12608163,[WINDOWS] MiniZookeeperCluster multiple daemons bind to the same port,"TestHBaseTestingUtility.testMiniZooKeeper() tests whether the mini zk cluster is working by launching 5 threads corresponding to zk servers. 

NIOServerCnxnFactory.configure() configures the socket as:

{code}
        this.ss = ServerSocketChannel.open();
        ss.socket().setReuseAddress(true);
{code}

setReuseAddress() is set, because it allows the server to come back up and bind to the same port before the socket is timed-out by the kernel.

Under windows, the behavior on ServerSocket.setReuseAddress() is different than on linux, in which it allows any process to bind to an already-bound port. This causes ZK nodes starting on the same node, to be able to bind to the same port. 

The following part of the patch at https://issues.apache.org/jira/browse/HADOOP-8223 deals with this case for Hadoop:

{code}
if(Shell.WINDOWS) {
+      // result of setting the SO_REUSEADDR flag is different on Windows
+      // http://msdn.microsoft.com/en-us/library/ms740621(v=vs.85).aspx
+      // without this 2 NN's can start on the same machine and listen on 
+      // the same port with indeterminate routing of incoming requests to them
+      ret.setReuseAddress(false);
+    }
{code}

We should do the same in Zookeeper (I'll open a ZOOK issue). But in the meantime, we can fix hbase tests to not rely on BindException to resolve for bind errors. Especially, in  MiniZKCluster.startup() when starting more than 1 servers, we already know that we have to increment the port number. ",enis,enis,Major,Closed,Fixed,18/Sep/12 23:55,23/Sep/13 18:31
Bug,HBASE-6823,12608164,[WINDOWS] TestSplitTransaction fails due to the Log handle not released by a call to DaughterOpener.start(),"There are two unit test cases in HBase RegionServer test failed in the clean up stage that failed to delete the files/folders created in the test. 
testWholesomeSplit(org.apache.hadoop.hbase.regionserver.TestSplitTransaction): Failed delete of ./target/test-
data/1c386abc-f159-492e-b21f-e89fab24d85b/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/a588d813fd26280c2b42e93565ed960c
testRollback(org.apache.hadoop.hbase.regionserver.TestSplitTransaction): Failed delete of ./target/test-data/6
1a1a14b-0cc9-4dd6-93fd-4dc021e2bfcc/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/8090abc89528461fa284288c257662cd
The root cause is triggered by ta call to the DaughterOpener.start() in \src\hbase\src\main\java\org\apache\hadoop\hbase\regionserver\SplitTransactopn.Java (openDaughters() function). It left handles to the splited folder/file and causing deleting of the file/folder failed in the Windows OS.

Windows does not allow to delete a file, while there are open file handlers.",enis,enis,Major,Closed,Fixed,19/Sep/12 00:00,15/Oct/13 04:46
Bug,HBASE-6824,12608166,Introduce ${hbase.local.dir} and save coprocessor jars there,"We need to make the temp directory where coprocessor jars are saved configurable. For this we will add hbase.local.dir configuration parameter. 

Windows tests are failing due to the pathing problems for coprocessor jars:
Two HBase TestClassLoading unit tests failed due to a failiure in loading the test file from HDFS:
{code}
testClassLoadingFromHDFS(org.apache.hadoop.hbase.coprocessor.TestClassLoading): Class TestCP1 was missing on a region
testClassLoadingFromLibDirInJar(org.apache.hadoop.hbase.coprocessor.TestClassLoading): Class TestCP1 was missing on a region
{code}

The problem is that CoprocessorHost.load() copies the jar file locally, and schedules the local file to be deleted on exit, but calling FileSystem.deleteOnExit(). However, the filesystem is not the file system of the local file, it is the distributed file system, so on windows, the Path fails.",enis,enis,Major,Closed,Fixed,19/Sep/12 00:02,26/Feb/13 08:27
Bug,HBASE-6826,12608170,[WINDOWS] TestFromClientSide failures,"The following tests fail for TestFromClientSide: 
{code}
testPoolBehavior()
testClientPoolRoundRobin()
testClientPoolThreadLocal()
{code}

The first test fails due to the fact that the test (wrongly) assumes that ThredPoolExecutor can reclaim the thread immediately. 

The second and third tests seem to fail because that Put's to the table does not specify an explicit timestamp, but on windows, consecutive calls to put happen to finish in the same milisecond so that the resulting mutations have the same timestamp, thus there is only one version of the cell value.  ",enis,enis,Major,Closed,Fixed,19/Sep/12 00:14,21/Aug/13 00:08
Bug,HBASE-6827,12608171,[WINDOWS] TestScannerTimeout fails expecting a timeout,"TestScannerTimeout.test2481() fails with:
{code}
java.lang.AssertionError: We should be timing out
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hbase.client.TestScannerTimeout.test2481(TestScannerTimeout.java:117)
{code}",enis,enis,Major,Closed,Fixed,19/Sep/12 00:16,23/Sep/13 18:31
Bug,HBASE-6828,12608172,[WINDOWS] TestMemoryBoundedLogMessageBuffer failures,"TestMemoryBoundedLogMessageBuffer fails because of a suspected \n line ending difference.
",enis,enis,Major,Closed,Fixed,19/Sep/12 00:18,23/Sep/13 18:30
Bug,HBASE-6829,12608173,[WINDOWS] Tests should ensure that HLog is closed,"TestCacheOnWriteInSchema and TestCompactSelection fails with 
{code}
java.io.IOException: Target HLog directory already exists: ./target/test-data/2d814e66-75d3-4c1b-92c7-a49d9972e8fd/TestCacheOnWriteInSchema/logs
	at org.apache.hadoop.hbase.regionserver.wal.HLog.<init>(HLog.java:385)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.<init>(HLog.java:316)
	at org.apache.hadoop.hbase.regionserver.TestCacheOnWriteInSchema.setUp(TestCacheOnWriteInSchema.java:162)
{code}",enis,enis,Major,Closed,Fixed,19/Sep/12 00:24,23/Sep/13 18:31
Bug,HBASE-6830,12608174,[WINDOWS] Tests should not rely on local temp dir to be available in DFS,"Some of the tests resolve the local temp directory for temporary test data, but use this directory path in dfs. Since on windows, local temp dir is resolved to something like: c:\\<path_to_local_dir>, DistributedFileSystem.getPathName() throws an IllegalArgumentException complaining that it is not a valid path name. 

Instead of relying on a local temp dir name, we should create a temp dir on dfs, and use this as a basis dir for test data. 

At least the following test cases are affected by this: 
{code}
TestHFileOutputFormat
TestHRegionServerBulkLoad
{code}
",enis,enis,Major,Closed,Fixed,19/Sep/12 00:27,23/Sep/13 18:31
Bug,HBASE-6831,12608175,[WINDOWS] HBaseTestingUtility.expireSession() does not expire zookeeper session,"TestReplicationPeer fails because it forces the zookeeper session expiration by calling HBaseTestingUtilty.expireSesssion(), but that function fails to do so.
",enis,enis,Major,Closed,Fixed,19/Sep/12 00:29,23/Sep/13 18:30
Bug,HBASE-6832,12608178,"[WINDOWS] Tests should use explicit timestamp for Puts, and not rely on implicit RS timing  ","TestRegionObserverBypass.testMulti() fails with 
{code}
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.coprocessor.TestRegionObserverBypass.checkRowAndDelete(TestRegionObserverBypass.java:173)
	at org.apache.hadoop.hbase.coprocessor.TestRegionObserverBypass.testMulti(TestRegionObserverBypass.java:166)
{code}
",enis,enis,Major,Closed,Fixed,19/Sep/12 01:04,23/Sep/13 18:30
Bug,HBASE-6834,12608181,HBaseClusterManager shell code breaks hadoop-2.0 build,"I get the following error:
{noformat}
HBaseClusterManager.java:[73,23] getExecString() in org.apache.hadoop.hbase.HBaseClusterManager.RemoteShell cannot override getExecString() in org.apache.hadoop.util.Shell.ShellCommandExecutor; attempting to assign weaker access privileges; was public
{noformat}

the issue is that getExecString() is declared public in hadoop-2.0, but protected in hadoop-1.0.  In HBase, it is declared protected, and you can't downgrade from public to protected.

We can just declare the function public and that seems to work, but given that in hadoop the class is declared
{code}
@InterfaceAudience.LimitedPrivate({""HDFS"", ""MapReduce""})
{code}
perhaps we should just copy the class into hbase source.",enis,gchanan,Major,Closed,Fixed,19/Sep/12 01:38,23/Sep/13 18:30
Bug,HBASE-6835,12608183,HBaseAdmin.flush claims to be asynchronous but appears to be synchronous,"Relevant comment:
{code}
   * Flush a table or an individual region.
   * Asynchronous operation.
{code}

but it looks like it's synchronous.  In fact, it returns whether the flush ran or not:
{code}
message FlushRegionResponse {
  required uint64 lastFlushTime = 1;
  optional bool flushed = 2;
}
{code}",gchanan,gchanan,Minor,Closed,Fixed,19/Sep/12 01:58,23/Sep/13 18:30
Bug,HBASE-6838,12608204,Regionserver may generate identical scanner name,"In current implementation of HRegionserver#addScanner, it may generate same scanner name, thus make scanner confusion.",zjushch,zjushch,Major,Closed,Fixed,19/Sep/12 08:17,23/Sep/13 18:31
Bug,HBASE-6839,12608206,Operations may be executed without holding rowLock,"HRegion#internalObtainRowLock will return null if timed out,
but many place which call this method don't handle this case

The bad result is operation will be executed even if it havn't obtained the row lock. Such as put、delete、increment。。。",zjushch,zjushch,Critical,Closed,Fixed,19/Sep/12 08:25,07/Apr/13 05:00
Bug,HBASE-6840,12608280,SplitLogManager should reassign tasks even on a clean RS shutdown.,"SplitLogManager does not reassign tasks if the regionserver does a clean
shutdown. We should reassign the task even if there is a clean shutdown.

This is a problem if the shutting down RS is the 3rd splitlog worker. Master
just sits there in a loop waiting for the task to finish, as the timeout
will not reassign the task any further.
Tue, Sep 18, 7:41 PM · D578411#test-plan
",amitanand,amitanand,Minor,Closed,Fixed,19/Sep/12 17:05,14/Jun/22 21:16
Bug,HBASE-6842,12608378,the jar used in  coprocessor is not deleted in local which will exhaust  the space of /tmp ,"FileSystem fs = path.getFileSystem(HBaseConfiguration.create());
      Path dst = new Path(System.getProperty(""java.io.tmpdir"") +
          java.io.File.separator +""."" + pathPrefix +
          ""."" + className + ""."" + System.currentTimeMillis() + "".jar"");
fs.copyToLocalFile(path, dst);
fs.deleteOnExit(dst);


change to 

File tmpLocal = new File(dst.toString());
tmpLocal.deleteOnExit();

            ",zhou wen jian,zhou wen jian,Critical,Closed,Fixed,20/Sep/12 04:01,07/Apr/13 05:01
Bug,HBASE-6843,12608380,loading lzo error when using coprocessor,"After applying HBASE-6308,we found error followed

2012-09-06 00:44:38,341 DEBUG org.apache.hadoop.hbase.coprocessor.CoprocessorClassLoader: Finding class: com.hadoop.compression.lzo.LzoCodec
2012-09-06 00:44:38,351 ERROR com.hadoop.compression.lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: Native Library /home/zhuzhuang/hbase/0.94.0-ali-1.0/lib/native/Linux-amd64-64/libgplcompression.so already loaded in another classloade
r
at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1772)
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1732)
at java.lang.Runtime.loadLibrary0(Runtime.java:823)
at java.lang.System.loadLibrary(System.java:1028)
at com.hadoop.compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java:32)
at com.hadoop.compression.lzo.LzoCodec.<clinit>(LzoCodec.java:67)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:113)
at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:107)
at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:243)
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:85)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:3793)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3782)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3732)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:332)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
2012-09-06 00:44:38,355 DEBUG org.apache.hadoop.hbase.coprocessor.CoprocessorClassLoader: Skipping exempt class java.io.PrintWriter - delegating directly to parent
2012-09-06 00:44:38,355 ERROR com.hadoop.compression.lzo.LzoCodec: Cannot load native-lzo without native-hadoop",zhou wen jian,zhou wen jian,Critical,Closed,Fixed,20/Sep/12 04:09,26/Feb/13 08:20
Bug,HBASE-6844,12608381,upgrade 0.23 version dependency in 0.94,"hadoop 0.23 has been promoted to stable. The snapshot jar no longer exists in maven.

https://repository.apache.org/content/repositories/releases/org/apache/hadoop/hadoop-common/0.23.3/",toffer,toffer,Major,Closed,Fixed,20/Sep/12 04:20,24/Oct/12 05:40
Bug,HBASE-6846,12608479,BitComparator bug - ArrayIndexOutOfBoundsException,"The HBase 0.94.1 BitComparator introduced a bug in the method ""compareTo"":
{code}
@Override
  public int compareTo(byte[] value, int offset, int length) {
    if (length != this.value.length) {
      return 1;
    }
    int b = 0;
    //Iterating backwards is faster because we can quit after one non-zero byte.
    for (int i = value.length - 1; i >= 0 && b == 0; i--) {
      switch (bitOperator) {
        case AND:
          b = (this.value[i] & value[i+offset]) & 0xff;
          break;
        case OR:
          b = (this.value[i] | value[i+offset]) & 0xff;
          break;
        case XOR:
          b = (this.value[i] ^ value[i+offset]) & 0xff;
          break;
      }
    }
    return b == 0 ? 1 : 0;
  }
{code}

I've encountered this problem when using a BitComparator with a configured this.value.length=8, and in the HBase table there were KeyValues with keyValue.getBuffer().length=207911 bytes. In this case:
{code}

    for (int i = 207910; i >= 0 && b == 0; i--) {
      switch (bitOperator) {
        case AND:
          b = (this.value[207910] ... ==> ArrayIndexOutOfBoundsException
          break;
{code}

That loop should use:
{code}
  for (int i = length - 1; i >= 0 && b == 0; i--) { (or this.value.length.)
{code}

Should I provide a patch for correcting the problem?",lucian.iordache,lucian.iordache,Major,Closed,Fixed,20/Sep/12 14:33,26/Feb/13 08:21
Bug,HBASE-6847,12608496,HBASE-6649 broke replication,"After running with HBASE-6646 and replication enabled I encountered this:

{noformat}
2012-09-17 20:04:08,111 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication va1r3s24%2C10304%2C1347911704238.1347911706318 at 78617132
2012-09-17 20:04:08,120 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Break on IOE: hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2C10304%2C1347911704238.1347911706318, entryStart=78641557, pos=78771200, end=78771200, edit=84
2012-09-17 20:04:08,120 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: currentNbOperations:164529 and seenEntries:84 and size: 154068
2012-09-17 20:04:08,120 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Replicating 84
2012-09-17 20:04:08,146 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Going to report log #va1r3s24%2C10304%2C1347911704238.1347911706318 for position 78771200 in hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2C10304%2C1347911704238.1347911706318
2012-09-17 20:04:08,158 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Removing 0 logs in the list: []
2012-09-17 20:04:08,158 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Replicated in total: 93234
2012-09-17 20:04:08,158 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication va1r3s24%2C10304%2C1347911704238.1347911706318 at 78771200
2012-09-17 20:04:08,163 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Unexpected exception in ReplicationSource, currentPath=hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2C10304%2C1347911704238.1347911706318
java.lang.IndexOutOfBoundsException
        at java.io.DataInputStream.readFully(DataInputStream.java:175)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2001)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1901)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1947)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:235)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.readAllEntriesToReplicateOrNextFile(ReplicationSource.java:394)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:307)
{noformat}

There's something weird at the end of the file and it's killing replication. We used to just retry.",ddas,jdcryans,Blocker,Closed,Fixed,20/Sep/12 17:25,07/Apr/13 05:02
Bug,HBASE-6851,12608547,Race condition in TableAuthManager.updateGlobalCache(),"When new global permissions are assigned, there is a race condition, during which further authorization checks relying on global permissions may fail.

In TableAuthManager.updateGlobalCache(), we have:
{code:java}
    USER_CACHE.clear();
    GROUP_CACHE.clear();
    try {
      initGlobal(conf);
    } catch (IOException e) {
      // Never happens
      LOG.error(""Error occured while updating the user cache"", e);
    }
    for (Map.Entry<String,TablePermission> entry : userPerms.entries()) {
      if (AccessControlLists.isGroupPrincipal(entry.getKey())) {
        GROUP_CACHE.put(AccessControlLists.getGroupName(entry.getKey()),
                        new Permission(entry.getValue().getActions()));
      } else {
        USER_CACHE.put(entry.getKey(), new Permission(entry.getValue().getActions()));
      }
    }
{code}

If authorization checks come in following the .clear() but before repopulating, they will fail.

We should have some synchronization here to serialize multiple updates and use a COW type rebuild and reassign of the new maps.

This particular issue crept in with the fix in HBASE-6157, so I'm flagging for 0.94 and 0.96.",ghelmling,ghelmling,Critical,Closed,Fixed,20/Sep/12 23:03,07/Apr/13 05:02
Bug,HBASE-6853,12608605,IllegalArgumentException is thrown when an empty region is splitted,"This is w.r.t a mail sent in the dev mail list.

Empty region split should be handled gracefully.  Either we should not allow the split to happen if we know that the region is empty or we should allow the split to happen by setting the no of threads to the thread pool executor as 1.
{code}
int nbFiles = hstoreFilesToSplit.size();
ThreadFactoryBuilder builder = new ThreadFactoryBuilder();
    builder.setNameFormat(""StoreFileSplitter-%1$d"");
    ThreadFactory factory = builder.build();
    ThreadPoolExecutor threadPool =
      (ThreadPoolExecutor) Executors.newFixedThreadPool(nbFiles, factory);
    List<Future<Void>> futures = new ArrayList<Future<Void>>(nbFiles);

{code}
Here the nbFiles needs to be a non zero positive value.

 ",priyadarshini,ram_krish,Major,Closed,Fixed,21/Sep/12 09:56,07/Apr/13 05:03
Bug,HBASE-6854,12608625,Deletion of SPLITTING node on split rollback should clear the region from RIT,"If a failure happens in split before OFFLINING_PARENT, we tend to rollback the split including deleting the znodes created.
On deletion of the RS_ZK_SPLITTING node we are getting a callback but not remvoving from RIT. We need to remove it from RIT, anyway SSH logic is well guarded in case the delete event comes due to RS down scenario.",ram_krish,ram_krish,Major,Closed,Fixed,21/Sep/12 13:25,09/Nov/12 23:44
Bug,HBASE-6858,12608693,Fix the incorrect BADVERSION checking in the recoverable zookeeper,Thanks for Stack and Kaka's reporting that there is a bug in the recoverable zookeeper when handling BADVERSION exception for setData(). It shall compare the ID payload of the data in zk with its own identifier.,liyin,liyin,Critical,Closed,Fixed,21/Sep/12 20:56,23/Sep/13 18:31
Bug,HBASE-6868,12608728,Skip checksum is broke; are we double-checksumming by default?,"The HFile contains checksums for decrease the iops, so when Hbase read HFile , that dont't need to read the checksum from meta file of HDFS.  But HLog file of Hbase don't contain the checksum, so when HBase read the HLog, that must read checksum from meta file of HDFS.  We could  add setSkipChecksum per file to hdfs or we could write checksums into WAL if this skip checksum facility is enabled ",larsh,liulei.cn,Blocker,Closed,Fixed,22/Sep/12 03:20,07/Apr/13 05:03
Bug,HBASE-6869,12608769,Update our hadoop-2 to 2.0.1-alpha,,stack,stack,Major,Closed,Fixed,23/Sep/12 00:37,23/Sep/13 18:30
Bug,HBASE-6871,12608848,HFileBlockIndex Write Error in HFile V2 due to incorrect split into intermediate index blocks,"After writing some data, compaction and scan operation both failure, the exception message is below:
2012-09-18 06:32:26,227 ERROR org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest: Compaction failed regionName=hfile_test,,1347778722498.d220df43fb9d8af4633bd7f547613f9e., storeName=page_info, fileCount=7, fileSize=1.3m (188.0k, 188.0k, 188.0k, 188.0k, 188.0k, 185.8k, 223.3k), priority=9, time=45826250816757428java.io.IOException: Could not reseek StoreFileScanner[HFileScanner for reader reader=hdfs://hadoopdev1.cm6:9000/hbase/hfile_test/d220df43fb9d8af4633bd7f547613f9e/page_info/b0f6118f58de47ad9d87cac438ee0895, compression=lzo, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false], firstKey=http://com.truereligionbrandjeans.www/Womens_Dresses/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/4010.html/page_info:anchor_sig/1347764439449/DeleteColumn, lastKey=http://com.trura.www//page_info:page_type/1347763395089/Put, avgKeyLen=776, avgValueLen=4, entries=12853, length=228611, cur=http://com.truereligionbrandjeans.www/Womens_Exclusive_Details/pl/c/4970.html/page_info:is_deleted/1347764003865/Put/vlen=1/ts=0] to key http://com.truereligionbrandjeans.www/Womens_Exclusive_Details/pl/c/4970.html/page_info:is_deleted/OLDEST_TIMESTAMP/Minimum/vlen=0/ts=0
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(StoreFileScanner.java:178)        
        at org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.doRealSeek(NonLazyKeyValueScanner.java:54)        
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:299)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:244)        
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:521)        
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:402)
        at org.apache.hadoop.hbase.regionserver.Store.compactStore(Store.java:1570)        
        at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:997)        
        at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1216)
        at org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.run(CompactionRequest.java:250)        
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)        
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Expected block type LEAF_INDEX, but got INTERMEDIATE_INDEX: blockType=INTERMEDIATE_INDEX, onDiskSizeWithoutHeader=8514, uncompressedSizeWithoutHeader=131837, prevBlockOffset=-1, dataBeginsWith=\x00\x00\x00\x9B\x00\x00\x00\x00\x00\x00\x03#\x00\x00\x050\x00\x00\x08\xB7\x00\x00\x0Cr\x00\x00\x0F\xFA\x00\x00\x120, fileOffset=218942        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.validateBlockType(HFileReaderV2.java:378)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:331)        at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.seekToDataBlock(HFileBlockIndex.java:213)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:455)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.reseekTo(HFileReaderV2.java:493)        
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseekAtOrAfter(StoreFileScanner.java:242)        
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(StoreFileScanner.java:167)

After some debug works，I found that when hfile closing, if the rootChunk is empty, the only one curInlineChunk will upgrade to root chunk. But if the last block flushing make curInlineChunk exceed max index block size, the root chunk(upgrade from curInlineChunk) will be splited into intermediate index blocks, and the index level is set to 2. So when BlockIndexReader read the root index, it expects the next level index block is leaf index(index level=2), but the on disk index block is intermediate block, the error happened. 

After I add some code to check curInlineChunk's size when rootChunk is empty in shouldWriteBlock(boolean closing), this bug can be fixed.

",mikhail,feng wang,Critical,Closed,Fixed,24/Sep/12 09:59,07/Apr/13 05:04
Bug,HBASE-6879,12609098,Add HBase Code Template,Add a standard code template to do along with the code formatter for HBase. This helps make sure people have the correct license and general commenting for auto-generated elements.,jesse_yates,jesse_yates,Major,Closed,Fixed,25/Sep/12 18:31,23/Sep/13 18:31
Bug,HBASE-6881,12609109,All regionservers are marked offline even there is still one up,"{noformat}
+        RegionPlan newPlan = plan;
+        if (!regionAlreadyInTransitionException) {
+          // Force a new plan and reassign. Will return null if no servers.
+          newPlan = getRegionPlan(state, plan.getDestination(), true);
+        }
+        if (newPlan == null) {
           this.timeoutMonitor.setAllRegionServersOffline(true);
           LOG.warn(""Unable to find a viable location to assign region "" +
             state.getRegion().getRegionNameAsString());
{noformat}

Here, when newPlan is null, plan.getDestination() could be up actually.

",jxiang,jxiang,Major,Closed,Fixed,25/Sep/12 19:37,23/Sep/13 18:30
Bug,HBASE-6888,12609309,HBase scripts ignore any HBASE_OPTS set in the environment,"hbase-env.sh which is sourced by hbase-config.sh which is eventually sourced by the main 'hbase' script defines HBASE_OPTS form scratch, ignoring any previous value set in the environment.

This prevents from passing additional JVM parameters to HBase programs (shell, hbck, etc) launched through these scripts.",adityakishore,adityakishore,Minor,Closed,Fixed,26/Sep/12 21:37,07/Apr/13 05:04
Bug,HBASE-6889,12609321,Ignore source control files with apache-rat,"Running 'mvn apache-rat:check' locally causes a failure because it finds the source control files, making it hard to check that you didn't include a file without a source header.",jesse_yates,jesse_yates,Major,Closed,Fixed,26/Sep/12 23:11,07/Apr/13 05:05
Bug,HBASE-6894,12609489,Adding metadata to a table in the shell is both arcane and painful,"In production we have hundreds of tables w/ whack names like 'aliaserv', 'ashish_bulk', 'age_gender_topics', etc.  It be grand if you could look in master UI and see stuff like owner, eng group responsible, miscellaneous description, etc.

Now, HTD has support for this; each carries a dictionary.  Whats a PITA though is adding attributes to the dictionary.  Here is what seems to work on trunk (though I do not trust it is doing the right thing):

{code}
hbase> create 'SOME_TABLENAME', {NAME => 'd', VERSION => 1, COMPRESSION => 'LZO'}
hbase> # Here is how I added metadata
hbase> disable 'SOME_TABLENAME'
hbase> alter 'SOME_TABLENAME', METHOD => 'table_att', OWNER => 'SOMEON', CONFIG => {'ENVIRONMENT' => 'BLAH BLAH', 'SIZING' => 'The size should be between 0-10K most of the time with new URLs coming in and getting removed as they are processed unless the pipeline has fallen behind', 'MISCELLANEOUS' => 'Holds the list of URLs waiting to be processed in the parked page detection analyzer in ingestion pipeline.'}
...
describe...
enable...
{code}

The above doesn't work in 0.94.  Complains about the CONFIG, the keyword we are using for the HTD dictionary.

It works in 0.96 though I'd have to poke around some more to ensure it is doing the right thing.

But this METHOD => 'table_att' stuff is really ugly.... can we fix it?

And I can't add table attributes on table create seemingly.

A little bit of thought and a bit of ruby could clean this all up.
",sershe,stack,Major,Closed,Fixed,28/Sep/12 05:14,05/Aug/14 20:11
Bug,HBASE-6896,12609568,sync bulk and regular assigment handling socket timeout exception,"In regular assignment, in case of socket network timeout, it tries to call openRegion again and again without change the region plan, ZK offline node,
till the region is out of transition, in case the region server is still up.

We may need to sync them up and make sure bulk assignment does the same in this case.",jxiang,jxiang,Minor,Closed,Fixed,28/Sep/12 17:43,14/Jun/22 21:24
Bug,HBASE-6900,12609643,RegionScanner.reseek() creates NPE when a flush or compaction happens before the reseek.,"HBASE-5520 introduced reseek() on the RegionScanner.  
Now when a scanner is created we have the StoreScanner heap.  After this if a flush or compaction happens parallely all the StoreScannerObservers are cleared so that whenever a new next() call happens we tend to recreate the scanner based on the latest store files.
The reseek() in StoreScanner expects the heap not to be null because always reseek would be called from next()
{code}
public synchronized boolean reseek(KeyValue kv) throws IOException {
    //Heap cannot be null, because this is only called from next() which
    //guarantees that heap will never be null before this call.
    if (explicitColumnQuery && lazySeekEnabledGlobally) {
      return heap.requestSeek(kv, true, useRowColBloom);
    } else {
      return heap.reseek(kv);
    }
  }
{code}
Now when we call RegionScanner.reseek() directly using CPs we tend to get a NPE.  In our case it happened when a major compaction was going on.  I will also attach a testcase to show the problem.

",ram_krish,ram_krish,Major,Closed,Fixed,29/Sep/12 11:26,07/Apr/13 05:05
Bug,HBASE-6901,12609667,Store file compactSelection throws ArrayIndexOutOfBoundsException,"When setting <hbase.mapreduce.hfileoutputformat.compaction.exclude> to true, and run compaction to exclude bulk loaded files could cause ArrayIndexOutOfBoundsException since all files are excluded.",jxiang,jxiang,Major,Closed,Fixed,29/Sep/12 19:54,07/Apr/13 05:06
Bug,HBASE-6904,12609715,"In the HBase shell, an error is thrown that states replication-related znodes already exist","On a replication-enabled cluster, querying the list_peers produces the error lines shown below. It doesn't appear that anything is broken in terms of functionality.

Stack trace:

hbase(main):001:0> list_peers
12/09/29 14:41:03 ERROR zookeeper.RecoverableZooKeeper: Node /hbase/replication/peers already exists and this is not a retry
12/09/29 14:41:03 ERROR zookeeper.RecoverableZooKeeper: Node /hbase/replication/rs already exists and this is not a retry
PEER ID CLUSTER KEY
0 row(s) in 0.4650 seconds
",gchanan,aleksshulman,Minor,Closed,Fixed,01/Oct/12 05:57,26/Feb/13 08:20
Bug,HBASE-6906,12609769,TestHBaseFsck#testQuarantine* tests are flakey due to TableNotEnabledException,"This test fails periodically (1 out of 10) times on our internal jenkins instance.

{code}
FAILED TESTS
================================================
1 tests failed.
REGRESSION: org.apache.hadoop.hbase.util.TestHBaseFsck.testQuarantineMissingRegionDir
Error Message:
org.apache.hadoop.hbase.TableNotEnabledException: testQuarantineMissingRegionDir at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75) at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1170) at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)
Stack Trace:
org.apache.hadoop.hbase.TableNotEnabledException: org.apache.hadoop.hbase.TableNotEnabledException: testQuarantineMissingRegionDir
at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75)
at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1170)
at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
at org.apache.hadoop.hbase.client.HBaseAdmin.disableTableAsync(HBaseAdmin.java:766)
at org.apache.hadoop.hbase.util.TestHBaseFsck.deleteTable(TestHBaseFsck.java:344)
at org.apache.hadoop.hbase.util.TestHBaseFsck.doQuarantineTest(TestHBaseFsck.java:1351)
at org.apache.hadoop.hbase.util.TestHBaseFsck.testQuarantineMissingRegionDir(TestHBaseFsck.java:1433)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hbase.TableNotEnabledException): org.apache.hadoop.hbase.TableNotEnabledException: testQuarantineMissingRegionDir
at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75)
at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1170)
at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)
at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:918)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
at $Proxy25.disableTable(Unknown Source)
at org.apache.hadoop.hbase.client.HBaseAdmin.disableTableAsync(HBaseAdmin.java:764)
... 12 more
{code}",jmhsieh,jmhsieh,Major,Closed,Fixed,01/Oct/12 16:40,07/Apr/13 05:06
Bug,HBASE-6907,12609777,KeyValue equals and compareTo methods should match,"KeyValue.KVComparator includes the memstoreTS when comparing, however the KeyValue.equals() method ignores the memstoreTS.

The Comparator interface has always specified that comparator return 0 when equals would return true and vice versa.  Obeying that rule has been sort of optional in the past, but Java 7 introduces a new default collection sorting algorithm called Tim Sort which relies on that behavior.  http://bugs.sun.com/view_bug.do?bug_id=6804124

Possible problem spots:
* there's a Collections.sort(KeyValues) in RedundantKVGenerator.generateTestKeyValues(..)
* TestColumnSeeking compares two collections of KeyValues using the containsAll method.  It is intentionally ignoring memstoreTS, so will need an alternative method for comparing the two collections.
",yuzhihong@gmail.com,mcorgan,Major,Closed,Fixed,01/Oct/12 17:36,23/Sep/13 18:31
Bug,HBASE-6912,12609839,Filters are not properly applied in certain cases,"Steps to reproduce:

Create a table, load data into it. Flush the table.

Do a scan with
1. Some filter which should not match the first entry in the scan
2. Where one specifies a family and column.
You will notice that the first entry is returned even though it doesn't match the filter.

It looks like the when the first KeyValue of a scan in the column from the point of view of the code

HRegion.java
{code}
} else if (kv != null && !kv.isInternal() && filterRowKey(currentRow)) {
{code}
Is generated by
{code}
public static KeyValue createLastOnRow(final byte [] row,
final int roffset, final int rlength, final byte [] family,
final int foffset, final int flength, final byte [] qualifier,
final int qoffset, final int qlength) { return new KeyValue(row, roffset, rlength, family, foffset, flength, qualifier, qoffset, qlength, HConstants.OLDEST_TIMESTAMP, Type.Minimum, null, 0, 0); }
{code}
So it is always internal from that point of the code.

Only later from within
StoreScanner.java
{code}
public synchronized boolean next(List<KeyValue> outResult, int limit, String metric) throws IOException {
....
LOOP: while((kv = this.heap.peek()) != null) {
{code}
( The second time through)

Do we get the actual kv, with a proper type and timestamp. This seems to mess with filtering.
",larsh,posix4e,Major,Closed,Fixed,01/Oct/12 23:40,07/Apr/13 05:07
Bug,HBASE-6915,12609846,String and ConcurrentHashMap sizes change on jdk7; makes TestHeapSize fail,,stack,stack,Major,Closed,Fixed,02/Oct/12 00:30,23/Sep/13 18:30
Bug,HBASE-6916,12609852,HBA logs at info level errors that won't show in the shell,"There is a weird interaction between the shell and HBA. When you try to close a region that doesn't exist, it doesn't throw any error:

{noformat}
hbase(main):029:0> close_region 'thisisaninvalidregion'
0 row(s) in 0.0580 seconds
{noformat}

Normally one should get UnknownRegionException. Starting the shell with ""-d"" I see what a non-shell user would see along with a ton of logging from ZK (skipped here):

{noformat}
INFO client.HBaseAdmin: No server in .META. for thisisaninvalidregion; pair=null
{noformat}

But again this is not the right message, it should have shown

{noformat}
INFO client.HBaseAdmin: No server in .META. for thisisaninvalidregion; pair=null
{noformat}

And this is because that part of the code treats both UnknownRegionException and NoServerForRegionException like if it was the same thing.

There is also some ugliness in flush, compact, and split but it normally doesn't show since the code treats everything like it's a table and sends a TableNotFoundException.

This jira is about making sure that the exceptions are correctly coming out.",jdcryans,jdcryans,Minor,Closed,Fixed,02/Oct/12 01:35,07/Apr/13 05:07
Bug,HBASE-6918,12609871,Debugging to help figure what is different up on jenkins when TestHeapSize runs,"TestHeapSize will pass locally then fail up on jenkins.  Add debugging to the test that prints detail on environment and jvm in particular to can figure what is different between two environments (it used to be that the d32 flag was needed to mimic jenkins but now we are jdk7, the d32 flag doesn't work w/ my oracle jvm... let me see what jenkins has).",stack,stack,Major,Closed,Fixed,02/Oct/12 06:09,23/Sep/13 18:31
Bug,HBASE-6920,12609939,"On timeout connecting to master, client can get stuck and never make progress","HBASE-5058 appears to have introduced an issue where a timeout in HConnection.getMaster() can cause the client to never be able to connect to the master.  So, for example, an HBaseAdmin object can never successfully be initialized.

The issue is here:
{code}
if (tryMaster.isMasterRunning()) {
  this.master = tryMaster;
  this.masterLock.notifyAll();
  break;
}
{code}

If isMasterRunning times out, it throws an UndeclaredThrowableException, which is already not ideal, because it can be returned to the application.

 But if the first call to getMaster succeeds, it will set masterChecked = true, which makes us never try to reconnect; that is, we will set this.master = null and just throw MasterNotRunningExceptions, without even trying to connect.

I tried out a 94 client (actually a 92 client with some 94 patches) on a cluster with some network issues, and it would constantly get stuck as described above.",gchanan,gchanan,Critical,Closed,Fixed,02/Oct/12 17:06,05/Nov/12 01:35
Bug,HBASE-6926,12609983,Cleanup some of the javadoc warnings.,,stack,stack,Major,Closed,Fixed,02/Oct/12 20:42,23/Sep/13 18:30
Bug,HBASE-6927,12609993,WrongFS using HRegionInfo.getTableDesc() and different fs for hbase.root and fs.defaultFS,"Calling HRegionInfo.getTableDesc() with different fs schema for hbase.root and fs.defaultFS raises ""IllegalArgumentException: Wrong FS"" exception.

HRegionInfo.getTableDesc() is called only by bin/region_mover.rb to get the table name and can be easily replaced, getTableDesc() is also deprecated.

The main problem is that getTableDesc() doesn't replace fs.defaultFS with hbase.root as all the other hbase code (all the code does this, except getTableDesc)
{code}
Configuration c = HBaseConfiguration.create();
c.set(""fs.defaultFS"", c.get(HConstants.HBASE_DIR));
c.set(""fs.default.name"", c.get(HConstants.HBASE_DIR));
{code}",mbertozzi,mbertozzi,Major,Closed,Fixed,02/Oct/12 21:42,07/Apr/13 05:08
Bug,HBASE-6946,12610350,JavaDoc missing from release tarballs,,larsh,larsh,Major,Closed,Fixed,04/Oct/12 16:26,24/Oct/12 05:40
Bug,HBASE-6948,12610371,shell create table script cannot handle split key which is expressed in raw bytes,,tychang,zhihyu@ebaysf.com,Major,Closed,Fixed,04/Oct/12 18:47,23/Sep/13 18:30
Bug,HBASE-6949,12610372,Automatically delete empty directories in CleanerChore,"Currently the CleanerChore asks cleaner delegates if both directories and files should be deleted. However, this leads to somewhat odd behavior in some delegates - you don't actually care if the directory hierarchy is preserved, the files; this means you always will delete directories and then implement the logic you actually want for preserving files. Instead we can handle this logic one layer higher in the CleanerChore and let the delegates just worry about preserving files.",jesse_yates,jesse_yates,Major,Closed,Fixed,04/Oct/12 18:52,23/Sep/13 18:30
Bug,HBASE-6950,12610402,TestAcidGuarantees system test now flushes too aggressively,"HBASE-6552 caused the TestAcidGuarantees system test to flush more aggressively, because flushes are where ACID problems have occurred in the past.

After some more cluster testing, it seems like this too aggressive; my clusters eventually can't keep up with the number of flushes/compactions and start getting SocketTimeoutExceptions.  We could try to optimize the flushes/compactions, but since this workload would never occur in practice, I don't think it is worth the effort.  Instead, let's just only flush once a minute.  This is arbitrary, but seems to work.

Here is my comment in the (upcoming) patch:
{code}
// Flushing has been a source of ACID violations previously (see HBASE-2856), so ideally,
// we would flush as often as possible.  On a running cluster, this isn't practical:
// (1) we will cause a lot of load due to all the flushing and compacting
// (2) we cannot change the flushing/compacting related Configuration options to try to
// alleviate this
// (3) it is an unrealistic workload, since no one would actually flush that often.
// Therefore, let's flush every minute to have more flushes than usual, but not overload
// the running cluster.
{code}",gchanan,gchanan,Minor,Closed,Fixed,04/Oct/12 21:22,23/Sep/13 18:30
Bug,HBASE-6953,12610410,Incorrect javadoc description of HFileOutputFormat regarding multiple column families,"The javadoc for HFileOutputFormat states that the class does not support writing multiple column families; however, this hasn't been the case since HBASE-1861 was resolved.",,gabriel.reid,Minor,Closed,Fixed,04/Oct/12 21:38,23/Sep/13 18:30
Bug,HBASE-6958,12610679,TestAssignmentManager sometimes fails,"From https://builds.apache.org/job/HBase-TRUNK/3432/testReport/junit/org.apache.hadoop.hbase.master/TestAssignmentManager/testBalanceOnMasterFailoverScenarioWithOpenedNode/ :
{code}
Stacktrace

java.lang.Exception: test timed out after 5000 milliseconds
	at java.lang.System.arraycopy(Native Method)
	at java.lang.ThreadGroup.remove(ThreadGroup.java:969)
	at java.lang.ThreadGroup.threadTerminated(ThreadGroup.java:942)
	at java.lang.Thread.exit(Thread.java:732)
...
2012-10-06 00:46:12,521 DEBUG [MASTER_CLOSE_REGION-mockedAMExecutor-0] zookeeper.ZKUtil(1141): mockedServer-0x13a33892de7000e Retrieved 81 byte(s) of data from znode /hbase/unassigned/dc01abf9cd7fd0ea256af4df02811640 and set watcher; region=t,,1349484359011.dc01abf9cd7fd0ea256af4df02811640., state=M_ZK_REGION_OFFLINE, servername=master,1,1, createTime=1349484372509, payload.length=0
2012-10-06 00:46:12,522 ERROR [MASTER_CLOSE_REGION-mockedAMExecutor-0] executor.EventHandler(205): Caught throwable while processing event RS_ZK_REGION_CLOSED
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.TestAssignmentManager$MockedLoadBalancer.randomAssignment(TestAssignmentManager.java:773)
	at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:1709)
	at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:1666)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1435)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1155)
	at org.apache.hadoop.hbase.master.TestAssignmentManager$AssignmentManagerWithExtrasForTesting.assign(TestAssignmentManager.java:1035)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1130)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1125)
	at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:106)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:202)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
2012-10-06 00:46:12,522 DEBUG [pool-1-thread-1-EventThread] master.AssignmentManager(670): Handling transition=M_ZK_REGION_OFFLINE, server=master,1,1, region=dc01abf9cd7fd0ea256af4df02811640, current state from region state map ={t,,1349484359011.dc01abf9cd7fd0ea256af4df02811640. state=OFFLINE, ts=1349484372508, server=null}
{code}
Looks like NPE happened on this line:
{code}
      this.gate.set(true);
{code}",jxiang,yuzhihong@gmail.com,Major,Closed,Fixed,06/Oct/12 02:08,26/Feb/13 16:36
Bug,HBASE-6961,12610912,In place scripts fail if mvn install hasn't been run,bin/hbase tries to get dependencies of the project however it fails if mvn install hasn't already satisfied hbase-hadoop-compat test-jar.,eclark,eclark,Major,Closed,Fixed,09/Oct/12 00:20,14/Jun/22 22:07
Bug,HBASE-6962,12610994,Upgrade hadoop 1 dependency to hadoop 1.1,,yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,09/Oct/12 14:52,23/Sep/13 18:30
Bug,HBASE-6963,12611047,unable to run hbck on a secure cluster,"{noformat}
12/10/02 11:49:07 WARN util.HBaseFsck: Got AccessControlException when preCheckPermission 
org.apache.hadoop.security.AccessControlException: Permission denied: action=WRITE path=hdfs://...:8020/hbase/-ROOT- user=hbase/...
	at org.apache.hadoop.hbase.util.FSUtils.checkAccess(FSUtils.java:882)
	at org.apache.hadoop.hbase.util.HBaseFsck.preCheckPermission(HBaseFsck.java:1230)
	at org.apache.hadoop.hbase.util.HBaseFsck.exec(HBaseFsck.java:3343)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3205)
{noformat}",jxiang,jxiang,Major,Closed,Fixed,09/Oct/12 19:47,23/Sep/13 18:30
Bug,HBASE-6969,12611107,Allow for injecting MiniZookeeperCluster instance,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the MiniZookeeperCluster when executing tests.

Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4).  Or if you want to control which specific port it starts on vs random.

Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class  ",,mkwhitacre,Major,Closed,Fixed,10/Oct/12 02:34,14/Jun/22 22:00
Bug,HBASE-6971,12611117,.META. directory does not contain .tableinfo serialization,"This is not very critical, but I've noticed that we do not serialize the HTableDescriptor to <hbase.root.dir>/.META./.tableinfo, although we do it for ROOT. 

MasterFileSystem.createRootTableInfo() should probably have an META equivalent.  ",,enis,Major,Closed,Fixed,10/Oct/12 04:16,05/Aug/14 20:11
Bug,HBASE-6972,12611250,HBase Shell deleteall should not require column to be defined ,"It appears that the shell does not allow users to delete a row without specifying a column (deleteall). It looks like the deleteall.rb used to pre-define column as nil, making it optional. 

I've created a patch and confirmed it to be working in standalone mode, I will upload it shortly. ",rickysaltzer,rickysaltzer,Major,Closed,Fixed,10/Oct/12 21:40,23/Sep/13 18:30
Bug,HBASE-6974,12611278,Metric for blocked updates,"When the disc subsystem cannot keep up with a sustained high write load, a region will eventually block updates to throttle clients.
(HRegion.checkResources).

It would be nice to have a metric for this, so that these occurrences can be tracked.",mdrzal,larsh,Critical,Closed,Fixed,11/Oct/12 00:34,23/Nov/14 01:37
Bug,HBASE-6978,12611406,Minor typo in ReplicationSource SocketTimeoutException error handling,"The user gets an error message on socket timeout exception:

The words ""the"" and ""call"" need a space between them. Fix is trivial.

2012-10-11 11:09:06,154 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Encountered a SocketTimeoutException. Since thecall to the remote cluster timed out, which is usually caused by a machine failure or a massive slowdown, sleeping 1000 times 100

The error is present in .92 and onward all the way through to the trunk. Fix should go into all those branches.
",aleksshulman,aleksshulman,Trivial,Closed,Fixed,11/Oct/12 18:16,26/Feb/13 08:20
Bug,HBASE-6989,12611618,"Avoid unneceseary regular expression matching in ruby code for ""list"" command","HBaseAdmin already has an API which can accept a regular expression string and return the matching tables.

The list command in HBase shell should make use of this and should not pull the entire table list and do a matching of its own",adityakishore,adityakishore,Minor,Closed,Fixed,12/Oct/12 19:10,23/Sep/13 18:30
Bug,HBASE-6991,12611683,"Escape ""\"" in Bytes.toStringBinary() and its counterpart Bytes.toBytesBinary()","Since ""\"" is used to escape non-printable character but not treated as special character in conversion, it could lead to unexpected conversion.

For example, please consider the following code snippet.

{code}
public void testConversion() {
  byte[] original = {
      '\\', 'x', 'A', 'D'
  };
  String stringFromBytes = Bytes.toStringBinary(original);
  byte[] converted = Bytes.toBytesBinary(stringFromBytes);
  System.out.println(""Original: "" + Arrays.toString(original));
  System.out.println(""Converted: "" + Arrays.toString(converted));
  System.out.println(""Reversible?: "" + (Bytes.compareTo(original, converted) == 0));
}

Output:
-------
Original: [92, 120, 65, 68]
Converted: [-83]
Reversible?: false
{code}

The ""\"" character needs to be treated as special and must be encoded as a non-printable character (""\x5C"") to avoid any kind of ambiguity during conversion.",adityakishore,adityakishore,Major,Closed,Fixed,13/Oct/12 10:29,23/Sep/13 18:30
Bug,HBASE-6994,12611780,minor doc update about DEFAULT_ACCEPTABLE_FACTOR,"Per trunk code, in LruBlockCache.java:
static final float DEFAULT_ACCEPTABLE_FACTOR = 0.99f;

but the site doc still :
""number of region servers * heap size * hfile.block.cache.size * 0.85""

seems the HBASE-6312 forgot to update this doc:)",xieliang007,xieliang007,Minor,Closed,Fixed,15/Oct/12 03:48,06/Apr/18 17:54
Bug,HBASE-6998,12612021,Uncaught exception in main() makes the HMaster/HRegionServer process suspend,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)",xieliang007,xieliang007,Major,Closed,Fixed,16/Oct/12 12:04,23/Sep/13 18:31
Bug,HBASE-7000,12612170,"Fix the ""INT_VACUOUS_COMPARISON"" WARNING in KeyValue class",,xieliang007,xieliang007,Minor,Closed,Fixed,17/Oct/12 08:36,23/Sep/13 18:31
Bug,HBASE-7001,12612204,Fix the RCN Correctness Warning in MemStoreFlusher class,"https://builds.apache.org/job/PreCommit-HBASE-Build/3057//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html#Warnings_CORRECTNESS
shows :
	
Bug type RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE (click for details)
In class org.apache.hadoop.hbase.regionserver.MemStoreFlusher
In method org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher$FlushRegionEntry)
Value loaded from region
Return value of org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.access$000(MemStoreFlusher$FlushRegionEntry)
At MemStoreFlusher.java:[line 346]
Redundant null check at MemStoreFlusher.java:[line 363]",xieliang007,xieliang007,Minor,Closed,Fixed,17/Oct/12 11:46,23/Sep/13 18:30
Bug,HBASE-7002,12612219,Fix all 4 findbug performance warnings,Fix the perf warning from this report : https://builds.apache.org/job/PreCommit-HBASE-Build/3057//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html#Warnings_PERFORMANCE,xieliang007,xieliang007,Minor,Closed,Fixed,17/Oct/12 13:06,23/Sep/13 18:31
Bug,HBASE-7005,12612299,Upgrade Thrift lib to 0.9.0,,jfarrell,jfarrell,Minor,Closed,Fixed,17/Oct/12 20:04,23/Sep/13 18:30
Bug,HBASE-7008,12612371,"Set scanner caching to a better default, disable Nagles","per http://search-hadoop.com/m/qaRu9iM2f02/Set+scanner+caching+to+a+better+default%253F&subj=Set+scanner+caching+to+a+better+default+
let's set to 100 by default",xieliang007,xieliang007,Major,Closed,Fixed,18/Oct/12 06:15,15/Oct/13 04:46
Bug,HBASE-7017,12612758,"Backport ""[replication] The replication-executor should make sure the file that it is replicating is closed before declaring success on that file"" to 0.94",,ddas,ddas,Major,Closed,Fixed,19/Oct/12 19:08,03/Dec/12 21:47
Bug,HBASE-7018,12612786,Fix and Improve TableDescriptor caching for bulk assignment,"HBASE-6214 backported HBASE-5998 (Bulk assignment: regionserver optimization by using a temporary cache for table descriptors when receiving an open regions request), but it's buggy on 0.94 (0.96 appears correct):

{code}
    HTableDescriptor htd = null;
    if (htds == null) {
      htd = this.tableDescriptors.get(region.getTableName());
    } else {
      htd = htds.get(region.getTableNameAsString());
      if (htd == null) {
        htd = this.tableDescriptors.get(region.getTableName());
        htds.put(region.getRegionNameAsString(), htd);
      }
    }
{code}

i.e. we get the tableName from the map but write the regionName.

Even fixing this, it looks like there are areas for improvement:
1) FSTableDescriptors already has a cache (though it goes to the NameNode each time through to check we have the latest copy.  May as well combine these two caches, might be a performance win as well since we don't need to write to multiple caches.
2) FSTableDescriptors makes two RPCs to the NameNode when it encounters a new table.  So the total number of RPCs necessary for a bulk assign (without caching is):
#regions + #tables
(with caching):
min(#regions,#tables) + #tables = #tables + #tables = 2 * #tables

We can make this only one RPC, yielding:
#tables

Probably not a big deal for most users, but in a multi-tenant situation where the number of regions being bulk assigned approaches the number of tables being bulk assigned, this could be a nice performance win.

Benchmarks coming.",gchanan,gchanan,Major,Closed,Fixed,19/Oct/12 21:34,26/Feb/13 08:20
Bug,HBASE-7019,12612803,Can't pass SplitAlgo in hbase shell,"{noformat}
hbase(main):002:0> create 't1', 'f1', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}

ERROR: uninitialized constant Hbase::Admin::RegionSplitter
{noformat}",gchanan,gchanan,Major,Closed,Fixed,20/Oct/12 00:42,23/Sep/13 18:31
Bug,HBASE-7021,12612808,Default to Hadoop 1.0.4 in 0.94 and add Hadoop 1.1 profile,"Hadoop 1.0.4 was released, we should default to that.",larsh,larsh,Major,Closed,Fixed,20/Oct/12 01:22,03/Dec/12 21:47
Bug,HBASE-7034,12613097,"Bad version, failed OPENING to OPENED but master thinks it is open anyways","I have this in RS log:

{code}
2012-10-22 02:21:50,698 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed transitioning node b9,\xEE\xAE\x9BiQO\x89]+a\xE0\x7F\xB7'X?,1349052737638.9af7cfc9b15910a0b3d714bf40a3248f. from OPENING to OPENED -- closing region
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f
{code}

Master says this (it is bulk assigning):

{code}
....
2012-10-22 02:21:40,673 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:10302-0xb3a862e57a503ba Set watcher on existing znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f
...

then this
....

2012-10-22 02:23:47,089 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:10302-0xb3a862e57a503ba Set watcher on existing znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f
....
2012-10-22 02:24:34,176 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:10302-0xb3a862e57a503ba Retrieved 112 byte(s) of data from znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f and set watcher; region=b9,\xEE\xAE\x9BiQO\x89]+a\xE0\x7F\xB7'X?,1349052737638.9af7cfc9b15910a0b3d714bf40a3248f., origin=sv4r17s44,10304,1350872216778, state=RS_ZK_REGION_OPENED

etc.
{code}

Disagreement as to what is going on here.
",anoopsamjohn,stack,Major,Closed,Fixed,23/Oct/12 06:37,26/Feb/13 08:27
Bug,HBASE-7037,12613176,ReplicationPeer logs at WARN level aborting server instead of at FATAL,,xieliang007,stack,Major,Closed,Fixed,23/Oct/12 18:37,05/Aug/14 20:11
Bug,HBASE-7046,12613377,Fix resource leak in TestHLogSplit#testOldRecoveredEditsFileSidelined,This method creates a writer but never closes one.,v.himanshu,v.himanshu,Major,Closed,Fixed,24/Oct/12 22:06,23/Sep/13 18:31
Bug,HBASE-7048,12613400,Regionsplitter requires the hadoop config path to be in hbase classpath,"When hadoop config path isn't included in hbase classpath, you will get the following:
{code}
Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: hdfs://t3.e.com/hbase/usertable/_balancedSplit, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:454)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:67)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:431)
at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:301)
at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1005)
at org.apache.hadoop.hbase.util.RegionSplitter.getSplits(RegionSplitter.java:643)
at org.apache.hadoop.hbase.util.RegionSplitter.rollingSplit(RegionSplitter.java:367)
at org.apache.hadoop.hbase.util.RegionSplitter.main(RegionSplitter.java:295)
{code}",,yuzhihong@gmail.com,Major,Closed,Fixed,24/Oct/12 23:18,26/Feb/13 08:20
Bug,HBASE-7051,12613427,CheckAndPut should properly read MVCC,"See, for example:
{code}
// TODO: Use MVCC to make this set of increments atomic to reads
{code}

Here's an example of what I can happen (would probably be good to write up a test case for each read/update):
Concurrent update via increment and put.

The put grabs the row lock first and updates the memstore, but releases the row lock before the MVCC is advanced.  Then, the increment grabs the row lock and reads right away, reading the old value and incrementing based on that.

There are a few options here:
1) Waiting for the MVCC to advance for read/updates: the downside is that you have to wait for updates on other rows.

2) Have an MVCC per-row (table configuration): this avoids the unnecessary contention of 1)

3) Transform the read/updates to write-only with rollup on read..  E.g. an increment  would just have the number of values to increment.",larsh,gchanan,Major,Closed,Fixed,25/Oct/12 03:27,26/Feb/13 08:20
Bug,HBASE-7060,12613933,Region load balancing by table does not handle the case where a table's region count is lower than the number of the RS in the cluster,"When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.

This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:

avg-regions-per-RS=0.5
min-RS-per-RS=0
max-RS-per-RS=1

when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always >=min, so it will always skip, therefore, no underloaded RS are found.

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}

Later the function returns since underloaded RS size is 0

if (serverUnerloaded ==0) return regionsToReturn;
",zhihyu@ebaysf.com,tychang,Major,Closed,Fixed,29/Oct/12 18:29,26/Feb/13 08:21
Bug,HBASE-7061,12613934,"region balance algorithm should do ""contiue"" instead of ""break"" when calculating the underloaded server","I found this issue when investigating HBASE-7060. Basically, I think  the intention of this code below is to find all the underloaded server. By using break, it will exit earlier, depending on where the non-overloaded server show up in the list.  ""break"" should be changed to ""continue"". 

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}",tychang,tychang,Major,Closed,Fixed,29/Oct/12 18:35,14/Jun/22 22:28
Bug,HBASE-7069,12613991,HTable.batch does not have to be synchronized,"This was raised on the mailing list by Yousuf.
HTable.batch(...) is synchronized and there appears to be no reason for it.

(flushCommits makes the same call to connection.processBatch and it also is not synchronized)

This is pretty bad actually marking critical.

0.96 is fine BTW.",larsh,larsh,Critical,Closed,Fixed,30/Oct/12 05:07,03/Dec/12 21:47
Bug,HBASE-7070,12614004,Scanner may retry forever  after HBASE-5974,"After the patch of HBASE-5974
Suppose The process is as the following:
1.A next request is very large, so first time it is failed because of timeout
2.The second time it is failed again because of CallSequenceOutOfOrderException
3.We will retry this next request again after reset scanner, so it is also very large and failed again because of timeout
4.CallSequenceOutOfOrderException again
5.Repeated the above forever...

See the comment in HBASE-5974 for more
https://issues.apache.org/jira/browse/HBASE-5974?focusedCommentId=13486658&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13486658",zjushch,zjushch,Major,Closed,Fixed,30/Oct/12 08:18,15/Oct/14 14:24
Bug,HBASE-7072,12614065,HBase-5256 breaks 0.92-0.94 compatibility,"HBase-5286 changes RegionLoad writable in 94.0, making it incompatible with 92. A fix was made in HBase-5795 where a 94 client can read response from a 92 server, but not vice versa. Currently, if a 92 client tries to do read RegionLoad (HBase shell ""status"" command, or, 92 master and 94 regionserver), it just hangs. ",v.himanshu,v.himanshu,Major,Closed,Fixed,30/Oct/12 18:44,14/Jun/22 22:37
Bug,HBASE-7083,12614296,SSH#fixupDaughter should force re-assign missing daughter,"In looking into flaky test TestSplitTransactionOnCluster#testShutdownSimpleFixup, I found out that a missing daughter is not assigned by SSH properly.  It could be open on the dead server.  We need to force re-assign it.",jxiang,jxiang,Minor,Closed,Fixed,01/Nov/12 00:00,23/Sep/13 18:31
Bug,HBASE-7086,12614539,Enhance ResourceChecker to log stack trace for potentially hanging threads,"Currently ResourceChecker logs a line similar to the following if it detects potential thread leak:
{code}
2012-11-02 10:18:59,299 INFO  [main] hbase.ResourceChecker(157): after master.cleaner.TestHFileCleaner#testTTLCleaner: 44 threads (was 43), 145 file descriptors (was 145). 0 connections,  -thread leak?-
{code}
We should enhance the log to include stack trace of the potentially hanging thread(s)

This work was motivated when I investigated test failure in HBASE-6796",zhihyu@ebaysf.com,zhihyu@ebaysf.com,Major,Closed,Fixed,02/Nov/12 17:23,26/Feb/13 08:20
Bug,HBASE-7091,12614619,support custom GC options in hbase-env.sh,"When running things like bin/start-hbase and bin/hbase-daemon.sh start [master|regionserver|etc] we end up setting HBASE_OPTS property a couple times via calling hbase-env.sh. This is generally not a problem for most cases, but when you want to set your own GC log properties, one would think you should set HBASE_GC_OPTS, which get added to HBASE_OPTS. 

NOPE! That would make too much sense.

Running bin/hbase-daemons.sh will run bin/hbase-daemon.sh with the daemons it needs to start. Each time through hbase-daemon.sh we also call bin/hbase. This isn't a big deal except for each call to hbase-daemon.sh, we also source hbase-env.sh twice (once in the script and once in bin/hbase). This is important for my next point.

Note that to turn on GC logging, you uncomment:

{code}
# export HBASE_OPTS=""$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps $HBASE_GC_OPTS"" 
{code}

and then to log to a gc file for each server, you then uncomment:
{code}
# export HBASE_USE_GC_LOGFILE=true
{code}

in hbase-env.sh

On the first pass through hbase-daemon.sh, HBASE_GC_OPTS isn't set, so HBASE_OPTS doesn't get anything funky, but we set HBASE_USE_GC_LOGFILE, which then sets HBASE_GC_OPTS to the log file (-Xloggc:...). Then in bin/hbase we again run hbase-env.sh, which now hs HBASE_GC_OPTS set, adding the GC file. 

This isn't a general problem because HBASE_OPTS is set without prefixing the existing HBASE_OPTS (eg. HBASE_OPTS=""$HBASE_OPTS ...""), allowing easy updating. However, GC OPTS don't work the same and this is really odd behavior when you want to set your own GC opts, which can include turning on GC log rolling (yes, yes, they really are jvm opts, but they ought to support their own param, to help minimize clutter).

The simple version of this patch will just add an idempotent GC option to hbase-env.sh and some comments that uncommenting 

{code}
# export HBASE_USE_GC_LOGFILE=true
{code}

will lead to a custom gc log file per server (along with an example name), so you don't need to set ""-Xloggc"".

The more complex solution does the above and also solves the multiple calls to hbase-env.sh so we can be sane about how all this works. Note that to fix this, hbase-daemon.sh just needs to read in HBASE_USE_GC_LOGFILE after sourcing hbase-env.sh and then update HBASE_OPTS. Oh and also not source hbase-env.sh in bin/hbase. 

Even further, we might want to consider adding options just for cases where we don't need gc logging - i.e. the shell, the config reading tool, hcbk, etc. This is the hardest version to handle since the first couple will willy-nilly apply the gc options.",jesse_yates,jesse_yates,Major,Closed,Fixed,03/Nov/12 00:01,05/Aug/14 20:12
Bug,HBASE-7095,12614690,Cannot set 'lenAsVal' for KeyOnlyFilter from shell,"Current implementation of createFilterFromArguments() in KeyOnlyFilter rejects the Boolean argument, effectively preventing from specifying this option from HBase shell.",adityakishore,adityakishore,Minor,Closed,Fixed,04/Nov/12 10:00,26/Feb/13 08:20
Bug,HBASE-7098,12614843,Fix minor typos and formatting issues in HFileArchiver/HFileLink,There are a few minor typographical issues. I have a patch in place for these.,aleksshulman,aleksshulman,Minor,Closed,Fixed,05/Nov/12 19:35,23/Sep/13 18:30
Bug,HBASE-7103,12614929,Need to fail split if SPLIT znode is deleted even before the split is completed.,"This came up after the following mail in dev list
'infinite loop of RS_ZK_REGION_SPLIT on .94.2'.
The following is the reason for the problem
The following steps happen
-> Initially the parent region P1 starts splitting.
-> The split is going on normally.
-> Another split starts at the same time for the same region P1. (Not sure why this started).
-> Rollback happens seeing an already existing node.
-> This node gets deleted in rollback and nodeDeleted Event starts.
-> In nodeDeleted event the RIT for the region P1 gets deleted.
-> Because of this there is no region in RIT.
-> Now the first split gets over.  Here the problem is we try to transit the node to SPLITTING to SPLIT. But the node even does not exist.
But we don take any action on this.  We think it is successful.
-> Because of this SplitRegionHandler never gets invoked.",ram_krish,ram_krish,Major,Closed,Fixed,06/Nov/12 10:09,26/Feb/13 08:20
Bug,HBASE-7104,12614939,HBase includes multiple versions of netty: 3.5.0; 3.2.4; 3.2.2,"We've got 3 of them on trunk.

[INFO] org.apache.hbase:hbase-server:jar:0.95-SNAPSHOT
[INFO] +- io.netty:netty:jar:3.5.0.Final:compile
[INFO] +- org.apache.zookeeper:zookeeper:jar:3.4.3:compile
[INFO] |  \- org.jboss.netty:netty:jar:3.2.2.Final:compile

[INFO] org.apache.hbase:hbase-hadoop2-compat:jar:0.95-SNAPSHOT
[INFO] +- org.apache.hadoop:hadoop-client:jar:2.0.2-alpha:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.0.2-alpha:compile
[INFO] |  |  \- org.jboss.netty:netty:jar:3.2.4.Final:compile


The patch attached:
- fixes this for hadoop 1 profile
- bump the netty version to 3.5.9
- does not fix it for hadoop 2. I don't know why, but I haven't investigate: as it's still alpha may be they will change the version on hadoop side anyway.

Tests are ok.

I haven't really investigated the differences between netty 3.2 and 3.5. A quick search seems to say it's ok, but don't hesitate to raise a warning...",nkeywal,nkeywal,Minor,Closed,Fixed,06/Nov/12 11:33,23/Sep/13 18:31
Bug,HBASE-7111,12615067,hbase zkcli will not start if the zookeeper server chosen to connect to is unavailable,"there are 3 zookeeper servers in my cluster.
s1
s2
s3
after killing  s3, i found the hbase zkcli will not start again.
it will try to connect to s3 continuely. 

/11/07 11:01:01 INFO zookeeper.ClientCnxn: Opening socket connection to server s3
12/11/07 11:01:01 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused


from the code 
{code}
  public String parse(final Configuration c) {
    // Note that we do not simply grab the property
    // HConstants.ZOOKEEPER_QUORUM from the HBaseConfiguration because the
    // user may be using a zoo.cfg file.
    Properties zkProps = ZKConfig.makeZKProps(c);
    String host = null;
    String clientPort = null;
    for (Entry<Object, Object> entry: zkProps.entrySet()) {
      String key = entry.getKey().toString().trim();
      String value = entry.getValue().toString().trim();
      if (key.startsWith(""server."") && host == null) {
        String[] parts = value.split("":"");
        host = parts[0];
      } else if (key.endsWith(""clientPort"")) {
        clientPort = value;
      }
      if (host != null && clientPort != null) break;
    }
    return host != null && clientPort != null? host + "":"" + clientPort: null;
  }
{code}

the code will choose the fixed zookeeper server (here is the unavailable s3), which leads to the script fails",zhou wen jian,zhou wen jian,Major,Closed,Fixed,07/Nov/12 03:08,07/May/13 03:51
Bug,HBASE-7114,12615206,Increment does not extend Mutation but probably should,"Increment is the only operation in the class of mutators that does not extend Mutation. It mostly duplicates what Mutation provides, but not quite. The signatures for setWriteToWAL and getFamilyMap are slightly different. This can be inconvenient because it requires special case code and therefore could be considered an API design nit. Unfortunately it is not a simple change: The interface is marked stable and the internals of the family map are different from other mutation types. The latter is why I suspect this was not addressed when Mutation was introduced.",stack,apurtell,Critical,Closed,Fixed,07/Nov/12 18:19,23/Sep/13 18:30
Bug,HBASE-7122,12615260,Proper warning message when opening a log file with no entries (idle cluster),"In case the cluster is idle and the log has rolled (offset to 0), replicationSource tries to open the log and gets an EOF exception. This gets printed after every 10 sec until an entry is inserted in it.
{code}
2012-11-07 15:47:40,924 DEBUG regionserver.ReplicationSource (ReplicationSource.java:openReader(487)) - Opening log for replication c0315.hal.cloudera.com%2C40020%2C1352324202860.1352327804874 at 0
2012-11-07 15:47:40,926 WARN  regionserver.ReplicationSource (ReplicationSource.java:openReader(543)) - 1 Got: 
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readFully(DataInputStream.java:152)
	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1508)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1486)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1470)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:55)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:175)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:716)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.openReader(ReplicationSource.java:491)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:290)
2012-11-07 15:47:40,927 WARN  regionserver.ReplicationSource (ReplicationSource.java:openReader(547)) - Waited too long for this file, considering dumping
2012-11-07 15:47:40,927 DEBUG regionserver.ReplicationSource (ReplicationSource.java:sleepForRetries(562)) - Unable to open a reader, sleeping 1000 times 10

{code}
We should reduce the log spewing in this case (or some informative message, based on the offset).",v.himanshu,v.himanshu,Major,Closed,Fixed,08/Nov/12 00:20,04/Jun/13 00:06
Bug,HBASE-7124,12615272,"typo in pom.xml with ""exlude"", no definition of ""test.exclude.pattern""","There is a typo in pom.xml with ""exlude"", and there is no definition of ""test.exclude.pattern"".",michelle,michelle,Minor,Closed,Fixed,08/Nov/12 02:33,26/Mar/13 11:55
Bug,HBASE-7130,12615376,NULL qualifier is ignored,HBASE-6206 ignored NULL qualifier so the qualifier list could be empty. But the request converter skips empty qualifier list too.,jxiang,jxiang,Major,Closed,Fixed,08/Nov/12 20:35,23/Sep/13 18:31
Bug,HBASE-7134,12615415,incrementColumnValue hooks no longer called from anywhere,"incrementColumnValue has been removed from RegionServer, the corresponding coprocessor hooks for this operation are no longer called.",apurtell,apurtell,Major,Closed,Fixed,09/Nov/12 02:29,23/Sep/13 18:30
Bug,HBASE-7143,12615575,TestMetaMigrationRemovingHTD fails when used with Hadoop 0.23/2.x,"TestMetaMigrationRemovingHTD fails when build is done with ""-Dhadoop.profile=23"" option. 

The reason is the changes of defaults in ""-mkdir"" CLI call. In 0.23/2.x, it doesn't create parent directories by default anymore.

The patch will be submitted shortly.",aklochkov,aklochkov,Major,Closed,Fixed,10/Nov/12 01:05,26/Feb/13 08:20
Bug,HBASE-7148,12615752,Some files in hbase-examples module miss license header,"Trunk build 3530 got to building hbase-examples module but failed:
{code}
[INFO] HBase - Examples .................................. FAILURE [3.222s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 29:21.569s
[INFO] Finished at: Sun Nov 11 15:17:35 UTC 2012
[INFO] Final Memory: 68M/642M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.8:check (default) on project hbase-examples: Too many unapproved licenses: 20 -> [Help 1]
{code}
Looks like license headers are missing in some of the files in hbase-examples module",enis,yuzhihong@gmail.com,Major,Closed,Fixed,12/Nov/12 17:04,23/Sep/13 18:30
Bug,HBASE-7153,12615842,print gc option in hbase-env.sh affects hbase zkcli,"I un-commented the -verbose:gc option in hbase-env.sh, which print out the gc info.
but when I use hbase zkcli to check zk, it can not connect to the server.
the problem is zkcli uses ""hbase org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServerArg"" to get the server_arg in the script hbase. when gc verbose option is open, the output of ZooKeeperMainServerArg is with gc info, which masses up with server_arg. and this is the reason stop zkcli working.
I think the easiest way to fix this is to trim the gc info out of server_arg in the hbase script.",davelatham,wonderyl,Major,Closed,Fixed,13/Nov/12 02:16,05/Apr/13 01:00
Bug,HBASE-7158,12615980,Allow CopyTable to identify the source cluster (for replication scenarios),"When I worked on HBASE-2195 I added a mechanism for an edit to identify its source cluster, so that replication would not bounce it back to the source.
See: {{this.clusterId = zkHelper.getUUIDForCluster(zkHelper.getZookeeperWatcher());}} in ReplicationSource, and {{put.setClusterId(entry.getKey().getClusterId());}} in ReplicationSink.

In master-master replication scenarios, it would very useful if CopyTable would identify the source cluster (by tagging each Put/Delete with the source clusterId before applying it).",jdcryans,larsh,Major,Closed,Fixed,13/Nov/12 23:49,26/Feb/13 08:23
Bug,HBASE-7159,12615983,Upgrade zookeeper dependency to 3.4.5,"zookeeper 3.4.5 works with Oracle JDK 1.7
We should upgrade to zookeeper 3.4.5 in trunk",stack,yuzhihong@gmail.com,Major,Closed,Fixed,14/Nov/12 00:24,23/Sep/13 18:30
Bug,HBASE-7165,12616174,TestSplitLogManager.testUnassignedTimeout is flaky,"This is the most frequently failing test in the recent 0.94 jenkins builds.
Looking at the code it seem the timeouts are a bit too aggressive (there's only a 500ms window for the test to detect that a ZK was removed). I think we should just double the timeouts.",larsh,larsh,Major,Closed,Fixed,15/Nov/12 01:04,26/Feb/13 08:23
Bug,HBASE-7166,12616186,TestSplitTransactionOnCluster tests are flaky,"There's a variety of tests in this class that fail occasionally.
I think this is caused by incorrect waiting for the split to finish.

The local split method in the test does not wait until both daughters are online, and in some tests there's an assert following immediately that the two daughters exist.
",larsh,larsh,Major,Closed,Fixed,15/Nov/12 04:39,26/Feb/13 08:23
Bug,HBASE-7167,12616260,Thrift's deleteMultiple() raises exception instead of returning list of failed deletes,"Thrift API claims deleteMultiple() returns the list of failed Deletes, but the current implementation throws a TIOError instead.",dferro,dferro,Major,Closed,Fixed,15/Nov/12 16:38,23/Sep/13 19:08
Bug,HBASE-7168,12616297,"[dev] in the script called 'hbase', we don't check for errors when generating the classpath with mvn","When it happens, it's difficult to guess. Let's fix this.",nkeywal,nkeywal,Minor,Closed,Fixed,15/Nov/12 20:04,23/Sep/13 18:31
Bug,HBASE-7172,12616344,TestSplitLogManager.testVanishingTaskZNode() fails when run individually and is flaky,"TestSplitLogManager.testVanishingTaskZNode fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. 

The reason is a rare race condition, which somehow does not happen that much when the whole class is run.

The sequence of events is smt like this:
 - we create 1 log file to split
 - we call splitLogDistributed() in its own thread. 
 - splitLogDistributed() is waiting in waitForSplittingCompletion() since there are no splitlogworkers, it keep waiting.
 - we delete the task znode from zk
 - SplitLogManager receives the zk callback from GetDataAsyncCallback, which will call setDone() and mark the task as success. 
 - However, meanwhile the waitForSplittingCompletion() loops sees that remainingInZK == 0, and calls return concurrently to the above. 
 - on return from waitForSplittingCompletion(), splitLogDistributed() fails because the znode delete callback has not completed yet. 

This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",enis,enis,Major,Closed,Fixed,16/Nov/12 00:38,26/Feb/13 08:23
Bug,HBASE-7177,12616355,TestZooKeeperScanPolicyObserver.testScanPolicyObserver is flaky,"Yes, this is my own test :(

Looking at the point where it fails frequently, it looks like the problem is just that the ZK does not hit the (asynchronous) ZKWatcher in the PolicyObserver in time.
",larsh,larsh,Major,Closed,Fixed,16/Nov/12 04:38,26/Feb/13 08:23
Bug,HBASE-7180,12616561,RegionScannerImpl.next() is inefficient.,"We just came across a special scenario.

For our Phoenix project (SQL runtime for HBase), we push a lot of work into HBase via coprocessors. One method is to wrap RegionScanner in coprocessor hooks and then do processing in the hook to avoid returning a lot of data to the client unnecessarily.

In this specific case this is pretty bad. Since the wrapped RegionScanner's next() does not ""know"" that it is called this way is still does all of this on each invocation:
# Starts a RegionOperation
# Increments the request count
# set the current read point on a thread local (because generally each call could come from a different thread)
# Finally does the next on its StoreScanner(s)
# Ends the RegionOperation

When this is done in a tight loop millions of times (as is the case for us) it starts to become significant.

Not sure what to do about this, really. Opening this issue for discussion.

One way is to extend the RegionScanner with an ""internal"" next() method of sorts, so that all this overhead can be avoided. The coprocessor could call the regular next() methods once and then just call the cheaper internal version.

Are there better/cleaner ways?
",larsh,larsh,Major,Closed,Fixed,18/Nov/12 06:39,26/Feb/13 08:22
Bug,HBASE-7192,12616872,Move TestHBase7051.java into TestAtomicOperation.java,"This test class only has one test function and would be better served by being located in TestAtomic.java, since anyone looking for atomicity-related tests would know to look there.

",aleksshulman,aleksshulman,Minor,Closed,Fixed,20/Nov/12 01:29,23/Sep/13 19:08
Bug,HBASE-7198,12617029,fix lastSeqId logic in regionserver,,amitanand,amitanand,Minor,Closed,Fixed,21/Nov/12 00:02,15/Jun/22 17:28
Bug,HBASE-7202,12617126,Family Store Files are not archived on admin.deleteColumn(),"using HBaseAdmin.deleteColumn() the files are not archived but deleted directory.

This causes problems with snapshots, and other systems that relies on files to be archived.",mbertozzi,mbertozzi,Major,Closed,Fixed,21/Nov/12 16:10,23/Sep/13 18:31
Bug,HBASE-7203,12617217,Move example CoProcessor into hbase-examples,Move the example co-processor into the hbase-examples module.  Also move the protobuf definition files into the module.,eclark,eclark,Major,Closed,Fixed,21/Nov/12 21:28,23/Sep/13 18:30
Bug,HBASE-7205,12617239,Coprocessor classloader is replicated for all regions in the HRegionServer,"HBASE-6308 introduced a new custom CoprocessorClassLoader to load the coprocessor classes and a new instance of this CL is created for each single HRegion opened. This leads to OOME-PermGen when the number of regions go above hundres / region server. 
Having the table coprocessor jailed in a separate classloader is good however we should create only one for all regions of a table in each HRS.
",zhihyu@ebaysf.com,amuraru,Critical,Closed,Fixed,21/Nov/12 23:41,14/Jan/15 18:58
Bug,HBASE-7210,12617346,Backport HBASE-6059 to 0.94,"HBASE-6059 seems to be an important issue.  Chunhui has already given a patch for 94. Need to rebase if it does not apply cleanly.
Raising a new one as the old issue is already closed.",yuzhihong@gmail.com,ram_krish,Major,Closed,Fixed,22/Nov/12 14:20,04/Jun/13 00:07
Bug,HBASE-7211,12617354,Improve hbase ref guide for the testing part.,"Here is some stuff I saw. I will propose a fix in a week or so, please add the comment or issues you have in mind.

??15.6.1. Apache HBase Modules??
=> We should be able to use categories in all modules. The default should be small; but any test manipulating the time needs to be in a specific jvm (hence medium), so it's not always related to minicluster.

??15.6.3.6. hbasetests.sh??
=> We can remove this chapter, and the script
 The script is not totally useless, but I think nobody actually uses it.

=> Add a chapter on flakiness.
Some tests are, unfortunately, flaky. While there number decreases, we still have some. Rules are:
- don't write flaky tests! :-)
- small tests cannot be flaky, as it blocks other test execution. Corollary: if you have an issue with a small test, it's either your environment either a severe issue.
- rerun the test a few time to validate, check the ports and file descriptors used.

??mvn test -P localTests -Dtest=MyTest??
=> We could actually activate the localTests profile whenever -Dtest is used. If we do that, we can remove the reference from localTests in the doc.

??mvn test -P runSmallTests?? ??mvn test -P runMediumTests??
=> I'm not sure it's actually used. We could remove them from the pom.xml (and the doc).

??The HBase build uses a patched version of the maven surefire plugin?? 
=> Hopefully, we will be able to remove this soon :-)


 ??Integration tests are described TODO: POINTER_TO_INTEGRATION_TEST_SECTION??
 => Should be documented",nkeywal,nkeywal,Minor,Closed,Fixed,22/Nov/12 15:17,04/Apr/18 23:05
Bug,HBASE-7214,12617480,"CleanerChore logs too much, so much so it obscures all else that is going on","Testing 0.94.3RC0, I see loads of this in logs:

{code}
2012-11-23 13:39:40,488 DEBUG org.apache.hadoop.hbase.master.cleaner.CleanerChore: file:/tmp/hbase-stack/hbase/.oldlogs/192.168.1.73%2C61033%2C1353705880078.1353706219757 is not deletable according t#
 14 2012-11-23 13:39:40,488 DEBUG org.apache.hadoop.hbase.master.cleaner.CleanerChore: file:/tmp/hbase-stack/hbase/.oldlogs/192.168.1.73%2C61033%2C1353705880078.1353706221210 is not deletable according t#
 1
{code}

There is too much of it.  Can we cut back some?",jesse_yates,stack,Major,Closed,Fixed,23/Nov/12 21:41,26/Feb/13 08:23
Bug,HBASE-7215,12617554,"Put, Delete, Increment, Result, all all HBase M/R classes still implement/use Writable","Making blocker as suggested by Stack.

At least the following still use Put/Delete as writables.

* IdentityTableReduce.java
* MultiPut.java
* HRegionServer.checkAndMutate
",larsh,larsh,Blocker,Closed,Fixed,26/Nov/12 02:33,23/Sep/13 18:31
Bug,HBASE-7220,12617666,Creating a table with 3000 regions on 2 nodes fails after 1 hour,"I'm trying to create a table with 3000 regions on two regions servers, from the shell.

It's ok on trunk a standalone config.
It's ok on 0.94
It's not ok on trunk: it fails after around 1 hour.

If I remove all the code related to metrics in HRegion, the 3000 regions are created in 3 minutes (twice faster than the 0.94).

On trunk, the region server spends its time in ""waitForWork"", while the master is in the tcp connection related code. It's a 1Gb network.

I haven't looked at the metric code itself.

Patch used to remove the metrics from HRegion:
{noformat}
index c70e9ab..6677e65 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -364,7 +364,7 @@ public class HRegion implements HeapSize { // , Writable{
   private HTableDescriptor htableDescriptor = null;
   private RegionSplitPolicy splitPolicy;
 
-  private final MetricsRegion metricsRegion;
+  private final MetricsRegion metricsRegion = null;
 
   /**
    * Should only be used for testing purposes
@@ -388,7 +388,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.coprocessorHost = null;
     this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
 
-    this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
+    //this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
   }
 
   /**
@@ -451,7 +451,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.regiondir = getRegionDir(this.tableDir, encodedNameStr);
     this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
 
-    this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
+    //this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
 
     /*
      * timestamp.slop provides a server-side constraint on the timestamp. This
@@ -1024,7 +1024,7 @@ public class HRegion implements HeapSize { // , Writable{
         status.setStatus(""Running coprocessor post-close hooks"");
         this.coprocessorHost.postClose(abort);
       }
-      this.metricsRegion.close();
+      //this.metricsRegion.close();
       status.markComplete(""Closed"");
       LOG.info(""Closed "" + this);
       return result;
@@ -2331,11 +2331,11 @@ public class HRegion implements HeapSize { // , Writable{
       if (noOfPuts > 0) {
         // There were some Puts in the batch.
         double noOfMutations = noOfPuts + noOfDeletes;
-        this.metricsRegion.updatePut();
+        //this.metricsRegion.updatePut();
       }
       if (noOfDeletes > 0) {
         // There were some Deletes in the batch.
-        this.metricsRegion.updateDelete();
+        //this.metricsRegion.updateDelete();
       }
       if (!success) {
         for (int i = firstIndex; i < lastIndexExclusive; i++) {
@@ -4270,7 +4270,7 @@ public class HRegion implements HeapSize { // , Writable{
 
     // do after lock
 
-    this.metricsRegion.updateGet();
+    //this.metricsRegion.updateGet();
 
     return results;
   }
@@ -4657,7 +4657,7 @@ public class HRegion implements HeapSize { // , Writable{
       closeRegionOperation();
     }
 
-    this.metricsRegion.updateAppend();
+    //this.metricsRegion.updateAppend();
 
 
     if (flush) {
@@ -4795,7 +4795,7 @@ public class HRegion implements HeapSize { // , Writable{
         mvcc.completeMemstoreInsert(w);
       }
       closeRegionOperation();
-      this.metricsRegion.updateIncrement();
+      //this.metricsRegion.updateIncrement();
     }
 
     if (flush) {
{noformat}",eclark,nkeywal,Major,Closed,Fixed,26/Nov/12 18:55,15/Jun/22 00:25
Bug,HBASE-7226,12617932,"HRegion.checkAndMutate uses incorrect comparison result for <, <=, > and >=","in HRegion.checkAndMutate, incorrect comparison results are used for <, <=, > and >=, as below:

          switch (compareOp) {
          case LESS:
            matches = compareResult <= 0;  // should be '<' here
            break;
          case LESS_OR_EQUAL:
            matches = compareResult < 0;  // should be '<=' here
            break;
          case EQUAL:
            matches = compareResult == 0;
            break;
          case NOT_EQUAL:
            matches = compareResult != 0;
            break;
          case GREATER_OR_EQUAL:
            matches = compareResult > 0;  // should be '>=' here
            break;
          case GREATER:
            matches = compareResult >= 0;  // should be '>' here
            break;",fenghh,fenghh,Major,Closed,Fixed,28/Nov/12 05:45,21/Jan/14 01:31
Bug,HBASE-7229,12618031,"ClassFinder finds compat tests during mvn package, causing TestCheckTestClasses to fail",Caused by HBASE-7109,sershe,sershe,Major,Closed,Fixed,28/Nov/12 18:28,23/Sep/13 18:31
Bug,HBASE-7230,12618033,port HBASE-7109 integration tests on cluster are not getting picked up from distribution to 0.94,,sershe,sershe,Major,Closed,Fixed,28/Nov/12 18:34,13/Jan/13 06:28
Bug,HBASE-7232,12618053,Remove HbaseMapWritable,Its used by hfile fileinfo only so need to convert fileinfo to remove this.,stack,stack,Major,Closed,Fixed,28/Nov/12 20:17,23/Sep/13 18:31
Bug,HBASE-7234,12618062,Remove long-deprecated HServerAddress and HServerInfo Writables,These classes have been deprecated since 0.92 or before.  Remove them.  Remove them too because they are Writable.,stack,stack,Blocker,Closed,Fixed,28/Nov/12 21:04,23/Sep/13 18:30
Bug,HBASE-7235,12618068,TestMasterObserver is flaky,"TestMasterObserver failed on windows builds at least a couple of times, although this is not windows specific. 

Upon further inspection, it seems that we are affected by the race condition for the table operations.
The handling of modify table occurs in its dedicated executor, MASTER_TABLE_OPERATIONS, and happens in async, although the handling of add/delete/modify columns are handled in the IPC handler thread. When we do not wait for the first modify table to finish, subsequent changes from the add/modify column operations are overriden by the earlier modify table executor.

{code}
at org.apache.hadoop.hbase.master.handler.TableEventHandler.hasColumnFamily(TableEventHandler.java:182)
	at org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler.<init>(TableDeleteFamilyHandler.java:42)
	at org.apache.hadoop.hbase.master.HMaster.deleteColumn(HMaster.java:1255)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1400)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:96)
	at org.apache.hadoop.hbase.client.HBaseAdmin.deleteColumn(HBaseAdmin.java:1024)
	at org.apache.hadoop.hbase.coprocessor.TestMasterObserver.testTableOperations(TestMasterObserver.java:616)
{code}",enis,enis,Major,Closed,Fixed,28/Nov/12 21:58,26/Feb/13 08:22
Bug,HBASE-7243,12618261,Test for creating a large number of regions,"After HBASE-7220, I think it will be good to write a unit test/IT to create a large number of regions. We can put a reasonable timeout to the test. ",ndimiduk,enis,Major,Closed,Fixed,30/Nov/12 02:13,05/Aug/14 20:11
Bug,HBASE-7251,12618430,Avoid flood logs during client disconnect during batch get operation,"Background:

A smart guy in the company want to read data from the HBASE in batch, the code like the following:(just demonstrate, not runnable):

	List<Get> gets = new ArrayList<Get>();

	for(int i =0; i < n; ++i){
		gets.add(""some row key here"");

		if (i % 10000 == 0){
			Results[] results = htable.get(gets);
			//process results here.  so delete some code
		}
	}



Yes, you know that, this guy forgot ""gets.clear()""  after each ""htable.get()"" operation in his code.

One region server becomes very slow, and crashed after 30mins becauseof OOM, but we got 15GB log file.
there are flood logs as following:

ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
org.apache.hadoop.hbase.ipc.CallerDisconnectedException: Aborting call multi(org.apache.hadoop.hbase.client.MultiAction@49540d8d), rpc version=1, client version=29, methodsFingerPrint=-56040613 from 10.1.1.1:57933 after 3980 ms, since caller disconnected
        at org.apache.hadoop.hbase.ipc.HBaseServer$Call.throwExceptionIfCallerDisconnected(HBaseServer.java:436)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3468)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3425)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3449)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:4198)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:4171)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1993)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3410)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1409)

Fix:
Server is stop ""get"" but cannot exit from the ""for"" loop, so write flood logs here.
My patch just log one exception log instead of flood logs. 
Importantly, server stop processing immediately if client timeout or disconnect.

Test:
I used this guy's wrong code read data( NO ""gets.clear()"" ) from the HBASE, when it becomes very slow to get results, I pressed ctrl+C, then there is only ONE CallerDisconnectedException exception log and the server stop reading immediately, LOG generate the last log entry:

 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server handler 1 on 60020 caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null


",,azuryy,Major,Closed,Fixed,01/Dec/12 14:05,15/Jan/13 01:57
Bug,HBASE-7252,12618451,TestSizeBasedThrottler fails occasionally,"Every now and then TestSizeBasedThrottler fails reaching the test internal timeouts.
I think the timeouts (200ms) are too short for the Jenkins machines.

On my (reasonably fast) machine I get timeout reliably when I set the timeout to 50ms, and occasionally at 100ms.

Let's double the timeout from 200ms to 400ms.",larsh,larsh,Major,Closed,Fixed,01/Dec/12 20:08,13/Jan/13 06:28
Bug,HBASE-7255,12618484,KV size metric went missing from StoreScanner.,"In trunk due to the metric refactor, at least the KV size metric went missing.
See this code in StoreScanner.java:
{code}
    } finally {
      if (cumulativeMetric > 0 && metric != null) {

      }
    }
{code}
Just an empty if statement, where the metric used to be collected.",eclark,larsh,Critical,Closed,Fixed,02/Dec/12 06:56,23/Sep/13 19:08
Bug,HBASE-7256,12618486,"Quick Start Guide shows stable version as 0.95, in the stable folder it is 0.94","In the Quick Start Guide for HBase - http://hbase.apache.org/book/quickstart.html
The stable version is mentioned as - 0.95 in the line -
""Choose a download site from this list of Apache Download Mirrors. Click on the suggested top link. This will take you to a mirror of HBase Releases. Click on the folder named stable and then download the file that ends in .tar.gz to your local filesystem; e.g. hbase-0.95-SNAPSHOT.tar.gz.""

But in the download folder at - http://apache.techartifact.com/mirror/hbase/stable/
The version that can be found is -  hbase-0.94.2-security.tar.gz   i.e. 0.94
So either the documentation or the download folder needs to be updated.",stack,spawgi,Minor,Closed,Fixed,02/Dec/12 11:05,23/Sep/13 18:30
Bug,HBASE-7258,12618523,Hbase needs to create baseZNode recursively,"In deploy env, multi small hbase clusters may share a same zk cluster. So, for hbase cluster1, its parent znode is /hbase/cluster1. But in hbase version 0.94.1, hbase use ZKUtil.createAndFailSilent(this, baseZNode) to create parent path and it will throw a NoNode exception if znode /hbase donot exist.
We want to change it to ZKUtil.createWithParents(this, baseZNode); to suport create baseZNode recursivly. 

The NoNode exception is:

java.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster
        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:1792)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:146)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:103)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:77)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1806)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/cluster1
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:778)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:420)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:402)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:905)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.createBaseZNodes(ZooKeeperWatcher.java:166)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:159)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:282)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.H",liushaohui,liushaohui,Minor,Closed,Fixed,03/Dec/12 03:28,23/Sep/13 18:30
Bug,HBASE-7259,12618536,Deadlock in HBaseClient when KeeperException occured,"HBaseClient was running after a period of time, all of get operation became too slow.

From the client logs I could see the following:

1. Unable to get data of znode /hbase/root-region-server
{code}
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1253)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1129)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:264)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataInternal(ZKUtil.java:522)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:498)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.getData(ZooKeeperNodeTracker.java:156)
        at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation(RootRegionTracker.java:62)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:933)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:832)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:48)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:126)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:99)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:894)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:948)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
{code}

2. Catalina.out found one Java-level deadlock:
{code}
=============================

""catalina-exec-800"":
  waiting to lock monitor 0x000000005f1f6530 (object 0x0000000731902200, a java.lang.Object),
  which is held by ""catalina-exec-710""
""catalina-exec-710"":
  waiting to lock monitor 0x00002aaab9a05bd0 (object 0x00000007321f8708, a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation),
  which is held by ""catalina-exec-29-EventThread""
""catalina-exec-29-EventThread"":
  waiting to lock monitor 0x000000005f9f0af0 (object 0x0000000732a9c7e0, a org.apache.hadoop.hbase.zookeeper.RootRegionTracker),
  which is held by ""catalina-exec-710""
Java stack information for the threads listed above:

===================================================

""catalina-exec-800"":
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:943)
        - waiting to lock <0x0000000731902200> (a java.lang.Object)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:807)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
""catalina-exec-710"":
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.resetZooKeeperTrackers(HConnectionManager.java:599)
        - waiting to lock <0x00000007321f8708> (a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1660)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.getData(ZooKeeperNodeTracker.java:158)
        - locked <0x0000000732a9c7e0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation(RootRegionTracker.java:62)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:933)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:832)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:48)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:126)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:99)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:894)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:948)
        - locked <0x0000000731902200> (a java.lang.Object)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:807)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
""catalina-exec-29-EventThread"":
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.stop(ZooKeeperNodeTracker.java:98)
        - waiting to lock <0x0000000732a9c7e0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.resetZooKeeperTrackers(HConnectionManager.java:604)
        - locked <0x00000007321f8708> (a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1660)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:374)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:271)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497)
Found 1 deadlock.
{code}
From the source code , the reason for this problem is doing ZooKeeperNodeTracker.getData that lead to KeeperException. And try to resetZookeeperTracker. At the same time, ClientCnxn.EventThread  also do resetZookeeperTracker ,too. Because of getData have already held the lock of  ZooKeeperNodeTracke , that lead to the order of the lock two threads to obtain does not accord. So deadlock happened.

In order to avoid the problem, we can add if reseting condition in abortable.abort()
See the patch.",,boneylw,Critical,Closed,Fixed,03/Dec/12 07:38,13/Jan/13 06:28
Bug,HBASE-7260,12618577,Upgrade hadoop 1 dependency to hadoop 1.1.1,hadoop 1.1.1 has been released with 20 bug fixes and improvements,yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,03/Dec/12 14:40,26/Feb/13 08:22
Bug,HBASE-7264,12618622,Improve Snappy installation documentation,"Snappy installation process is lacking some details. I tried to give some.

There is also some mistakes ""it's"" vs ""its"".",jmspaggi,jmspaggi,Minor,Closed,Fixed,03/Dec/12 19:46,23/Sep/13 18:31
Bug,HBASE-7265,12618637,Make Maven skip module test properties consistent,,eclark,eclark,Minor,Closed,Fixed,03/Dec/12 21:40,23/Sep/13 18:30
Bug,HBASE-7268,12618684,correct local region location cache information can be overwritten (or deleted) w/stale information from an old server,"Discovered via HBASE-7250; related to HBASE-5877.
Test is writing from multiple threads.
Server A has region R; client knows that.
R gets moved from A to server B.
B gets killed.
R gets moved by master to server C.
~15 seconds later, client tries to write to it (on A?).
Multiple client threads report from RegionMoved exception processing logic ""R moved from C to B"", even though such transition never happened (neither in nor before the sequence described below). Not quite sure how the client learned of the transition to C, I assume it's from meta from some other thread...
Then, put fails (it may fail due to accumulated errors that are not logged, which I am investigating... but the bogus cache update is there nonwithstanding).

I have a patch but not sure if it works, test still fails locally for yet unknown reason.",sershe,sershe,Minor,Closed,Fixed,04/Dec/12 03:25,23/Sep/13 18:31
Bug,HBASE-7269,12618687,Testing in place does not work if not building with default profile,"If I build with the Hadoop 2 profile, for example:

{{mvn -Dhadoop.profile=2.0 -Dhadoop.version=2.0.3-SNAPSHOT}}

and then try to run daemons like so:

{{./bin/hbase master start}}

this will fail, because the launch script will invoke Maven to build the cached classpath selecting whatever is the default profile, currently Hadoop 1. Startup will actually get pretty far, until:

{noformat}
12/12/04 11:42:13 WARN regionserver.HRegionServer: error telling master we are up
com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: org/apache/hadoop/net/SocketInputWrapper
	at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:189)
	at $Proxy10.regionServerStartup(Unknown Source)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:1844)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:843)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/net/SocketInputWrapper
	at org.apache.hadoop.hbase.ipc.HBaseClient.createConnection(HBaseClient.java:317)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1415)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1278)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:177)
	... 4 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.net.SocketInputWrapper
{noformat}

There doesn't appear to be a way to supply additional arguments to the launch script for directing Maven which profile(s) to activate.",nkeywal,apurtell,Major,Closed,Fixed,04/Dec/12 03:52,23/Sep/13 18:30
Bug,HBASE-7270,12618697,Remove MultiPut and MultiPutResponse to satisfy rat-check,,jesse_yates,jesse_yates,Major,Closed,Fixed,04/Dec/12 06:46,23/Sep/13 18:30
Bug,HBASE-7271,12618740,Have a single executor for all zkWorkers in the assignment manager,"The current strategy is to have an array of monothreaded executor, and hash the zk path to ensure that there are no two events on the same region executed in parallel  

I think a single executor, as presented in the attached patch, is better because:
- we're guaranteed to use all threads at any time
- if managing one of the event takes longer that expected, the slowness is limited to this region, and not to all regions that have the same hashed/moduloed code
- For the nodeChildrenChanged, there is no need to choose randomly one of the worker (or, once again, the risk to get stuck if one of the event takes time to be managed).

",nkeywal,nkeywal,Major,Closed,Fixed,04/Dec/12 15:06,23/Sep/13 18:30
Bug,HBASE-7273,12618769,Upgrade zookeeper dependency to 3.4.5 for 0.94,"0.94 is using zookeeper 3.4.3
We should upgrade to 3.4.5

HBASE-4791, e.g., needs 3.4.5",,zhihyu@ebaysf.com,Major,Closed,Fixed,04/Dec/12 18:55,13/Jan/13 06:28
Bug,HBASE-7277,12618809,Thrift default JMX port should be 10103 instead of 8093,"HBASE-5879 set Thrift JMX port to 8093.  In conf/hbase-env.sh, the default one is 10103",jxiang,jxiang,Major,Closed,Fixed,05/Dec/12 00:49,23/Sep/13 18:31
Bug,HBASE-7279,12618823,"Avoid copying the rowkey in RegionScanner, StoreScanner, and ScanQueryMatcher","Did some profiling again.
I we can gain some performance [1] when passing buffer, rowoffset, and rowlength instead of making a copy of the row key.
That way we can also remove the row key caching (and this patch also removes the timestamps caching). Considering the sheer number in which we create KVs, every byte save is good.

[1] (15-20% when data is in the block cache we setup a Filter such that only a single row is returned to the client).",larsh,larsh,Major,Closed,Fixed,05/Dec/12 03:38,07/Feb/14 23:12
Bug,HBASE-7285,12618959,HMaster fails to start with secure Hadoop,"In current trunk, HMaster will fail to start with secure Hadoop if the user starting the process has not obtained a kerberos TGT.  The user starting the process should not be required to have a TGT, as the HMaster process self logs in using the configured keytab and principal.

This is due to a log line in the HMaster constructor executing prior to the {{User.login()}} step:
{code}
    LOG.info(""hbase.rootdir="" + FSUtils.getRootDir(this.conf) +
        "", hbase.cluster.distributed="" + this.conf.getBoolean(""hbase.cluster.distributed"", false));
{code}

Here the FSUtils.getRootDir() winds up hitting the NameNode.  The fix is trivial, moving the log line to follow {{User.login()}}.",ghelmling,ghelmling,Major,Closed,Fixed,05/Dec/12 22:57,23/Sep/13 18:31
Bug,HBASE-7290,12619005,Online snapshots ,"HBASE-6055 will be closed when the offline snapshots pieces get merged with trunk.  This umbrella issue has all the online snapshot specific patches.  This will get merged once one of the implementations makes it into trunk.  Other flavors of online snapshots can then be done as normal patches instead of on a development branch.  (was: HBASE-6055 will be closed when the online snapshots pieces get merged with trunk.  This umbrella issue has all the online snapshot specific patches.  This will get merged once one of the implementations makes it into trunk.  Other flavors of online snapshots can then be done as normal patches instead of on a development branch.)

(not a fan of the quick edit descirption jira feature)",jmhsieh,jmhsieh,Blocker,Closed,Fixed,06/Dec/12 08:31,15/Oct/13 04:46
Bug,HBASE-7293,12622832,[replication] Remove dead sinks from ReplicationSource.currentPeers and pick new ones,"I happened to look at a log today where I saw a lot lines like this:

{noformat}
2012-12-06 23:29:08,318 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Slave cluster looks down: This server is in the failed servers list: sv4r20s49/10.4.20.49:10304
2012-12-06 23:29:15,987 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of a local or network error: 
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:519)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:484)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupConnection(HBaseClient.java:416)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:462)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1150)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1000)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
	at $Proxy14.replicateLogEntries(Unknown Source)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:627)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)
2012-12-06 23:29:15,988 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Slave cluster looks down: Connection refused
{noformat}

What struck me as weird is this had been going on for some days, I would expect the RS to find new servers if it wasn't able to replicate. But the reality is that only a few of the chosen sink RS were down so eventually the source hits one that's good and is never able to refresh its list of servers.

We should remove the dead servers, it's spammy and probably adds some slave lag.",larsh,jdcryans,Major,Closed,Fixed,06/Dec/12 23:32,26/Feb/13 08:27
Bug,HBASE-7296,12622903,Add hbase.master.loadbalancer.class in the documentation,hbase.master.loadbalancer.class information is missing from the documentation. Might be useful to add it.,jmspaggi,jmspaggi,Minor,Closed,Fixed,07/Dec/12 13:28,20/Nov/15 11:54
Bug,HBASE-7299,12622954,TestMultiParallel fails intermittently in trunk builds,"From trunk build #3598:
{code}
 testFlushCommitsNoAbort(org.apache.hadoop.hbase.client.TestMultiParallel): Count of regions=8
{code}
It failed in 3595 as well:
{code}
java.lang.AssertionError: Server count=2, abort=true expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.hadoop.hbase.client.TestMultiParallel.doTestFlushCommits(TestMultiParallel.java:267)
	at org.apache.hadoop.hbase.client.TestMultiParallel.testFlushCommitsWithAbort(TestMultiParallel.java:226)
{code}",zjushch,yuzhihong@gmail.com,Critical,Closed,Fixed,07/Dec/12 19:19,23/Sep/13 18:30
Bug,HBASE-7300,12622958,HbckTestingUtil needs to keep a static executor to lower the number of threads used,"I can't run TestHBaseFsck on my machine without running out of threads, that's because each new HBaseFsck creates 50 threads. It has a constructor that takes an executor and HbckTestingUtil could keep a static one to pass in. I was able to cut thousands of threads with this.",jdcryans,jdcryans,Major,Closed,Fixed,07/Dec/12 19:38,26/Feb/13 08:22
Bug,HBASE-7301,12622965,Force ipv4 for unit tests,"These two tests are failing when I run them locally:



Failed tests:   testMultiSlaveReplication(org.apache.hadoop.hbase.replication.TestMultiSlaveReplication): Waited too much time for put replication
  testCyclicReplication(org.apache.hadoop.hbase.replication.TestMasterReplication): Waited too much time for put replication
  testSimplePutDelete(org.apache.hadoop.hbase.replication.TestMasterReplication): Waited too much time for put replication


The TestMasterReplication is NPE'ing

Mighty JD said he'd take a looksee.",jdcryans,stack,Major,Closed,Fixed,07/Dec/12 20:30,26/Feb/13 08:23
Bug,HBASE-7304,12622990,assembly:assembly doesn't include the correct hbase-hadoop compat jars for hadoop 2,"When executing 
{code}mvn clean package assembly:assembly -Dhadoop.profile=2.0{code}
hbase-hadoop1-compat is placed in the tar.gz erroneously.",eclark,eclark,Major,Closed,Fixed,07/Dec/12 23:56,15/Jun/22 20:50
Bug,HBASE-7305,12622994,ZK based Read/Write locks for table operations,"This has started as forward porting of HBASE-5494 and HBASE-5991 from the 89-fb branch to trunk, but diverged enough to have it's own issue. 

The idea is to implement a zk based read/write lock per table. Master initiated operations should get the write lock, and region operations (region split, moving, balance?, etc) acquire a shared read lock. 

",enis,enis,Major,Closed,Fixed,08/Dec/12 00:16,15/Oct/13 04:46
Bug,HBASE-7307,12623011,MetaReader.tableExists should not return false if the specified table regions has been split,"If a region is split parent we are not adding it to META scan results during full scan. 
{code}
        if (!isInsideTable(this.current, tableNameBytes)) return false;
        if (this.current.isSplitParent()) return true;
        // Else call super and add this Result to the collection.
        super.visit(r);
{code}

If all regions of a table has been split then result size will be zero and returning false.
{code} 
    fullScan(catalogTracker, visitor, getTableStartRowForMeta(tableNameBytes));
    // If visitor has results >= 1 then table exists.
    return visitor.getResults().size() >= 1;
{code}
Even table is present we are returning false which is not correct(its highly possible in case of tables with one region).",rajesh23,rajesh23,Major,Closed,Fixed,08/Dec/12 13:21,26/Feb/13 08:23
Bug,HBASE-7309,12623039,"Metrics refresh-task is not canceled when regions are closed, leaking HRegion objects","While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
",amuraru,amuraru,Critical,Closed,Fixed,09/Dec/12 01:52,23/Sep/13 18:30
Bug,HBASE-7314,12623198,Can't start REST/Thrift server if HBASE_JMX_OPTS not set,"By default JMX is enabled for REST server and Thrift server.  However, if HBASE_JMX_OPTS is not set, and JMX remote access rule is not set, we can't bring up the REST/Thrift server due to JMX remote access rule errors.

We need to enhance the hbase script not to enable JMX for REST/Thrift server is HBASE_JMX_OPTS is not set.",jxiang,jxiang,Minor,Closed,Fixed,10/Dec/12 19:28,23/Sep/13 18:31
Bug,HBASE-7325,12623406,Replication reacts slowly on a lightly-loaded cluster,"ReplicationSource uses a backing-off algorithm to sleep for an increasing duration when an error is encountered in the replication run loop. However, this backing-off is also performed when there is nothing found to replicate in the HLog.

Assuming default settings (1 second base retry sleep time, and maximum multiplier of 10), this means that replication takes up to 10 seconds to occur when there is a break of about 55 seconds without anything being written. As there is no error condition, and there is apparently no substantial load on the regionserver in this situation, it would probably make more sense to not back off in non-error situations.",gabriel.reid,gabriel.reid,Minor,Closed,Fixed,11/Dec/12 16:33,23/Sep/13 19:28
Bug,HBASE-7332,12623459,[webui] HMaster webui should display the number of regions a table has.,Pre-0.96/trunk hbase displayed the number of regions per table in the table listing.  Would be good to have this back.,octo47,jmhsieh,Minor,Closed,Fixed,11/Dec/12 21:33,16/Sep/15 06:08
Bug,HBASE-7338,12623610,Fix flaky condition for org.apache.hadoop.hbase.TestRegionRebalancing.testRebalanceOnRegionServerNumberChange,"The balancer doesn't run in case a region is in-transition. The check to confirm whether there all regions are assigned looks for region count > 22, where the total regions are 27. This may result in a failure:
{code}
java.lang.AssertionError: After 5 attempts, region assignments were not balanced.
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hbase.TestRegionRebalancing.assertRegionsAreBalanced(TestRegionRebalancing.java:203)
	at org.apache.hadoop.hbase.TestRegionRebalancing.testRebalanceOnRegionServerNumberChange(TestRegionRebalancing.java:123)

.....
2012-12-11 13:47:02,231 INFO  [pool-1-thread-1] hbase.TestRegionRebalancing(120): Added fourth server=p0118.mtv.cloudera.com,44414,1355262422083
2012-12-11 13:47:02,231 INFO  [RegionServer:3;p0118.mtv.cloudera.com,44414,1355262422083] regionserver.HRegionServer(3769): Registered RegionServer MXBean
2012-12-11 13:47:02,231 DEBUG [pool-1-thread-1] master.HMaster(987): Not running balancer because 1 region(s) in transition: {c786446fb2542f190e937057cdc79d9d=test,kkk,1355262401365.c786446fb2542f190e937057cdc79d9d. state=OPENING, ts=1355262421037, server=p0118.mtv.cloudera.com,54281,1355262419765}
2012-12-11 13:47:02,232 DEBUG [pool-1-thread-1] hbase.TestRegionRebalancing(165): There are 4 servers and 26 regions. Load Average: 13.0 low border: 9, up border: 16; attempt: 0
2012-12-11 13:47:02,232 DEBUG [pool-1-thread-1] hbase.TestRegionRebalancing(171): p0118.mtv.cloudera.com,51590,1355262395329 Avg: 13.0 actual: 11
2012-12-11 13:47:02,232 DEBUG [pool-1-thread-1] hbase.TestRegionRebalancing(171): p0118.mtv.cloudera.com,52987,1355262407916 Avg: 13.0 actual: 15
2012-12-11 13:47:02,233 DEBUG [pool-1-thread-1] hbase.TestRegionRebalancing(171): p0118.mtv.cloudera.com,48044,1355262421787 Avg: 13.0 actual: 0
2012-12-11 13:47:02,233 DEBUG [pool-1-thread-1] hbase.TestRegionRebalancing(179): p0118.mtv.cloudera.com,48044,1355262421787 Isn't balanced!!! Avg: 13.0 actual: 0 slop: 0.2
2012-12-11 13:47:12,233 DEBUG [pool-1-thread-1] master.HMaster(987): Not running balancer because 1 region(s) in transition: 
{code}",v.himanshu,v.himanshu,Minor,Closed,Fixed,12/Dec/12 19:34,26/Feb/13 08:23
Bug,HBASE-7342,12623631,Split operation without split key incorrectly finds the middle key in off-by-one error,"I took a deeper look into issues I was having using region splitting when specifying a region (but not a key for splitting).

The midkey calculation is off by one and when there are 2 rows, will pick the 0th one. This causes the firstkey to be the same as midkey and the split will fail. Removing the -1 causes it work correctly, as per the test I've added.

Looking into the code here is what goes on:

1. Split takes the largest storefile
2. It puts all the keys into a 2-dimensional array called blockKeys[][]. Key i resides as blockKeys[i]
3. Getting the middle root-level index should yield the key in the middle of the storefile
4. In step 3, we see that there is a possible erroneous (-1) to adjust for the 0-offset indexing.
5. In a result with where there are only 2 blockKeys, this yields the 0th block key. 
6. Unfortunately, this is the same block key that 'firstKey' will be.
7. This yields the result in HStore.java:1873 (""cannot split because midkey is the same as first or last row"")
8. Removing the -1 solves the problem (in this case). ",aleksshulman,aleksshulman,Minor,Closed,Fixed,12/Dec/12 22:00,26/Feb/13 08:22
Bug,HBASE-7343,12623645,Fix flaky condition for TestDrainingServer,"The assert statement in setUpBeforeClass() may fail in case the region distribution is not even (a particular rs has 0 regions).

{code}
junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:48)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertFalse(Assert.java:34)
	at junit.framework.Assert.assertFalse(Assert.java:41)
	at org.apache.hadoop.hbase.TestDrainingServer.setUpBeforeClass(TestDrainingServer.java:83)

{code}

This is already fixed in trunk with HBASE-5992, but as that's a bigger change and uses 5877, this jira fixes that issue instead of backporting 5992.",v.himanshu,v.himanshu,Minor,Closed,Fixed,13/Dec/12 00:31,13/Jan/13 06:28
Bug,HBASE-7350,12623846,Flakey tests make CI unreliable,"Most PreCommit and PostCommit builds are failing these days. Keeping an eye on usual suspects reveals them to be mostly longer-running tests. Either the tests need to me made more rigorous or the erroneous Jenkins configuration needs addressed.

The usual suspects:
- org.apache.hadoop.hbase.client.TestMultiParallel
- org.apache.hadoop.hbase.TestDrainingServer
- org.apache.hadoop.hbase.regionserver.TestSplitTransaction
- org.apache.hadoop.hbase.client.TestMultiParallel.testFlushCommitsNoAbort
- org.apache.hadoop.hbase.replication.TestReplication
- org.apache.hadoop.hbase.util.TestHBaseFsck",nkeywal,ndimiduk,Major,Closed,Fixed,13/Dec/12 22:33,15/Jun/22 20:56
Bug,HBASE-7355,12624025,NPE in ClusterStatus PB conversion,,apurtell,apurtell,Major,Closed,Fixed,14/Dec/12 19:07,23/Sep/13 18:31
Bug,HBASE-7357,12624050,HBaseClient and HBaseServer should use hbase.security.authentication when negotiating authentication,"This came up in the context of testing HBASE-6788.  Currently HBaseClient and HBaseServer call UserGroupInformation.isSecurityEnabled() when determining whether or not to use SASL to negotiate connections.  This means they are using the hadoop.security.authentication configuration value.  Since this is in the context of HBase RPC connections, it seems more correct to use the hbase.security.authentication configuration value by calling User.isHBaseSecurityEnabled().",ghelmling,ghelmling,Major,Closed,Fixed,14/Dec/12 21:25,09/Jun/14 04:01
Bug,HBASE-7363,12624106,Fix javadocs warnings for hbase-server packages from master to end,,nkeywal,nkeywal,Minor,Closed,Fixed,15/Dec/12 10:33,23/Sep/13 18:30
Bug,HBASE-7370,12624346,Remove Writable From ScanMetrics.,Right now ScanMetrics uses Writable to be able to set MapReduce counters.  We should remove this and use protobuf.,eclark,eclark,Critical,Closed,Fixed,17/Dec/12 20:51,15/Jun/22 20:57
Bug,HBASE-7373,12624374,table should not be required in AccessControlService,"We should fix the proto file, add unit test for this case, and verify it works from hbase shell with table to be nil.
",jxiang,jxiang,Minor,Closed,Fixed,18/Dec/12 00:01,23/Sep/13 18:30
Bug,HBASE-7376,12624394,Acquiring readLock does not apply timeout in HRegion#flushcache,"{code}
HRegion
  public boolean flushcache() throws IOException {
       lock(lock.readLock());
  }
{code}
The HRegion.flushcache is called by the normal flush cache, so if we use a timeout, the MemStoreFlusher may be get a RegionTooBusyException, it is safe to do not use a timeout.",binlijin,binlijin,Major,Closed,Fixed,18/Dec/12 02:03,02/May/13 02:30
Bug,HBASE-7380,12624409,"[replication] When transferring queues, check if the peer still exists before copying the znodes","Right now it's a pain if you remove a peer and still have rogue queues because they get moved on and on and on. NodeFailoverWorker needs to run the check:

bq. if (!zkHelper.getPeerClusters().containsKey(src.getPeerClusterId())) {

before this:

bq. SortedMap<String, SortedSet<String>> newQueues = zkHelper.copyQueuesFromRS(rsZnode);

And test.",jdcryans,jdcryans,Major,Closed,Fixed,18/Dec/12 04:33,23/Sep/13 19:08
Bug,HBASE-7382,12624522,Port ZK.multi support from HBASE-6775 to 0.96,"HBASE-6775 adds support for ZK.multi ZKUtil and uses it for the 0.92/0.94 compatibility fix implemented in HBASE-6710.

ZK.multi support is most likely useful in 0.96, but since HBASE-6710 is not relevant for 0.96, perhaps we should find another use case first before we port.",v.himanshu,gchanan,Critical,Closed,Fixed,18/Dec/12 20:16,23/Sep/13 18:30
Bug,HBASE-7383,12624535,create integration test for HBASE-5416 (improving scan performance for certain filters),HBASE-5416 is risky and needs an integration test.,sershe,sershe,Major,Closed,Fixed,18/Dec/12 21:33,23/Sep/13 18:30
Bug,HBASE-7390,12624658,Add extra test cases for assignement on the region server and fix the related issues,"We don't have a lot of tests on the region server itself.
Here are some.
Some of them are failing, feedback welcome.
See as well the attached state diagram for the ZK nodes on assignment.",nkeywal,nkeywal,Major,Closed,Fixed,19/Dec/12 16:02,23/Sep/13 18:31
Bug,HBASE-7391,12624661,Review/improve HLog compression's memory consumption,"From Ram in http://mail-archives.apache.org/mod_mbox/hbase-dev/201205.mbox/%3C00bc01cd31e6$7caf1320$760d3960$%25vasudevan@huawei.com%3E:

{quote}
One small observation after giving +1 on the RC.
The WAL compression feature causes OOME and causes Full GC.

The problem is, if we have 1500 regions and I need to create recovered.edits
for each of the region (I don’t have much data in the regions (~300MB)).
Now when I try to build the dictionary there is a Node object getting
created.
Each node object occupies 32 bytes.
We have 5 such dictionaries.

Initially we create indexToNodes array and its size is 32767.

So now we have 32*5*32767 = ~5MB.

Now I have 1500 regions.

So 5MB*1500 = ~7GB.(Excluding actual data).  This seems to a very high
initial memory foot print and this never allows me to split the logs and I
am not able to make the cluster up at all.

Our configured heap size was 8GB, tested in 3 node cluster with 5000
regions, very less data( 1GB in hdfs cluster including replication), some
small data is spread evenly across all regions.

The formula is 32(Node object size)*5(No of dictionary)*32767(no of node
objects)*noofregions.
{quote}",ram_krish,jdcryans,Major,Closed,Fixed,19/Dec/12 16:16,23/Sep/13 19:22
Bug,HBASE-7398,12624756,[0.94 UNIT TESTS] TestAssignmentManager fails frequently on CentOS 5,"TestAssignmentManager#testBalance() fails pretty frequently on CentOS 5 for 0.94. The root cause is that ClosedRegionHandler is executed by an executor, and before it finishes, the region transition is done for OPENING and OPENED.

This seems to be just a test problem, not an actual bug, since the region server won't open the region unless it get's it from the assign call on ClosedRegionHandler.process(). 

I've seen that HBASE-6109 has a fix for this already, will just backport those changes. This is 0.94 only. ",enis,enis,Major,Closed,Fixed,20/Dec/12 01:57,13/Jan/13 06:28
Bug,HBASE-7401,12624769,Remove warning message about running 'hbase migrate',"I was switching version to 0.94 (from 0.95-SNAPSHOT) and got this message which refers to a program that does not exist.

{code}
org.apache.hadoop.hbase.util.FileSystemVersionException: File system needs to be upgraded.  You have version null and I want version 7.  Run the '${HBASE_HOME}/bin/hbase migrate' script.
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:324)
        at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:352)
        at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:128)
        at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:113)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:505)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:362)
        at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:226)
        at java.lang.Thread.run(Thread.java:662)

{code}",jmhsieh,jmhsieh,Major,Closed,Fixed,20/Dec/12 05:56,27/Apr/13 15:55
Bug,HBASE-7407,12624864,TestMasterFailover under tests some cases and over tests some others,"The tests are done with this settings:
    conf.setInt(""hbase.master.assignment.timeoutmonitor.period"", 2000);
    conf.setInt(""hbase.master.assignment.timeoutmonitor.timeout"", 4000);


As a results:
1) some tests seems to work, but in real life, the recovery would take 5 minutes or more, as in production there always higher. So we don't see the real issues.
2) The tests include specific cases that should not happen in production. It works because the timeout catches everything, but these scenarios do not need to be optimized, as they cannot happen. 
",nkeywal,nkeywal,Minor,Closed,Fixed,20/Dec/12 18:07,19/May/14 16:40
Bug,HBASE-7412,12624914,Fix how HTableDescriptor handles default max file size and flush size,"If the region flush size is not set in the table, IncreasingToUpperBoundRegionSplitPolicy will most likely always use the default value: 128MB, even if the flush size is set to a different value in hbase-site.xml.",jxiang,jxiang,Minor,Closed,Fixed,20/Dec/12 23:06,26/Feb/13 08:22
Bug,HBASE-7417,12624942,TestReplication is flaky,"See discussion at the end of HBASE-5778.
TestReplication failed in all recent 0.94 jenkins builds.",larsh,larsh,Critical,Closed,Fixed,21/Dec/12 06:06,13/Jan/13 06:28
Bug,HBASE-7418,12625006,HFileLink flaky tests,TestStoreFile and TestHFileLinkCleaner seems flaky.,mbertozzi,mbertozzi,Minor,Closed,Fixed,21/Dec/12 14:52,15/Jun/22 21:07
Bug,HBASE-7421,12625031,TestHFileCleaner->testHFileCleaning  has an aggressive timeout,"As the description, increasing it to 1 min to have some upper limit.
{code}
FAILED:  org.apache.hadoop.hbase.master.cleaner.TestHFileCleaner.testHFileCleaning

Error Message:
test timed out after 2000 milliseconds

Stack Trace:
java.lang.Exception: test timed out after 2000 milliseconds
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.ipc.Client.call(Client.java:1210)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
        at $Proxy17.create(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
        at $Proxy17.create(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:192)
        at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1298)
        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1317)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1243)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1200)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:273)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:262)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:79)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:811)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:792)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:754)
        at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1054)
        at org.apache.hadoop.hbase.master.cleaner.TestHFileCleaner.testHFileCleaning(TestHFileCleaner.java:119)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
{code}",v.himanshu,v.himanshu,Minor,Closed,Fixed,21/Dec/12 17:36,26/Feb/13 08:22
Bug,HBASE-7422,12625037,MasterFS doesn't set configuration for internal FileSystem,"The call stack ends on HFileArchiver.archveRegion, which uses fs.getConf() to get the configuration object. This returns a different conf object (not the one used in MasterFileSystem), which may have the root file system as file (depending on the test environment). 
{code}
from META
2012-12-20 13:46:09,540 DEBUG [MASTER_TABLE_OPERATIONS-p0122.mtv.cloudera.com,53027,1356039958141-0] backup.HFileArchiver(91): ARCHIVING region file:/tmp/hbase-jenkins/hbase/tableB/1c0ce809562dcefde6086d4f8549ed8f
2012-12-20 13:46:09,540 DEBUG [MASTER_TABLE_OPERATIONS-p0122.mtv.cloudera.com,53027,1356039958141-0] backup.HFileArchiver(109): Have an archive directory, preparing to move files
2012-12-20 13:46:09,543 ERROR [MASTER_TABLE_OPERATIONS-p0122.mtv.cloudera.com,53027,1356039958141-0] executor.EventHandler(174): Caught throwable while processing event C_M_DELETE_TABLE
java.lang.IllegalArgumentException: Wrong FS: file:/tmp/hbase-jenkins/hbase/tableB/1c0ce809562dcefde6086d4f8549ed8f, expected: hdfs://localhost:39594
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:550)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1376)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1416)
	at org.apache.hadoop.hbase.util.FSUtils.listStatus(FSUtils.java:1164)
	at org.apache.hadoop.hbase.backup.HFileArchiver.archiveRegion(HFileArchiver.java:122)
	at org.apache.hadoop.hbase.backup.HFileArchiver.archiveRegion(HFileArchiver.java:72)
	at org.apache.hadoop.hbase.master.MasterFileSystem.deleteRegion(MasterFileSystem.java:444)
	at org.apache.hadoop.hbase.master.handler.DeleteTableHandler.handleTableOperation(DeleteTableHandler.java:71)
	at org.apache.hadoop.hbase.master.handler.TableEventHandler.process(TableEventHandler.java:96)
{code}",v.himanshu,v.himanshu,Major,Closed,Fixed,21/Dec/12 18:27,13/Jan/13 06:28
Bug,HBASE-7423,12625049,HFileArchiver should not use the configuration from the Filesystem,"HFileArchiver gets the configuration from the FileSystem in 
{code}
 public static void archiveRegion(FileSystem fs, HRegionInfo info)
      throws IOException {
    Path rootDir = FSUtils.getRootDir(fs.getConf());
{code}

In Pig's test cases, they construct a MiniDFSCluster and pass it to HBaseTestingUtil, which causes the delete table to fail because it will refer to the FileSystem's configuration rather than HBase's one. 




",enis,enis,Major,Closed,Fixed,21/Dec/12 20:01,26/Feb/13 08:27
Bug,HBASE-7432,12625195,TestHBaseFsck prevents testsuite from finishing,"Seen twice in a row in a local build:
{code}
""pool-1-thread-1"" prio=10 tid=0x00007fb4b8410000 nid=0x4481 waiting on condition [0x00007fb4b7d48000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hbase.util.TestHBaseFsck.deleteTable(TestHBaseFsck.java:359)
        at org.apache.hadoop.hbase.util.TestHBaseFsck.testNotInHdfs(TestHBaseFsck.java:1002)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
        at org.junit.rules.RunRules.evaluate(RunRules.java:18)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.junit.runners.Suite.runChild(Suite.java:128)
        at org.junit.runners.Suite.runChild(Suite.java:24)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{code}",larsh,larsh,Critical,Closed,Fixed,24/Dec/12 22:59,13/Jan/13 06:28
Bug,HBASE-7435,12625207,BuiltInGzipDecompressor is only released during full GC,"That seems to be bug in Hadoop, actually.

BuiltInGzipDecompressor.end() needs to be called to release it's resource, but it is not called anywhere in CodecPool.
Instead the end() is called by finalize(), which is only called during a full gc (or never, depending on JVM).

This is only an issue in test. In real life most folks will have the native GzipDecompressor",larsh,larsh,Major,Closed,Fixed,25/Dec/12 07:05,05/Apr/13 01:01
Bug,HBASE-7440,12625299,ReplicationZookeeper#addPeer is racy,"While adding a peer, ReplicationZK does the znodes creation in three transactions. Create :
a) peers znode
b) peerId specific znode, and
c) peerState znode

There is a PeerWatcher which invokes getPeer() (after steps b) and c)). If it happens that while adding a peer, the control flows to getPeer() and step c) has not been processed, it may results in a state where the peer will not be added. This happens while running TestMasterReplication#testCyclicReplication().
{code}
2012-12-26 07:36:35,187 INFO  [RegionServer:0;p0120.XXXXX,38423,1356536179470-EventThread] zookeeper.RecoverableZooKeeper(447): Node /2/replication/peers/1/peer-state already exists and this is not a retry
2012-12-26 07:36:35,188 ERROR [RegionServer:0;p0120.XXXXX,38423,1356536179470-EventThread] regionserver.ReplicationSourceManager$PeersWatcher(527): Error while adding a new peer
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /2/replication/peers/1/peer-state
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:428)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:410)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndWatch(ZKUtil.java:1044)
	at org.apache.hadoop.hbase.replication.ReplicationPeer.startStateTracker(ReplicationPeer.java:82)
	at org.apache.hadoop.hbase.replication.ReplicationZookeeper.getPeer(ReplicationZookeeper.java:344)
	at org.apache.hadoop.hbase.replication.ReplicationZookeeper.connectToPeer(ReplicationZookeeper.java:307)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$PeersWatcher.nodeChildrenChanged(ReplicationSourceManager.java:519)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:315)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
2012-12-26 07:36:35,188 DEBUG [RegionServer:0;p0120.XXXXX,55742,1356536171947-EventThread] zookeeper.ZKUtil(1545): regionserver:55742-0x13bd7db39580004 Retrieved 36 byte(s) of data from znode /1/hbaseid; data=9ce66123-d3e8-4ae9-a249-afe03...

{code}
",v.himanshu,v.himanshu,Major,Closed,Fixed,27/Dec/12 00:07,26/Feb/13 16:21
Bug,HBASE-7442,12625341,HBase remote CopyTable not working when security enabled,"When security is enabled, HBase CopyTable fails with Kerberos exception:

{code}
FATAL org.apache.hadoop.ipc.SecureClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
{code}

This is only when copying to remote HBase cluster (using either MRv1 or YARN), local copy works fine.",jrkinley,jrkinley,Major,Closed,Fixed,27/Dec/12 15:57,13/Mar/13 13:08
Bug,HBASE-7443,12625344,More findbugs fixes,"It   
- adds the dependency to findbugs for the annotations. It's a compile only dependency. License is LGPL.
- suppresses a few not critical warnings
- fixes a few others

Locally, I'm now at 144.",nkeywal,nkeywal,Minor,Closed,Fixed,27/Dec/12 17:20,23/Sep/13 18:30
Bug,HBASE-7446,12625383,Make the reference guide utf8 rather than 8859,,stack,stack,Major,Closed,Fixed,28/Dec/12 04:44,23/Sep/13 18:31
Bug,HBASE-7450,12625408,"orphan RPC connection in HBaseClient leaves ""null"" out member, causing NPE in HCM","Just like:　https://issues.apache.org/jira/browse/HADOOP-7428
Exceptions except IOException thrown in setupIOstreams would leave the connection half-setup. But the connection would not close utill it become timeout. The orphane connection cause NPE when is used in HCM.",docete,docete,Major,Closed,Fixed,28/Dec/12 09:22,23/Sep/13 18:30
Bug,HBASE-7455,12625471,Increase timeouts in TestReplication and TestSplitLogWorker,"When I measure the times in TestReplication.queueFailover, it takes about 15s on my (reasonably fast) Laptop.
The timeout in queueFailover currently is 1500*2*15 = 45000ms.
For setup before each test (which truncates the table and waits for the changes to replicate) it is 1500*15 = 22500ms.

Interestingly I see queueFailover failures where the wait time is measured as 64260ms and some at 72316ms.
Since these numbers are not even close to 45000ms, the machine or JVM must have been stuck for 15 or almost 30s (otherwise we'd get a timeout and the total time spent should be close to the timeout).

So I would suggest that we increase the timeouts further.
We could set SLEEP_TIME to 2000 and retries to 20. Would lead to 2000*2*20 = 80000ms.

Any objections?",larsh,larsh,Major,Closed,Fixed,29/Dec/12 06:07,26/Feb/13 16:24
Bug,HBASE-7457,12625476,"Fix javadoc warnings in hadoopqa tool, it complains about unsafe accesses","I see this in  hadoopqa output and it seems to be causing the two warnings we currently see in hadoopqa reports:

{code}
2 warnings
[WARNING] Javadoc Warnings
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java:43: warning: sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[WARNING] import sun.misc.Unsafe;
[WARNING] ^
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java:1032: warning: sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[WARNING] static final Unsafe theUnsafe;
[WARNING] ^
{code}",stack,stack,Major,Closed,Fixed,29/Dec/12 09:22,23/Sep/13 18:30
Bug,HBASE-7458,12625485,TestReplicationWithCompression fails intermittently in both PreCommit and trunk builds,"TestReplicationWithCompression has been failing often.
Here are few examples:
https://builds.apache.org/job/PreCommit-HBASE-Build/3755/testReport/

https://builds.apache.org/job/HBase-TRUNK/3672/testReport/org.apache.hadoop.hbase.replication/TestReplicationWithCompression/testDeleteTypes/
https://builds.apache.org/job/HBase-0.94/677/testReport/junit/org.apache.hadoop.hbase.replication/TestReplicationWithCompression/queueFailover/",jeffreyz,yuzhihong@gmail.com,Critical,Closed,Fixed,29/Dec/12 14:00,23/Sep/13 18:31
Bug,HBASE-7459,12625489,NPE in HMaster TestlocalHBaseCluster,"TestLocalHBaseCluster has failed intermittently on a unit test run with this exception stack.

{code}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.HMaster.shutdown(HMaster.java:2064)
	at org.apache.hadoop.hbase.util.JVMClusterUtil.shutdown(JVMClusterUtil.java:238)
	at org.apache.hadoop.hbase.LocalHBaseCluster.shutdown(LocalHBaseCluster.java:430)
	at org.apache.hadoop.hbase.TestLocalHBaseCluster.testLocalHBaseCluster(TestLocalHBaseCluster.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	
{code}",jmhsieh,jmhsieh,Major,Closed,Fixed,29/Dec/12 17:03,23/Sep/13 18:31
Bug,HBASE-7464,12625537,[REST] Sending HTML for errors is unhelpful,"The large HTML 404 page returned by Stargate is not helpful.  The REST interface is not intended for humans to read, esp. when the client is known to be a program because it's asking for binary, but really any time.  Nice big readable error pages use bandwidth and clutter network traces to no purpose.

Please allow the 404 and other error pages to be configured away, or just stop sending them (my preference).  If some body must be sent, a simple text/plain ""Not found"" would be fine, I think.
",apurtell,chipdude,Minor,Closed,Fixed,30/Dec/12 23:51,26/Feb/13 16:28
Bug,GROOVY-5224,12816024,groovy.util.Node plus operator assuming all children are not strings,"The attached code triggers a class cast exception because in the groovy.util.Node plus operator, lines 179-180
https://github.com/groovy/groovy-core/blob/fc33b105f172629fc99d5b62da9e3c1adafff85c/src/main/groovy/util/Node.java

it is assumed that all children are nodes, but the parser does not seem to create textnodes but keeping them as string, either that or some strange conversion is happening because in 179 a class cast exception is raised for a java.lang.String to groovy.util.Node illegal conversion. 

The proof of concept is the attached junit code. If the commented line is uncommented, the cast exception appears. If the text in the xml is removed, the failure does not show.

XMLParser is being used.",paulk,escalope,Major,Closed,Fixed,02/Jan/12 13:34,12/Feb/12 04:03
Bug,GROOVY-5226,12816020,Variables which type is determined thanks to instanceof checks should be usable as arguments without casts,"The current implementation of the type checker allows to use methods of an object without the need of an explicit cast if it is wrapped in a proper {{instanceof}} check:

{code}
class A {
   void foo() {}
}
def o
...
if (o instanceof A) {
   o.foo()
}
{code}

But it doesn't work if the object is used as an argument:

{code}
class A {}
void m(A a) { ... }
def o
...
if (o instanceof A) {
   m(o) // requires m((A) o) which should not be necessary
}
{code}
",melix,melix,Major,Closed,Fixed,03/Jan/12 12:27,13/May/12 03:30
Bug,GROOVY-5228,12816025,Prefix/Postfix operations are not type checked,"Instructions like:

{code}
i++
date++
object++
++i
++date
++object
{code}

are not type checked.",melix,melix,Major,Closed,Fixed,04/Jan/12 10:36,13/May/12 03:30
Bug,GROOVY-5229,12816050,Some properties are not recognized as properties by the type checker,"Some class define ""implicit"" properties by defining a getter and a setter. If the class is a Java class, like {{java.util.Date}} and the {{time}} property, the type checker fails to recognize the property properly, thinking it's a read-only property.
",melix,melix,Major,Closed,Fixed,04/Jan/12 16:10,13/May/12 03:30
Bug,GROOVY-5231,12816021,Problem with @TupleConstructor with static type checking,"When using the @TupleConstructor with the static type checking, it doesn't like when a constructor without all the fields is used.
Consider the following example:
{code}
import groovy.transform.*

@TupleConstructor
@TypeChecked
class Person {
    String name, city
    static Person create() {
        new Person(""Guillaume"")
    }
}

Person.create()
{code}
The type checker complains because it could not find a zero-arg constructor:
{code}
[Static type checking] - Cannot find matching method Person#<init>(java.lang.String)
 at line: 8, column: 9
{code}
But actually, @TupleConstructor allows to use less args than all the declared.
Person can be instantiated with new Person(""Guillaume"", ""Paris""), new Person(""Guillaume""), or even new Person()",melix,guillaume,Major,Closed,Fixed,05/Jan/12 09:22,13/May/12 03:30
Bug,GROOVY-5232,12815492,Static type checking fails on setter call,"The setter generated by Groovy properties doesn't seem to be recognized by the type checker.

Given the following example:

{code}
import groovy.transform.*

@TypeChecked
class Person {
    String name
    
    static Person create() {
        def p = new Person()
        p.setName(""Guillaume"")
        // but p.name = ""Guillaume"" works
        return p
    }
}

Person.create()
{code}

It complains with:
{code}
[Static type checking] - Cannot find matching method Person#setName(java.lang.String)
 at line: 9, column: 9
{code}

Note that using the property notation for setting the property works, but the plain setter call fails.",melix,guillaume,Major,Closed,Fixed,05/Jan/12 09:43,13/May/12 03:30
Bug,GROOVY-5233,12816030,Static type checker has problems with args coerced to arrays,"{code}
import groovy.transform.*

@TypeChecked
def m() {
    try {
        throw new Exception()
    } catch (Throwable t) {
        def newTrace = []
        def clean = newTrace.toArray(newTrace as StackTraceElement[])
    }
}

m()
{code}

Given the above program which works when not type checked, the checker complains with:

{code}
[Static type checking] - Cannot call java.util.List#toArray([Ljava.lang.Object;) with arguments [[Ljava.lang.StackTraceElement; -> [Ljava.lang.StackTraceElement;]
 at line: 9, column: 21
{code}",melix,guillaume,Major,Closed,Fixed,05/Jan/12 12:05,13/May/12 03:30
Bug,GROOVY-5235,12816006,Static compilation of method call expressions with missing default parameters,The static compiler won't compile method call expressions for which some default parameters are missing properly.,melix,melix,Major,Closed,Fixed,05/Jan/12 14:34,13/May/12 03:30
Bug,GROOVY-5237,12815781,Static type checker complains on assignment on generics fields,"The static type checker complains on the following class:
{code}
@groovy.transform.TypeChecked
class Container<T> {
    private T initialValue
    Container(T initialValue) { this.initialValue = initialValue }
    T get() { initialValue }
}
{code}
With the message:
{code}
[Static type checking] - Incompatible generic argument types. Cannot assign java.lang.Object <T> to: java.lang.Object <T>
 at line: 4, column: 33
{code}",melix,guillaume,Major,Closed,Fixed,06/Jan/12 05:42,13/May/12 03:30
Bug,GROOVY-5238,12816074,Methods belonging to a different source unit get visited and report errors at the wrong place,"If a method call expression references a method from another class, the method called gets visited (if it is statically checked) even if the class doesn't belong to the same source unit. This triggers incorrect error reporting (errors in the wrong file).

Fixing completely this requires a large amount of work, so return type inference will only work for methods in the same source unit at first.

See TODO in StaticTypeCheckingVisitor.",melix,melix,Major,Closed,Fixed,06/Jan/12 06:02,13/May/12 03:30
Bug,GROOVY-5239,12816066,Statically imported method vs other methods with the same name,"Assume you have a class test.Foo:
{code}
package test

class Foo {
  static foo() {'foo'}
}
{code}


Let's consider some code snippets. IMHO Groovy incorrectly resolves reference 'foo' to statically imported method in all of these cases. 

{code} 
import static test.Foo.foo

class Bar {
  def foo() {'bar'}

  class Inner {
    def abc() {
      assert foo() == 'bar' //statically imported method will be invoked here
    }
  }

}
{code}

{code} 
import static test.Foo.foo

class Bar {
  def foo() {'bar'}
}

new Bar().with {
  assert foo() == 'bar' //statically imported method will be executed
}
{code}

{code} 
import static test.Foo.foo

class Base {
  def foo(){'base'}
}

class Bar extends Base {
  def abc() {
    assert foo() == 'base' //statically imported method will be executed 
  }
}
{code}",emilles,mxm-groovy,Major,Closed,Fixed,06/Jan/12 06:16,25/Feb/22 20:38
Bug,GROOVY-5240,12815853,Static type checker confuses Class and type,"When compiling the following code:

{code}
@groovy.transform.TypeChecked
class Container {
    static lookup(Class clazz) { }
    static m() {
        lookup(Date)
    }
}

Container.m()
{code}

The static type checker complains with:
{code}
[Static type checking] - Cannot find matching method Container#lookup(java.util.Date)
 at line: 5, column: 9
{code}

Although it shouldn't complain here, since we're passing the Date.class as argument to the lookup method, which is correct.",melix,guillaume,Major,Closed,Fixed,06/Jan/12 09:46,13/May/12 03:30
Bug,GROOVY-5243,12816026,@Canonical @TupleConstructor can't handle Object or Map properties,"Given the following code, I would expect the assertions to pass. They currently don't. However, if the {{a}} and {{b}} properties are typed to String, for example, everything goes as expected.

It might be challenging to get this to work properly for more complex types, but it would make the annotations *really* transparent. The way it currently behaves is very unsettling (a is set with a Map containing both a and b with their respective values, while b is not set to anything)

{code}
import groovy.transform.Canonical

def weird = new CanonicalIsWeird(a: 'first letter', b: 'second letter')
println ""a: ${weird.a}""
println ""b: ${weird.b}""

assert weird.a == 'first letter'
assert weird.b == 'second letter'

@Canonical
class CanonicalIsWeird {
    def a, b
}
{code}
https://gist.github.com/1565938",paulk,gjoseph,Major,Closed,Fixed,09/Jan/12 05:08,25/Oct/22 16:49
Bug,GROOVY-5244,12818137,MarkupBuilderHelper#comment() Javadoc not up to date - wrong info,"The API says:
To create an element with the name 'comment', you need to supply empty attributes, e.g.:

 comment('hello1')
 
or
 mkp.comment('hello1')
 
will produce:
 <!-- hello1 -->
 
while:
 comment('hello2', [:])
 
will produce:
 <comment>hello2</comment>


This doesn't seem to be the case, as the code:

new MarkupBuilder().root {
   mkp.comment('rafal')
}
produces:
<root>
  <comment>cmt</comment>
</root>
as expected. The Javadoc might be a remnant from the past, where this didn't work.",,wujek,Trivial,Closed,Fixed,09/Jan/12 17:21,10/Nov/12 13:02
Bug,GROOVY-5245,12816022,Inconsistency with accessing issers as properties,"In the following snippet, the first two statements work as expected,
but the last one throws a MissingPropertyException.  It seems to me
that no exceptions should be thrown.  This is on Groovy 1.8.4.

{code}
class Isser {
   boolean isWorking() { true }
}
class IsserCat {
   static boolean getWorking2(Isser b) { true }
   static boolean isNotWorking(Isser b) { true }
}

use (IsserCat) {
   println new Isser().working
   println new Isser().working2
   println new Isser().notWorking  // Missing property exception...why?
}

{code}
",emilles,werdna,Major,Closed,Fixed,10/Jan/12 12:00,28/Jul/22 15:24
Bug,GROOVY-5247,12815904,Sorting a map then passing it to JsonBuilder results in a NullPointerException only in 1.8.5,"
{code}
import groovy.json.*

aMap = [a:1, b:2, c:3]
new JsonBuilder(aMap) // works as expected

aMap = [a:1, b:2, c:3].sort()
new JsonBuilder(aMap) // fails with a NullPointerException in 1.8.5. Works as expected in 1.8.4
{code}
",guillaume,fxg,Major,Closed,Fixed,12/Jan/12 09:43,12/Feb/12 04:03
Bug,GROOVY-5248,12818134,call site caching missing null check,"Luke brought GRADLE-2045 to my attention and I was able to reproduce the problem using this program:
{code:Java}
class Cat {
  public static findAll(Integer x, Closure cl) {1}   
}

 def foo(x) {
     x.findAll {}
 }
 
 use (Cat) {
     println foo(1)
     println foo(null)
 }
{code}
It should actually not matter what method or base class the category uses, important is more that a PojoMetaClassSite will be produced. The code here does this because the receiver in foo is the Pojo integer and since categories are active, the normal way of getting a MetaMethod and using that will not be applied. Instead the fallback action with the long path through the MetaClassImpl will be chosen. PojoMetaClassSite has, as all of those, a check for invalidation which includes meta class changes and a check for the receiver being still of the same class. But the class check requires a non-null receiver, which is not the case anymore at the time that foo is called the second time.",blackdrag,blackdrag,Major,Closed,Fixed,12/Jan/12 12:24,13/Jan/12 08:55
Bug,GROOVY-5249,12816086,Avoid unnecessary locking in ClassInfo.getMetaClass,"We have a Grails application serving hundreds of requests per second and this seems to be the most critical hot spot for us. Under high load, most threads are blocked in the following call stack:
{code}
""http-apr-8080""-exec-144
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
org.codehaus.groovy.util.LockableObject.lock(LockableObject.java:34)
org.codehaus.groovy.reflection.ClassInfo.lock(ClassInfo.java:268)
org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:193)
org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:214)
org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:747)
org.codehaus.groovy.runtime.InvokerHelper.invokePojoMethod(InvokerHelper.java:780)
org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:772)
org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToBoolean(DefaultTypeTransformation.java:156)
org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.booleanUnbox(DefaultTypeTransformation.java:65)
{code}

Grails uses {{InvokerHelper}} a lot, which calls {{ClassInfo.getMetaClass}} which uses locking. This is stop-the-world lock affecting all threads (they all hit the same {{ClassInfo}} instance). Note that 99,999% of time the locking is useless as nothing is modified (typically all metaclasses getting modified on startup). 

There are several related tickets: GROOVY-3557 and GROOVY-5059, not really solving the issue.

The solution could be to use more fine-grained locks (ReadWriteLock) or Atomics. Should be easy to implement, but need to isolate modification part from read-only parts.

Doing so can be a good boost to overall Grails performance.",blackdrag,snekoval,Critical,Closed,Fixed,12/Jan/12 15:15,06/Feb/18 11:34
Bug,GROOVY-5256,12814764,Wrong generation of stubs with Generics,"When a Groovy class uses generics and is based on classes from other modules which use generics as well, the compiler generates wrong method signatures in the stub.  When all classes are located in the same module, the stub is generated correctly.

The attached archive contains a Maven2 project with two sub-modules, where module2 contains a Groovy class and depends on module1.  During {{mvn clean install}}, a Groovy stub with the following constructor declaration is generated in {{module2/target}}:
{code}
public Hl7v3ContinuationAwareProducer
(module1.cont.Hl7v3ContinuationAwareEndpoint endpoint, module1.core.JaxWsClientFactory clientFactory) {
super ((module1.core.AbstractWsEndpoint<ComponentType extends module1.core.AbstractWsComponent<java.lang.Object extends java.lang.Object>>)null, (module1.core.JaxWsClientFactory)null, (java.lang.Class<InType>)null, (java.lang.Class<OutType>)null);
}
{code}
",paulk,rnd,Major,Closed,Fixed,19/Jan/12 04:27,13/May/12 03:30
Bug,GROOVY-5257,12815900,Node.ReplaceNode method fails cannot remove itself,"Perhaps this is not a frequent operation, but replaceNode method fails because of a nullpointer exception. The case corresponds to removing a root node from a xml document. This node has no parent (parent==null) and that is the point where the error is triggered. This may be considered a bad programming, but, anyway, the code should return something different from an exception.",paulk,escalope,Minor,Closed,Fixed,21/Jan/12 02:46,12/Feb/12 04:03
Bug,GROOVY-5258,12816034,Static type checker incorrectly handles assignments of variables used as parameters in control structures,"Imagine the following code:
{code}
Double foo(Integer x) { x+1 }
Integer foo(Double x) { x+1 }
def x = 0
for (int i=0;i<10;i++) {
   // there are two possible target methods. This is not a problem for STC, but it is for static compilation
   x = foo(x)
}
{code}

The type checker should not allow this to compile. There are more cases (even more complex) like this in the patch ({{groovy.transform.stc.LoopsSTCTest}}).",melix,melix,Major,Closed,Fixed,23/Jan/12 07:37,13/May/12 03:30
Bug,GROOVY-5259,12816063,VerifyError - cannot access outer class static final fields from an inner class,"Trying to access an outer class 'constant' (i.e. static final) from within an inner class results in a java.lang.VerifyError.  

The problem only manifests if the inner class is extending something; if it is not extending another class, access to the outer class member will work.",melix,rumour,Major,Closed,Fixed,24/Jan/12 12:38,12/Feb/12 04:03
Bug,GROOVY-5260,12818428,Groovyc stub generation doesn't handle all primitive types properly,"The problem manifests when a long value is effectively being changed to an 'int' and results in an exception during compilation.

Example class:
{code}
class AnObject {
    public static final long serialVersionUID = -5239748510188117876L
}
{code}
Using groovy 1.8.5, the generated stub (during joint compilation using the 'Groovyc' ant taskdef) looks like (sans imports):

{code}
public class AnObject implements
    groovy.lang.GroovyObject {
public static final long serialVersionUID = -5239748510188117876;
}
{code}

The problem is that this yields a compilation exception 'integer number too large'.

If you exclude the 'public' keyword on the member definition and re-compile, the stub instead looks like:
{code}
public class AnObject implements
    groovy.lang.GroovyObject {
public static final  long getSerialVersionUID() { return (long)0;}
}
{code}

This can then compile successfully. ",melix,rumour,Major,Closed,Fixed,24/Jan/12 14:17,12/Feb/12 04:03
Bug,GROOVY-5261,12816067,A static method call inside a static closure bypasses closure delegate behavior,"Given the following code:

{code}

class Website {
    Long id
    String url
    Load load
    
    static constraints = {
        Website.load 1
        load nullable:true
        url url:true
    }
    
    static Website load(Long id) {
        Website.get(id)
    }
}

class Load {
    Long id
    Integer connections
}
{code}

The call to:

{code}
load nullable:true
{code}

Will not go through the closure and instead dispatch directly as a static method call. This means it is impossible to control DSL definition. At the AST level the above call seems to be a StaticMethodCallExpression. This is strange because:

{code}
Website.load 1
{code}

Is a normal method call expression. Method calls defined in static closures should never be StaticMethodCallExpression instances otherwise a DSL author cannot control the method dispatch.",blackdrag,graemerocher,Major,Closed,Fixed,25/Jan/12 07:30,01/Jul/14 02:53
Bug,GROOVY-5262,12818135,Problem currying null parameters,"When I apply the spread operator to a List containing a single null and use the result of that as an argument to .curry, an NPE is thrown with 1.8.x but not with 1.7.10.

{noformat:borderStyle=solid|title=script1.groovy}
c = { x ->
    println ""X is ${x}""
}

args = [42]

c = c.curry(*args)

c()
{noformat}

{noformat:borderStyle=solid|title=script2.groovy}
c = { x, y ->
    println ""X is ${x}""
    println ""Y is ${y}""
}

args = [42, 2112]

c = c.curry(*args)

c()
{noformat}

{noformat:borderStyle=solid|title=script3.groovy}
c = { x, y ->
    println ""X is ${x}""
    println ""Y is ${y}""
}

args = [null, null]

c = c.curry(*args)

c()
{noformat}

(This next script is the problematic one...)

{noformat:borderStyle=solid|title=script4.groovy}
c = { x ->
    println ""X is ${x}""
}

args = [null]

c = c.curry(*args)

c()
{noformat}

With Groovy 1.7.10:

{noformat}
curry $ groovy -version
Groovy Version: 1.7.10 JVM: 1.6.0_29
curry $ 
curry $ groovy script1
X is 42
curry $ 
curry $ groovy script2
X is 42
Y is 2112
curry $ 
curry $ groovy script3
X is null
Y is null
curry $ 
curry $ groovy script4
X is null
curry $
{noformat}

With Groovy 1.8.5:

{noformat}
curry $ groovy -version
Groovy Version: 1.8.5 JVM: 1.6.0_29 Vendor: Apple Inc. OS: Mac OS X
curry $ groovy script1
X is 42
curry $ groovy script2
X is 42
Y is 2112
curry $ groovy script3
X is null
Y is null
curry $ groovy script4
Caught: java.lang.NullPointerException
java.lang.NullPointerException
	at script4.run(script4.groovy:7)
{noformat}
",paulk,brownj,Major,Closed,Fixed,25/Jan/12 08:51,12/Feb/12 04:03
Bug,GROOVY-5267,12816097,getting java.lang.VerifyError depending on some simple method content,"This is a very strange issue.  Everything was working but then I added some very basic code in a method and I'm now seeing this issue with 1.7.10, 1.8.0 & 1.8.4:  The openStore() method is throwing java.lang.VerifyError when I activate line 104 below but it works if I comment 104 and activate 103.

I've attached a zip file with lucene jars, two groovy files and one runme.sh script.

Thanks for looking into this!

BTW, I'm seeing the same issue with groovy-2.0.0-beta-2

In LuceneStore.groovy (attached)
{code}
    97          /* FIXME: Bug triggers here
    98             with this line, everything works:
    99                  writer.setRAMBufferSizeMB(400);
   100             with this line, we get the java.lang.VerifyError
   101                 writer.setRAMBufferSizeMB(defaultRamBufferSize());
   102          */       
   103         // writer.setRAMBufferSizeMB(400);
   104         writer.setRAMBufferSizeMB(defaultRamBufferSize());
{code}",melix,jprobichaud,Critical,Closed,Fixed,27/Jan/12 09:53,12/Feb/12 04:03
Bug,GROOVY-5272,12818140,Intermittant/random incorrect resolution of sub-interface constant values,"If a constant in a sub-interface replaces/shadows a constant in a super interface (both Java), when trying to access the sub-interface's constant value from Groovy, the behaviour is intermittantly incorrect - sometimes the sub-interface's value is returned (correct) and sometimes the super interface's value is returned (incorrect).
In Java the sub-interface's value is always returned, as expected.

See source code below that demonstrates this: InterfaceA.java, InterfaceB.java, ShowBugGroovy.groovy, ShowBugJava.java
If ShowBugGroovy is repeatedly run, sometimes the assertion fails, sometimes it passes (randomly?). I believe the behaviour is the same for concrete implementations of the sub-interface.

{code}
package groovybug;

public interface InterfaceA {
    String FOO=""Foo A"";
}
{code}

{code}
package groovybug;

public interface InterfaceB extends InterfaceA {
    String FOO=""Foo B"";
}
{code}

{code}
package groovybug

class ShowBugGroovy {
    static main(args) {
        println(""Interface A: "" + InterfaceA.FOO);
        println(""Interface B: "" + InterfaceB.FOO);

        // Fails randomly
        assert(InterfaceA.FOO!=InterfaceB.FOO)
    }
}
{code}

{code}
package groovybug;

public class ShowBugJava {
    public static void main(String[] args) {
        System.out.println(""Interface A: "" + InterfaceA.FOO);
        System.out.println(""Interface B: "" + InterfaceB.FOO);

        // Always passes
        assert(!InterfaceA.FOO.equals(InterfaceB.FOO));
    }
}
{code}
",melix,ssummer,Major,Closed,Fixed,31/Jan/12 07:54,13/May/12 03:30
Bug,GROOVY-5274,12816096,CLONE - Problem with @InheritConstructors with multiple level of inheritance,"See test eclipse project attached.

- Class Hierarchy : A <-- inherit -- B <-- inherit -- C

- A define constructor public A(String dummy){...}

- B and C use  @InheritConstructors.

- In the main(...) : C.class.newInstance(""tata"")

- Result : Exception in thread ""main"" groovy.lang.GroovyRuntimeException: Could not find matching constructor for: data.C(java.lang.String)
	at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1474)
	at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1390)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeConstructorOf(InvokerHelper.java:824)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.newInstance(DefaultGroovyMethods.java:17689)
	at org.codehaus.groovy.runtime.dgm$511.doMethodInvoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.invoke(StaticMetaMethodSite.java:43)
	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.call(StaticMetaMethodSite.java:88)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at Main.MainLaunch.main(MainLaunch.groovy:7)
",paulk,ronan_michaux,Major,Closed,Fixed,02/Feb/12 03:04,12/Feb/12 04:03
Bug,GROOVY-5277,12816100,SecureASTCustomizer doesn't check class methods,"The ""call"" - method in SecureASTCustomzer doesn't check class methods content
Instead of 
{code}
  BlockStatement bstmt = ast.getStatementBlock();
  bstmt.visit(new SecuringCodeVisitor());
{code}
should be:
{code}
 BlockStatement bstmt = ast.getStatementBlock();
        SecuringCodeVisitor visitor = new SecuringCodeVisitor();
        bstmt.visit(visitor);
        for (ClassNode clNode : ast.getClasses()) {
            for ( MethodNode methodNode : clNode.getMethods()) {
                if (methodNode.getCode() instanceof BlockStatement) {
                    BlockStatement blst = (BlockStatement) methodNode.getCode();
                    blst.visit(visitor);
                }
            }
{code}",melix,michael971,Critical,Closed,Fixed,02/Feb/12 10:16,12/Feb/12 04:03
Bug,GROOVY-5278,12816056,groovy allows top-level classes be marked as private,"Groovy allows to make a class like this: private class X{}.
The result is a top level class, that will be having the private modifier set. But the JVM spec does not include this modifier in the allowed modifiers for classes. Instead this modifier is supposed to be a reserved flag for future use. The violation of this causes internal errors when using indy.",blackdrag,blackdrag,Major,Closed,Fixed,02/Feb/12 14:28,14/Feb/12 07:55
Bug,GROOVY-5279,12816104,groovysh holds a grudge - ,"Make a simple typo (in this case, opening a string with a doublequote and closing it with a singlequote) and groovysh holds it against you for the rest of the session, refusing to do anything except remind you of your mistake:


jasonf-mbp:~ jasonf$ groovysh
Groovy Shell (1.8.5, JVM: 1.6.0_29)
Type 'help' or '\h' for help.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
groovy:000> println ""hello""
hello
===> null
groovy:000> println ""hello'
groovy:001> println ""hello""
ERROR org.codehaus.groovy.control.MultipleCompilationErrorsException:
startup failed:
groovysh_parse: 1: expecting anything but ''\n''; got it anyway @ line 1, column 16.
   println ""hello'
                  ^

1 error

        at java_lang_Runnable$run.call (Unknown Source)
groovy:001> println ""hello""
ERROR org.codehaus.groovy.control.MultipleCompilationErrorsException:
startup failed:
groovysh_parse: 1: expecting anything but ''\n''; got it anyway @ line 1, column 16.
   println ""hello'
                  ^

1 error

        at java_lang_Runnable$run.call (Unknown Source)
groovy:001> 
",guillaume,jfager,Major,Closed,Fixed,03/Feb/12 12:44,12/Feb/12 04:03
Bug,GROOVY-5280,12816058,DOMCategory: NullPointerException in setValue() when element has no text,"NullPointerException at groovy.xml.dom.DOMCategory.setValue(DOMCategory.java:261) if setting value on element that has no text. 

Example is to modify input in the example shown on the ""Updating XML with DOMCategory"" page (http://groovy.codehaus.org/Updating+XML+with+DOMCategory) to include an empty<item/>  element, like this: 

 ...
      <category type=""groceries"">
          <item>Chocolate</item>
          <item>Coffee</item>
          <item/>
      </category>
 ...

 This modification to input causes NullPointerException at this line in the
 example:

          g.value = 'Luxury ' + g.text() ",paulk,dbwhitaker,Minor,Closed,Fixed,04/Feb/12 01:17,12/Feb/12 04:03
Bug,GROOVY-5285,12816105,super.setMetaClass() generates StackOverflowError when there is no base class,"The following code will generate a StackOverflowError. It seems ""setMetaClass(MetaClass)"" is called recursivly, although ""super.setMetaClass(MetaClass)"" is used. Note that this problem only occures when ""Test"" has no base class.

{code}
class Test {
    void setMetaClass(MetaClass metaClass) {
        super.setMetaClass(metaClass)
    }
}

def obj = new Test()
obj.metaClass = obj.metaClass
{code}

On the other hand, the following code works just fine:

{code}
class Base {}

class Test extends Base {
    void setMetaClass(MetaClass metaClass) {
        super.setMetaClass(metaClass)
    }
}

def obj = new Test()
obj.metaClass = obj.metaClass
{code}
",melix,hojerst,Major,Closed,Fixed,07/Feb/12 05:29,13/May/12 03:30
Bug,GROOVY-5286,12818141,Constant pool is used for not final fields too,"As a side effect to GROOVY-5150, the class constant pool is used for non final fields too:

I have this class:
{code}
public class A {
  public static int i = 5;
}
{code}
compile it with javac and you get the following related to i (using javap):

{code}
public static int i;

static {};
  Code:
   Stack=1, Locals=0, Args_size=0
   0:   iconst_5
   1:   putstatic       #2; //Field i:I
   4:   return
{code}

Compile it with groovyc and these days I seem to get this:
{code}
public static int i;
  Constant value: int 5
{code}
Now I thought the 'constant value' attribute for field objects was for proper constants (i.e. final fields), not for initialization values.  That seems to be what it indicates here: 
===
4.7.2 The ConstantValue Attribute

The ConstantValue attribute is a fixed-length attribute used in the attributes table of the field_info (§4.5) structures. A ConstantValue attribute represents the value of a constant field that must be (explicitly or implicitly) static;
===

Indeed javac will only use that attribute if I make the field final.

Now the JVM doesn't seem to be particularly strict here and the static does get the right value initially and allow it to be changed (at least on the VM version I'm using, haven't tried any others).  It just seems to be a slight abuse of the meaning of constant value.  I guess I just wanted to check it was deliberate.

thanks,
Andy
",melix,melix,Minor,Closed,Fixed,08/Feb/12 03:31,12/Feb/12 04:03
Bug,GROOVY-5287,12816094,invalid unboxing in compare and other operations if primopts are enabled,"the program{code:Java}
def proceed(){}
def checkResponse() {return null}

Integer responseCode = checkResponse()
if (responseCode == 200) {1.8.4 running in Grails 2.0.0     
 proceed()
} 
{code}
fails with a NPE in the == operation because it tries to unbox the response code, where it should have instead boxed the constant to do the dynamic compare.",blackdrag,blackdrag,Critical,Closed,Fixed,08/Feb/12 04:12,08/Feb/12 06:17
Bug,GROOVY-5288,12816071,invalid cast of null to int,"the program{code:Java}
ExpandoMetaClass.enableGlobally()
def proceed(){}
def checkResponse() {return null}

Integer responseCode = checkResponse()
if (responseCode == 200) {1.8.4 running in Grails 2.0.0     
 proceed()
}
{code}
will cause a ClassCastException because it tries to case null to in in a Groovy cast. This is due to responseCode being tried to inbox illegally using a GroovyCast. Instead no such cast should happen.",blackdrag,blackdrag,Critical,Closed,Fixed,08/Feb/12 04:15,08/Feb/12 06:17
Bug,GROOVY-5289,12816089,Using GroovyServlet deployed to Tomcat with a multi-level context causes 404 errors,"When deploying an application which use the GroovyServlet to Tomcat with a simple context, there are no problems.  However, when deploying the same application to the same Tomcat instance using a multi-level context a 404 error is returned.

Attaching a sample application which demonstrates this behavior.  Simply run ""mvn package"" to create a WAR from the project.  Take the ""groovyweb.war"" file from the ""target"" directory and deploy it to a Tomcat server.  This will work just fine.  Now take the same WAR file, rename it to ""apps#groovyweb.war"" and deploy it to Tomcat.  Now a 404 is presented and in the log, a message indicates that the groovy script cannot be found.

----------------

Looking into this, it appears that the ""#"" in the name is causing the problem.  In the ""loadScriptName"" method of GroovyScriptEngine, it is calling ""conn.getURL().getPath()"" on the URLConnection to the resource.  Because there is a ""#"" character in the URL, this only returns the part of the URL up to the ""#"" character.

{code:title=GroovyScriptEngine.java|borderStyle=solid}
public Class loadScriptByName(String scriptName) throws ResourceException, ScriptException {
    URLConnection conn = rc.getResourceConnection(scriptName);
    String path = conn.getURL().getPath();
    ScriptCacheEntry entry = scriptCache.get(path);
    Class clazz = null;
    if (entry != null) clazz = entry.scriptClass;
    try {
        if (isSourceNewer(entry)) {
            try {
                String encoding = conn.getContentEncoding() != null ? conn.getContentEncoding() : ""UTF-8"";
                clazz = groovyLoader.parseClass(DefaultGroovyMethods.getText(conn.getInputStream(), encoding), path);
            } catch (IOException e) {
                throw new ResourceException(e);
            }
        }
    } finally {
        forceClose(conn);
    }
    return clazz;
}
{code}",blackdrag,dmikusa,Minor,Closed,Fixed,08/Feb/12 14:18,22/Dec/12 01:10
Bug,GROOVY-5292,12818418,Stub generation doesn't handle 'protected' multi-line String correctly.,"When using the groovyc joint compilation feature, the generated stub is incorrect when the groovy source includes a protected (possibly affects public too, I didn't test) multi-line String 'constant'.

For example,

class Example {
  protected static final String CONSTANT_VALUE = """"""
    I am a constant value
    Thank you
  """"""
} 

would generate a stub:

public Example extends java.lang.Object implements groovy.lang.GroovyObject {
protected static final java.lang.String CONSTANT_VALUE = ""
I am a constant value
Thank you
"";
}

This is a syntactically incorrect Java statement.",paulk,rumour,Major,Closed,Fixed,09/Feb/12 14:51,13/May/12 03:30
Bug,GROOVY-5293,12816036,ASTTransformationVisitor not closing URL streams,"I'm using Groovy as a script language in EJB in Glassfish 3.1 
After using GroovyShell, when i'm shutting down the server i'm getting a lot of 'open stream' errors. 
I've tested this bug with Groovy 1.8.5, 1.9 and 2 beta 2  - still the same

There's a line in ASTTransformationVisitor.java - line 202 in 2.0 beta 2
BufferedReader svcIn = new BufferedReader(new InputStreamReader(service.openStream()));

Looks like that stream is never closed ? 


Below stack trace:

WARNING: Input stream has been finalized or forced closed without being explicitly closed; stream instantiation reported in following stack trace
java.lang.Throwable
	at com.sun.enterprise.loader.ASURLClassLoader$SentinelInputStream.<init>(ASURLClassLoader.java:1230)
	at com.sun.enterprise.loader.ASURLClassLoader$InternalJarURLConnection.getInputStream(ASURLClassLoader.java:1338)
	at java.net.URL.openStream(URL.java:1010)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.doAddGlobalTransforms(ASTTransformationVisitor.java:202)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addGlobalTransforms(ASTTransformationVisitor.java:190)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addPhaseOperations(ASTTransformationVisitor.java:154)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:187)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:118)
	at groovy.lang.GroovyClassLoader.createCompilationUnit(GroovyClassLoader.java:436)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:271)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:638)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:650)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:677)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:668)

",guillaume,piotrek@mente.pl,Major,Closed,Fixed,10/Feb/12 04:11,13/May/12 03:30
Bug,GROOVY-5294,12816118,This reference is null during construction of object of abstract class implemented as closure map,"The following code produces NPE when the second object is created:

{code}
abstract class C {
    public abstract String f(Object o);

    private void g(Object val) { 
        println f(val) // NPE when 'y' is created !!!
    }

    public C() {this('DEF')}

    public C(Object val) { g(val) }
}

class D extends C {
    public String f(Object o) { return o ? 'Not null' : 'Is null' }
}

def x = new D()

def y = [ 'f' : { it -> return '123' + it } ] as C
{code}

Somehow 'this' becomes null when 'f' is invoked in 'g'.
",melix,almo,Minor,Closed,Fixed,10/Feb/12 05:39,13/May/12 03:30
Bug,GROOVY-5295,12816060,Static type checker cannot choose between two covariant return type methods,"The following code fails because {{getMethods}} returns two covariant return type methods:

{code}
StringBuffer sb = new StringBuffer()
sb.append('string')
{code}",melix,melix,Major,Closed,Fixed,10/Feb/12 10:48,13/May/12 03:30
Bug,GROOVY-5303,12816078,"page title on groovydoc index page is ""{todo.title}""",,ldaley,ldaley,Minor,Closed,Fixed,14/Feb/12 03:13,13/May/12 03:30
Bug,GROOVY-5304,12815943,groovydoc output html does not specify character encoding,,paulk,ldaley,Major,Closed,Fixed,14/Feb/12 03:18,13/May/12 03:30
Bug,GROOVY-5308,12818151,Caught: BUG! exception in phase 'conversion' in source unit '${file}' null,"I'm new to Groovy, so I simply stumbled upon the following, which indicated it was a bug. I couldn't find it as an existing issue.

Source (Test.groovy):
foo = new String[] [ ""x"" ]

$ groovy -d Test.groovy
Caught: BUG! exception in phase 'conversion' in source unit '/home/taylor/Test.groovy' null
BUG! exception in phase 'conversion' in source unit '/home/taylor/Test.groovy' null
        at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:847)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:548)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:524)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:501)
        at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:283)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:267)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:197)
        at groovy.lang.GroovyShell$2.run(GroovyShell.java:215)
        at groovy.lang.GroovyShell$2.run(GroovyShell.java:213)
        at java.security.AccessController.doPrivileged(Native Method)
        at groovy.lang.GroovyShell.run(GroovyShell.java:213)
        at groovy.lang.GroovyShell.run(GroovyShell.java:159)
        at groovy.ui.GroovyMain.processOnce(GroovyMain.java:550)
        at groovy.ui.GroovyMain.run(GroovyMain.java:337)
        at groovy.ui.GroovyMain.process(GroovyMain.java:323)
        at groovy.ui.GroovyMain.processArgs(GroovyMain.java:120)
        at groovy.ui.GroovyMain.main(GroovyMain.java:100)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)
Caused by: java.lang.NullPointerException
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:1652)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1636)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1632)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.arraySizeExpression(AntlrParserPlugin.java:2512)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.arraySizeExpression(AntlrParserPlugin.java:2510)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.constructorCallExpression(AntlrParserPlugin.java:2463)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:1673)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1636)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1632)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.binaryExpression(AntlrParserPlugin.java:2265)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:1794)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1636)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1632)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:1655)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1636)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1632)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.statement(AntlrParserPlugin.java:1259)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.convertGroovy(AntlrParserPlugin.java:304)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.buildAST(AntlrParserPlugin.java:241)
        at org.codehaus.groovy.control.SourceUnit.convert(SourceUnit.java:272)
        at org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:622)
        at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:843)
        ... 23 more",paulk,metasyntax,Minor,Closed,Fixed,15/Feb/12 13:55,13/May/12 03:30
Bug,GROOVY-5309,12816114,Wrong line/col info for binary expressions,"{code}
assert annotationsOfIdField[0]    instanceof Id
{code}

The binary expression between the assert and instanceof: the end of the line / col info is right before instance of, instead of being after the last closing square bracket",daniel_sun,guillaume,Minor,Closed,Fixed,16/Feb/12 04:34,06/Mar/18 23:25
Bug,GROOVY-5310,12816037,Wrong line/col info for statements with whitespace before EOL,"If there are spaces at the end of a line statement, the space is also accounted in the lin / col info, instead of being the last (non-whitespace) character of the statement",daniel_sun,guillaume,Minor,Closed,Fixed,16/Feb/12 04:36,06/Mar/18 23:25
Bug,GROOVY-5311,12816117,Wrong line/col info for GStrings,"GStrings have problems with their various sub parts: the constant strings and the variables interpolated.
With this example:
{code}
def gs1 = ""-$a-${b}-${->c}-""
{code}
The first constant string represents ""-$ instead of just the minus sign.
The second and third constant are -$ instead of just the minus sign again.
And the last constant is -"", instead of just the minus sign, it's going one character too far.
Now on to the variables.
Variable a is okay.
Varible b is actually {b} and I think it should be just b
And the closure expression is jut right too.",daniel_sun,guillaume,Minor,Closed,Fixed,16/Feb/12 04:38,06/Mar/18 23:25
Bug,GROOVY-5312,12815988,Wrong line/col info for MapEntryExpressions,"Whenever there's a map entry, a named parameter, etc, a MapEntryExpression is used, but the line / col of MapEntryExpression is just surrounding the column, instead of the whole key + colon + value.

Another problem with MEE is that the value goes up to the next comma or closing square bracket (including all potential whitespace), instead of just stopping at the end of the expression or constant.",daniel_sun,guillaume,Minor,Closed,Fixed,16/Feb/12 04:38,06/Mar/18 23:25
Bug,GROOVY-5313,12816124,Wrong line/col info for SpreadMapExpressions,"{code}
def m = [a  :1  , b: 2 ]
def map = [abc: 1, bcd: 2, *:m , cde: 3]
{code}
The SpreadMapExpression highlights just the star *, and the m variable is highlighted till the comma, instead of just the m character (ie. including further whitespace).",daniel_sun,guillaume,Minor,Closed,Fixed,16/Feb/12 04:39,06/Mar/18 23:25
Bug,GROOVY-5318,12816013,generic types in fully-qualified class names parsing error,"The following code compiles and runs in Groovy 1.8.6...

{code}
def a= new java.util<Integer>.ArrayList<ArrayList<Integer>>()
{code}

The parser allows a generic type after each name in the fully-qualified class name. The generic type should only be after the final name.

The relevant part of the Antlr parser that needs fixing is:

{code}
typeParameters: LT typeParameter (COMMA typeParameter)* (typeArgumentsOrParametersEnd)?
typeParameter: IDENT (typeParameterBounds)?
typeParameterBounds: ""extends"" classOrInterfaceType (BAND! classOrInterfaceType)*
{code}
",daniel_sun,gavingrover,Minor,Closed,Fixed,18/Feb/12 20:49,02/May/17 02:03
Bug,GROOVY-5320,12816079,groovyc ant task cannot compile Servlet using Java EE 6 reference API libaries (results in ClassFormatError Missing Code attribute),"
Compiling a simple servlet against the Java EE 6 reference APIs results in a groovyc compilation failure. I have attached a project with both a Java Servlet and a Groovy Servlet.

The Java compiler works ok but the groovyc fails. Using Maven's eclipse-groovy-plugin also works ok. It seems that the groovyc compiler is introspecting the class in a different way than the maven eclipse-groovy-compiler.

{code}

Buildfile: X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\build.xml

clean:

init:

compile:
    [mkdir] Created dir: X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\build\classes\main
    [javac] Compiling 1 source file to X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\build\classes\main
    [javac] X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\src\main\java\com\acme\test\servlet\JavaServlet.java
  [groovyc] Compiling 1 source file to X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\build\classes\main
  [groovyc] X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\src\main\groovy\com\acme\test\servlet\GroovyServlet.groovy

  [groovyc] >>> a serious error occurred: javax/servlet/ServletException : Missing Code attribute
  [groovyc] >>> stacktrace:
  [groovyc] java.lang.ClassFormatError: javax/servlet/ServletException : Missing Code attribute
  [groovyc]     at java.lang.ClassLoader.defineClass1(Native Method)
  [groovyc]     at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
  [groovyc]     at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
  [groovyc]     at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
  [groovyc]     at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
  [groovyc]     at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
  [groovyc]     at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
  [groovyc]     at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
  [groovyc]     at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
  [groovyc]     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
  [groovyc]     at java.lang.ClassLoader.loadClass(ClassLoader.java:295)
  [groovyc]     at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:696)
  [groovyc]     at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:564)
  [groovyc]     at org.codehaus.groovy.control.ResolveVisitor.resolveToClass(ResolveVisitor.java:709)
  [groovyc]     at org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:275)
  [groovyc]     at org.codehaus.groovy.control.ResolveVisitor.visitClass(ResolveVisitor.java:1241)
  [groovyc]     at org.codehaus.groovy.control.ResolveVisitor.startResolving(ResolveVisitor.java:148)
  [groovyc]     at org.codehaus.groovy.control.CompilationUnit$9.call(CompilationUnit.java:605)
  [groovyc]     at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:843)
  [groovyc]     at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:548)
  [groovyc]     at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
  [groovyc]     at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:480)
  [groovyc]     at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:60)
  [groovyc]     at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:216)
  [groovyc]     at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:149)
  [groovyc]     at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.j
ava:179)
  [groovyc]     at org.codehaus.groovy.ant.FileSystemCompilerFacade.main(FileSystemCompilerFacade.java:27)

BUILD FAILED
X:\workspaces\test\krasmussen\GroovyBug\GroovyBug\build.xml:42: Forked groovyc returned error code: 1
{code}


Maven build

{code}
X:\workspaces\test\krasmussen\GroovyBug>mvn clean compile
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building CPMS Parent Module 3.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ parent-pom ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 0.760s
[INFO] Finished at: Mon Feb 20 14:58:45 CST 2012
[INFO] Final Memory: 28M/64M
[INFO] ------------------------------------------------------------------------
X:\workspaces\test\krasmussen\GroovyBug>mvn clean compile
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building CPMS Parent Module 3.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ groovy-bug ---
[INFO]
[INFO] --- maven-resources-plugin:2.4.3:resources (default-resources) @ groovy-bug ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory X:\workspaces\test\krasmussen\GroovyBug\src\main\resources
[INFO]
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ groovy-bug ---
[INFO] Using Groovy-Eclipse compiler to compile both Java and Groovy files
[INFO] Compiling 2 source files to X:\workspaces\test\krasmussen\GroovyBug\target\classes
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.421s
[INFO] Finished at: Mon Feb 20 14:59:20 CST 2012
[INFO] Final Memory: 15M/64M
[INFO] ------------------------------------------------------------------------
X:\workspaces\test\krasmussen\GroovyBug>
{code}

",,krasmussen,Major,Closed,Fixed,20/Feb/12 15:01,24/Jul/22 15:04
Bug,GROOVY-5321,12818146,GroovyScriptEngine getResourceConnection treats resourceName argument as URI path spec,"If a script name that begins with a leading ""/"" is passed to GroovyScriptEngie.run, then getResourceConnection may fail. The resulting output message will indicate some invalid path that (intuitively) seems like it should not have been constructed by groovy. (This makes it extremely difficult to debug some issues like dynamic script names).

I am currently running embedded groovy running in java application. Problem observed when groovy scripts (.groovy) are stored within a jar file with the application, but the issue is probably not be limited to this case.

Example roots provided to groovy:
""jar:file:/home/user/blah.jar!/org/someFolder"" (which actually contains groovy scripts)
""jar:file:/home/user/blah.jar!/org/someOtherFolder"" (which also contains groovy scripts)

If I incorectly pass ""/coolscript.groovy"" to GroovyScriptEngine.run instead of ""coolscript.groovy"" then I will receive an exception to the effect:
""Cannot open URL: file:/home/user/blah.jar!/coolscript.groovy""

The fix (for user application) is to, of course, remove the leading ""/"". The hope is that groovy can indicate that it is not a well formed name for a script, and throw some more meaningful exception. The alternative of continuing to use the name when it is not well formed (as though it was a path), leaves the possibility of obscure bugs.

This is caused by the constructor for URL used in GroovyScriptEngine.getResourceConnection as described in the following javadoc:
http://docs.oracle.com/javase/6/docs/api/java/net/URL.html#URL(java.net.URL, java.lang.String)",blackdrag,denvercoder9,Minor,Closed,Fixed,21/Feb/12 13:12,11/Sep/12 01:15
Bug,GROOVY-5322,12816128,MinimumRecompilationInterval is not used as an interval in GroovyScriptEngine,"Example:

If I set minimumRecompilationInteval to 60 seconds, GroovyScriptEngine will load the script on first access (as expected).
It will then skip checking the script for modifications for 60 seconds (as expected).

After the 60 seconds has passed it will check the the script for modifications EVERY time it's accessed.
I would expect that the source file would be left alone for another 60 seconds after it has been checked once again.

Is the current behaviour correct, or is it a bug?",blackdrag,mattiasr,Major,Closed,Fixed,22/Feb/12 07:55,11/Sep/12 01:15
Bug,GROOVY-5323,12816135,JsonOutput.prettyPrint breaks escaped special characters,"I suppose that json text filtered with groovy.json.JsonOutput.prettyPrint should produce again valid JSON string. Try this example/testcase and see how prettyPrint breaks escaped characters:

{code}
String json = new groovy.json.JsonBuilder('a': 'x""y').toString()
String pretty = groovy.json.JsonOutput.prettyPrint(json)

// comment this println in final test case
println json
println pretty

def slurper = new groovy.json.JsonSlurper()
// this is OK in 1.8.6
try {
  def test1 = slurper.parseText(json)
  assert 'x""y' == test1.a
} catch(Exception e) {
  assert false, ""parsing generated JSON failed""
}
// this fails in 1.8.6
try {
  def test = slurper.parseText(pretty)
  println test.a
} catch(Exception e) {
  assert false, ""parsing pretty printed JSON failed""
}
{code}",guillaume,jakub.neubauer,Major,Closed,Fixed,23/Feb/12 06:07,13/May/12 03:30
Bug,GROOVY-5330,12816035,Typo in exception message at JsonSlurper.parseObject(),"groovy.json.JsonException: Expected a string key on line: 1, column: 2. Bug got '42' instead.
at groovy.json.JsonSlurper.parseObject(JsonSlurper.java:187)

Message is misspelled: ""Bug"" should be ""But"".",guillaume,goeh,Trivial,Closed,Fixed,24/Feb/12 12:40,13/May/12 03:30
Bug,GROOVY-5332,12816130,"Map<String,Integer> is recognized as Map<String,String> by the type checker","The following code fails:
{code}
Map<String,Integer> foo = new HashMap<String,Integer>()
foo.put('foo', 1)
{code}

because the type checker aligns the V placeholder in <K,V> with {{String}} instead of {{Integer}}.",melix,melix,Critical,Closed,Fixed,27/Feb/12 06:50,13/May/12 03:30
Bug,GROOVY-5339,12816154,groovy.sql.Sql.executeQuery(String) should be non-final for extension,"The method ""protected groovy.sql.Sql.executeQuery(String sql) throws SQLException"" is intended for extensibility and would indeed be a useful extension point, but marked as ""final"".
From the method's JavaDoc: ""Hook to allow derived classes to access ResultSet returned from query.""",paulk,spannjp,Minor,Closed,Fixed,01/Mar/12 02:53,13/May/12 03:30
Bug,GROOVY-5340,12818160,AbstractQueryCommand constructor should be protected for subclassing,"To customize the behavior of groovy.sql.Sql we override ""protected groovy.sql.Sql.createQueryCommand(String)"". Next, we attempt to implement the inner class groovy.sql.Sql.AbstractQueryCommand. Its only constructor ""AbstractQueryCommand(String sql)"" is package-protected. therefore subclassing is impossible unless we put our code into package ""groovy.sql"" which is against best practice.

As a side note, AbstractQueryCommand is non-static, therefore requires an instance of groovy.sql.Sql for instantiation. 
I guess one would only want to subclass AbstractQueryCommand after subclassing Sql first, so this won't be an issue. Still, an instance of Sql could be provided on the constructor explicitly and let the inner class be static.",paulk,spannjp,Minor,Closed,Fixed,01/Mar/12 03:55,13/May/12 03:30
Bug,GROOVY-5349,12816145,ICO file in distribution is broken/missing,"The groovy.ico file in the distributions (source zip, binary zip and also Windows installer) is broken.
The one from https://fisheye.codehaus.org/browse/~raw,r=9382/groovy/trunk/groovy/groovy-core/src/tools/org/codehaus/groovy/tools/groovy.ico is fine though.
Comparing a hexdump of both files it seems there are Unicode replace characters inserted.
I guess you use a <copy> task with nested <filterset> or with some implicit filter from a stand-alone <filter> task to copy that file. This is not according to the <copy> tasks documentation, which states that filter may only be used with text files and will destroy binary files.

{quote}
Note: If you employ filters in your copy operation, you should limit the copy to text files. Binary files will be corrupted by the copy operation. This applies whether the filters are implicitly defined by the filter task or explicitly provided to the copy operation as filtersets. See encoding note. 
{quote}",pschumacher,vampire,Trivial,Closed,Fixed,07/Mar/12 07:57,14/Jun/13 05:10
Bug,GROOVY-5356,12817866,When using Grab concurrently the underlying ivy library can lock a metadata file causing grab failure,"This doesn't occur very often - seems to be a problem on heavily used slow machines, e.g. CI servers. When the issue does occur, there will be a FileNotFoundException (Access is denied) as one process has ""~/.groovy/grapes/resolved-caller-all-caller-working.properties"" locked when the other also tries to write to it.

Proposal is to add a couple of digits from System.currentTimeMillis into the name of the metadata file plus also wrap the call to Ivy with a MAX_TRIES wait loop to allow the first Grab to finish - to handle the rare case where two process attempt a grab within the same millisecond.",paulk,paulk,Minor,Closed,Fixed,08/Mar/12 16:20,14/Jul/15 10:46
Bug,GROOVY-5358,12816172,Different ways of extending a class functionality esp. adding getProperty inconsistent,"When getProperty is used e.g., to mimic a Map behaviour, the result differs depending on whether getProperty was defined directly in the class, added through a mixin, or through a metaclass.

{code}
class FooWorksAsMap { // class names the same length as LinkedHashMap to make output pretty
 def getProperty(String foo) { ""OK:FooWorksAsMap.$foo"" }
}
class BarWorksAsMap {}
class BaxWorksAsMap {}
@Category(BarWorksAsMap) class C {
 def getProperty(String foo) { ""OK:BarWorksAsMap.$foo"" }
}
BarWorksAsMap.mixin C
BaxWorksAsMap.metaClass.getProperty = { foo -> ""OK:BaxWorksAsMap.$foo"" }
def maps = [new FooWorksAsMap(), new BarWorksAsMap(), new BaxWorksAsMap(),
           [foo:'OK:LinkedHashMap.foo', class:'OK:LinkedHashMap.class']]
for (def prop in ['foo','class']) {
 for (def m in maps) {
   def op = ""${m.getClass().getSimpleName()}.$prop""
   try { println ""$op -> "" + m.""$prop"" }
   catch (t) { println ""$op -> FAIL:$t"" }
 }
}
{code}",emilles,oc,Minor,Closed,Fixed,11/Mar/12 15:19,03/Feb/22 22:35
Bug,GROOVY-5361,12816179,XmlParser and XmlSlurper should support XML Schema validation,"XmlSlurper and XmlParser both support a validation capability, but this only works for DTDs. It should work for XMLSchemas as well.",paulk,russel,Major,Closed,Fixed,12/Mar/12 12:14,22/Jan/13 16:33
Bug,GROOVY-5364,12816044,Static property reference from static context in script does not compile,"The following script:
{code}
static getStaticProperty() {}
static staticMethod() { staticProperty }
{code}

Does not compile and produces the following error:
{noformat}
You misspelled a classname or statically imported field. Please check the spelling.
You attempted to use a method 'staticProperty' but left out brackets in a place not allowed by the grammar.
 @ line 4, column 25.
   static staticMethod() { staticProperty }
                           ^

1 error
{noformat}",emilles,btiernay,Major,Closed,Fixed,13/Mar/12 19:10,18/Feb/22 22:14
Bug,GROOVY-5366,12816121,unable to add assertions via compile time transform,See: http://groovy.329449.n5.nabble.com/Problem-with-implicit-assertion-transform-td5568080.html,paulk,ldaley,Major,Closed,Fixed,15/Mar/12 09:46,18/Jan/13 16:06
Bug,GROOVY-5367,12816161,Closure property in Binding used as Closure delegate cannot be called,"In a script, this works:

c = { println 'works' }
c()

whereas this one doesn't:

def bind = new Binding(c: { println 'does not work' })
def t = { c() }
t.delegate = bind
t()

They both define a 'c' property in the binding which happens to be a closure. In the first case, the closure get's called as Groovy apparently tries to get the binding property and invoke it if it's a closure; in the latter, the delegate of the closure is a binding with a property 'c' that is a closure, but c() still doesn't work.

I debugged this and there is actually an if in MetaClassImpl that checks if the 'this' object is a Script, and if so, a property with the name is retrieved and a call() method is called. (MetaClassImpl#invokePropertyOrMissing:1089 in Groovy 1.8.6-all from maven central).",,wujek,Major,Closed,Fixed,15/Mar/12 15:03,29/Jan/22 18:39
Bug,GROOVY-5371,12818154,Sql DataSet fails to work with non-literals in queries (fix error message/doco),"All the examples of using _findAll_ in the _Sql_ _DataSet_ class use literals for the search values of queries. Using free variables causes failure as Groovy does not implement lexical closure automatically.  However this can be realized using the Closure delegate field. I therefore believe that the following example fails because the _Sql.SqlWhereVisitor_ fails to lookup variables but assumes that all query values are literals.
{code}
import groovy.sql.DataSet
import groovy.sql.Sql

@Grapes ( [    
            @Grab ( 'org.xerial:sqlite-jdbc:3.7.2' ),
            @GrabConfig ( systemClassLoader = true )
          ] )
def database
final words = [ ]
try {
  database = Sql.newInstance ( 'jdbc:sqlite:database.db' , 'org.sqlite.JDBC')
  final wordsTable = new DataSet ( database , 'words' )
  ( 0 ..< 4 ).each { i ->
    final query = { item -> item.id == i }
    query.delegate = { i : i }
    query.resolveStrategy = Closure.DELEGATE_FIRST 
    words << wordsTable.findAll ( query ).firstRow ( ).word
  }
}
finally {
  database?.close ( )
}
println words.join ( '' )
{code}",paulk,russel,Critical,Closed,Fixed,17/Mar/12 06:24,23/Mar/12 02:47
Bug,GROOVY-5374,12816065,@TypeChecked does not recognize @Log annotation,"@TypeChecked rejects code that uses the @Log annotation. It should not. 


This code does not compile but should: 
{code}

import groovy.util.logging.Log
import groovy.transform.TypeChecked

@Log
class Example1 {

    @TypeChecked
    void method(String message) {
      log.info(message)
    }
}
{code}",melix,hamletdrc,Major,Closed,Fixed,20/Mar/12 04:29,15/Nov/12 00:21
Bug,GROOVY-5377,12816166,CLONE - Incomprehensible Error Message Passing Partial Evaluated Lambda Function (improved error message for normal closure not on classpath case),"When executing the code:
{code}
#! /usr/bin/env groovy

import groovy.sql.DataSet
import groovy.sql.Sql

@Grab ( 'org.xerial:sqlite-jdbc:3.7.2' )
@GrabConfig ( systemClassLoader = true )
def database
final words = [ ]
try {
  database = Sql.newInstance ( 'jdbc:sqlite:database.db' , 'org.sqlite.JDBC' )
  final wordsTable = new DataSet ( database , 'words' )
  ( 0 ..< 4 ).each { i ->
    words << wordsTable.findAll ( { j , item -> item.id == j }.curry ( i ) ).firstRow ( ).word
  }
}
finally {
  database?.close ( )
}
println words.join ( '' )
{code}
the result is the error:
{quote}
Caught: groovy.lang.GroovyRuntimeException: Could not find the ClassNode for MetaClass: groovy.lang.MetaClassImpl@6c1826dc[class org.codehaus.groovy.runtime.CurriedClosure]
groovy.lang.GroovyRuntimeException: Could not find the ClassNode for MetaClass: groovy.lang.MetaClassImpl@6c1826dc[class org.codehaus.groovy.runtime.CurriedClosure]
{quote}
which doesn't really tell the programmer anything useful about the executed code they wrote.
",paulk,russel,Major,Closed,Fixed,21/Mar/12 03:00,04/May/12 23:58
Bug,GROOVY-5383,12816120,@ListenerList changes the ArrayList classnode generics,"The {{@ListenerList}} AST transformation changes the generic types associated to the cached ArrayList class node. This is wrong because it has side effects on the subsequent compîlations.

This

{code}
def listenerListType = ClassHelper.make(ArrayList)
listenerListType.setGenericsTypes(types)
{code}

must be replaced with:

{code}
def listenerListType = ClassHelper.make(ArrayList).plainNodeReference
listenerListType.setGenericsTypes(types)
{code}",melix,melix,Critical,Closed,Fixed,28/Mar/12 14:14,13/May/12 03:30
Bug,GROOVY-5384,12811949,Type checker doesn't infer generics properly,"In the following code, the computed lowest upper bound of the list doesn't have the correct type.

{code}
class Test {
    static test2() {
        if (new Random().nextBoolean()) {
            def a = new ArrayList<String>()
            a << ""a"" << ""b"" << ""c""
            return a
        } else {
            def b = new LinkedList<Number>()
            b << 1 << 2 << 3
            return b
        }
    }

    static test() {
        def result = test2()
        result[0].toInteger()
        //result[0].toString()
    }
}
new Test()
{code}",melix,melix,Major,Closed,Fixed,28/Mar/12 14:18,13/May/12 03:30
Bug,GROOVY-5385,12818155,"StackOverflow Exception when calling super.method, which returns an instance of the corresponding class. ","You get a StackOverflow Exception when running following code:

{code:title=A.groovy|borderStyle=solid}
abstract class A {
    A attach(){
        System.out.println( ""A does something else"");
        return this;
    }
}
{code}
{code:title=B.groovy|borderStyle=solid}
class B extends A {
    @Override
    B attach(){
        System.out.println(""B does something else"");
        return (B) super.attach();
    }
}
{code}
{code:title=C.groovy|borderStyle=solid}
class C extends B {
    @Override
    C attach(){
        System.out.println(""C does something else"");
        return (C) super.attach();
    }
}
{code}

{code:title=Test.groovy|borderStyle=solid}
C c = new C(); c.attach();
{code}

This bug only occurs when the depth of the class hierarchy exceeds two classes.
Moreover this bug seems depend on the method name! If you try {{attachMe()}} as method name the Stackoverflow Exception won't be raised.
The whole ErrorOutput can be found attached.

{code:title=Error.output|borderStyle=solid}
C does something else
C does something else
C does something else
C does something else
Exception in thread ""main"" java.lang.StackOverflowError
	at groovy.lang.MetaClassImpl.getMetaProperty(MetaClassImpl.java:2441)
	at groovy.lang.MetaClassImpl.getMetaProperty(MetaClassImpl.java:2415)
	at groovy.lang.MetaClassImpl.getProperty(MetaClassImpl.java:1510)
	at groovy.lang.MetaClassImpl.getProperty(MetaClassImpl.java:3308)
	at org.codehaus.groovy.runtime.callsite.ClassMetaClassGetPropertySite.getProperty(ClassMetaClassGetPropertySite.java:48)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGetProperty(AbstractCallSite.java:227)
	at C.attach(C.groovy:20)
	at C.attach(C.groovy)
	at B.attach(B.groovy)
	at C.super$3$attach(C.groovy)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1047)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuperN(ScriptBytecodeAdapter.java:128)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuper0(ScriptBytecodeAdapter.java:148)
{code}




",blackdrag,stefan_leo,Major,Closed,Fixed,29/Mar/12 05:51,15/Oct/15 18:19
Bug,GROOVY-5386,12816202,@Delegate - sometimes compilation fails for generic types,"we have some classes using @Delegate on generic classes:

@Commons
class FilterList {

@Delegate List<Map<Dimension, List<String>>> filterList = []

...
}

Sometime compilation fails with compiler errors like 

BUG! exception in phase 'class generation' in source unit '/Users/andre/Development/Projects/Adternity/grf/grails-app/jobs/com/grf/reports/ScheduledReportExecutionJob.groovy' Type is null. Most probably you let a transform reuse existing ClassNodes with generics information, that is now used in a wrong context. (Use --stacktrace to see the full trace)

I assume this has to do with DelegateASTTransformation#nonGeneric handling of generic types, which is setting the generic type to null, but it's hard to reproduce the error in a determined way.",blackdrag,andre.steingress,Major,Closed,Fixed,29/Mar/12 05:55,28/May/14 03:04
Bug,GROOVY-5388,12814822,Website code samples do not display,"*I* would rate this as MAJOR/BLOCKER.

In IE8 (either plain or compatibility modes) and Firefox 7.0.1, when I browse to ANY page with code examples, I see no code. 

See attached.

I can't even copy/paste hidden text. The code IS in the source...just not shown. 

I note that the code blocks are wrapped in <script type=""syntaxhilighter""> tags so a javascript bug?

It was pointed out to me when I sent a colleague to look at:

http://groovy.codehaus.org/Reading+XML+using+Groovy's+XmlSlurper

With IE8 this is an issue for nearly all code examples on the site...except on the front page, which are formatted differently!

In Recent FF/Safari 5/IE 9.0.8xxx (native mode) all is OK. But of course, IE8 (recently updated from IE6) is the corporate standard for the organisation I am currently at...

Thus my comment about the bug being major/blocker priority. As my colleagues throw at me: ""Why adopt a technology from a group that can't even make a working website?"" Not my words. And yes, harsh. Makes my evangelising rather hard/futile.

Thought I'd seen this happen before...in the early days of the last site makover. Thought it was fixed? my recollection says that it is something that the mysterious ""Codehaus Site Administrators"" have to take a look at?

I posted at http://groovy.329449.n5.nabble.com/Website-code-samples-missing-td5602131.html but heard nothing, hence opened this JIRA.

Cheers,

BOB

",guillaume,alpheratz,Blocker,Closed,Fixed,31/Mar/12 18:41,13/May/12 03:30
Bug,GROOVY-5394,12816157,Grape's Grab ignores 'ext' when classifier is not set,"The value of ext (and also type) in Grab is only taken into account if classifier is set, I believe this to be a mistake. For example Grape cannot be used to retrieve the following artifact, which has packaging=orbit but an artifact with the extension .jar:
{code}
@Grab(group='org.eclipse.jetty.orbit', module='javax.servlet', version='3.0.0.v201112011016', ext='jar')
{code}
Therefore Grab should respect the value of ext even if classifier is not set. (And in any case it should at least report that there are some arguments that are ignored.)

Steps to reproduce:
{code}
$ groovysh
groovy:000> org.apache.ivy.util.Message.setDefaultLogger(new org.apache.ivy.util.DefaultMessageLogger(org.apache.ivy.util.Message.MSG_DEBUG));
groovy:000> groovy.grape.Grape.grab(autoDownload: true, group: 'org.eclipse.jetty.orbit', module: 'javax.servlet', version: '3.0.0.v201112011016', ext: 'jar')
{code}
See https://github.com/groovy/groovy-core/blob/master/src/main/groovy/grape/GrapeIvy.groovy#L298",paulk,malyvelky,Major,Closed,Fixed,02/Apr/12 16:08,19/Sep/12 04:40
Bug,GROOVY-5396,12818157,Groovy classes can't see package-local properties from a super class,"If I have a standard gradle project setup (attached):

{code}
build.gradle
src/
  main/
    java/
      test/
        AbstractBase.java
        SamePackageJavaBase.java
    groovy/
      test/
        GroovyBase.groovy
        GroovyBaseWithSuper.groovy
{code}

I have a single Abstract base class:

{code:title=AbstractBase.java}
package test ;

public abstract class AbstractBase {
  int base ;

  public AbstractBase( int base ) {
    this.base = base ;
  }

  public abstract int mult( int n ) ;
}
{code}

And a class {{GroovyBase}} that extends this class:

{code:title=GroovyBase.groovy}
package test

public class GroovyBase extends AbstractBase {
  public GroovyBase( int n ) {
    super( n )
  }

  public int mult( int n ) {
    n * base
  }

  static main( args ) {
    println new GroovyBase( 10 ).mult( 3 )
  }
}
{code}

When running this (using the Gradle script in the attachment), I get the exception:

{code}
Exception in thread ""main"" groovy.lang.MissingPropertyException: No such property: base for class: test.GroovyBase
Possible solutions: class
{code}

Changing the line
{code}
    n * base
{code}

to

{code}
    n * super.base
{code}

Or making the {{base}} field public in the {{AbstractBase}} class makes it work.

It's as if the classes are considered to be in different packages for some things, but not for others.

Thinking about it, not sure if this is a Gradle bug or a Groovy cross compiler one.

To run the tests, unpack the attachment, and run:

{code}
# Test the above failing example
gradle -Pmain=test.GroovyBase
# Test the addition of super.base
gradle -Pmain=test.GroovyBaseWithSuper
# Test the Java extension of the Abstract class
gradle -Pmain=test.SamePackageJavaBase
{code}

You can change the groovy version from 1.8.6 by passing (for example) {{-Pgroovy=groovy=2.0.0-beta-2}}",paulk,tim_yates,Major,Closed,Fixed,05/Apr/12 04:54,01/Feb/17 23:18
Bug,GROOVY-5411,12816207,Type checker doesn't seem to realise a non-static method won't satisfy a call from a static context,"Just got the 2.0 compiler into groovy-eclipse, so testing things out:

{code}
import groovy.transform.TypeChecked;


@TypeChecked
class TC {

	public static void main(String[] args) {
		foo();
	}
	
	public void foo() {}
}

{code}

No type checking error is reported here - I presume there should be one? (Since foo is not static)",melix,aclement,Blocker,Closed,Fixed,17/Apr/12 14:43,13/May/12 03:30
Bug,GROOVY-5413,12816222,groovyc should emit errors when @Override is not respected,"The following compiles without warning or error:

{code}
class OverrideAnnotatationTest {
    @Override
    def notOverriden() {}
}
{code}",paulk,btiernay,Major,Closed,Fixed,20/Apr/12 19:51,09/Apr/14 18:40
Bug,GROOVY-5415,12816123,Static Type checker reporting invalid error,"{code:title=ClassA.java}

public class ClassA<T> {
    <X> Class<X> foo(Class<X> classType){
        return classType;
    }
}
{code}

{code:title=ClassB.groovy}

import groovy.transform.CompileStatic

@CompileStatic
class ClassB {
    void bar() {
        def ClassA<Long> a = new ClassA<Long>();
        a.foo(this.getClass());
    }
}
{code}

  ClassB.groovy: 13: [Static type checking] - Cannot find matching method ClassA#foo(java.lang.Class <java.lang.Object extends java.lang.Object>)
 @ line 13, column 9.
             a.foo(this.getClass());
             ^










",melix,mmrath,Critical,Closed,Fixed,21/Apr/12 18:53,13/May/12 03:30
Bug,GROOVY-5416,12816168,Transform loaded by wrong class loader when @GroovyASTTransformationClass(classes = ...) is used,"When @GroovyASTTransformationClass(classes = ...) is used, the transform will be loaded by CompilationUnit.classLoader rather than CompilationUnit.transformLoader. This may cause the transform not to work in environments (like Gradle) where transformLoader != classLoader.

Pull request will follow.",melix,pniederw,Major,Closed,Fixed,21/Apr/12 22:09,13/May/12 03:32
Bug,GROOVY-5418,12818456,Compilation error with covariant return types (incompatible types),"Given the following files:

{code:title=pkg/Base.java}
package pkg;
public interface Base {
    Base doSomething();
}
{code}

{code:title=pkg/Child.java}
package pkg;
public interface Child extends Base {
    Child doSomething();
}
{code}

{code:title=pkg/GChild.groovy}
package pkg;
class GChild implements Child {
    Child doSomething() {
        return this
    }
}
{code}

When I try to compile this, groovyc complains about incompatible return types:

{noformat}
$ javac pkg/*.java
$ groovyc pkg/*.groovy
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
pkg/GChild.groovy: 3: The return type of pkg.Child doSomething() in pkg.GChild is incompatible with pkg.Base doSomething() in pkg.Base
. At [3:2]  @ line 3, column 2.
        Child doSomething() {
    ^

1 error

$ javac -version
javac 1.6.0_31
$ groovyc -version
Groovy compiler version 1.8.6
Copyright 2003-2011 The Codehaus. http://groovy.codehaus.org/

$
{noformat}

A very similar (almost identical) issue has already been reported here: https://jira.codehaus.org/browse/GROOVY-2829 (marked as fixed in 1.5.7 and 1.6-beta2)",melix,rherzog,Major,Closed,Fixed,25/Apr/12 05:11,21/Sep/12 16:58
Bug,GROOVY-5423,12818163,Wrong Method Being Invoked - Related To Static Imports,"When a static import brings in a method which conflicts with a method defined in a class' super class Java invokes the method from the super class but Groovy invokes the method from the static import.

{code:title=src/main/java/com/demo/Utils.java|borderStyle=solid}
package com.demo;

public class Utils {
    
    public static String getValue() {
        return ""Utils.getValue"";
    }
}
{code}

{code:title=src/main/java/com/demo/MyBaseClass.java|borderStyle=solid}
package com.demo;

public class MyBaseClass {
    
    public static String getValue() {
        return ""MyBaseClass.getValue"";
    }
}
{code}

{code:title=src/main/java/com/demo/JavaSubClass.java|borderStyle=solid}
package com.demo;

import static com.demo.Utils.*;

public class JavaSubClass extends MyBaseClass {
    
    public String retrieveValue() {
        return getValue();
    }
}
{code}

{code:title=src/main/groovy/com/demo/GroovySubClass.groovy|borderStyle=solid}
package com.demo

import static com.demo.Utils.*

class GroovySubClass extends MyBaseClass {
    
    String retrieveValue() {
        getValue()
    }
}
{code}


{code:title=src/test/groovy/com/demo/ImportTests.groovy|borderStyle=solid}
package com.demo

class ImportTests extends GroovyTestCase {
    
    void testJava() {
        def subClass = new JavaSubClass()
        def result = subClass.retrieveValue()
        assertEquals 'MyBaseClass.getValue', result
    }
    
    void testGroovy() {
        def subClass = new GroovySubClass()
        def result = subClass.retrieveValue()
        assertEquals 'MyBaseClass.getValue', result
    }
}
{code}

The Java test passes but the Groovy test fails.

To run the code download the attached project (staticimport.zip) and run ""./gradlew test"".",emilles,brownj,Major,Closed,Fixed,26/Apr/12 09:17,03/Feb/22 22:51
Bug,GROOVY-5425,12816134,ObjectRange doesn't have BigInteger optimisation for size(),"The ObjectRange.size() method has optimisations for BigDecimal, Integer and Long, but not BigInteger

https://github.com/groovy/groovy-core/blob/master/src/main/groovy/lang/ObjectRange.java#L262

This means that calling:

{code}
(1G..2147483647G).size()
{code}

Takes ages

Could we change:

{code}
} else if (from instanceof BigInteger || to instanceof BigInteger) {
{code}

to

{code}
} else if (from instanceof BigDecimal ||
           to   instanceof BigDecimal ||
           from instanceof BigInteger ||
           to   instanceof BigInteger ) {
{code}

To catch this?",tim_yates,tim_yates,Major,Closed,Fixed,27/Apr/12 10:45,16/Feb/13 00:47
Bug,GROOVY-5428,12816228,Performance problem: ClassCastExceptions are created in normal execution flow which kills performance,"This problem is a major performance bottleneck in Grails 2.0 applications. 
Currently this problem shows up when resources plugin has been installed and used in a Grails app. (see attached screenshot)

Exceptions are relatively heavy weight in Java. Filling the stacktrace takes most of the time. Normal execution flow should never throw exceptions.
(it is possible to disable filling the stacktrace like has been done for MissingMethodExceptionNoStack, in that case the performance overhead is usually acceptable.)

The problem shows up whenever there is 2 classes sharing the same base class and when the other class is a Java class and the other one is a Groovy class (GroovyObject). 

I was able to reproduce the problem with this code. Run this code in a debugger and make a breakpoint for the construction of ClassCastException.
In this example the base class is java.util.Map , the Java class is LinkedHashMap and the Groovy class is MyMap (extends LinkedHashMap).

{code}
class ClassCastProblem {
    def doCalls() {
        def m1=[abc:'test']
        def m2=new MyMap()
        doSomething(m1)
        doSomething(m2)
        doSomething(m1)
        doSomething(m2)
        doSomething(m1)
    }
    
    void doSomething(m) {
        m.remove('test')
    }

    public static void main(String[] args) {
        def ccp=new ClassCastProblem()
        ccp.doCalls()
    }
}

class MyMap extends LinkedHashMap {
    
}
{code}

In a Grails app the 2 different java.util.Map implementations usually triggering this problem are LinkedHashMap and GrailsParameterMap (GroovyObject).

In Grails core I've worked around some of the problem with this type of hack:
https://github.com/grails/grails-core/commit/62626921ebd80e3cdeb9776fd79be7eb92f88763
At that time I didn't have time to investigate the reason that caused the problem and just made some changes until the problem went away.

",blackdrag,lhotari,Major,Closed,Fixed,28/Apr/12 00:55,04/May/12 10:47
Bug,GROOVY-5433,12816169,Node is missing documentation,"The Groovy class Node is missing a lot of documentation:

http://groovy.codehaus.org/api/groovy/util/Node.html

Take, for example, the parent() method. What does it return for the root Node? Is it null or this?",paulk,balor123,Major,Closed,Fixed,02/May/12 08:40,22/Dec/12 01:10
Bug,GROOVY-5439,12816171,error: incompatible types on Java stubs returning generics,"The following class definition fails to compile the Java stubs:
{code:java}
class GenericReturn {
    public <T extends List> T foo() {
        null
    }
}
{code}

Error:
{noformat}
tmp/groovy-java-stubs/GenericReturn.java:16: error: incompatible types
public <T extends java.util.List> T foo() { return (java.util.List)null;}
                                                   ^
  required: T
  found:    List
  where T is a type-variable:
    T extends List declared in method <T>foo()

{noformat}

It compiles with no errors on 2.0-beta-2 and 1.8.6",guillaume,rgarcia,Blocker,Closed,Fixed,03/May/12 17:32,21/Sep/12 16:58
Bug,GROOVY-5440,12816620,using dataset queries not supported in groovyConsole,"Running the script it is ok, but executing in groovyConsole not.
{code}
def cc = new DataSet(sql, 'cc')
cc.each{println(it)} // works fine
cc.findAll{ it.prg_ordine > 0 }.findAll{it.prg_ordine > 0} // works fine
cc.findAll{ it.prg_ordine > 0 }.each{println(it)} throws 
{code}
{noformat}
Exception thrown
May 04, 2012 10:22:24 AM org.codehaus.groovy.runtime.StackTraceUtils sanitize
WARNING: Sanitizing stacktrace:
groovy.lang.GroovyRuntimeException: Could not find the ClassNode for MetaClass: org.codehaus.groovy.runtime.metaclass.ClosureMetaClass@118a6e8[class ConsoleScript2$_run_closure1]
	at groovy.sql.DataSet.visit(DataSet.java:278)
	at groovy.sql.DataSet.getSqlWhereVisitor(DataSet.java:261)
	at groovy.sql.DataSet.getSqlWhere(DataSet.java:208)
	at groovy.sql.DataSet.getSql(DataSet.java:235)
	at groovy.sql.DataSet.each(DataSet.java:198)
	at groovy.sql.DataSet$each.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at ConsoleScript2.run(ConsoleScript2:7)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:266)
	at groovy.lang.GroovyShell.run(GroovyShell.java:517)
	at groovy.lang.GroovyShell.run(GroovyShell.java:172)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:924)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:884)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:884)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:722)

groovy.lang.GroovyRuntimeException: Could not find the ClassNode for MetaClass: org.codehaus.groovy.runtime.metaclass.ClosureMetaClass@118a6e8[class ConsoleScript2$_run_closure1]
	at ConsoleScript2.run(ConsoleScript2:7)
{noformat}
",,gon,Major,Closed,Fixed,04/May/12 03:29,08/Nov/13 10:53
Bug,GROOVY-5443,12816194,Static type checking not working for fields assigment,"The following classes should fail to compile:

{code:java} 
@TypeChecked
class StaticGroovy1 {
  Date foo = """"
}
{code}
{code:java}
@TypeChecked
class StaticGroovy2 {
    Closure<List> cls = { Date aDate ->  aDate.getTime() }
}
{code}
{code:java}
@TypeChecked
class StaticGroovy3 {
    
    static Closure<Long> cls = { Date aDate ->  aDate.getTime() }
    
    def bar() {
        cls("""")
    }
}
{code}

[See discussion associated to this bug|http://groovy.329449.n5.nabble.com/TypeChecked-and-StaticCompile-usage-td5692547.html]



",melix,rgarcia,Major,Closed,Fixed,08/May/12 16:33,15/Jun/12 22:56
Bug,GROOVY-5444,12818111,"Message ""This method should not have been called"" on script with nested loops and @CompileStatic","I was creating a small benchmarking for BigDecimal operations under @CompileStatic and after a nested loop the following message came:

{noformat}
Caught: BUG! exception in phase 'class generation' in source unit '/media/CHRONICLES/info/groovy/scripts/BugG2b3.groovy' At line 13 column 13
On receiver: curr.call() with message: minus and arguments: t0
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY
BUG! exception in phase 'class generation' in source unit '/media/CHRONICLES/info/groovy/scripts/BugG2b3.groovy' At line 13 column 13
On receiver: curr.call() with message: minus and arguments: t0
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY
{noformat}


Happens with or without --indy, and with or without indy jar. 
Happens only with @CompileStatic.


Commenting the {{5.times {}} ""solves"" the problem.

Swapping the ""5.times"" by 
{{for (int i = 0; i < 5; i++)}}
or by
{{for (i in 1..5)}}
or
{{(1..5).each}}
doesn't solve the problem.

",melix,willpiasecki,Minor,Closed,Fixed,12/May/12 10:49,15/Jun/12 22:56
Bug,GROOVY-5447,12811972,Dead links for 2.0.0-beta-3 release,"The following URLs are dead :
http://dist.groovy.codehaus.org/distributions/groovy-src-2.0.0-beta-3.zip
http://dist.groovy.codehaus.org/distributions/groovy-docs-2.0.0-beta-3.zip

Consequently, one has to check out the repo to get them.",melix,igosuki,Major,Closed,Fixed,13/May/12 07:48,15/Jun/12 22:56
Bug,GROOVY-5449,12816215,NullPointerException in compiler when using @Immutable,"When using @Immutable with a certain combination of properties, I get a NullPointerException as shown in the stack trace below.

The most basic way I've found to reproduce this is with the two classes.  Note that if I remove the 'final' keyword from properties, or don't include an implementation of toString() in the TestBean class, then the error goes away.

java.lang.NullPointerException
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:179)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitIfElse(ClassCodeExpressionTransformer.java:80)
	at org.codehaus.groovy.ast.stmt.IfStatement.visit(IfStatement.java:41)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:102)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:113)
	at org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitConstructorOrMethod(ClassCodeExpressionTransformer.java:50)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructor(ClassCodeVisitorSupport.java:120)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1166)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:51)
	at org.codehaus.groovy.control.OptimizerVisitor.visitClass(OptimizerVisitor.java:49)
	at org.codehaus.groovy.control.CompilationUnit$6.call(CompilationUnit.java:778)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1122)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:594)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:572)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:549)
	at org.codehaus.jdt.groovy.internal.compiler.ast.GroovyCompilationUnitDeclaration.processToPhase(GroovyCompilationUnitDeclaration.java:171)
	at org.codehaus.jdt.groovy.internal.compiler.ast.GroovyCompilationUnitDeclaration.generateCode(GroovyCompilationUnitDeclaration.java:1526)
	at org.eclipse.jdt.internal.compiler.Compiler.process(Compiler.java:831)
	at org.eclipse.jdt.internal.compiler.ProcessTaskManager.run(ProcessTaskManager.java:137)
	at java.lang.Thread.run(Thread.java:680)
",melix,jans70,Minor,Closed,Fixed,14/May/12 14:40,01/Jun/13 06:11
Bug,GROOVY-5453,12815676,Incorrect resolve of category property for String,"Assume you have two category methods with String and CharSequence as first parameters respectively. The bug is that Groovy prefers the 'CharSequence' method for String qualifier. 
The bug is reproduced only for String and CharSequence types.
Direct accessor invoking returns correct result.

{code}
class Cat {
  static getFoo(String s) {'String'}
  static getFoo(CharSequence s) {'CharSequence'}
}

use (Cat) {
  assert 'abc'.getFoo() == 'String'   //works
  assert 'abc'.foo      == 'String'   //fails
}
{code}",emilles,mxm-groovy,Major,Closed,Fixed,16/May/12 06:39,22/Apr/22 19:48
Bug,GROOVY-5454,12811973,Static type checker reports ambiguous method call on DGM method,"The following code produces a static type checking error:

{code}
(0..10).find { int i -> i<5 }
{code}

It reports an ambiguous call between two ""find"" methods from DGM, one defined on {{Object}}, the other one defined on {{Collection}}.",melix,melix,Major,Closed,Fixed,16/May/12 07:21,15/Jun/12 22:56
Bug,GROOVY-5456,12816196,Static type compiler failure when defining a closure using the / operator,"The following class definition results in the error:

{code}
@CompileStatic
class StaticGroovy2 {
    def foo(Closure cls) {}
    def bar() {	foo{ it / 2 } }
}
{code}

>>> a serious error occurred: BUG! exception in phase 'class generation' in source unit '../src/StaticGroovy2.groovy' At line 10 column 8
On receiver: it with message: div and arguments: 2
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY

It seem to fail only with the /, because it works with other operators like: + - *
",melix,rgarcia,Major,Closed,Fixed,16/May/12 18:09,15/Jun/12 22:56
Bug,GROOVY-5457,12816201,The example code in the docs of Closure class regarding OWNER_ONLY resolution strategy is wrong,"In the docs of Closure (groovy.lang.Closure), for the resolution strategy OWNER_ONLY, the example code is wrong.
The line def cl = { y = x + y } should be def cl = { y = x + y + z} which would throw the exception as mentioned later in the example. The current example runs fine without exception and hence does not explain the concept correctly.",paulk,shailendra,Minor,Closed,Fixed,17/May/12 02:49,15/Jun/12 22:56
Bug,GROOVY-5459,12811974,grape command and help is wrong command line structure,"The grape command treats help as an option and not a command, which I would claim to be the wrong thing.
{code}
grape help
{code}
should be they way of getting help. To have to type:
{code}
grape --help
{code}
confuses the role of the tokens in the command line.",paulk,russel,Minor,Closed,Fixed,18/May/12 01:15,28/Apr/14 19:30
Bug,GROOVY-5463,12816247,"""Caught: BUG! exception in phase 'class generation'""  when @CompileStatic present on class definition","Receive the following error when I attempt to run the attached file (Find.groovy):

Caught: BUG! exception in phase 'class generation' in source unit '/home/jboone/scripts/Find.groovy' operand stack contains 4 elements, but we expected only 5
BUG! exception in phase 'class generation' in source unit '/home/jboone/scripts/Find.groovy' operand stack contains 4 elements, but we expected only 5

Also, when the static members are refactored as instance members I receive the same error (see Find2.groovy).

Both files compile and execute correctly when the @CompileStatic annotation is removed.
",melix,jboone,Major,Closed,Fixed,21/May/12 11:35,21/Jun/12 17:03
Bug,GROOVY-5465,12812084,groovyc Ant task fails to compile any transform if includeAntRuntime=false,"I stumbled over this during my work on Gradle Groovy integration. It's not a big deal for Gradle, but I thought I'd let you know. It's a class loading problem.",,pniederw,Minor,Closed,Fixed,22/May/12 07:50,08/Mar/22 12:16
Bug,GROOVY-5466,12816238,Invalid code crashes the compiler,"here is piece of code which return bug

BUG! exception in phase 'class generation' in source unit 'Script7.groovy' tried to get a variable with the name number as stack variable, but a variable with this name was not created

if (false)
    int number = 0

1..3.each{
    println number
}
",blackdrag,hudi1,Major,Closed,Fixed,22/May/12 09:30,26/Jul/12 16:49
Bug,GROOVY-5467,12818129,catch Exception from loading from a jar to avoid BUG! message and do proper error reporting,"I have a testsuite within a soapui project which runs perfectly using soapUI (4.5.0). Trying to run the testsuite in a JUnit test fails: ""java.lang.AssertionError: expected:<FINISHED> but was:<FAILED>"".

I Assume its due to the error message i get in the console:
""ERROR [SoapUI] An error occured [BUG! exception in phase 'semantic analysis' in source unit 'Script1.groovy' Invalid index], see error log for details
BUG! exception in phase 'semantic analysis' in source unit 'Script1.groovy' Invalid index""

This i my code:
{code}
@Test
public void allTestCases() throws Exception {
  WsdlProject project = new WsdlProject(""someproject.xml"");
  List<TestSuite> testSuites = project.getTestSuiteList();
  for (TestSuite suite : testSuites) {
    List<TestCase> testCases = suite.getTestCaseList();
    for (TestCase testCase : testCases) {
      TestRunner runner2 = testCase.run(new PropertiesMap(), false);
      assertEquals(Status.FINISHED, runner2.getStatus());
    }
  }
}
{code}
At first i was using groovy-all-1.8.0 and I also tried it with 1.8.6. afterwards.Nothing changed...

I only get this error message when I'm using groovy scripts in my testsuite.",,juljanmay,Minor,Closed,Fixed,23/May/12 06:13,06/Aug/16 09:52
Bug,GROOVY-5468,12811705,Documentation error for ExpandoMetaClass,"
The following javadoc has a warning that was true under Grails from way back but I don't believe applies anymore when this functionality was ported to Groovy 1.1:

http://groovy.codehaus.org/api/groovy/lang/ExpandoMetaClass.html

{quote}
WARNING: This MetaClass uses a thread-bound ThreadLocal instance to store and retrieve properties. In addition properties stored use soft references so they are both bound by the life of the Thread and by the soft references. The implication here is you should NEVER use dynamic properties if you want their values to stick around for long periods because as soon as the JVM is running low on memory or the thread dies they will be garbage collected.
{quote}

It is also mentioned here:

http://groovy.codehaus.org/ExpandoMetaClass+-+Properties

{quote}
However, using this technique the property is stored in a ThreadLocal, WeakHashMap so don't expect the value to stick around forever!
{quote}


My guess is when Expando was ported into Groovy 1.1 the javadocs weren't updated appropriately to reflect the changes as discussed here:

http://groovy.329449.n5.nabble.com/Adding-properties-failed-with-ExpandoMetaClass-td343953.html

It was added here:

http://jira.codehaus.org/browse/GROOVY-1720

Using ThreadLocale wouldn't be amiable to server side RIA frameworks such as Vaadin where objects live beyond the request/response cycle.

This test case clearly shows that it isn't bound to a ThreadLocale or this would fail:

{code}
String.metaClass.swapCase = {->
      def sb = new StringBuffer()
      delegate.each {
           sb << (Character.isUpperCase(it as char) ? Character.toLowerCase(it as char) : 
                   Character.toUpperCase(it as char))
      }
      sb.toString()
}


println 'THIS IS A TEST'.swapCase()

new Thread(new Runnable() {
   
    void run() {
        println 'ANOTHER'.swapCase()
    }
}).start()

Thread.sleep(1000L)

{code}
",paulk,krasmussen,Major,Closed,Fixed,23/May/12 15:27,22/Dec/12 01:10
Bug,GROOVY-5470,12811980,@TypeChecked Closure methods/properties not recognized inside closure defition,"The compiler fails to recognize the delegate variable and getOwner method on the class definition below:

{code}
@TypeChecked
class StaticGroovy {
    def bar() {
        def foo = {[ this, delegate, getOwner()]}
    }
}
{code}",melix,rgarcia,Minor,Closed,Fixed,24/May/12 02:47,15/Jun/12 22:56
Bug,GROOVY-5471,12816233,"Add ""indy"" option to Groovy Console","If ""invokedynamic"" support is available, the groovy console should show an option allowing the scripts written in the console to be compiled with indy support too.

Otherwise, the user might think that because he's using a ""indy"" jar, the Groovy Console will compile scripts with indy activated, but in reality, only core groovy classes will use indy.",jwagenleitner,melix,Major,Closed,Fixed,24/May/12 03:13,02/May/17 02:03
Bug,GROOVY-5473,12812112,groovysh 2.0.0-beta-3 ERROR java.lang.ClassCastException: required class java.lang.Class but encountered class org.fusesource.jansi.AnsiRenderer,"Guillaume requested the I create a ticket for this issue originally reported on the groovy-user mailing list...

I'm trying out the invoke dynamic (indy) version of groovy.  When running groovysh 2.0.0-beta-3 with Java 1.7u4, the following error is continuously printing to the console 

C:\>groovysh 
Groovy Shell (2.0.0-beta-3, JVM: 1.7.0_04) 
Type 'help' or '\h' for help. 
------------------------------------------------------------------------------- 
ERROR java.lang.ClassCastException: required class java.lang.Class but encountered class org.fusesource.jansi.AnsiRenderer 

I'm running this on Windows XP SP3. 

I followed CÃÆÃâÃâÃÂ©dric Champeau's instructions to copy the indy jar to the lib and rename it (http://groovy.329449.n5.nabble.com/ANN-Groovy-2-0-0-beta-3-td5691151.html).  groovyConsole starts without these errors. ",blackdrag,bbrooks,Blocker,Closed,Fixed,24/May/12 13:42,12/Jan/13 20:40
Bug,GROOVY-5474,12812088,groovy-all-2.0.0-beta-3-indy.jar invoke dynamic jar lacks osgi attributes,"We would like to embed groovy into our OSGi-based server running on Java 7. However, as discussed on the groovy-2.0.0-beta-3 announcement thread

http://groovy.markmail.org/message/5t5hmvb36b3v5hl3

The 2.0.0 beta 3 lacks the OSGI MANIFEST.MF entries because 'bnd' doesn't
understand invokedynamic byte codes.",guillaume,bbrooks,Major,Closed,Fixed,25/May/12 09:49,10/Jul/13 04:42
Bug,GROOVY-5479,12812090,indy looses safe navigation flag,"for the following script:{code:Java}
class X {
    def value
}
def m(x) {
    x?.getValue()
}
def x = new X(value:1)
m(x)
m(null)
{code}
Indy fails to keep the safeNavigation flag and causes a NPE for the getValue() call on the m(null) invocation",blackdrag,blackdrag,Major,Closed,Fixed,29/May/12 13:27,29/May/12 14:11
Bug,GROOVY-5480,12816280,@TypeChecked type inference does not work with closure,"The following code when annotated with @TypeChecked:
{code}
List<Integer> foo = [1, 2, 3]
Integer bar = foo.find { it == 2 }
^
{code}
Will not compile with the error message:
Groovy:[Static type checking] - Cannot assign value of type java.lang.Object
to variable of type java.lang.Integer	 on the second line.",melix,rgarcia,Major,Closed,Fixed,31/May/12 04:05,07/Aug/13 12:23
Bug,GROOVY-5482,12812118,Type inference on lists,"{code}
@TypeChecked
class StaticGroovy2 {

    def bar() {

        def foo = [new Date(), 1, new C()]
        foo.add( 2 ) // Compiles
        foo.add( new Date() )
        foo.add( new C() )

        foo = [new Date(), 1]
        foo.add( 2 ) // Does not compile
    }
}
class C{
}
{code}

This code fails to compile on the last line foo.add(2) with the error:
Groovy:[Static type checking] - Cannot find matching method
java.util.List#add(int)    ",melix,rgarcia,Major,Closed,Fixed,31/May/12 05:46,15/Jun/12 22:56
Bug,GROOVY-5486,12812127,Distribution misses some dependencies,"The distribution misses several jars that were bundled in the distribution before:

* ant-antlr
* ant-junit
* ant-launcher
* bsf
* commons-logging
* hamcrest-core
* jsp-api
* junit
* servlet-api

",melix,melix,Blocker,Closed,Fixed,01/Jun/12 06:34,15/Jun/12 22:56
Bug,GROOVY-5487,12812124,jarjar task in Gradle doesn't exclude some classes,"After switching to Gradle, some classes that were previously excluded are no longer excluded, which causes a problem to existing framework relying on the normal path of classes.
",melix,melix,Blocker,Closed,Fixed,01/Jun/12 06:35,15/Jun/12 22:56
Bug,GROOVY-5488,12816283,groovy-all pom doesn't include modules dependencies,"The generated {{groovy-all}} pom should add dependencies from modules, but it doesn't.",melix,melix,Blocker,Closed,Fixed,01/Jun/12 10:55,15/Jun/12 22:56
Bug,GROOVY-5491,12815696,HashMap subclass property access inconsistent,"Creating a subclass of java.util.HashMap with properties defined results in inconsistent and confusing behavior. 
{code:java}
class MapSub extends HashMap {
  def foo
  
}

// Using dot notation to get the property results in a map key lookup.
def m = new MapSub(foo: 'bar')
println m.foo    // prints 'null'
println m.getFoo() // prints 'bar'

// Using same dot notation to set the property results in property setter.
m.foo = 'baz'
println m.foo // still prints 'null'
println m.getFoo() // prints 'baz'
{code}
This behavior results in subtle and hard to track bugs. I saw another Jira that stated that a design decision was made that Map dot property getters always resulted in a map key lookup. For consistency, I think either the same policy should be applied to Map property setters or the original design decision should be reconsidered to first check for property existence.
",emilles,mrfritz379,Minor,Resolved,Fixed,04/Jun/12 13:16,25/Jan/22 18:21
Bug,GROOVY-5494,12816275,JsonOutput.toJson(def) fails with MissingMethodException if the object defines getProperties(),"If an object defines {{getProperties()}}, {{JsonOutput.toJson(def)}} fails with the following:

{noformat}
groovy.lang.MissingMethodException: No signature of method: com.foo.FileProperties.remove() is applicable for argument types: (java.lang.String) values: [class]
Possible solutions: getAt(java.lang.String)
...
	at groovy.json.JsonOutput.toJson(JsonOutput.groovy:131)
...
{noformat}

Repro:

{noformat}
class MyClass { String properties }
groovy.json.JsonOutput.toJson(new MyClass(properties: 'properties'))
{noformat}",guillaume,sukhyun.cho,Major,Closed,Fixed,04/Jun/12 15:15,21/Jan/13 04:36
Bug,GROOVY-5495,12818162,@TypeChecked and @CompileStatic is not aware of interfaces hierarchy,"@TypeChecked and @CompileStatic doesn't compile code which use interface hierarchy. It only checks methods defined in used interface, not inherited from ancestors.

Example:
{code:java}
import groovy.transform.TypeChecked

class ClassUnderTest {

    @TypeChecked
    void methodFromString(SecondInterface si) {
        si.methodFromSecondInterface();
        si.methodFromFirstInterface();
    }
}

interface FirstInterface {
    void methodFromFirstInterface();
}

interface SecondInterface extends FirstInterface {
    void methodFromSecondInterface();
}
{code}

Compiler throws an error:
bq. [Static type checking] - Cannot find matching method SecondInterface#methodFromFirstInterface()

For class hierarchy it works and code below compiles:
{code:java}
import groovy.transform.TypeChecked

class ClassUnderTest {

    @TypeChecked
    void methodFromString(SecondInterface si) {
        si.methodFromSecondInterface();
        si.methodFromFirstInterface();
    }
}

class FirstInterface {
    void methodFromFirstInterface() { };
}

class SecondInterface extends FirstInterface {
    void methodFromSecondInterface() { };
}
{code}
",melix,filus,Major,Closed,Fixed,05/Jun/12 09:24,15/Jun/12 22:56
Bug,GROOVY-5497,12816322,"Returning ""null"" in a function reports an invalid type checking error","The following code is valid, but it reports some compilation errors:

{code}
@groovy.transform.CompileStatic
class Test {
 
  List getList() {
    null
  }
  
}
{code}


1 compilation error:

[Static type checking] - Cannot return value of type java.lang.Object on method returning type java.util.List -> java.util.List <E extends java.lang.Object -> java.lang.Object>
 at line: 5, column: 5
",melix,ariel.andres.morelli,Major,Closed,Fixed,05/Jun/12 13:14,15/Jun/12 22:56
Bug,GROOVY-5498,12816315,Static type check reports an error when accessing some property,"{code}
@groovy.transform.CompileStatic
class Test {
 
  List getListVar() {
    new ArrayList()
  }
 
  void someMethod() {
     def t = new Object()
     t = this
     
     t.getListVar()     //No error here
     t.listVar          //error is being reported here
  } 
}
{code}


1 compilation error:

Access to java.lang.Object#listVar is forbidden at line: 13, column: 6
",melix,ariel.andres.morelli,Major,Closed,Fixed,05/Jun/12 13:19,15/Jun/12 22:56
Bug,GROOVY-5500,12816264,GroovyBugError thrown when it shouldn't be,"The following code throws a {{GroovyBugError}}:
{code}
print ""Foo"" ===~ "".*""
{code}
but this seems wrong to me.  A {{GroovyBugError}}, as I understand it, is supposed to be only thrown for internal compiler errors, meaning that there is a bug in the compiler.  This seems to me to be a candidate for a {{SyntaxError}}.

It seems that there is a simple fix. In the default block of {{org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.eval(BinaryExpression)}}, change {{throws new GroovyBugError(..)}} to {{throws new SyntaxException(..)}}.

I only tried this on Groovy 2.0.0 beta 3 and Groovy 1.8.6.  ",paulk,werdna,Major,Closed,Fixed,05/Jun/12 16:14,15/Jun/12 22:56
Bug,GROOVY-5507,12816308,rcurry for overloaded MethodClosure throws an exception,"def a(b,c){}
def a(b,c,d){}
b = (this.&a).rcurry(0) 
b(1,2) // works
b(1) // throws ArrayIndexOutOfBoundsException",paulk,zhaber,Major,Closed,Fixed,07/Jun/12 15:59,15/Jun/12 22:56
Bug,GROOVY-5508,12816298,Verifier fails to check property types with covariant override,"The following code Does not throw an compiler exception

{code:java}
interface Addon {
   Map getFactories()
}

class AddonImpl implements Addon {
    Map factories
}

class SubAddon extends AddonImpl {
    def factories = [a:1]
}
{code}

whereas the method call will output the correct compiler error

{code:java}
interface Addon {
   Map getFactories()
}

class AddonImpl implements Addon {
    Map getFactories() { [:] }
}

class SubAddon extends AddonImpl {
    def getFactories() { [:] }
}
{code}

the error being

{code}
The return type of java.lang.Object getFactories() in SubAddon is incompatible with java.util.Map getFactories() in AddonImpl
{code}
",blackdrag,aalmiray,Major,Closed,Fixed,08/Jun/12 02:28,26/Jul/12 16:49
Bug,GROOVY-5512,12816316,[NPE] Creating a range in an inner class using @CompileStatic,"The following code causes an NPE during compilation, with 2.0.0-rc-2:

{code}
class Outer {
                static class Inner {
                    @groovy.transform.CompileStatic
                    int m() {
                        int x = 0
                        for (int i in 1..10) {x++}
                        x
                    }
                }
            }
{code}",melix,melix,Major,Closed,Fixed,13/Jun/12 02:28,15/Jun/12 22:56
Bug,GROOVY-5516,12816285,Type checker fails verification of generic types with addAll,"The following code produces an error:
{code}
List<String> list = ['a','b','c']
Collection<String> e = list.findAll { it }
list.addAll(e)
{code}
",melix,melix,Major,Closed,Fixed,14/Jun/12 03:50,15/Jun/12 22:56
Bug,GROOVY-5517,12816296,Type inference doesn't work with a Java class and a static property,"Example:

{code}

@Grab(group='org.grails', module='grails-bootstrap', version='2.0.4')
import grails.util.Metadata
import groovy.transform.CompileStatic

@CompileStatic
def foo() {
   Metadata m = Metadata.current
}
{code}

Results in:

{code}
[Static type checking] - Cannot assign value of type java.lang.Object to variable of type grails.util.Metadata
 at line: 7, column: 4
{code}

The class Metadata has a Java static method that looks roughly like:

{code}
public static Metadata getCurrent() { ... }
{code}",melix,graemerocher,Major,Closed,Fixed,14/Jun/12 07:58,11/Sep/22 14:10
Bug,GROOVY-5519,12811806,Static Type Checker: Exception type not inferred for catch blocks,"The following code:

{code}
import groovy.transform.CompileStatic

@CompileStatic
File foo() {
   try {
   
   } catch(e) {
       handleError(e)
   }
}

def handleError(Throwable e) {
  println e.message
}
{code}

Fails to compile with:

{code}

[Static type checking] - Cannot find matching method ConsoleScript2#handleError(java.lang.Object)
 at line: 8, column: 8
{code}",melix,graemerocher,Major,Closed,Fixed,14/Jun/12 08:22,21/Jun/12 17:03
Bug,GROOVY-5520,12816328,Static Type Checker: Cannot check for null against a Java interface ,"Following code:

{code}
@GrabResolver('http://repo.grails.org/grails/core')
@Grab(group='org.grails', module='grails-bootstrap', version='2.0.4')
import org.codehaus.groovy.grails.plugins.*
import groovy.transform.CompileStatic

@CompileStatic
def foo() {
   GrailsPluginInfo gpi = null
   
   if(gpi != null) {
       println ""good""
   }
}
{code}

Results in 

{code}

[Static type checking] - Cannot find matching method org.codehaus.groovy.grails.plugins.GrailsPluginInfo#equals(<unknown parameter type>)
 at line: 10, column: 7
{code}",melix,graemerocher,Major,Closed,Fixed,14/Jun/12 09:00,21/Jun/12 17:03
Bug,GROOVY-5521,12816333,Static Type Checker: Type not inferred from maps with generics,"Following code:
{code}
@GrabResolver('http://repo.grails.org/grails/core')
@Grab(group='org.grails', module='grails-bootstrap', version='2.0.4')
import org.codehaus.groovy.grails.plugins.*
import groovy.transform.CompileStatic



@CompileStatic
GrailsPluginInfo getPluginName(String pluginName) {
    Map<String, GrailsPluginInfo> pluginInfosMap = new HashMap<String, GrailsPluginInfo>()
    return pluginInfosMap[pluginName]
}
{code}

Produces:

{code}


[Static type checking] - Cannot return value of type java.lang.Object on method returning type org.codehaus.groovy.grails.plugins.GrailsPluginInfo -> org.codehaus.groovy.grails.plugins.GrailsPluginInfo
 at line: 11, column: 12
{code}",melix,graemerocher,Major,Closed,Fixed,14/Jun/12 09:08,21/Jun/12 17:03
Bug,GROOVY-5522,12811814,Add array DGM variants for find and findAll to assist Static Type Checker,"Following code:

{code}
import groovy.transform.CompileStatic

@CompileStatic
File findFile() {
    new File(""user.home"").listFiles().find { File f -> f.hidden } 
}
{code}

Produces

{code}

[Static type checking] - Cannot return value of type java.lang.Object on method returning type java.io.File -> java.io.File
 at line: 7, column: 5
{code}

Since listFiles() returns a File[] I would expect the call to find { } to infer the type",melix,graemerocher,Minor,Closed,Fixed,14/Jun/12 09:25,21/Jun/12 17:03
Bug,GROOVY-5523,12816310,Static Type Checker: Ternary operator doesn't allow nulls,"Following code:

{code}
import groovy.transform.CompileStatic
@CompileStatic
File findFile() {
    String str = """"
    File f = str ? new File(str) : null
}
{code}

Produces:

{code}
[Static type checking] - Cannot assign value of type java.lang.Object to variable of type java.io.File
 at line: 8, column: 5
{code}",melix,graemerocher,Major,Closed,Fixed,14/Jun/12 09:31,21/Jun/12 17:03
Bug,GROOVY-5524,12816325,rcurry fails using with closures and default parameters,"Next code fails (ArrayOutOfBoundException):

{code}
def log = {String msg, arg0 = null, arg1 = null, String level ->
    println ""Doing $level $msg $arg0 $arg1""
}

def trace = log.rcurry('TRACE')

trace('Trace world')
{code}

If I change parameters order (using ""level"" as first parameter) and use curry instead of rcurry this example works.",,guindous,Major,Closed,Fixed,18/Jun/12 06:39,01/Jan/15 09:58
Bug,GROOVY-5525,12816311,Type checker doesn't find a method defined in Arrays,"The following code produces a type checking error:

{code}

import groovy.transform.TypeChecked
import java.util.Arrays

@TypeChecked
class ArrayCopying {

    public static void main(String[] args) {
        def acopy = Arrays.copyOf(args, 1)
    }

}
{code}

Error:
{{Cannot find matching
method java.lang.Class#copyOf([Ljava.lang.String;, int)}}

The type checker doesn't seem to be aware that copyOf is defined on {{Arrays}}.",melix,melix,Major,Closed,Fixed,18/Jun/12 09:20,21/Jun/12 17:03
Bug,GROOVY-5526,12816312,VerifyError using @CompileStatic and assertEquals,"The following code produces a VerifyError:

{code}

import static org.junit.Assert.*;
import groovy.transform.CompileStatic;

@CompileStatic
class CompilerBugs {

  public static void main(String[] args) {
    int expected = 0
    assertEquals(expected, args.length)
  }

}
{code}

Bytecode shows that the selected method is incorrect:

{code}
public static void main(java.lang.String[]);
  Code:
   0:   iconst_0
   1:   istore_1
   2:   iload_1
   3:   pop
   4:   iload_1
   5:   invokestatic    #41; //Method
java/lang/Integer.valueOf:(I)Ljava/lang/Integer;
   8:   ldc     #43; //class ""[Ljava/lang/Object;""
   10:  invokestatic    #49; //Method
org/codehaus/groovy/runtime/ScriptBytecodeAdapter.castToType:(Ljava/lang/Object;Ljava/lang/Class;)Ljava/lang/Object;
   13:  checkcast       #43; //class ""[Ljava/lang/Object;""
   16:  iconst_1
   17:  anewarray       #4; //class java/lang/Object
   20:  dup
   21:  iconst_0
   22:  aload_0
   23:  arraylength
   24:  aastore
   25:  invokestatic    #55; //Method
org/junit/Assert.assertEquals:([Ljava/lang/Object;[Ljava/lang/Object;)V
   28:  aconst_null
   29:  pop
   30:  return
{code}",melix,melix,Major,Closed,Fixed,18/Jun/12 09:21,21/Jun/12 17:03
Bug,GROOVY-5528,12816313,wrong generics check for classes extending another class and giving through the generics parameter,"This fails to compile, while it should:
{code:Java}
class MyList<T> extends LinkedList<T> {}
@groovy.transform.CompileStatic
def foo() {
	List<String> list = new MyList<String>()
}
{code}
",melix,blackdrag,Major,Closed,Fixed,19/Jun/12 06:43,21/Jun/12 17:03
Bug,GROOVY-5529,12816279,Closure not statically compiled and owner access issue,"{code}
import groovy.transform.CompileStatic

interface Row {
    int getKey()
}

class RowImpl implements Row {
    int getKey() { 1 }
}

@CompileStatic
def test() {
    def rows = [new RowImpl(), new RowImpl(), new RowImpl()]
    
    rows.each { Row row ->
        println row.key
    }
}

test()
{code}
When executing the above script, with the statically compiled test() method, I get an exception saying that the owner property could not be found:
{code}
groovy.lang.MissingPropertyException: No such property: owner for class: ConsoleScript47
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:50)
	at org.codehaus.groovy.runtime.callsite.PogoGetPropertySite.getProperty(PogoGetPropertySite.java:49)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGroovyObjectGetProperty(AbstractCallSite.java:231)
	at ConsoleScript47$_test_closure1.doCall(ConsoleScript47:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:423)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1323)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1295)
	at ConsoleScript47.test(ConsoleScript47:17)
{code}
Furthermore, if you look closely at the stacktrace, it seems strange to see some call sites appearing, as if the closure wasn't compiled statically.",melix,guillaume,Major,Closed,Fixed,19/Jun/12 09:24,21/Jun/12 17:03
Bug,GROOVY-5530,12816297,No compilation error for GString values in named parameter consturctors for String fields,"The static type checker currently complains about constructors taking named parameters when a value is a GString for a field which is a String. It should coerce transparently.

Here's an example showing the problem:
{code}
import groovy.transform.CompileStatic

class User {
    String login
    String username
    String domain
    String firstName
    String lastName
}

@CompileStatic
class UserNamedParameterIssueWithStaticCompilation {
    List<User> getUsers() {
        [1, 2, 3].collect { Number num ->
             new User(
                    login:      ""login$num"",
                    username:   ""username$num"",
                    domain:     ""domain$num"",
                    firstName:  ""first$num"",
                    lastName:   ""last$num""
            )
        }
    }
}

def service = new UserNamedParameterIssueWithStaticCompilation()
service.getUsers()
{code}",melix,guillaume,Major,Closed,Fixed,19/Jun/12 12:47,21/Jun/12 17:03
Bug,GROOVY-5531,12813341,Problem accessing closure parameter inside named arg constructor values inside a closure in a @CompileStatic class,"The following example passes static type checking fine.
But at runtime, the generated bytecode seems to have some issue, as I get:
{code}
groovy.lang.MissingPropertyException: No such property: row for class: UserRepository
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:50)
	at UserRepository$getUsers.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
	at ConsoleScript20.run(ConsoleScript20:27)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:257)
	at groovy.lang.GroovyShell.run(GroovyShell.java:481)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:931)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
{code}

Here's the offending class:
{code}
import groovy.transform.CompileStatic

class User {
    String login
}

interface Row<R> {
    R getKey()
}

class RowImpl<R> implements Row<R> {
    R getKey() { null }
}

@CompileStatic
class UserRepository implements Serializable {
    Collection<User> getUsers() {
        def rows = [new RowImpl<String>()]

        return rows.collect { Row<String> row ->
            new User(login: row.getKey())
        }
    }
}

def rep = new UserRepository()
rep.getUsers()
{code}",melix,guillaume,Major,Closed,Fixed,19/Jun/12 13:30,21/Jun/12 17:03
Bug,GROOVY-5532,12812139,Static compilation complains when accessing a property with a property notation inside a named arg ctor inside a closure inside a @CompileStatic class,"Similarly to GROOVY-5531, I'm facing another problem with the same sample.
Static type checking the class is fine, but I get a compilation error coming from StaticTypesCallSiteWriter#makeGetPropertySite() when trying to access the closure's parameter property.

The offending sample:
{code}
import groovy.transform.CompileStatic

class User {
    String login
}

interface Row<R> {
    R getKey()
}

class RowImpl<R> implements Row<R> {
    R getKey() { null }
}

@CompileStatic
class UserRepository implements Serializable {
    Collection<User> getUsers() {
        def rows = [new RowImpl<String>()]

        return rows.collect { Row<String> row ->
            new User(login: row.key)
        }
    }
}

def rep = new UserRepository()
rep.getUsers()
{code}

Error I get:
{code}
Access to java.lang.Object#key is forbidden at line: 21, column: 29
{code}",melix,guillaume,Major,Closed,Fixed,19/Jun/12 13:37,21/Jun/12 17:03
Bug,GROOVY-5533,12816240,Static compiler doesn't write valid bytecode for list.property,"Given the following code:
{code}
class Elem { int value }
List<Elem> list = new LinkedList<Elem>()
list.add(new Elem(value:123))
list.add(new Elem(value:456))
assert list.value == [ 123, 456 ]
{code}

The static compiler doesn't produce bytecode for the {{list.value}} call.",melix,melix,Major,Closed,Fixed,19/Jun/12 16:05,21/Jun/12 17:03
Bug,GROOVY-5534,12816253,Static Compilation: Groovy @CompileStatic doesn't support safe dereference,"Following code:

{code}
import groovy.transform.*

@CompileStatic
def foo() {
   File bar
   println bar?.name
} 

foo()
{code}

Produces a NPE",melix,graemerocher,Critical,Closed,Fixed,20/Jun/12 03:26,21/Jun/12 17:03
Bug,GROOVY-5535,12816326,When variable is not initialized @CompileStatic can fail compilation,"The following code:

{code}
import groovy.transform.*

@CompileStatic
URL getDescriptorForPlugin(File pluginDir) {
    URL descriptor = null
    File baseFile = pluginDir.canonicalFile
    File basePluginFile = (File)baseFile.listFiles().find {  File it -> it.name.endsWith(""GrailsPlugin.groovy"")}

    if (basePluginFile?.exists()) {
         descriptor = null
    }
    return descriptor
}
{code}

Fails with an error:

{code}

[Static type checking] - Cannot return value of type java.lang.Object on method returning type java.net.URL -> java.net.URL
 at line: 12, column: 12
{code}

Initializing to null shouldn't cause this problem",melix,graemerocher,Major,Closed,Fixed,20/Jun/12 03:43,21/Jun/12 17:03
Bug,GROOVY-5536,12816218,Using @CompileStatic causes VerifierError,"The following code:

{code}
import groovy.transform.*

@CompileStatic
URL getDescriptorForPlugin(File pluginDir) {
    URL descriptor = null
    File baseFile = pluginDir.canonicalFile
    File basePluginFile = (File)baseFile.listFiles().find {  File it -> it.name.endsWith(""GrailsPlugin.groovy"")}

    if (basePluginFile?.exists()) {
         descriptor = new URL(""http://grails.org"")
    }
    return descriptor
}
{code}

Results in 

{code}

java.lang.VerifyError: (class: ConsoleScript6, method: getDescriptorForPlugin signature: (Ljava/io/File;)Ljava/net/URL;) Expecting to find integer on stack
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2389)
	at java.lang.Class.getConstructor0(Class.java:2699)
{code}",melix,graemerocher,Critical,Closed,Fixed,20/Jun/12 03:44,21/Jun/12 17:03
Bug,GROOVY-5537,12816354,"Using @CompileStatic results in ""Illegal use of nonvirtual function call""","Uncommenting the following @CompileStatic declaration:

https://github.com/grails/grails-core/blob/master/grails-bootstrap/src/main/groovy/grails/util/PluginBuildSettings.groovy#L840

Results in the following error message:

{code}
Error executing script Compile: (class: grails/util/PluginBuildSettings$_resolvePluginResourcesAndAdd_closure27, method: doCall signature: (Lorg/springframework/core/io/Resource;)Ljava/lang/Object;) Illegal use of nonvirtual function call
{code}

I couldn't reproduce this outside of Grails, but have attached the compiled byte code ",melix,graemerocher,Major,Closed,Fixed,20/Jun/12 03:55,21/Jun/12 17:03
Bug,GROOVY-5538,12816368,Using @CompileStatic produces NoSuchMethodError,"The following code:

{code}
@GrabResolver('http://repo.grails.org/grails/core')
@Grab(group='org.grails', module='grails-bootstrap', version='2.0.4')
import org.codehaus.groovy.grails.plugins.*
import org.springframework.core.io.*
import groovy.transform.*



@CompileStatic
List<GrailsPluginInfo> getCompileScopedSupportedPluginInfos() {
    def basicInfo = new BasicGrailsPluginInfo(new FileSystemResource(new File("""")))
    basicInfo.name = ""foo""
    def compilesScopedInfo = new BasicGrailsPluginInfo(new FileSystemResource(new File("""")))
    compilesScopedInfo.name = ""bar""
    GrailsPluginInfo[] pluginInfos = [basicInfo] as GrailsPluginInfo[]
    def compileScopePluginInfos = []
    compileScopePluginInfos.add compilesScopedInfo 
    compileScopePluginInfos = compileScopePluginInfos.findAll { GrailsPluginInfo info -> pluginInfos.any { GrailsPluginInfo it -> it.name == info.name } }
}
    
    
getCompileScopedSupportedPluginInfos()
{code}

Results in 

{code}
ava.lang.NoSuchMethodError: java.lang.Object.any(Lgroovy/lang/Closure;)Z
	at ConsoleScript20$_getCompileScopedSupportedPluginInfos_closure1.doCall(ConsoleScript20:18)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:423)
	at org.c
{code}

I also find it odd that there are some MOP calls inside the stack trace since the method is supposed to be statically compiled?",melix,graemerocher,Major,Closed,Fixed,20/Jun/12 04:08,21/Jun/12 17:03
Bug,GROOVY-5539,12813340,@CompileStatic throws verify error using power operator,"The following code produces a {{VerifyError}}

{code}
int squarePlusOne(int num) {
   num ** num + 1
}
assert squarePlusOne(2) == 5
{code}

The reason is that the helper {{power}} method takes a {{Long}} as first argument, but we call it with an {{Integer}}:

{code}
squarePlusOne(I)I
00000 LTestScripttestPowerShouldNotThrowVerifyError0; I  :  :     ILOAD 1
00001 LTestScripttestPowerShouldNotThrowVerifyError0; I  : I  :     INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer;
00002 LTestScripttestPowerShouldNotThrowVerifyError0; I  : Integer  :     ILOAD 1
00003 LTestScripttestPowerShouldNotThrowVerifyError0; I  : Integer I  :     INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer;
00004 LTestScripttestPowerShouldNotThrowVerifyError0; I  : Integer Integer  :     INVOKESTATIC org/codehaus/groovy/runtime/DefaultGroovyMethods.power (Ljava/lang/Long;Ljava/lang/Integer;)Ljava/lang/Number;
00005 ?     :     ICONST_1
00006 ?     :     INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer;
00007 ?     :     INVOKESTATIC org/codehaus/groovy/runtime/dgmimpl/NumberNumberPlus.plus (Ljava/lang/Number;Ljava/lang/Number;)Ljava/lang/Number;
00008 ?     :     INVOKESTATIC org/codehaus/groovy/runtime/typehandling/DefaultTypeTransformation.intUnbox (Ljava/lang/Object;)I
00009 ?     :     IRETURN
00010 ?     :     LDC 0
00011 ?     :     IRETURN
{code}",melix,melix,Major,Closed,Fixed,20/Jun/12 09:47,21/Jun/12 17:03
Bug,GROOVY-5540,12813384,Static compiler doesn't choose the right method in hierarchy,"Using the following code seems to indicate the compiler forgets a method in the middle of the hierarchy!

{code:title=foo/Parent.groovy}
package foo

@groovy.transform.CompileStatic
class Parent {
   protected void getChildren() { throw new UnsupportedOperationException() }
}
{code}

{code:title=foo/Child.groovy}
package foo

@groovy.transform.CompileStatic
class Child extends Parent {
   public void getChildren() {   
    throw new UnsupportedOperationException('Child')
   }
}
{code}

{code:title=foo/SubChild.groovy}
package foo

@groovy.transform.CompileStatic
class SubChild extends Child {
}
{code}

{code:title=Usage.groovy}
import foo.*

@groovy.transform.CompileStatic
class Usage {
    public static void main(String... args) {
       def child = new SubChild()
       child.getChildren()
    }
}
{code}

Then it fails with:
{{Caught: java.lang.IllegalAccessError: tried to access method foo.Parent.getChildren()V from class Usage
java.lang.IllegalAccessError: tried to access method foo.Parent.getChildren()V from class Usage}}


",melix,melix,Critical,Closed,Fixed,20/Jun/12 12:15,21/Jun/12 17:03
Bug,GROOVY-5541,12816347,AstBrowser doesn't show any labels,The AST browser doesn't show any label in the tree anymore.,melix,melix,Major,Closed,Fixed,20/Jun/12 15:03,21/Jun/12 17:03
Bug,GROOVY-5543,12816337,Grab a Module Extension fails to match Map objects,"There seems to be an issue with Modular Extensions not selecting the correct classes to decorate when @Grabbed

In my groovy-stream project, I define my extension class as:

{code}
package groovy.stream ;

import groovy.lang.Closure ;
import java.util.* ;

public class StreamExtension {
  public static Stream toStream( Closure delegate ) {
    return Stream.from( delegate ) ;
  }

  public static Stream toStream( Iterator delegate ) {
    return Stream.from( delegate ) ;
  }

  public static Stream toStream( Iterable delegate ) {
    return Stream.from( delegate ) ;
  }

  public static Stream toStream( Map delegate ) {
    return Stream.from( delegate ) ;
  }
}
{code}

If you then run a test class:

{code}
def m = [ x:1..3, y:5..7 ].toStream().where { x + 4 == y }
assert [[x:1,y:5],[x:2,y:6],[x:3,y:7]] == m.collect()
{code}

With {{groovy -cp groovy-stream-0.3-map.jar testscript.groovy}}, it all works fine, however if you push the jar to a maven repo, and change the test script to:

{code}
@GrabResolver( name='bloidonia', root='https://raw.github.com/timyates/bloidonia-repo/master' )
@Grab('com.bloidonia:groovy-stream:0.3-map')
import groovy.stream.Stream 

def m = [ x:1..3, y:5..7 ].toStream().where { x + 4 == y }
assert [[x:1,y:5],[x:2,y:6],[x:3,y:7]] == m.collect()
{code}

Then running {{groovy testscript.groovy}}, you get the exception:

{code}
Caught: groovy.lang.MissingMethodException: No signature of method: java.util.LinkedHashMap.toStream() is applicable for argument types: () values: []
Possible solutions: toString(), toString(), toString(), toString(), toSpreadMap(), spread()
{code}

If I change the extension class so that the Map function is replaced with:

{code}
  public static Stream toStream( LinkedHashMap delegate ) {
    return Stream.from( delegate ) ;
  }
{code}

Then running:

{code}
@GrabResolver( name='bloidonia', root='https://raw.github.com/timyates/bloidonia-repo/master' )
@Grab('com.bloidonia:groovy-stream:0.3')
import groovy.stream.Stream 

def m = [ x:1..3, y:5..7 ].toStream().where { x + 4 == y }
assert [[x:1,y:5],[x:2,y:6],[x:3,y:7]] == m.collect()
{code}

Works.",melix,tim_yates,Major,Closed,Fixed,22/Jun/12 04:53,26/Jul/12 16:49
Bug,GROOVY-5547,12816378,Groovy RC release zip download contains duplicate files,"When unzipping the download archive, the command line reports duplicate files to be overwritten. On closer examination, the following two files are duplicated inside the archive:
groovy-2.0.0-rc-4.jar
groovy.icns

To replicate, simply run the following on cli:
unzip groovy-binary-2.0.0-rc-4.zip",paulk,marco.vermeulen,Major,Closed,Fixed,24/Jun/12 11:41,08/Jul/12 04:51
Bug,GROOVY-5552,12816356,Stackoverflow in JsonSlurper.parseText,"Jun 27, 2012 1:59:43 PM org.codehaus.groovy.runtime.StackTraceUtils sanitize
WARNING: Sanitizing stacktrace:
{code}
java.lang.StackOverflowError
	at java.lang.Character.codePointAt(Character.java:2335)
	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3344)
	at java.util.regex.Pattern$Curly.match0(Pattern.java:3760)
	at java.util.regex.Pattern$Curly.match(Pattern.java:3744)
	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
	at java.util.regex.Pattern$Curly.match0(Pattern.java:3782)
{code}
This will reproduce the error in the console (the first few times, then i starts to work for no reason):
{code}
def url = new URL(""http://vip.regionh.dk/VIP/Redaktoer/133007.nsf/aLoadInfoDokRead?OpenAgent&id=XCA7E63D0762F1706C125791E0045E51B"")
def jsonStr = url.getText(""UTF-8"")
def slurper = new groovy.json.JsonSlurper()
slurper.parseText(jsonStr)
{code}",guillaume,jonnoerrelykke,Critical,Closed,Fixed,27/Jun/12 07:02,26/Jul/12 16:49
Bug,GROOVY-5555,12816530,groovy.sql is missing from the 2.0 api docs online and in the docs .zip file,"The groovy API documents at http://groovy.codehaus.org/gapi/ do not have the groovy.sql package, nor is this package in the download groovy-docs-2.0.0.zip:

{noformat}
/Xfer/Groovy$ unzip -l groovy-docs-2.0.0.zip | grep sql
        0  06-28-2012 08:48   groovy-2.0.0/html/groovy-jdk/java/sql/
     6185  06-28-2012 08:48   groovy-2.0.0/html/groovy-jdk/java/sql/Timestamp.html
     8608  06-28-2012 08:48   groovy-2.0.0/html/groovy-jdk/java/sql/Date.html
     5246  06-28-2012 08:48   groovy-2.0.0/html/groovy-jdk/java/sql/ResultSetMetaData.html
     5183  06-28-2012 08:48   groovy-2.0.0/html/groovy-jdk/java/sql/ResultSet.html
      950  06-28-2012 08:48   groovy-2.0.0/html/groovy-jdk/java/sql/package-frame.html
/Xfer/Groovy$ 
{noformat}

",melix,dannyturns,Major,Closed,Fixed,28/Jun/12 10:10,13/Jul/12 17:11
Bug,GROOVY-5556,12811682,GAPI on codehaus doesn't appear to have subproject groovydoc (check gradle build dist target),,paulk,paulk,Major,Closed,Fixed,28/Jun/12 10:21,07/Apr/15 19:07
Bug,GROOVY-5557,12818166,@Slf4j throws error in Groovy 2.0,"groovyc throws a stacktrace compiling a class with the @Slf4j in Groovy 2.0. The same source compiles in 1.8.6.

Source file:
{code}
import groovy.util.logging.Slf4j
@Slf4j
class Test {
    Test() { log.debug ""Here"" }
}
{code}

groovyc output:

{noformat}
>>> a serious error occurred: BUG! exception in phase 'class generation' in source unit 'Test.groovy' ClassNode#getTypeClass for org.slf4j.Logger is called before the type class is set 
>>> stacktrace:
BUG! exception in phase 'class generation' in source unit 'Test.groovy' ClassNode#getTypeClass for org.slf4j.Logger is called before the type class is set 
	at org.codehaus.groovy.ast.ClassNode.getTypeClass(ClassNode.java:1327)
	at org.codehaus.groovy.classgen.asm.BytecodeHelper.box(BytecodeHelper.java:596)
	at org.codehaus.groovy.classgen.asm.OperandStack.box(OperandStack.java:205)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.prepareSiteAndReceiver(CallSiteWriter.java:233)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.prepareSiteAndReceiver(CallSiteWriter.java:221)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.makeCallSite(CallSiteWriter.java:270)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:229)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:334)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:649)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:354)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:510)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:155)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:456)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:320)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:277)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructor(ClassCodeVisitorSupport.java:119)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructor(AsmClassGenerator.java:392)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1052)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:181)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:496)
	at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
	at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:213)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:146)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:176)
	at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:160)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)
{noformat}",melix,cholick,Minor,Closed,Fixed,28/Jun/12 21:34,26/Jul/12 16:49
Bug,GROOVY-5559,12816265,Type checker should treat gstrings as strings for generic collections,"Following code:

{code}
@groovy.transform.TypeChecked
def test() {
    def bar = 1
    List<String> list = [""foo"", ""$bar"" ]
}
{code}

Produces

{code}

[Static type checking] - Incompatible generic argument types. Cannot assign java.util.List <java.io.Serializable> to: java.util.List <String>
 at line: 5, column: 5
{code}",melix,graemerocher,Major,Closed,Fixed,29/Jun/12 08:44,15/Jan/13 08:47
Bug,GROOVY-5562,12818165,"GroovyFX - Class Cast Exception on BigDecimal to double when using Groovy 2.0 Indy=""true""","Setting indy=""true"" causes JavaFX binds to fail on failure to convert
BigDecimal to double. This works ok when indy=""false""

=====
""hourAngleProperty.bind((hours() * 30.0) + (minutes() * 0.5))""

this actually translates to this:
hourAngleProperty.bind(hours().multiply(30.0).add(minutes().multiply(0.5)));

where hourAngleProperty, hours() and minutes() are type javafx.beans.property.DoubleProperty.

The problem is the 30.0 and 0.5 are treated as BigDecimal
and with Indy, there seems to be no unboxing of BigDecimal to ""double"".

If i do this:
hourAngleProperty.bind((hours() * (double)30.0) + (minutes() * (double)0.5))

it works. 

But this error is all over the GroovyFX code base.

======
java.lang.ClassCastException: Cannot cast java.math.BigDecimal to java.lang.Double
	at java.lang.Class.cast(Class.java:3005)
	at sun.invoke.util.ValueConversions.primitiveConversion(ValueConversions.java:236)
	at sun.invoke.util.ValueConversions.unboxDouble(ValueConversions.java:118)
	at java.lang.invoke.MethodHandleImpl$GuardWithCatch.invoke_L2(MethodHandleImpl.java:1130)
	at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:684)
	at Time.<init>(AnalogClockDemo.groovy:38)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:77)
	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:102)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:57)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:186)
	at AnalogClockDemo$_run_closure1.doCall(AnalogClockDemo.groovy:64)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:809)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:792)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeClosure(InvokerHelper.java:95)
	at groovyx.javafx.GroovyFX.start(GroovyFX.java:35)
	at com.sun.javafx.application.LauncherImpl$5.run(LauncherImpl.java:319)
	at com.sun.javafx.application.PlatformImpl$5.run(PlatformImpl.java:206)
	at com.sun.javafx.application.PlatformImpl$4.run(PlatformImpl.java:173)
	at com.sun.glass.ui.InvokeLaterDispatcher$Future.run(InvokeLaterDispatcher.java:76)
Exception in Application start method
Exception in thread ""main"" java.lang.RuntimeException: Exception in Application start method
	at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:403)
	at com.sun.javafx.application.LauncherImpl.access$000(LauncherImpl.java:47)
	at com.sun.javafx.application.LauncherImpl$1.run(LauncherImpl.java:115)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ClassCastException: Cannot cast java.math.BigDecimal to java.lang.Double
	at java.lang.Class.cast(Class.java:3005)
	at sun.invoke.util.ValueConversions.primitiveConversion(ValueConversions.java:236)
	at sun.invoke.util.ValueConversions.unboxDouble(ValueConversions.java:118)
	at java.lang.invoke.MethodHandleImpl$GuardWithCatch.invoke_L2(MethodHandleImpl.java:1130)
	at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:684)
	at Time.<init>(AnalogClockDemo.groovy:38)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:77)
	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:102)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:57)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:186)
	at AnalogClockDemo$_run_closure1.doCall(AnalogClockDemo.groovy:64)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:809)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:792)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeClosure(InvokerHelper.java:95)
	at groovyx.javafx.GroovyFX.start(GroovyFX.java:35)
	at com.sun.javafx.application.LauncherImpl$5.run(LauncherImpl.java:319)
	at com.sun.javafx.application.PlatformImpl$5.run(PlatformImpl.java:206)
	at com.sun.javafx.application.PlatformImpl$4.run(PlatformImpl.java:173)
	at com.sun.glass.ui.InvokeLaterDispatcher$Future.run(InvokeLaterDispatcher.java:76)
Java Result: 1",blackdrag,jimclarke5,Major,Closed,Fixed,29/Jun/12 09:21,26/Jul/12 16:49
Bug,GROOVY-5564,12816366,@CompileStatic(SKIP) throws an error,"If you annotate a class with @CompileStatic, then use @CompileStatic(SKIP) on a method, then calling that method throws an error. However, annotating each method separately with @CompileStatic works.

Example:
{code}
import groovy.transform.CompileStatic
import static groovy.transform.TypeCheckingMode.SKIP

@CompileStatic
class A {
    @CompileStatic(SKIP)
    void m() {}
}

new A().m()
{code}

throws:

{noformat}
java.lang.NoSuchMethodError: A.$getCallSiteArray()[Lorg/codehaus/groovy/runtime/callsite/CallSite;
	at A.m(ConsoleScript0)
	at A$m.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
	at ConsoleScript0.run(ConsoleScript0:10)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:257)
	at groovy.lang.GroovyShell.run(GroovyShell.java:481)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:124)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:951)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:722)

{noformat}",melix,melix,Major,Closed,Fixed,02/Jul/12 02:23,26/Jul/12 16:49
Bug,GROOVY-5565,12818429,NPE when using TypeChecking and AIC with field,"compiling this code:
{code}
@groovy.transform.TypeChecked
def foo() {
    Serializable s = new Serializable() { List things = [] }
    println s.things.size()
}

foo()
{code}
yields:
{noformat}
java.lang.NullPointerException
    at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.inferDiamondType(StaticTypeCheckingVisitor.java:456)
    at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBinaryExpression(StaticTypeCheckingVisitor.java:394)
    at org.codehaus.groovy.ast.CodeVisitorSupport.visitDeclarationExpression(CodeVisitorSupport.java:245)
    ...
{noformat}",melix,paulk,Major,Closed,Fixed,02/Jul/12 06:16,07/Apr/15 19:06
Bug,GROOVY-5566,12811684,possible bug when applying static typing to AIC referencing an outer local variable,"compiling this code:
{code}
@groovy.transform.TypeChecked
def foo() {
    List things = []
    Serializable s = new Serializable() {
      def size() {
        things.size()
      }
    }
    s.size()
}

println foo()
{code}
yields this error:
{noformat}
[Static type checking] - Cannot find matching method ConsoleScript51$1#<init>(ConsoleScript51, java.util.List)
 at line: 4, column: 22
{noformat}
If you make the {{things}} list be something more specific, e.g. {{['a', 'b']}}, then the error message becomes more specific:
{noformat}
[Static type checking] - Cannot find matching method ConsoleScript54$1#<init>(ConsoleScript54, java.util.List <java.lang.String>)
 at line: 4, column: 22
{noformat}
",melix,paulk,Major,Closed,Fixed,02/Jul/12 06:22,07/Apr/15 19:07
Bug,GROOVY-5567,12816379,Types not inferred correctly for static fields,"Example:

{code}
import org.apache.commons.logging.LogFactory

class Foo {
    private static final LOG = LogFactory.getLog(this)
    
    @groovy.transform.CompileStatic
    static foo() {
        def bar = 1
        LOG.debug ""number is $bar""
    }
}
{code}

Produces

{code}
[Static type checking] - Cannot find matching method java.lang.Object#debug(groovy.lang.GString)
 at line: 9, column: 9
{code}",melix,graemerocher,Major,Closed,Fixed,02/Jul/12 07:22,26/Jul/12 16:49
Bug,GROOVY-5568,12816381,DGM properties not available in interfaces?,"Example:

{code}
class Foo {
    
    @groovy.transform.CompileStatic
    static foo() {
        InputStream input
        println input.text
    }
}
{code}

Produces:

{code}

[Static type checking] - No such property: text for class: java.io.InputStream
 at line: 8, column: 17
{code}",melix,graemerocher,Major,Closed,Fixed,02/Jul/12 07:29,09/Nov/22 14:41
Bug,GROOVY-5569,12816369,Can't call method with concrete map implementation and generic arguments,"Example:

{code}

import java.util.concurrent.*

class Foo {
    
    private static ThreadLocal<Map<Integer, String>> cachedConfigs = new ThreadLocal<Map<Integer, String>>()
    @groovy.transform.CompileStatic
    static foo() {
           def configs = new ConcurrentHashMap<Integer, String>()
           cachedConfigs.set configs

    }
}

{code}

Error

{code}
[Static type checking] - Cannot call java.lang.ThreadLocal#set(java.util.Map <Integer, String>) with arguments [java.util.concurrent.ConcurrentHashMap <Integer, String>] 
 at line: 9, column: 12
{code}",melix,graemerocher,Major,Closed,Fixed,02/Jul/12 07:39,26/Jul/12 16:49
Bug,GROOVY-5570,12818172,CompileStatic gives VerifyError: Register 1 contains wrong type,"If you enter the following into the GroovyConsole:

{code}
import groovy.transform.*

@CompileStatic
class Test {
  List<Integer> elements
  
  public Test( List<Integer> elements ) {
    this.elements = elements
  }
  
  void regularEach() {
    int idx = 0
    elements.each { Integer elem ->
      ""$elem:${idx++}""
    }
  }
}

new Test( [1,2,3] )
{code}

You get the Error:

{code}
java.lang.VerifyError: (class: Test, method: regularEach signature: ()V) Register 1 contains wrong type
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2389)
	at java.lang.Class.getDeclaredConstructors(Class.java:1836)
	at org.codehaus.groovy.reflection.CachedClass$2$1.run(CachedClass.java:69)
	at java.security.AccessController.doPrivileged(Native Method)
	at org.codehaus.groovy.reflection.CachedClass$2.initValue(CachedClass.java:66)
	at org.codehaus.groovy.reflection.CachedClass$2.initValue(CachedClass.java:64)
	at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
	at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
	at org.codehaus.groovy.reflection.CachedClass.getConstructors(CachedClass.java:258)
	at groovy.lang.MetaClassImpl.<init>(MetaClassImpl.java:185)
	at groovy.lang.MetaClassImpl.<init>(MetaClassImpl.java:189)
	at groovy.lang.MetaClassRegistry$MetaClassCreationHandle.createNormalMetaClass(MetaClassRegistry.java:157)
	at groovy.lang.MetaClassRegistry$MetaClassCreationHandle.createWithCustomLookup(MetaClassRegistry.java:147)
	at groovy.lang.MetaClassRegistry$MetaClassCreationHandle.create(MetaClassRegistry.java:130)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:165)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:182)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:302)
	at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:767)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallConstructorSite(CallSiteArray.java:84)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:57)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:190)
	at ConsoleScript6.run(ConsoleScript6:19)
        ...

{code}
",melix,tim_yates,Major,Closed,Fixed,02/Jul/12 08:56,26/Jul/12 16:49
Bug,GROOVY-5571,12816252,Missing or wrong osgi headers since 2.0-rc1,"Since 2.0-rc1 osgi import-package and export-package headers are not correct anymore so groovy-all.jar is not really usable in osgi environment anymore.
Previous version were fine.
Those headers are extracted in attached file, beta3 version seems the correct one, and some are missing in release.

Also groovy.jar is osgified, but none of modules groovy-xxx.jar are, so they probably should in order to be able to use them in osgi environment
",paulk,amergey,Major,Closed,Fixed,03/Jul/12 03:24,26/Jul/12 16:49
Bug,GROOVY-5572,12816397,Invalid behaviour of ?: operator within an 'if' statement in closure,"The invalid result appears only in 2.0.0 version of Groovy:

dm@dmc:~/bin/groovy-2.0.0/bin> ./groovysh
Groovy Shell (2.0.0, JVM: 1.6.0_29)
Type 'help' or '\h' for help.
---------------------------------------------
groovy:000> def c = {def s; if (it.y == null) s = true; else s = it.z < it.x - 1 ? null : true; s}; c(z: 0, y: 2, x: 3)
===> false
groovy:000>

In Groovy 1.8.6 the result is correct.

dm@dmc:~/bin/groovy-1.8.6/bin> ./groovysh
Groovy Shell (1.8.6, JVM: 1.6.0_29)
Type 'help' or '\h' for help.
----------------------------------------------
groovy:000> def c = {def s; if (it.y == null) s = true; else s = it.z < it.x - 1 ? null : true; s}; c(z: 0, y: 2, x: 3)
===> null
groovy:000> 

It can be fixed by replacing the ternary operator with 'else if' and 'else'.",melix,dmovchinn,Critical,Closed,Fixed,03/Jul/12 07:11,26/Jul/12 16:49
Bug,GROOVY-5573,12816257,Type checker incorrectly selecting DGM return type instead of actual return type,"The following code:

{code}
import java.lang.reflect.Array

@groovy.transform.CompileStatic
def arrayTest() {
   Object[] joinedArray = (Object[]) Array.newInstance(Integer.class, 10);
}
{code}

Results in 

{code}
[Static type checking] - Inconvertible types: cannot cast java.lang.reflect.Array to [Ljava.lang.Object;
 at line: 5, column: 27
{code}

The DGM newInstance method is being used instead of http://docs.oracle.com/javase/6/docs/api/java/lang/reflect/Array.html#newInstance(java.lang.Class,%20int)",melix,graemerocher,Major,Closed,Fixed,03/Jul/12 07:36,26/Jul/12 16:49
Bug,GROOVY-5574,12812113,Log4j annotation causes class generation BUG!,"Running this script:

{code}
import groovy.util.logging.Log4j

@Log4j
class Test {
  void doit() {
    log.debug( 'woo' )
  }
  
  static main( args ) {
    new Test().doit()
  }
}
{code}

Causes:

{code}
BUG! exception in phase 'class generation' in source unit 'log4jannotation.groovy' ClassNode#getTypeClass for org.apache.log4j.Logger is called before the type class is set 
	at org.codehaus.groovy.ast.ClassNode.getTypeClass(ClassNode.java:1327)
	at org.codehaus.groovy.classgen.asm.BytecodeHelper.box(BytecodeHelper.java:596)
	at org.codehaus.groovy.classgen.asm.OperandStack.box(OperandStack.java:205)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.prepareSiteAndReceiver(CallSiteWriter.java:233)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.prepareSiteAndReceiver(CallSiteWriter.java:221)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.makeCallSite(CallSiteWriter.java:270)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:229)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:334)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:649)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:354)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:510)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:155)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:456)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:320)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:277)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:397)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:181)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:613)
	at groovy.lang.GroovyShell.run(GroovyShell.java:480)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:931)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
{code}",melix,tim_yates,Major,Closed,Fixed,04/Jul/12 05:36,26/Jul/12 16:49
Bug,GROOVY-5578,12816374,Cannot use default map constructor with Java classes and type checking,"Example:

{code}
@GrabResolver('http://repo.grails.org/grails/core')
@Grab(group='org.grails', module='grails-bootstrap', version='2.0.4')
import org.codehaus.groovy.grails.resolve.SnapshotAwareM2Resolver

@groovy.transform.CompileStatic
def foo() {
 def url = ""blah""
 def resolver = new SnapshotAwareM2Resolver(name: url, root: url, m2compatible: true, changingPattern: "".*SNAPSHOT"")
}
{code}

Results in:

{code}

[Static type checking] - Cannot find matching method org.codehaus.groovy.grails.resolve.SnapshotAwareM2Resolver#<init>(java.util.Map <java.lang.String, java.io.Serializable>)
 at line: 8, column: 17
{code}",melix,graemerocher,Major,Closed,Fixed,06/Jul/12 07:19,26/Jul/12 16:49
Bug,GROOVY-5579,12816394,Static compiler sometimes use setProperty where it could make direct access,"Take the following code:
{code}
@groovy.transform.CompileStatic
class Test {
    A a = new A()
    void foo() {
        a.x = 1
    }
}
class A { 
    int x
}

new Test().foo()
{code}
The static compiler sets the ""x"" variable from A using {{ScriptBytecodeAdapter.setProperty}} although it could use {{A#setX}}.",melix,melix,Major,Closed,Fixed,06/Jul/12 08:37,26/Jul/12 16:49
Bug,GROOVY-5580,12818169,@CompileStatic doesn't support interfaces extending other interfaces,"[Major is the default priority, sorry if I'm supposed to assign to another]
This code fails during static type checking:
{noformat}
interface A {
    String getName();
}

interface B extends A {
    void foo();
}

@groovy.transform.CompileStatic
class C {
    String name
    void bar(B b)
    {
        b.foo()
        name = b.name
    }
}
new C()
{noformat}

With this error:
{noformat}
[Static type checking] - No such property: name for class: B
{noformat}

Changing the code to use getName() works, as does casting b to A.

I also encountered a related issue, in which the getName() workaround fails:
{noformat}
interface A {
    String getName();
}

interface A2 {
    String getName();
}

interface B extends A,A2 {
    void foo();
}

@groovy.transform.CompileStatic
class C {
    String name
    void bar(B c)
    {
        c.foo()
        name = c.getName()
    }
}
new C()
{noformat}
The casting workaround still works, but otherwise the error given is:
{noformat}
[Static type checking] - Reference to method is ambiguous. Cannot choose between [MethodNode@1813166140[java.lang.String getName()], MethodNode@877270685[java.lang.String getName()]]
{noformat}
",melix,scoreunder,Major,Closed,Fixed,06/Jul/12 20:54,26/Jul/12 16:49
Bug,GROOVY-5582,12816393,An AIC cannot extend a non static inner class,"Groovy doesn't support anonymous inner classes if they extend a *non static* inner class. For example:

{code}
class Outer {
    int outer() { 1 }
    abstract class Inner {
        abstract int inner()
    }
    int test() {
        Inner inner = new Inner() {
            int inner() { outer() }
        }
        inner.inner()
    }
}
assert new Outer().test() == 1
{code}",blackdrag,melix,Major,Closed,Fixed,10/Jul/12 07:02,08/Jul/14 15:28
Bug,GROOVY-5583,12816287,Parsing problem of number literal 0x8000000000000000L,"I have the following small part of a java class:
{code}
    public class GBitMaskTest {
    
        @Test
        public void checkFirstBitTest() {
            BitMask bm = new BitMask(0x8000000000000000L);
            assertEquals(true, bm.isBitSet(63));
        }
    
    ...
    }
{code}
which should be compilable in Groovy as well cause it's valid Java code. But i get the following:
{code}
    Groovy Bug --- exception in phase 'conversion' in source unit '/home/g-ut-example/src/test/groovy/com/soebes/training/maven/simple/GBitMaskTest.groovy' For input string: ""8000000000000000""
    BUG! exception in phase 'conversion' in source unit '/home/g-ut-example/src/test/groovy/com/soebes/training/maven/simple/GBitMaskTest.groovy' For input string: ""8000000000000000""
{code}
This has happened with Groovy 2.0 in relationship with Maven. It's this example https://github.com/khmarbaise/maui/tree/master/src/main/resources/g-ut-example which produces this problem. I'm compiling with the following part:
{code}
      <build>
        <plugins>
          <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <configuration>
              <compilerId>groovy-eclipse-compiler</compilerId>
              <compilerArgument>nowarn</compilerArgument>
              <verbose>true</verbose>
            </configuration>
            <dependencies>
              <dependency>
                <groupId>org.codehaus.groovy</groupId>
                <artifactId>groovy-eclipse-compiler</artifactId>
                <version>2.7.0-01</version>
              </dependency>
              <dependency>
                <groupId>org.codehaus.groovy</groupId>
                <artifactId>groovy-eclipse-batch</artifactId>
                <version>2.0.0-01</version>
              </dependency>
            </dependencies>
          </plugin>
        </plugins>
      </build>
{code}
The rest of the error message (EXception):
{code}
    GBitMaskTest.groovy' For input string: ""8000000000000000""
    	at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:1001)
    	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:624)
    	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:600)
    	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:577)
    	at org.codehaus.jdt.groovy.internal.compiler.ast.GroovyCompilationUnitDeclaration.processToPhase(GroovyCompilationUnitDeclaration.java:171)
    	at org.codehaus.jdt.groovy.internal.compiler.ast.GroovyParser.dietParse(GroovyParser.java:455)
    	at org.codehaus.jdt.groovy.integration.internal.MultiplexingParser.dietParse(MultiplexingParser.java:44)
    	at org.eclipse.jdt.internal.compiler.Compiler.internalBeginToCompile(Compiler.java:775)
    	at org.eclipse.jdt.internal.compiler.Compiler.beginToCompile(Compiler.java:395)
    	at org.eclipse.jdt.internal.compiler.Compiler.compile(Compiler.java:485)
    	at org.eclipse.jdt.internal.compiler.batch.Main.performCompilation(Main.java:3829)
    	at org.eclipse.jdt.internal.compiler.batch.Main.compile(Main.java:1682)
    	at org.codehaus.groovy.eclipse.compiler.GroovyEclipseCompiler.compile(GroovyEclipseCompiler.java:243)
    	at org.apache.maven.plugin.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:678)
    	at org.apache.maven.plugin.TestCompilerMojo.execute(TestCompilerMojo.java:161)
    	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
    	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
    	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
    	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
    	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
    	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
    	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)
    	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
    	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
    	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
    	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    	at java.lang.reflect.Method.invoke(Method.java:597)
    	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)
    	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)
    	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)
    	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)
    Caused by: java.lang.NumberFormatException: For input string: ""8000000000000000""
    	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
    	at java.lang.Long.parseLong(Long.java:422)
    	at org.codehaus.groovy.syntax.Numbers.parseInteger(Numbers.java:215)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.integerExpression(AntlrParserPlugin.java:3184)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:2153)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:2018)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:2008)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:2046)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:2018)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:2008)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.addArgumentExpression(AntlrParserPlugin.java:3071)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.arguments(AntlrParserPlugin.java:2994)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.constructorCallExpression(AntlrParserPlugin.java:2932)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:2069)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:2018)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:2008)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.declarationExpression(AntlrParserPlugin.java:1776)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.variableDef(AntlrParserPlugin.java:1792)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.statement(AntlrParserPlugin.java:1513)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.statementListNoChild(AntlrParserPlugin.java:1599)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.statementList(AntlrParserPlugin.java:1574)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.methodDef(AntlrParserPlugin.java:1045)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.objectBlock(AntlrParserPlugin.java:800)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.innerClassDef(AntlrParserPlugin.java:783)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.classDef(AntlrParserPlugin.java:677)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.convertGroovy(AntlrParserPlugin.java:361)
    	at org.codehaus.groovy.antlr.AntlrParserPlugin.buildAST(AntlrParserPlugin.java:269)
    	at org.codehaus.groovy.control.SourceUnit.convert(SourceUnit.java:302)
    	at org.codehaus.groovy.control.CompilationUnit$3.call(CompilationUnit.java:706)
    	at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:992)
    	... 35 more
{code}",guillaume,khmarbaise,Major,Closed,Fixed,10/Jul/12 15:34,26/Jul/12 16:49
Bug,GROOVY-5584,12816290,"@CompileStatic fails to compile Map.each { key, value -> } form","The following code (from Map#each documentation in the Groovy JDK) fails under static compilation:

{code}
import groovy.transform.CompileStatic

@CompileStatic def test() {
    def result = """"
    [a:1, b:3].each { key, value -> result += ""$key$value"" }
    assert result == ""a1b3""
}
test()
{code}

with the message:
{code}
Caught: groovy.lang.MissingMethodException: No signature of method: mapeach$_test_closure1.doCall() is applicable for argument types: (java.util.LinkedHashMap$Entry) values: [a=1]
Possible solutions: doCall(java.lang.Object, java.lang.Object), findAll(), findAll(), isCase(java.lang.Object), isCase(java.lang.Object)
groovy.lang.MissingMethodException: No signature of method: mapeach$_test_closure1.doCall() is applicable for argument types: (java.util.LinkedHashMap$Entry) values: [a=1]
Possible solutions: doCall(java.lang.Object, java.lang.Object), findAll(), findAll(), isCase(java.lang.Object), isCase(java.lang.Object)
	at mapeach.test(mapeach.groovy:5)
	at mapeach.run(mapeach.groovy:8)
{code}",melix,blendmaster,Major,Closed,Fixed,10/Jul/12 17:05,26/Jul/12 16:49
Bug,GROOVY-5585,12816286,interface.property not recognized by type checker if property is of Object,"The type checker fails to recognize properties like {{.class}} on an interface if the property is defined in the Object class.

{code}
Class test(Serializable arg) {
    Class<?> clazz = arg.class
    clazz
}
assert test('foo') == String
{code}
",melix,melix,Major,Closed,Fixed,11/Jul/12 06:34,26/Jul/12 16:49
Bug,GROOVY-5586,12818199,@Canonical class causes compile-time NPE under @StaticCompilation,"{code}
import groovy.transform.*
@CompileStatic
class CanonicalStaticTest extends GroovyTestCase {
  @Canonical class Thing {
    String stuff
  }

  void testCanonical() {
    def thing = new Thing()
  }
}
{code}

fails to compile with the message:

{noformat}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
General error during class generation: NPE while processing CanonicalStaticTest.groovy

groovy.lang.GroovyRuntimeException: NPE while processing CanonicalStaticTest.groovy
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:199)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:260)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:244)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:185)
	at groovy.lang.GroovyShell$2.run(GroovyShell.java:206)
	at groovy.lang.GroovyShell$2.run(GroovyShell.java:204)
	at java.security.AccessController.doPrivileged(Native Method)
	at groovy.lang.GroovyShell.run(GroovyShell.java:204)
	at groovy.lang.GroovyShell.run(GroovyShell.java:150)
	at groovy.ui.GroovyMain.processOnce(GroovyMain.java:557)
	at groovy.ui.GroovyMain.run(GroovyMain.java:344)
	at groovy.ui.GroovyMain.process(GroovyMain.java:330)
	at groovy.ui.GroovyMain.processArgs(GroovyMain.java:119)
	at groovy.ui.GroovyMain.main(GroovyMain.java:99)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)
Caused by: java.lang.NullPointerException
	at org.codehaus.groovy.classgen.asm.BytecodeHelper.getTypeDescription(BytecodeHelper.java:163)
	at org.codehaus.groovy.classgen.asm.BytecodeHelper.getTypeDescription(BytecodeHelper.java:126)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAttributeOrProperty(AsmClassGenerator.java:1013)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitPropertyExpression(AsmClassGenerator.java:1052)
	at org.codehaus.groovy.ast.expr.PropertyExpression.visit(PropertyExpression.java:55)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:115)
	at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeDirectMethodCall(StaticInvocationWriter.java:188)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:221)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76)
	at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.makeCall(StaticInvocationWriter.java:383)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:334)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:649)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:510)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:456)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:320)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:277)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:397)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:181)
	... 25 more

1 error
{noformat}",melix,blendmaster,Major,Closed,Fixed,11/Jul/12 10:45,26/Jul/12 16:49
Bug,GROOVY-5587,12818124,"Map.Entry<K,V>#key and #value fail to infer type correctly under @StaticCompile","{code}
import groovy.transform.CompileStatic
@CompileStatic
class BrokenCompileStaticTest extends GroovyTestCase {
  void testMapForEachAttribute() {
    def result = """"
    def sum = 0
    for ( Map.Entry<String, Integer> it in [a:1, b:3].entrySet() ) {
       result += it.key
       sum += it.value
    }
    assert result == ""ab""
    assert sum == 4
  }
}
{code}

fails to infer the type of Map.Entry#key and #value, regardless of the proper generic type in the for-in loop definition.

Message:

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/home/steven/BrokenCompileStaticTest.groovy: 9: [Static type checking] - Cannot find matching method int#plus(java.lang.Object <V>)
 @ line 9, column 8.
          sum += it.value
          ^

1 error
{code}

Also, shouldn't groovy report both static type errors, instead of just the first?",melix,blendmaster,Major,Closed,Fixed,11/Jul/12 10:57,26/Jul/12 16:49
Bug,GROOVY-5588,12816395,Multidimensional Arrays: Incompatible argument to function,"Hey, starting with Groovy 1.8.x, and now as well with Groovy 2.0, the following code produces an error

int[][] x = new int[10][10]
x[1][1] += 5

Error is:
Caught: java.lang.VerifyError: (class: test, method: run signature: ()Ljava/lang/Object;) Incompatible argument to function
java.lang.VerifyError: (class: test, method: run signature: ()Ljava/lang/Object;) Incompatible argument to function

(there were some issues with multidimensional arrays in earlier versions but it still seems to be a problem in the new version)",blackdrag,casp,Major,Closed,Fixed,11/Jul/12 11:11,14/Aug/12 07:25
Bug,GROOVY-5589,12816405,Type checker doesn't allow property notation as LHS when only a setter exists,"If a class has only a setter, but no getter:
{code}
class A {
   void setFoo(String arg) {}
}
{code}
Then the type checker doesn't allow the property notation for assignments:
{code}
def a = new A()
a.foo = 'bar'
{code}
",melix,melix,Major,Closed,Fixed,11/Jul/12 11:25,26/Jul/12 16:49
Bug,GROOVY-5590,12811898,"Caused by: org.codehaus.groovy.runtime.InvokerInvocationException: java.io.IOException: Cannot run program ""find"": java.io.IOException: error=24, Too many open files","A script I wrote starts many processes but all within a common GPars pool so no more than a few are running at a time. It fails with the following error:

Caught: java.util.concurrent.ExecutionException: org.codehaus.groovy.runtime.InvokerInvocationException: org.codehaus.groovy.runtime.InvokerInvocationException: java.io.IOException: Cannot run program ""find"": java.io.IOException: error=24, Too many open files

I verified that the problem is fixed by calling process.out.close() after every Process finishes. However, this solution is undesirable for a few two reasons. First, it adds an extra possibly avoidable line. Second, it prevents usage of convenience methods like getText(), where the Process might not have been exposed.

The fix works because the Process remains open as long as STDIN is open. I see that STDOUT and STDERR are closed already so probably someone tried to fix this problem in the past but closed the wrong streams. To fix the problem, I propose that the STDIN (out) stream be closed as well where those streams are already closed. Additionally, I propose adding the close() calls to getText() as well.

I've attached a diff with the change for review. I rebuilt Groovy and verified that the script no longer fails with these changes. A unit test is still needed but haven't delved into that part of Groovy source quite yet :)",guillaume,balor123,Major,Closed,Fixed,11/Jul/12 15:29,05/Apr/15 14:43
Bug,GROOVY-5591,12816254,groovy-all-2.0.0-sources.jar is missing lots of source files,"Not sure if this is known already, but I couldn't find anything on it in Jira or mailing list. The groovy-all-2.0.0-sources.jar as published to Maven Central seems to be missing lots of source files. For example, the only file under groovy/lang is GroovyLogTestCase.",paulk,pniederw,Critical,Closed,Fixed,11/Jul/12 18:36,26/Jul/12 16:49
Bug,GROOVY-5592,12816246,No recent snapshot for 2.0.1 available from http://snapshots.repository.codehaus.org,"Latest snapshot is from June 26th, which is before 2.0.0 was released. Having access to the latest snapshot would help a lot, e.g. for Gradle development.",paulk,pniederw,Major,Closed,Fixed,11/Jul/12 19:20,26/Jul/12 16:49
Bug,GROOVY-5593,12816271,groovy-all-2.0.0 does not work with JDK 1.5,"I assume this is not intended? The problem is that org.codehaus.groovy.jsr223.ScriptExtensions gets loaded, which uses JDK 1.6 classes. For example I get ""java.lang.NoClassDefFoundError: javax/script/ScriptEngine"". Seems like this makes groovy-all-2.0.0 unusable with JDK 1.5.",melix,pniederw,Blocker,Closed,Fixed,11/Jul/12 20:35,26/Jul/12 16:49
Bug,GROOVY-5594,12816300,Type checker doesn't infer property type if coming from a generified getter,"If someone uses the property notation to get the key or the value of a map entry, the property type is not inferred properly. The workaround is to use a getter instead.

Test case:
{code}
Map.Entry<Date, Integer> entry

@ASTTest(phase=INSTRUCTION_SELECTION, value={
    assert node.getNodeMetaData(INFERRED_TYPE) == make(Date)
})
def k = entry?.key

@ASTTest(phase=INSTRUCTION_SELECTION, value={
    assert node.getNodeMetaData(INFERRED_TYPE) == Integer_TYPE
})
def v = entry?.value
{code}",melix,melix,Major,Closed,Fixed,12/Jul/12 04:31,26/Jul/12 16:49
Bug,GROOVY-5595,12816367,Type checker doesn't fully infer Map.Entry type,"If you have a map:
{code}Map<Date,Integer>{code}
and that you use the {{entrySet()}} method, the inferred type is:
{code}Set<Map.Entry<K,V>>{code}
but it should be:
{code}Set<Map.Entry<Date,Integer>>{code}

(multiple levels of generics)

Here's the test case:
{code}
Map<Date, Integer> map

@ASTTest(phase=INSTRUCTION_SELECTION, value={
    def infType = node.getNodeMetaData(INFERRED_TYPE)
    assert infType == make(Set)
    def entryInfType = infType.genericsTypes[0].type
    assert entryInfType == make(Map.Entry)
    assert entryInfType.genericsTypes[0].type == make(Date)
    assert entryInfType.genericsTypes[1].type == Integer_TYPE
})
def entries = map?.entrySet()
{code}",melix,melix,Major,Closed,Fixed,12/Jul/12 04:52,26/Jul/12 16:49
Bug,GROOVY-5598,12816344,Cannot use @CompileStatic with Thread.start method,"Example:

{code}

@groovy.transform.CompileStatic
def foo() {
  Thread.start {
      println ""boo""
  }
}

foo()
{code}

Produces

{code}
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'class java.lang.Thread' with class 'java.lang.Class' to class 'java.lang.Thread'
	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToType(DefaultTypeTransformation.java:360)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.castToType(ScriptBytecodeAdapter.java:599)

{code}",guillaume,graemerocher,Major,Closed,Fixed,12/Jul/12 08:32,26/Jul/12 16:49
Bug,GROOVY-5601,12818173,static type checker NPE when instantiating generic interface (with and without diamond),"{code}
package compile

import groovy.transform.TypeChecked

interface Mapper<F, T> {
    T to(F from)
}

@TypeChecked
class Main {
    static void main(String[] args) {
        Mapper<String, Integer> mapper = new Mapper<String, Integer>() { //        Mapper<String, Integer> mapper = new Mapper<>() {
            Integer to(String from) {
                17
            }
        };
    }
}
{code}
Results in:

{code}
$ ~/Apps/groovy/bin/groovyc -e compile/Mapper.groovy 
null
>>> stacktrace:
java.lang.NullPointerException
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.inferDiamondType(StaticTypeCheckingVisitor.java:456)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBinaryExpression(StaticTypeCheckingVisitor.java:394)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitDeclarationExpression(CodeVisitorSupport.java:245)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitDeclarationExpression(ClassCodeVisitorSupport.java:107)
        at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:87)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:69)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:195)
        at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
        at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:1051)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:1252)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:1226)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:216)
        at org.codehaus.groovy.transform.StaticTypesTransformation.visit(StaticTypesTransformation.java:70)
        at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:132)
        at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:176)
        at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:496)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:213)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:146)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:176)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)
{code}

It doesn't matter if I explicitly specify the parameters during creation, or whether I let the diamong operator kick in. Without @TypeChecked, it works fine. I am not sure whether the problem happens only with anonymous inner classes or is more general.",melix,wujek.srujek,Major,Closed,Fixed,14/Jul/12 06:27,26/Jul/12 16:49
Bug,GROOVY-5605,12813399," Class Cast Exception when using Groovy 2.0 Indy=""true"" and using an integer with a double.","The following code causes a Class Cast Exception.
{code}
import java.awt.geom.Point2D;

def width = 240.0
def radius = width / 3.0
 
new Point2D.Double(x: 0, y: radius * 2 / 4)
{code}
{code}
Exception in thread ""main"" java.lang.ClassCastException: java.lang.Integer cannot be cast to java.math.BigInteger
	at org.codehaus.groovy.runtime.dgm$364.invoke(Unknown Source)
	at java.lang.invoke.MethodHandleImpl$GuardWithCatch.invoke_L2(MethodHandleImpl.java:1130)
	at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:681)
	at Main.run(Main.groovy:7)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1074)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:848)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:831)
	at org.codehaus.groovy.runtime.InvokerHelper.runScript(InvokerHelper.java:407)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeStaticMethod(MetaClassImpl.java:1307)
	at groovy.lang.MetaClassImpl.invokeStaticMethod(MetaClassImpl.java:1293)
	at java.lang.invoke.MethodHandleImpl$GuardWithCatch.invoke_L3(MethodHandleImpl.java:1138)
	at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:681)
	at Main.main(Main.groovy)
{code}

If you change the Groovy code to:
{code}
import java.awt.geom.Point2D;

def width = 240.0
def radius = width / 3.0
 
new Point2D.Double(x: 0, y: radius * 2D / 4D)
{code}
it works ok.",blackdrag,jimclarke5,Major,Closed,Fixed,16/Jul/12 06:56,26/Jul/12 16:49
Bug,GROOVY-5606,12816396,Problem with CompileStatic,"The following code compiles with 2.0.0 but not with the current 2.0.1-SNAPSHOT.

{code:title=src/main/groovy/com/demo/MyUtility.groovy|borderStyle=solid}
package com.demo

import groovy.transform.CompileStatic

class MyUtility {
    @CompileStatic
    def methodOne() {
        def someFiles = new ArrayList<File>()
        def someString = ''
        methodTwo someString, someFiles
    }

    @CompileStatic
    def methodTwo(String s, List<File> files) {}
}
{code}

Extract the attached project and execute the following command to see the error:

{noformat}
$ ./gradlew clean compileGroovy
:clean
:compileJava UP-TO-DATE
:compileGroovy
startup failed:
/Users/jeff/t/compilestatic/src/main/groovy/com/demo/MyUtility.groovy: 10: [Static type checking] - Cannot call com.demo.MyUtility#methodTwo(java.io.File) with arguments [java.lang.String, java.util.ArrayList <File>] 
 @ line 10, column 9.
           methodTwo someString, someFiles
           ^

1 error


FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':compileGroovy'.
> Compilation failed; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 3.453 secs
{noformat}

If you edit build.gradle and change 2.0.1-SNAPSHOT to 2.0.0 the code does compile.",melix,brownj,Major,Closed,Fixed,16/Jul/12 19:44,26/Jul/12 16:49
Bug,GROOVY-5607,12816444,@Compile static Sql.newInstance,"Don't know if this bug has been reported before (didn't find anything while looking)
{code}
@CompileStatic
class M
{
	public void r()
	{
	     Sql sql = Sql.newInstance(""jdbc:mysql://localhost/dummy"", ""dummy"", ""dummy"", ""com.mysql.jdbc.Driver"")
	}
}

(new M()).r()
{code}
fails with
{code}
Caught: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: groovy.sql.Sql(java.lang.String, java.lang.String, java.lang.String, java.lang.String)
groovy.lang.GroovyRuntimeException: Could not find matching constructor for: groovy.sql.Sql(java.lang.String, java.lang.String, java.lang.String, java.lang.String)
	at M.r(Test2.groovy:11)
	at M$r.call(Unknown Source)
	at Test2.run(Test2.groovy:18)
{code}

Looks like DefaultGroovyMethods.newInstance get's called instead.

Removing @CompileStatic fixes the problem
",guillaume,gordon277,Major,Closed,Fixed,16/Jul/12 21:17,26/Jul/12 16:49
Bug,GROOVY-5608,12816408,@CompileStatic casting problem,"Don't know if this bug has been reported before (didn't find anything while looking)
{code}
@CompileStatic
class M
{
	public void r()
	{
		List<Integer> a = [1, 3, 5]
		List<Integer> b = (List<Integer>)a[1..2]
	}
}

(new M()).r()
{code}

fails with
{code}
D:\dev\eclipse_workspace\groovy_test\src\Test2.groovy: 13: [Static type checking] - Inconvertible types: cannot cast java.lang.Integer to java.util.List
 @ line 13, column 21.
   		List<Integer> b = (List<Integer>)a[1..2]
                       ^
{code}
Thinks that the sublist is a single Integer

Removing @CompileStatic fixes the problem
",guillaume,gordon277,Major,Closed,Fixed,16/Jul/12 21:20,26/Jul/12 16:49
Bug,GROOVY-5609,12816419,Type checking of generic argument types of extension methods on abstract classes using generics,"If an extension method is using an abstract class or an array as the first argument and that argument makes use of a generic type defined in the signature of the extension method, the type checker won't be able to type check a method call properly.

Example:
{code}
// <T> is used in the first argument
public static <T> void foo(AbstractList<T> list, T[] elems)
// <T> is used in an array of the first argument
public static <T> void foo(T[] list, T[] elems)
{code}

Note that the limitation doesn't exist if the method is defined on an interface.",,melix,Minor,Closed,Fixed,17/Jul/12 14:37,08/Mar/22 12:16
Bug,GROOVY-5610,12816295,Type checking of method calls using generics and default values is not supported,"Imagine a method with the following signature:
{code}
def foo(List<? extends A> arg, String value='default')
{code}

And a method call:

{code}
foo(a)
{code}

Then in that case, the type checker will not verify that for argument {{a}}, the generics match the signature (only that {{a}} is a {{List}}).

A method call with all arguments set will not have this limitation.",blackdrag,melix,Minor,Closed,Fixed,17/Jul/12 14:41,14/Oct/13 16:54
Bug,GROOVY-5611,12816418,Annotating a method with @CompileStatic when the class is already annotated leads to errors,"If a method is annotated with {{@CompileStatic}} while its owner class is also annotated, the type checker reports incorrect errors.
",melix,melix,Major,Closed,Fixed,18/Jul/12 04:51,26/Jul/12 16:49
Bug,GROOVY-5612,12816410,"Fix ""gradle idea"" task","The ""gradle idea"" task doesn't generate a project that works out of the box.",melix,melix,Major,Closed,Fixed,18/Jul/12 14:45,26/Jul/12 16:49
Bug,GROOVY-5613,12816400,@CompileStatic - Caught: java.lang.VerifyError - Inconsistent stack height 1 != 2,"Hello,

I am trying to build an application and I am getting some âInconsistent stack heightâ errors, I donât really know if itâs an error in the libraries I am using or a Groovy bug.
I have managed the reproduce the error with the minimum code and I have also attached all the needed files to reproduce the error.


Note:
 - We donât get the Verify Error if we use âimport staticâ and change âmsg?.â with âmsg.â
 - We donât get the Verify Error if we use âimportâ and with âmsg?.â
 - We donât get the Verify Error if we run the code with @TypeChecked


{code}
import groovy.transform.CompileStatic
import groovy.transform.TypeChecked

import com.hp.openview.ib.api.jopc.JOpcServerMessage
import com.hp.openview.ib.api.jopc.JOpcMessage
import static com.hp.openview.ib.api.jopc.JOpcHelper.*  // Uncomment for example VerifyError and Works
//import com.hp.openview.ib.api.jopc.JOpcHelper  // Uncomment for example Works2


//@TypeChecked // All the examples work is we use TypeChecked instead of CompileStatic
@CompileStatic
class ErrorReproduction {
  JOpcServerMessage msg

  ErrorReproduction() {
    String VerifyError = long2EventType(msg?.getEventFlag())
    //String Works = long2EventType(msg.getEventFlag())  // This work if we remove the ""?""
    //String Works2 = JOpcHelper.long2EventType(msg?.getEventFlag()) // This work if we dont use the ""import static""
  }

  static void main(String[] args) {
    ErrorReproduction m = new ErrorReproduction()
  }
}
{code} 

{code}
Caught: java.lang.VerifyError: (class: ErrorReproduction, method: <init> signature: ()V) Inconsistent stack height 1 != 2
java.lang.VerifyError: (class: ErrorReproduction, method: <init> signature: ()V) Inconsistent stack height 1 != 2
{code}
",melix,syepes,Major,Closed,Fixed,19/Jul/12 04:27,26/Jul/12 16:49
Bug,GROOVY-5614,12814235,Diamond Inference broken with Static Compilation,"Example program:

{code}
package example

import groovy.transform.TypeChecked

@TypeChecked
class Rules {

    private final Map<String, String> bindings = new HashMap<>();

}
{code}

Causes Stacktrace:

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
General error during instruction selection: 0

java.lang.ArrayIndexOutOfBoundsException: 0
        at org.codehaus.groovy.ast.tools.GenericsUtils.extractPlaceholders(GenericsUtils.java:130)
        at org.codehaus.groovy.ast.tools.GenericsUtils.parameterizeInterfaceGenerics(GenericsUtils.java:166)
        at org.codehaus.groovy.ast.GenericsType$GenericsTypeMatcher.compareGenericsWithBound(GenericsType.java:301)
        at org.codehaus.groovy.ast.GenericsType$GenericsTypeMatcher.checkGenerics(GenericsType.java:261)
        at org.codehaus.groovy.ast.GenericsType$GenericsTypeMatcher.matches(GenericsType.java:228)
        at org.codehaus.groovy.ast.GenericsType.isCompatibleWith(GenericsType.java:153)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.typeCheckAssignment(StaticTypeCheckingVisitor.java:647)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitField(StaticTypeCheckingVisitor.java:951)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1048)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:216)
        at org.codehaus.groovy.transform.StaticTypesTransformation.visit(StaticTypesTransformation.java:70)
        at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:132)
        at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:176)
        at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:496)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:213)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:146)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:176)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)

1 error
{code}

This was using the 2.1.0-SNAPSHOT built from 333084c467c18f2afc9460e58d48c43909ad8d1a of git MASTER.  The problem disappears if you provide type arguments to the HashMap constructor or if you remove @TypeChecked.",melix,richardwarburton,Major,Closed,Fixed,19/Jul/12 08:07,26/Jul/12 16:49
Bug,GROOVY-5615,12816424,var.getProperties() cannot be invoked as var.properties when using @CompileStatic,"
{code}
@groovy.transform.CompileStatic
class MyClass {

 void method() {
    String var = ""Hola""
    println this.pepe
    println var.getProperties()    
    
    println var.properties  //Works without @CompileStatic, but it doesn't with it
 }
 
 
 String getPepe() {
 return ""Hola Pepe"";
 }
 
}

new MyClass().method()
{code}",melix,ariel.andres.morelli,Major,Closed,Fixed,19/Jul/12 09:18,26/Jul/12 16:49
Bug,GROOVY-5616,12816409,Compile static failing to allow cast of groovy type to GroovyObject,"I tried to see if this was already fixed or covered by another issue, but I couldn't find something like it...  Here is the code:

{code}
class Foo {
    @groovy.transform.CompileStatic
    public void blah() {
        ((GroovyObject)this);
    }
}
{code}

This is currently causing me an issue when trying to build grails-core in STS (it has a snippet somewhat similar to this, but obviously it does a bit more).  For some reason it works through the gradle build on the command line, but not if I just run groovy 2.0.0 groovyc against that class above.  I also grabbed a snapshot and tried it, which seemed to give the same result.

Oh, the error is:

{code}
> groovyc Foo.groovy
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Foo.groovy: 4: [Static type checking] - Inconvertible types: cannot cast Foo to groovy.lang.GroovyObject
 @ line 4, column 3.
   		((GroovyObject)this);
     ^

1 error
{code}
",melix,aclement,Major,Closed,Fixed,19/Jul/12 16:25,26/Jul/12 16:49
Bug,GROOVY-5617,12818435,strange behavior with lists involving generics,"this works fine:
{code}
@groovy.transform.TypeChecked
def test() {
    List<GString> dates = [""${new Date()-1}"", ""${new Date()}"", ""${new Date()+1}""]
    List<String> upper = dates*.toUpperCase()
}
test()
{code}
but this fails:
{code}
@groovy.transform.TypeChecked
def test() {
    List<GString> dates = [""${new Date()-1}"", ""${new Date()}"", ""${new Date()+1}""]
    List<GString> copied = []
    copied.addAll(dates)
    List<String> upper = copied*.toUpperCase()
}
test()
{code}
with this error message:
{noformat}
2 compilation errors:

[Static type checking] - Cannot find matching method groovy.lang.GString#toUpperCase(). Please check if the declared type is right and if the method exists.
 at line: 6, column: 26

[Static type checking] - Incompatible generic argument types. Cannot assign java.util.List <java.lang.Object> to: java.util.List <String>
 at line: 6, column: 5
{noformat}
",melix,paulk,Major,Closed,Fixed,20/Jul/12 04:39,07/Apr/15 19:06
Bug,GROOVY-5618,12816440,Type checker throws No such property: value for class: java.lang.Object ,"In the following example:
{code}
class MyUtility {
    protected String value

    @groovy.transform.TypeChecked
    void foo() {
        this.@value = 'new value'
    }
}
{code}
The compiler doesn't find the attribute. Removing ""@"" works.",melix,melix,Major,Closed,Fixed,20/Jul/12 10:53,26/Jul/12 16:49
Bug,GROOVY-5619,12816413,Static compiler calls setter even with attribute notation,"If compiled with {{@CompileStatic}}, in the following example, we use the attribute notation, so the static compiler should perform a direct field access. However, it calls the setter:

{code}
class A {
    boolean setterCalled = false

    protected int x
    public void setX(int a) {
        setterCalled = true
        x = a
    }
}
class B extends A {
    void m() {
        this.@x = 2
    }
}
B b = new B()
b.m()
assert b.isSetterCalled() == false
{code}",melix,melix,Major,Closed,Fixed,20/Jul/12 10:55,26/Jul/12 16:49
Bug,GROOVY-5620,12816281,Spread-safe operator unsupported on LHS of assignments,"Using the spread safe operator on the LHS of assignements is not supported for statically compiled code. For example:

{code}
@CompileStatic(List<A> list) {
   list*.foo = 1
}
{code}",melix,melix,Major,Closed,Fixed,23/Jul/12 09:42,26/Jul/12 16:49
Bug,GROOVY-5622,12816420,"Cannot loop using foreach or ""for in"" in @TypeChecked code","{code:java}
    @Test
    @TypeChecked
    void testForEach() {
        int i = 0;
        for (Field field : String.class.declaredFields) {
            i++;
        }
        assert i > 0
    }
{code}

fails on compile time with
{quote}[Static type checking] - Cannot loop with element of type java.lang.reflect.Field -> java.lang.reflect.Field with collection of type java.lang.ref.SoftReference <T extends java.lang.Object -> java.lang.Object>
{quote}",melix,nplekhanov,Major,Closed,Fixed,24/Jul/12 00:06,26/Jul/12 16:49
Bug,GROOVY-5623,12816415,"collection's ""each"" fails if compiled with @CompileStatic","{code}
@Test
@CompileStatic
void test() {
    int n = 0;
    String.getDeclaredFields().each {n++}
    assert n > 0
}
{code}

failed in runtime with

{noformat}
java.lang.VerifyError: (class: EachByClosureTest, method: test signature: ()V) Register 1 contains wrong type
        at java.lang.Class.getDeclaredMethods0(Native Method)
        at java.lang.Class.privateGetDeclaredMethods(Class.java:2427)
        at java.lang.Class.getMethod0(Class.java:2670)
        at java.lang.Class.getMethod(Class.java:1603)
        at org.apache.maven.surefire.util.ReflectionUtils.tryGetMethod(ReflectionUtils.java:57)
        at org.apache.maven.surefire.common.junit3.JUnit3TestChecker.isSuiteOnly(JUnit3TestChecker.java:65)
        at org.apache.maven.surefire.common.junit3.JUnit3TestChecker.isValidJUnit3Test(JUnit3TestChecker.java:60)
        at org.apache.maven.surefire.common.junit3.JUnit3TestChecker.accept(JUnit3TestChecker.java:55)
        at org.apache.maven.surefire.common.junit4.JUnit4TestChecker.accept(JUnit4TestChecker.java:52)
        at org.apache.maven.surefire.util.DefaultDirectoryScanner.locateTestClasses(DefaultDirectoryScanner.java:80)
        at org.apache.maven.surefire.junit4.JUnit4Provider.scanClassPath(JUnit4Provider.java:174)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:83)
        ... 9 more
{noformat}",melix,nplekhanov,Major,Closed,Fixed,24/Jul/12 02:13,26/Jul/12 16:49
Bug,GROOVY-5627,12816371,Problems with categories when using invoke dynamic,"If you have a call site, that has been visited before, it will not be updated with the category. 

If the callsite has been visited first with the category, it will not be invalidated once the category is not available anymore. 

And if Thread x uses a category, Thread y may see it as well although it shouldn't.",blackdrag,guillaume,Major,Closed,Fixed,25/Jul/12 05:21,11/Sep/12 01:15
Bug,GROOVY-5628,12816517,new JsonBuilder('<complex xml string>').toPrettyPrint() throws StackOverflowException,"When we run the following, 
{code}
new JsonBuilder('<a a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\"" a=\\""\\""/>').toPrettyPrint()
{code}
we get a StackOverflowException
{code}
 java.lang.StackOverflowError
	at java.lang.Character.codePointAt(Character.java:2335)
	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3344)
	at java.util.regex.Pattern$Curly.match0(Pattern.java:3760)
	at java.util.regex.Pattern$Curly.match(Pattern.java:3744)
	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
	at java.util.regex.Pattern$Curly.match0(Pattern.java:3782)
	at java.util.regex.Pattern$Curly.match(Pattern.java:3744)
	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
        ...
{code}
We have made a basic change to fix the problem as an override/temporary solution as follows...
{code}
JsonTokenType.STRING.validator = Pattern.compile('""(?>[^""\\\\]+|\\\\[""\\\\bfnrt\\/]|\\\\u[0-9a-fA-F]{4})*""', Pattern.DOTALL)
{code}
The main change is the addition of ?> and the change from * to +.",guillaume,crykal79,Minor,Closed,Fixed,25/Jul/12 11:25,26/Jul/12 14:45
Bug,GROOVY-5630,12818183,Java stub generator generates wrong cast for return value of generic method,"From org/gradle/util/HelperUtil.groovy:

{code}
class HelperUtil {
    ...
    static <T extends Task> T createTask(Class<T> type) {
        ...
    }
    ...
}
{code}

Generated method stub in Groovy 1.8.6:

{code}
public static <T extends org.gradle.api.Task> T createTask(java.lang.Class<T> type) { return (T)null;}
{code}

This stub compiles fine.

Generated method stub in Groovy 1.8.7:

{code}
public static <T extends org.gradle.api.Task> T createTask(java.lang.Class<T> type) { return (org.gradle.api.Task)null;}
{code}

This stub gives the following compile error:

{noformat}
/swd/prj/gradle/subprojects/core/build/tmp/groovy-java-stubs/org/gradle/util/HelperUtil.java:22: error: incompatible types
public static <T extends org.gradle.api.Task> T createTask(java.lang.Class<T> type) { return (org.gradle.api.Task)null;}
                                                                                             ^
  required: T
  found:    Task
  where T is a type-variable:
    T extends Task declared in method <T>createTask(Class<T>)
{noformat}",guillaume,pniederw,Critical,Closed,Fixed,25/Jul/12 17:18,21/Sep/12 16:58
Bug,GROOVY-5632,12818174,Closure default params can cause BUG! exception in phase 'class generation',"Default parameters for methods can be used in further default parameters, ie:

{code}
def f( int x, fn={ -> x } ) {
  fn()
}

f( 10 )
{code}
(outputs 10)

However, if I give {{x}} a default value:

{code}
def f( int x=3, fn={ -> x } ) {
  fn()
}

f( 10 )
{code}

We get:

{code}
BUG! exception in phase 'class generation' in source unit 'ConsoleScript96' tried to get a variable with the name x as stack variable, but a variable with this name was not created
	at org.codehaus.groovy.classgen.asm.CompileStack.getVariable(CompileStack.java:280)
	at org.codehaus.groovy.classgen.asm.ClosureWriter.loadReference(ClosureWriter.java:131)
	at org.codehaus.groovy.classgen.asm.ClosureWriter.writeClosure(ClosureWriter.java:102)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClosureExpression(AsmClassGenerator.java:546)
	at org.codehaus.groovy.ast.expr.ClosureExpression.visit(ClosureExpression.java:43)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitCastExpression(AsmClassGenerator.java:616)
	at org.codehaus.groovy.ast.expr.CastExpression.visit(CastExpression.java:66)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.loadArguments(InvocationWriter.java:183)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:129)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:221)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:334)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:648)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:582)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeReturn(OptimizingStatementWriter.java:316)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:505)
	at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:613)
	at groovy.lang.GroovyShell.run(GroovyShell.java:480)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:951)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor218.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
{code}
",blackdrag,tim_yates,Minor,Closed,Fixed,26/Jul/12 02:56,12/Jan/13 20:40
Bug,GROOVY-5633,12816468,indy transformation for receiver missing,"This code:{code}
import static java.lang.Math.*

  R = 1;  C1 = 0.4; C2 = 0.9;  C3 = 6


//niters = getInt(""How many iterations of the Ikeda map"");
niters = 200000
x = new double[niters]
y = new double[niters]
x[0]=0.12; y[0]=0.2

k=1
km=0
  tau=0.0;  sintau=0.0;  costau=0.0

while  (k< niters)  {
   km=k-1
   tau = C1-C3/(1+x[km]*x[km]+y[km]*y[km])
   sintau = sin(tau); costau = cos(tau);
   x[k] = R+C2*(x[km]*costau-y[km]*sintau)
   y[k] = C2*(x[km]*sintau+y[km]*costau)

   k++
}
{code}
fails with a cast exception complaining it cannot cast BigDecimal to double. The reason is that C1 - (C3/(1+x[km]*x[km]+y[km]*y[km])) is a BigDecimal-Double operation, for which a double,double method will be selected. This requires a transformation of the receiver from BigDecimal to double. The current code does this only for the arguments.
",blackdrag,blackdrag,Major,Closed,Fixed,26/Jul/12 07:14,11/Sep/12 01:15
Bug,GROOVY-5635,12816392,"SyntaxException.getEndColumn() returns ""getStartColumn() + 1""","The exception {{org.codehaus.groovy.syntax.SyntaxException.getEndColumn()}} returns always {{getStartColumn() + 1}}.
(you can check that in the source code)

I'm trying to integrate the compiler to an IDE (JDeveloper) and I cannot get the error length to underline it properly.

I'm sending you a patch that obtains the error length from all the ASTNode's.
In the patch, I haven't deleted any unused constructor (even if it isn't used any more by the compiler) to maintain API backward compatibility.

I would really appreciate if you could fix it from groovy 2.0.2 on.",paulk,ariel.andres.morelli,Minor,Closed,Fixed,26/Jul/12 14:37,05/Apr/15 14:44
Bug,GROOVY-5636,12818171,java.lang.ArrayIndexOutOfBoundsException: size==0,"Problem compiling a Groovy class using the @CompileStatic annotation

https://github.com/glaforge/gaelyk/tree/groovy2

:core:compileGroovy
index problem in /glaforge-gaelyk/core/src/main/groovyx/gaelyk/GaelykCategory.groovy
startup failed:
General error during class generation: size==0

java.lang.ArrayIndexOutOfBoundsException: size==0
	at org.codehaus.groovy.classgen.asm.OperandStack.getTopOperand(OperandStack.java:717)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateEqual(BinaryExpressionHelper.java:297)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesBinaryExpressionMultiTypeDispatcher.evaluateEqual(StaticTypesBinaryExpressionMultiTypeDispatcher.java:97)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.eval(BinaryExpressionHelper.java:78)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:527)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:509)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeIfElse(StatementWriter.java:296)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitIfElse(AsmClassGenerator.java:471)
	at org.codehaus.groovy.ast.stmt.IfStatement.visit(IfStatement.java:41)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:496)
	at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:109)
	at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:39)
	at org.gradle.api.internal.tasks.compile.daemon.CompilerDaemonServer.execute(CompilerDaemonServer.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.remote.internal.TypeCastDispatch.dispatch(TypeCastDispatch.java:30)
	at org.gradle.messaging.remote.internal.WorkerProtocol.handleIncoming(WorkerProtocol.java:53)
	at org.gradle.messaging.remote.internal.WorkerProtocol.handleIncoming(WorkerProtocol.java:31)
	at org.gradle.messaging.remote.internal.ProtocolStack$ProtocolStage.handleIncoming(ProtocolStack.java:167)
     rg.gradle.messaging.remote.internal.ProtocolStack$BottomStage.handleIncoming(ProtocolStack.java:277)
	at org.gradle.messaging.remote.internal.ProtocolStack$BottomConnection$1.run(ProtocolStack.java:299)
	at org.gradle.messaging.remote.internal.ProtocolStack$ExecuteRunnable.dispatch(ProtocolStack.java:120)
	at org.gradle.messaging.remote.internal.ProtocolStack$ExecuteRunnable.dispatch(ProtocolStack.java:116)
	at org.gradle.messaging.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132)
	at org.gradle.messaging.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33)
	at org.gradle.messaging.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72)
	at org.gradle.messaging.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:66)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

1 error
",guillaume,sdmurphy,Blocker,Closed,Fixed,27/Jul/12 00:25,11/Sep/12 01:15
Bug,GROOVY-5638,12816278,static compiler fails with bug massage for method call in loop increment,"This code{code:Java}
@CompileStatic
class X {
    static void bar(){
        for (int i=0;i<1000000; i+=foo()){}
    }
    static foo(){1}
}{code}
may not make so much sense, but should at least not fail during compilation with a GroovyBugError. The message complains about the method call in the i+=foo() part",melix,blackdrag,Major,Closed,Fixed,27/Jul/12 14:59,11/Sep/12 01:15
Bug,GROOVY-5639,12811610,"Spurious ""Closure shared variable has been assigned with various type"" exception","Compiling this fails:

{code}
import groovy.transform.TypeChecked

@TypeChecked
class Foo {

    private void doIt() {
        Closure<Void> c = {
            List<String> list = new ArrayList<String>()
            String s = ""foo""
            10.times {
                list.add(s)
            }
        }
    }
}
{code}

{noformat}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Foo.groovy: 11: [Static type checking] - A closure shared variable [list] has been assigned with various types and the method [add(java.lang.Object <E>)] does not exist in the lowest upper bound of those types: [java.util.ArrayList <java.lang.String>]. In general, this is a bad practice (variable reuse) because the compiler cannot determine safely what is the type of the variable at the moment of the call in a multithreaded context.
 @ line 11, column 17.
                   list.add(s)
                   ^

1 error
{noformat}




The error only occurs when a closure is involved. The following works fine:

{code}
import groovy.transform.TypeChecked

@TypeChecked
class Bar {
    void doIt() {
        List<String> list = new ArrayList<String>()
        String s = ""foo""
        10.times {
            list.add(s)
        }
    }
}
{code}",melix,mpierce,Major,Closed,Fixed,27/Jul/12 17:16,11/Sep/12 01:15
Bug,GROOVY-5640,12818179,static type checking fails in for-each loops,"I have some code that fail to compile with type checking. The example is not contrived, it comes from a real-life project that uses neo4j. The Node_ class is called Node, and I changed it here so that we exclude the possibility that Groovy somehow treats it as its Node class (which it doesn't anyways). The following code:

{code}
package test

import groovy.transform.TypeChecked

@TypeChecked
class Test {

    public void traverse() {
        println new Node_().class.name
        for (/*Object*/Node_ node : new MyTraverser().nodes()) {
            println node.class.name
        }
    }
}

class Node_ {}

interface Traverser {

    Iterable<Node_> nodes();
}

class MyTraverser implements Traverser {

    @Override
    Iterable<Node_> nodes() {
        []
    }
}
{code}

does not compile with the following error:

{noformat}
$ groovyc Test.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Test.groovy: 12: [Static type checking] - Cannot loop with element of type test.Node -> test.Node with collection of type java.util.List <java.lang.Object> -> java.util.List <E extends java.lang.Object -> java.lang.Object>
 @ line 12, column 9.
           for (/*Object*/Node node : new MyTraverser().nodes()) {
{noformat}

It compiles fine when the iteration uses Object instead of Node_, and when there is no type checking.",melix,wujek.srujek,Major,Closed,Fixed,28/Jul/12 08:05,11/Sep/12 01:15
Bug,GROOVY-5641,12816464,for variable omitted type makes Caught: BUG! exception in phase 'class generation',"Following code:
{code}
@groovy.transform.CompileStatic
def foo() {
    int[] perm = new int[10]
    for (i in 0..<10) {
      println(perm[i-0])
    }
}

{code}
generates error message:

{quote}Caught: BUG! exception in phase 'class generation' in source unit '/private/tmp/f.groovy' At line 13 column 20
On receiver: i with message: minus and arguments: 0
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY
BUG! exception in phase 'class generation' in source unit '/private/tmp/f.groovy' At line 13 column 20
On receiver: i with message: minus and arguments: 0
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY
{quote}

following is ok
{code}
@groovy.transform.CompileStatic
def foo() {
    int[] perm = new int[10]
    for (int i in 0..<10) {
      println(perm[i-0])
    }
}

{code}",melix,uehaj,Minor,Closed,Fixed,28/Jul/12 09:30,11/Sep/12 01:15
Bug,GROOVY-5642,12816447,"TypeChecked/CompileStatic says ambiguous about calling PrintWriter.write(byte[],int,int)","I tried TypeChecked/CompileStatic with calling PrintWriter.write(byte[], int, int) in following code:
{code}
//@groovy.transform.CompileStatic

@groovy.transform.TypeChecked
def main() {
  byte[] data = [1,2,3] as byte[]
  System.out.write((byte[])data, (int)0, (int)0)
}

main()
{code}

raises following static type check error:
{quote}
m.groovy: 11: [Static type checking] - Reference to method is ambiguous. Cannot choose between [MethodNode@1587779925[void write([B, int, int)], MethodNode@1727501758[void write([B)], MethodNode@957941600[void write([B)], MethodNode@902239314[void write([C)]]
 @ line 11, column 3.
     System.out.write((byte[])data, (int)0, (int)0)
     ^

1 error
{quote}
When tried without cast operator( System.out.write(data,0,0) ), the result was similar.
When I tried to call method which i defined, no error detected:
{code}
@groovy.transform.TypeChecked
def foo(byte[] c, int a, int b)
{}

@groovy.transform.TypeChecked
def main() {
  byte[] data = [1,2,3] as byte[]
  foo((byte[])data, (int)0, (int)0)
}

main()
{code}
",melix,uehaj,Major,Closed,Fixed,28/Jul/12 20:36,11/Sep/12 01:15
Bug,GROOVY-5643,12816422,array length is not correct with CompileStatic'd code.,"In following code, the second assert fails.

{code}
@groovy.transform.CompileStatic
def main(){
  boolean[] flags1 = new boolean[10]
  assert flags1.class == boolean[]
  assert flags1.size() == 10 // OK

  def flags2 = new boolean[10] // dynamic type
  assert flags2.class == boolean[]
  assert flags2.size() == 10 // LHS is 1(not expected)
}

main()
{code}

I tried also int[] instead of boolean[]
{code}
@groovy.transform.CompileStatic
def main(){
  int[] flags1 = new int[10]
  assert flags1.class == int[]
  assert flags1.size() == 10 // OK

  def flags2 = new int[10]  // dynamic type
  assert flags2.class == int[]
  assert flags2.size() == 10 // Caught: org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object '[I@3b845321' with class '[I' to class 'int'
}

main()

{code}

When remove CompileStatic annotation, above are no problem.
Using @TypeChecked instead of @CompileStatic claims nothing.
I think it is a problem about the code generated by CompileStatic or type inference.
",melix,uehaj,Major,Closed,Fixed,28/Jul/12 21:02,11/Sep/12 01:15
Bug,GROOVY-5644,12818180,UFO Operator(<=>) errors with CompileStatic and TypeChecked,"When I use UFO operator in following code:
{code}
@groovy.transform.CompileStatic
def main() {
  Closure c = { Integer l, Integer r -> l <=> r }
  // Closure c = { Integer l, Integer r -> l.compareTo(r) } // OK
  println([7, 4, 1, 2, 3].sort(c))
}
main()
{code}
reports:
{quote}
Caught: org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'ufo@7067f21' with class 'ufo' to class 'java.lang.Comparable'
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'ufo@7067f21' with class 'ufo' to class 'java.lang.Comparable'
	at ufo$_main_closure1.doCall(ufo.groovy)
	at ufo.main(ufo.groovy:6)
	at ufo.run(ufo.groovy:8)
{quote}
and I found another case, i don't know the relation of above case.
When running:
{code}
@groovy.transform.TypeChecked
def main() {
  Integer x = 3
  Integer y = 4
  println (x <=> y)
}
main()
{code}
Static type checker claims:
{quote}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/tool/ufo2.groovy: 6: [Static type checking] - Reference to method is ambiguous. Cannot choose between [MethodNode@2142220333[int compareTo(java.lang.Object)], MethodNode@153838582[int compareTo(java.lang.Integer)]]
 @ line 6, column 12.
     println (x <=> y)
              ^

1 error
{quote}",melix,uehaj,Major,Closed,Fixed,29/Jul/12 02:39,11/Sep/12 01:15
Bug,GROOVY-5645,12818181,CompileStatic cause inconsistent compilation error when varargs super method called (reference to method is ambiguous),"Consider an example base class:
{code:java}
class BaseObject {
    void trace(Integer logLevel, Object... args) { /* ... */ }
    void trace(Object... args) { /* ... */ }
}
{code}
and the extending class:
{code:java}
@CompileStatic
class ChildObject extends BaseObject {
    void doSomething() {
        trace('test2')          // OK
        super.trace('test2')    // OK
        trace(1, 'test1')       // OK
        super.trace(1, 'test1') // compilation error
    }
}
{code}

When a type checked compilation is enabled following error appears on the last line of 2nd example class:
{code}[Static type checking] - Reference to method is ambiguous. Cannot choose between [MethodNode@1022828757[void trace([Ljava.lang.Object;)], MethodNode@76324664[void trace(java.lang.Integer, [Ljava.lang.Object;)]]{code}

I've come across this problem when extending closed library class which has methods of such signatures. Only difference is no Integer passed, but some custom class object of the library API as the 1st param. Compilation error is the same.
Pay attention to the fact, that the +error is thrown only when 'super' keyword preceding method call+.",melix,topr,Minor,Closed,Fixed,30/Jul/12 07:32,11/Sep/12 01:15
Bug,GROOVY-5647,12816449,SKIP is not honored when an inner class is defined in a SKIP method,"{code}
    @CompileStatic(TypeCheckingMode.SKIP)
    static Closure memoize(MemcacheService memcache, Closure closure) {
        return new Closure(closure.owner) {
            Object call(Object[] args) {
                // a closure call is identified by its hashcode and its call argument values
                def key = [
                        closure: closure.hashCode(),
                        arguments: args.toList()
                ]
                // search for a result for such a call in memcache
                def result = memcache.get(key)
                if (result != null) {
                    // a previous invocation exists
                    return result
                } else {
                    // no previous invocation, so calling the closure and caching the result 
                    result = closure(* args)
                    put(memcache, key, result, Expiration.byDeltaSeconds(60), SetPolicy.SET_ALWAYS)
                    return result
                }
            }
        }
    }
{code}

results in

{code}
/glaforge-gaelyk/core/src/main/groovyx/gaelyk/GaelykCategory.groovy: 1687: [Static type checking] - The spread operator cannot be used as argument of method or closure calls with static type checking because the number of arguments cannot be determined at compile time
 @ line 1687, column 38.
                       result = closure(* args)
                                        ^

1 error
{code}

yet it has been annotated with
@CompileStatic(TypeCheckingMode.SKIP)",melix,sdmurphy,Blocker,Closed,Fixed,30/Jul/12 12:37,11/Sep/12 01:15
Bug,GROOVY-5649,12816461,Stack overflow when property accessor is annoated with @CompileStatic,"{code}
class HaveOption {

  private String helpOption;


  @CompileStatic
  public void setHelpOption(String helpOption) {
    this.helpOption = helpOption
  }

}
{code}
Calling setHelpOption cause stack overflow
{code}
Exception in thread ""main"" java.lang.StackOverflowError
	at utils.tests.HaveOption.setHelpOption(HaveOption.groovy:15)
	at utils.tests.HaveOption.setHelpOption(HaveOption.groovy:15)
	at utils.tests.HaveOption.setHelpOption(HaveOption.groovy:15)
	at utils.tests.HaveOption.setHelpOption(HaveOption.groovy:15)
	at utils.tests.HaveOption.setHelpOption(HaveOption.groovy:15)
{code}

",melix,boaznahum,Blocker,Closed,Fixed,31/Jul/12 10:26,11/Sep/12 01:15
Bug,GROOVY-5650,12818433,Groovy 2.0.1: Regression in generic type inference,"The following Groovy code compiles with @TypeChecked in 2.0.0, but not in 2.0.1:

JavaClass.java
{code}import java.util.List;
import java.util.ArrayList;
import java.util.Collection;

public class JavaClass {

    public static class Container<T> {
    }

    public static class StringContainer extends Container<String> {
    }

    public static <T> List<T> unwrap(Collection<? extends Container<T>> list) {
        return null;
    }

    public static void main(String[] args) {
        final List<StringContainer> containers = new ArrayList<>();
        final List<String> strings = unwrap(containers);
    }
}{code}

GroovyClass.groovy
{code}import groovy.transform.TypeChecked;

@TypeChecked
class GroovyClass {

    public static void javaCall() {
        final List<JavaClass.StringContainer> containers = new ArrayList<>();
        containers.add(new JavaClass.StringContainer()); 
        final List<String> strings = JavaClass.unwrap(containers);
    }

    public static void main(String[] args) { 
        javaCall();
    }
}{code}


Groovy 2.0.0 compiles the code without errors or warnings, but 2.0.1 gives the following error message:

{noformat}/home/.../GroovyClass.groovy: 8: [Static type checking] - Incompatible generic argument types. Cannot assign java.util.List <JavaClass> to: java.util.List <String>
 @ line 8, column 38.
           final List<String> strings = JavaClass.unwrap(containers);
                                        ^

1 error{noformat}

Somehow Groovy infers the type to the static method's class, instead of the container's type argument.
",melix,perplex79,Major,Closed,Fixed,01/Aug/12 02:38,11/Sep/12 01:15
Bug,GROOVY-5652,12818184,"Semicolon required after coercing to a parameterized (generic) type containing a parameterized (generic) type as its only or last type argument when there is no space between the ending "">>""","It seems that after coercing an object to a parameterized (generic) type containing a parameterized (generic) type as its only or last type argument without putting any space between the ending "">>"", a semicolon is needed.

Example:

$ groovy -v
Groovy Version: 1.8.6 JVM: 1.6.0_30 Vendor: Sun Microsystems Inc. OS: Linux
$ groovy -e ""def list = [1,2,3,4] as List<Integer>
println list
println 'bye'"" # This works fine.
[1, 2, 3, 4]
bye
$ groovy -e ""def list = [[1,2],[3,4]] as List<List<Integer>>
println list
println 'bye'"" # This should work fine, but does not.
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
script_from_command_line: 2: expecting EOF, found 'println' @ line 2, column 1.
   println list
   ^

1 error

$ groovy -e ""def list = [[1,2],[3,4]] as List<List<Integer> >
println list
println 'bye'"" # After adding a space between "">>"", this works fine.
[[1, 2], [3, 4]]
bye
$ groovy -e ""def list = [[1,2],[3,4]] as List<List<Integer>>;
println list
println 'bye'"" # Or, after adding a semicolon, this works fine.
[[1, 2], [3, 4]]
bye
$
",daniel_sun,adastragrl,Major,Closed,Fixed,01/Aug/12 15:58,06/Mar/18 23:25
Bug,GROOVY-5653,12816488,STC: non ambiguous method call reported ambiguous,"{code}
import groovy.transform.*

@TypeChecked
class Bug1 {
    void m(byte[] bytes) {}
    void m(byte[] bytes, int i1, int i2) {}
    void test() {
        m(""foo"".bytes, 1, 2)
    }
}

new Bug1().test()
{code}

In the example above, STC believes the call is ambiguous, although it doesn't really appear to be. This yields the following compilation error:

{code}
[Static type checking] - Reference to method is ambiguous. Cannot choose between [MethodNode@18204363[void m([B, int, int)], MethodNode@864642116[void m([B)]]
 at line: 8, column: 9
{code}",melix,guillaume,Major,Closed,Fixed,01/Aug/12 16:11,11/Sep/12 01:15
Bug,GROOVY-5654,12816345,SC: Problem accessing a Map string key with property notation,"{code}
import groovy.transform.*

@CompileStatic
class Bug2 {
    void test() {
        Map<String, Integer> m = ['abcd': 1234]
        assert m['abcd'] == 1234
        assert m.abcd == 1234
    }
}

new Bug1().test()
{code}

The code above works fine with static type checking.
But it's not happy with static compilation for the last line, where we're using the property notation for

{code}
1 compilation error:

Access to java.util.Map <java.lang.String, java.lang.Integer>#abcd is forbidden at line: 8, column: 16
{code}

Note that an assignment {{m.abcd = 1234}} seems to be okay though.
",melix,guillaume,Major,Closed,Fixed,01/Aug/12 16:22,11/Sep/12 01:15
Bug,GROOVY-5655,12816499,SC: byte[] and Byte[] typecasting issue,"{code}
import groovy.transform.*

@CompileStatic
class Bug3 {
    String test() {
        def b = ""foo"".bytes
        new String(b)
    }
}

new Bug3().test()
{code}
Static compilation is not happy with the above, thinking b is a Byte[] instead of byte[], yielding the following error:
{code}
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object '[B@727a9fa7' with class '[B' to class 'byte'
	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToNumber(DefaultTypeTransformation.java:146)
	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.byteUnbox(DefaultTypeTransformation.java:46)
	at Bug3.test(ConsoleScript34:7)

{code}
Whereas, if you explicitly type the variable with {{byte[]}} it's okay.",melix,guillaume,Major,Closed,Fixed,01/Aug/12 16:36,11/Sep/12 01:15
Bug,GROOVY-5656,12816482,STC: non ambiguous constructor reference when subclass is used,"{code}
import groovy.transform.*

class Expr {}
class VarExpr extends Expr {}

class TupList {
    TupList(Expr e1) {}
    TupList(Expr[] es) {}
}

class ArgList extends TupList {
    ArgList(Expr e1) { super(e1) }
    ArgList(Expr[] es) { super(es) }
}

@TypeChecked
class Bug4 {
    void test() {
        new ArgList(new VarExpr())
    }
}

new Bug4().test()
{code}

Yields:
{code}
[Static type checking] - Reference to method is ambiguous. Cannot choose between [MethodNode@745724997[void <init>(Expr)], MethodNode@685460687[void <init>(Expr[])]]
 at line: 19, column: 9
{code}

It's happening somehow because VarExpr is a child of Expr.
In the case of just creating and passing an Expr, things are fine.",melix,guillaume,Major,Closed,Fixed,01/Aug/12 17:17,11/Sep/12 01:15
Bug,GROOVY-5657,12816510,SC: Unable to pop operand off an empty stack with default/optional paramaters,"Here's an simplified example of what's in Gaelyk:
{code}
import groovy.transform.*
import org.codehaus.groovy.ast.ClassHelper
import org.codehaus.groovy.ast.ASTNode
import org.codehaus.groovy.ast.AnnotationNode
import org.codehaus.groovy.ast.ClassNode
import org.codehaus.groovy.ast.MethodNode
import org.codehaus.groovy.control.SourceUnit
import org.codehaus.groovy.transform.ASTTransformation
import org.codehaus.groovy.control.CompilePhase
import org.codehaus.groovy.transform.GroovyASTTransformation

@CompileStatic
@GroovyASTTransformation(phase = CompilePhase.SEMANTIC_ANALYSIS)
class EntityTransformation implements ASTTransformation {
    void visit(ASTNode[] nodes, SourceUnit source) {
        ClassNode parent = (ClassNode) nodes[1]
        parent.addMethod(addStaticDelegatedMethod([:]))
    }
    private MethodNode addStaticDelegatedMethod(Map params, ClassNode returnType = ClassHelper.DYNAMIC_TYPE) {}
}

def et = new EntityTransformation()
{code}
At runtime, we get the following problem:
{code}
Exception thrown

java.lang.VerifyError: (class: EntityTransformation, method: visit signature: ([Lorg/codehaus/groovy/ast/ASTNode;Lorg/codehaus/groovy/control/SourceUnit;)V) Unable to pop operand off an empty stack
	at ConsoleScript119.run(ConsoleScript119:23)
{code}
It seems to be related to the fact the method addStaticDelegatedMethod has an default parameter at the end. It also depends on the first parameter being a map (for instance it won't fail if the first parameter is a string). It also works if we explicitly give the optional parameter.",melix,guillaume,Major,Closed,Fixed,01/Aug/12 18:03,11/Sep/12 01:15
Bug,GROOVY-5658,12818185,SC: issue with optional parameters generating an NPE,"A problem which seems related to optional parameters (if you remove the optional param in the initPlugins signature, the NPE goes alway):

{code}
import groovy.transform.*

@CompileStatic
class Listener {
    void event() {
        initPlugins()
    }
    static initPlugins(ignoreBinary = false) {}
}

def l = new Listener()
l.event()
{code}
Yields:
{code}
2 ao?t 2012 01:31:47 org.codehaus.groovy.runtime.StackTraceUtils sanitize
ATTENTION: Sanitizing stacktrace:
java.lang.NullPointerException
	at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.loadArguments(StaticInvocationWriter.java:274)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:129)
	at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeDirectMethodCall(StaticInvocationWriter.java:188)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:221)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76)
	at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.makeCall(StaticInvocationWriter.java:406)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:334)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:648)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:509)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:613)
	at groovy.lang.GroovyShell.run(GroovyShell.java:480)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:951)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor239.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
2 ao?t 2012 01:31:47 org.codehaus.groovy.runtime.StackTraceUtils sanitize
ATTENTION: Sanitizing stacktrace:
groovy.lang.GroovyRuntimeException: NPE while processing ConsoleScript173
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:198)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:613)
	at groovy.lang.GroovyShell.run(GroovyShell.java:480)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:951)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor239.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NullPointerException
{code}",melix,guillaume,Major,Closed,Fixed,01/Aug/12 18:33,11/Sep/12 01:15
Bug,GROOVY-5659,12816473,VerifyError when CompileStatic with Increment Array Element,"I found errors when I use increment(prefix/postfix) operator to an element of array.
Following Code:
{code}
import groovy.transform.*

@CompileStatic
def foo()
{
  int[] p = new int[10]
  p[0]++
}

foo()

{code}

generates following error:
{quote}
$ /tool/groovy-2.0.1/target/install/bin/groovy ~/tmp/test2.groovy
Caught: java.lang.VerifyError: (class: test2, method: foo signature: ()Ljava/lang/Object;) Expecting to find array of objects or arrays on stack
java.lang.VerifyError: (class: test2, method: foo signature: ()Ljava/lang/Object;) Expecting to find array of objects or arrays on stack
{quote}
",melix,uehaj,Major,Closed,Fixed,06/Aug/12 02:19,11/Sep/12 01:15
Bug,GROOVY-5660,12816455,JsonOutput Date formatting is not threadsafe,"JsonOutput is using a static SimpleDateFormat object. This is not a threadsafe use of that class.

See http://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html#synchronization for more details.

I did create a test case for this, but these are hard cases to test and it looks pretty ugly. I will happily submit it, however, if it helps.",guillaume,johnnywey,Major,Closed,Fixed,06/Aug/12 18:01,11/Sep/12 01:15
Bug,GROOVY-5662,12816350,java.lang.VerifyError when accessing array passed to method,"this code reproduces the error:

void doSomething(long[] values){
  values[1] += 5
}

results in:
Caught: java.lang.VerifyError: (class: test, method: doSomething signature: ([J)V) Expecting to find long on stack
java.lang.VerifyError: (class: test, method: doSomething signature: ([J)V) Expecting to find long on stack

(it works fine in groovy 1.6.4)",blackdrag,casp,Major,Closed,Fixed,09/Aug/12 09:24,29/Aug/12 10:37
Bug,GROOVY-5664,12816507,"OSGi bundle hangs in ""starting"" state","When using the Groovy runtime as an OSGi bundle the bundle hangs in ""starting"" state when the framework (Felix) is started:
{code}
38|Starting   |    1|Groovy Runtime (2.0.1)
{code}

Stopping and starting the bundle again seems to fix the problem.",guillaume,paulbakker,Minor,Closed,Fixed,13/Aug/12 04:15,11/Sep/12 01:15
Bug,GROOVY-5665,12816465,Groovyc ant task fails with nested javac when using generics,"Unzip the attached project and run the ""compile"" ant target. The following error is given:

Compile error during compilation with javac.
C:\DOCUME~1\bborcha\LOCALS~1\Temp\groovy-generated-3979570706073693155-java-source\test\MyGroovyClass.java:18: incompatible types
found   : java.lang.String
required: T
public static <T extends java.lang.String> T getLast(java.util.List<T> list) { return (java.lang.String)null;}

This error did not occur on groovy 1.8.6 or earlier versions and is preventing us from upgrading.",guillaume,bborchardt,Major,Closed,Fixed,13/Aug/12 17:00,21/Sep/12 16:58
Bug,GROOVY-5666,12816372,"Incorrectly finding ""where"" in query string","groovy.sql.Sql.findWhereKeyword returns the index once all the characters in [w,h,e,r,e] are found but not necessarily adjacently.

This causes invalid sql to be generated - null values become conditions.

example of invalid generated sql:
update tbl set wh=null, ere=null, anotherfield is null where id=1",paulk,jschwartzbeck,Major,Closed,Fixed,16/Aug/12 19:25,11/Sep/12 01:15
Bug,GROOVY-5668,12818186,Input stream not closed,"I am using groovy as a language for jasper reports in an enterprise application. When undeploying the application after having created one or more reports, I get a lot of warnings like the ones that follow (the stack traces are with Groovy 2.0.1; I get similar ones with Groovy 1.8.7).

[#|2012-08-20T12:20:36.250+0300|WARNING|glassfish3.1.2|javax.enterprise.system.core.classloading.com.sun.enterprise.loader|_ThreadID=78;_ThreadName=Thread-7;|Input stream has been finalized or forced closed without being explicitly closed; stream instantiation reported in following stack trace
java.lang.Throwable
	at com.sun.enterprise.loader.ASURLClassLoader$SentinelInputStream.<init>(ASURLClassLoader.java:1230)
	at com.sun.enterprise.loader.ASURLClassLoader$InternalJarURLConnection.getInputStream(ASURLClassLoader.java:1338)
	at java.net.URL.openStream(URL.java:1035)
	at org.codehaus.groovy.control.SourceExtensionHandler.getRegisteredExtensions(SourceExtensionHandler.java:44)
	at org.codehaus.groovy.control.CompilerConfiguration.getScriptExtensions(CompilerConfiguration.java:636)
	at groovy.lang.GroovyClassLoader$1$1.run(GroovyClassLoader.java:76)
	at groovy.lang.GroovyClassLoader$1$1.run(GroovyClassLoader.java:74)
	at java.security.AccessController.doPrivileged(Native Method)
	at groovy.lang.GroovyClassLoader$1.loadGroovySource(GroovyClassLoader.java:74)
	at org.codehaus.groovy.control.ResolveVisitor.resolveToScript(ResolveVisitor.java:385)
	at org.codehaus.groovy.control.ResolveVisitor.resolveToClass(ResolveVisitor.java:712)
	at org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:275)
	at org.codehaus.groovy.control.ResolveVisitor.resolveFromModule(ResolveVisitor.java:648)
	at org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:275)
	at org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:243)
	at org.codehaus.groovy.control.ResolveVisitor.resolveOrFail(ResolveVisitor.java:227)
	at org.codehaus.groovy.control.ResolveVisitor.resolveOrFail(ResolveVisitor.java:239)
	at org.codehaus.groovy.control.ResolveVisitor.resolveOrFail(ResolveVisitor.java:235)
	at org.codehaus.groovy.control.ResolveVisitor.visitClass(ResolveVisitor.java:1276)
	at org.codehaus.groovy.control.ResolveVisitor.startResolving(ResolveVisitor.java:148)
	at org.codehaus.groovy.control.CompilationUnit$9.call(CompilationUnit.java:621)
	at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:900)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:513)
	at net.sf.jasperreports.compilers.JRGroovyCompiler.compileUnits(JRGroovyCompiler.java:96)
	at net.sf.jasperreports.engine.design.JRAbstractCompiler.compileReport(JRAbstractCompiler.java:188)
	at net.sf.jasperreports.engine.JasperCompileManager.compileReport(JasperCompileManager.java:212)
	at net.sf.jasperreports.engine.JasperCompileManager.compileReport(JasperCompileManager.java:198)
...



[#|2012-08-20T11:05:34.432+0300|WARNING|glassfish3.1.2|javax.enterprise.system.core.classloading.com.sun.enterprise.loader|_ThreadID=73;_ThreadName=Thread-7;|Input stream has been finalized or forced closed without being explicitly closed; stream instantiation reported in following stack trace
java.lang.Throwable
	at com.sun.enterprise.loader.ASURLClassLoader$SentinelInputStream.<init>(ASURLClassLoader.java:1230)
	at com.sun.enterprise.loader.ASURLClassLoader$InternalJarURLConnection.getInputStream(ASURLClassLoader.java:1338)
	at java.net.URL.openStream(URL.java:1035)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.registerExtensionModuleFromMetaInf(MetaClassRegistryImpl.java:163)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.registerClasspathModules(MetaClassRegistryImpl.java:153)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.<init>(MetaClassRegistryImpl.java:108)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.<init>(MetaClassRegistryImpl.java:70)
	at groovy.lang.GroovySystem.<clinit>(GroovySystem.java:33)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:162)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:192)
	at Blank32A4_1345449884504_251660.$getStaticMetaClass(calculator_Blank32A4_1345449884504_251660)
	at Blank32A4_1345449884504_251660.<init>(calculator_Blank32A4_1345449884504_251660)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at java.lang.Class.newInstance0(Class.java:372)
	at java.lang.Class.newInstance(Class.java:325)
	at net.sf.jasperreports.engine.design.JRAbstractJavaCompiler.loadEvaluator(JRAbstractJavaCompiler.java:98)
	at net.sf.jasperreports.engine.design.JRAbstractCompiler.loadEvaluator(JRAbstractCompiler.java:320)
	at net.sf.jasperreports.engine.JasperCompileManager.loadEvaluator(JasperCompileManager.java:237)
	at net.sf.jasperreports.engine.fill.JRFillDataset.createCalculator(JRFillDataset.java:428)
	at net.sf.jasperreports.engine.fill.JRBaseFiller.<init>(JRBaseFiller.java:363)
	at net.sf.jasperreports.engine.fill.JRVerticalFiller.<init>(JRVerticalFiller.java:77)
	at net.sf.jasperreports.engine.fill.JRVerticalFiller.<init>(JRVerticalFiller.java:87)
	at net.sf.jasperreports.engine.fill.JRVerticalFiller.<init>(JRVerticalFiller.java:57)
	at net.sf.jasperreports.engine.fill.JRFiller.createFiller(JRFiller.java:142)
	at net.sf.jasperreports.engine.fill.JRFiller.fillReport(JRFiller.java:52)
	at net.sf.jasperreports.engine.JasperFillManager.fillReport(JasperFillManager.java:417)
....",paulk,chrisie,Major,Closed,Fixed,20/Aug/12 04:28,11/Sep/12 01:15
Bug,GROOVY-5669,12816504,DOMCategory should provide name() convenience method for Node's not just Element's,"As {{Node}} is a super interface of {{Element}}, this should be a fully backward compatible change",paulk,paulk,Minor,Closed,Fixed,20/Aug/12 05:06,07/Apr/15 19:07
Bug,GROOVY-5671,12816484,Static compilation of calling increment operator on Integer fails,"in short: calling the ++ operator on an Integer does not increment the value when using static compilation.

Example:
{code}
class IntegerIncrementStaticCompilationBug {

    Integer index = 0

    @groovy.transform.CompileStatic
    void increment() {
	index++
    }
}

def b = new IntegerIncrementStaticCompilationBug()
b.increment(); b.increment(); b.increment()

println b.index
{code}

This script should print ""3"", but it prints ""0"".
Works find when CompileStatic annotation is removed.",melix,peterd,Critical,Closed,Fixed,22/Aug/12 06:57,11/Sep/12 01:15
Bug,GROOVY-5672,12816518,groovyc produces AIOOBE if @TypeChecked used with @CompileStatic in certain circumstances,"Simple source code to reproduce the problem

{code:title=SampleClass.groovy|borderStyle=solid}
import groovy.transform.CompileStatic
import groovy.transform.TypeChecked

@TypeChecked
@CompileStatic
class SampleClass {
	def a = ""some string""
	def b = a.toString()
}
{code}
{noformat}
$ groovyc SampleClass.groovy
{noformat}
Produces java.lang.ArrayIndexOutOfBoundsException (attached full trace).

It's enough to reorder the annotations and all works fine:
{code:title=SampleClass.groovy|borderStyle=solid}
import groovy.transform.CompileStatic
import groovy.transform.TypeChecked

@CompileStatic
@TypeChecked
class SampleClass {
	def a = ""some string""
	def b = a.toString()
}
{code}
",melix,imi,Minor,Closed,Fixed,22/Aug/12 07:50,11/Sep/12 01:15
Bug,GROOVY-5673,12816503,groovy does not detect correct Java version on Mac,"Groovy does not detect Oracle's Java 7 - instead, it will use only the Apple JDK6, unless JAVA_HOME is set.

I'll try to get around to doing a git pull, but for now, the solution is to change startGroovy

{code}
# Attempt to set JAVA_HOME if it's not already set.
if [ -z ""$JAVA_HOME"" ] ; then
    if $darwin ; then
        jdkhome=""`java -XshowSettings:properties 2>&1 | grep java\\.home | awk '{print $3}' | rev | cut -d '/' -f 2- | rev`""
        [ -d $jdkhome ] && export JAVA_HOME=$jdkhome
        [ -z ""$JAVA_HOME"" -a -d ""/Library/Java/Home"" ] && export JAVA_HOME=""/Library/Java/Home""
        [ -z ""$JAVA_HOME"" -a -d ""/System/Library/Frameworks/JavaVM.framework/Home"" ] && export JAVA_HOME=""/System/Library/Frameworks/JavaVM.framework/Home""
    else
{code}

I've tested this with both the Apple and Oracle JDKs as the default, and it works correctly.  My shell-fu is not strong however, and there's certainly a better way to do this.",,driscoll,Minor,Closed,Fixed,23/Aug/12 18:19,05/Apr/15 14:43
Bug,GROOVY-5675,12816454,Stub compiler expands generic-inner-class variable declaration incorrectly,"{noformat}
@Log4j
abstract class AbstractProcessingQueue<T> extends AbstractAgent {
    ...

    protected Queue<ProcessingQueueMember<T>> items
    ...

    private class ProcessingQueueMember<E> {
        ...
    }
}
{noformat}

Produces the following line in AbstractProcessingQueue.java:

{noformat}
protected java.util.Queue<nz.ac.auckland.digitizer.AbstractProcessingQueue.ProcessingQueueMember<T>> items;
{noformat}

Which produces the following compile error:
{noformat}
[ERROR] C:\Documents and Settings\Administrator\digitizer\target\generated-sources\groovy-stubs\main\nz\ac\auckland\digitizer\AbstractProcessingQueue.java:[14,96] error: improperly formed type, type arguments given on a raw type
{noformat}

Because {{ProcessingQueueMember}} is a non-static nested class of {{AbstractProcessingQueue}} and as such is instantiated as a subclass of a given *instance* of {{AbstractProcessingQueue}}, a generic class, the fully-qualified expansion of {{Queue<ProcessingQueueMember<T>>}} needs to be {{java.util.Queue<nz.ac.auckland.digitizer.AbstractProcessingQueue<T>.ProcessingQueueMember<T>>}}, with the type parameter given to both the inner and outer classes in the declaration.",guillaume,johansensen,Major,Closed,Fixed,28/Aug/12 18:12,21/Sep/12 16:58
Bug,GROOVY-5678,12816483,(primopts) increment on array not done as primopt,"be x a primtive array and i an int, then x[i]++ should be completely done in primopts, if in a primopts block. This is not the case, only the final set is done like that. The initial get and the increment are still dynamic method calls. ",blackdrag,blackdrag,Major,Closed,Fixed,30/Aug/12 06:41,11/Sep/12 01:15
Bug,GROOVY-5679,12816490,Enclosing method is not set for anonymous inner classes,"The enclosing method is not set for anonymous inner classes. AST transformations may rely on that information (actually, GROOVY-5647 has a workaround for this).

For example:
{code}
class A {
   void enclosingMethod() {
       Runnable r = new Runnable() { void run() {} }
   }
}
{code}

If you inspect the AST, you will see that encoding method for the anonymous runnable is {{null}}.",melix,melix,Major,Closed,Fixed,30/Aug/12 11:55,11/Sep/12 01:15
Bug,GROOVY-5681,12818188,Anonynous inner class as default argument value throws IOOBE,"The compiler doesn't allow using anonymous inner classes as default argument values.

{code}
class A {
    void bar(arg = new Runnable() { void run() {} }) {}
}
{code}

results in:

{noformat}
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:604)
	at java.util.ArrayList.get(ArrayList.java:382)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeAICCall(InvocationWriter.java:450)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeConstructor(InvocationWriter.java:430)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorCallExpression(AsmClassGenerator.java:909)
	at org.codehaus.groovy.ast.expr.ConstructorCallExpression.visit(ConstructorCallExpression.java:43)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitCastExpression(AsmClassGenerator.java:616)
	at org.codehaus.groovy.ast.expr.CastExpression.visit(CastExpression.java:66)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.loadArguments(InvocationWriter.java:185)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:130)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:223)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:336)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:648)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:354)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:509)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:786)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:613)
{noformat}",melix,melix,Major,Closed,Fixed,31/Aug/12 03:24,11/Sep/12 01:15
Bug,GROOVY-5683,12816466,Accessing length of an array of arrays occurs VerifyError,"{code}@groovy.transform.CompileStatic
def foo() { 
    int[][] array = [[1]] as int[][]
    array[0].length 
}
{code}

throws

{noformat}
Caught: java.lang.VerifyError: (class: script_from_command_line, method: foo signature: ()Ljava/lang/Object;) Expecting to find array on stack
java.lang.VerifyError: (class: script_from_command_line, method: foo signature: ()Ljava/lang/Object;) Expecting to find array on stack
{noformat}",melix,nagai_masato,Major,Closed,Fixed,02/Sep/12 12:56,13/Apr/23 15:32
Bug,GROOVY-5687,12811958,Constants defined in an interface not visible to implementing subclass in a static context,"The following code fails to compile:

{code}
public interface DateTimeFormatConstants {
    static final SimpleDateFormat AM_PM_TIME_FORMAT = new SimpleDateFormat(""h:mma"")

    static final SimpleDateFormat MILITARY_TIME_FORMAT = new SimpleDateFormat(""HH:mm"")
}

class DateTimeUtils implements DateTimeFormatConstants {
    static String convertMilitaryTimeToAmPm(String militaryTime) {
        Date date = MILITARY_TIME_FORMAT.parse(militaryTime)
        return AM_PM_TIME_FORMAT.format(date).toLowerCase()
    }
}
{code}

The error message is:

{code}
You attempted to reference a variable in the binding or an instance variable from a static context.
You misspelled a classname or statically imported field. Please check the spelling.
You attempted to use a method 'AM_PM_TIME_FORMAT' but left out brackets in a place not allowed by the grammar.
 @ line 13, column 16.
           return AM_PM_TIME_FORMAT.format(date).toLowerCase()
                  ^
{code}",paulk,behrangsa,Critical,Closed,Fixed,04/Sep/12 22:46,18/Sep/12 17:37
Bug,GROOVY-5688,12816519,NullPointerException in StaticTypeCheckingVisitor,"Upgrade from 2.0.0 to 2.0.1 produces following exception:
{noformat}
java.lang.NullPointerException
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.inferReturnTypeGenerics(StaticTypeCheckingVisitor.java:2618)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallExpression(StaticTypeCheckingVisitor.java:1791)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBinaryExpression(CodeVisitorSupport.java:144)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBinaryExpression(StaticTypeCheckingVisitor.java:390)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:195)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:1179)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:1403)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.silentlyVisitMethodNode(StaticTypeCheckingVisitor.java:1558)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallExpression(StaticTypeCheckingVisitor.java:1783)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitListOfExpressions(CodeVisitorSupport.java:273)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitTupleExpression(CodeVisitorSupport.java:178)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitArgumentlistExpression(CodeVisitorSupport.java:283)
	at org.codehaus.groovy.ast.expr.ArgumentListExpression.visit(ArgumentListExpression.java:74)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallExpression(StaticTypeCheckingVisitor.java:1610)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBinaryExpression(CodeVisitorSupport.java:144)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBinaryExpression(StaticTypeCheckingVisitor.java:390)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitDeclarationExpression(CodeVisitorSupport.java:245)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitDeclarationExpression(ClassCodeVisitorSupport.java:107)
	at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:87)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:195)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:1179)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:1403)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:1377)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:216)
	at org.codehaus.groovy.transform.StaticTypesTransformation.visit(StaticTypesTransformation.java:73)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:132)
	at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:176)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:43)
	at org.jetbrains.groovy.compiler.rt.GroovycRunner.main(GroovycRunner.java:150)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:108)
{noformat}

Reverting back to 2.0.0 works fine. No other feedback is available at the moment.",,andyg,Blocker,Closed,Fixed,05/Sep/12 04:30,11/Sep/12 01:15
Bug,GROOVY-5689,12816474,ClassCastException Double->Float with -= += operators and closures,"The ClassCastException is thrown only in combination of a -= (or +=) operator and a closure.
{code} 
void doesNotWork() {
  float myFloat = 40f
  myFloat -= 20f
  println ""doesNotWork() $myFloat"" // the exception is thrown here, but because of the combination of the -= and the closure
  (0..1).each { i -> // this can be any closure - doesn't matter
    println ""doesNotWork() in the closure: $myFloat""
  }
}
{code}

Did not occure with 1.8.3.",blackdrag,luukes,Major,Closed,Fixed,05/Sep/12 05:05,11/Sep/12 01:15
Bug,GROOVY-5690,12811944,Compiler exception while applying to @CompileStatic and subclasses,"An exception was thrown when try to compile two classes Subclass.groovy and Z.groovy, where Subclass extends Z. Both class are annotated with @CompileStatic

Which is really strange, is that, when you change the name Z to *a letter smaller than 'U'*, it compiles without any problem.

The source code is attached, 3 files, Z.groovy, Subclass.groovy, and P.groovy, in which P and Z is same class with different name. When Subclass extends P, it works.

The compilation command is: 
bq. groovyc *.groovy

Enviroment:

Ubuntu 12.10, 32-Bit
Groovy 2.0.1, installed from groovy-dev/groovy ppa

The exception is:
{quote}
java.lang.ArrayIndexOutOfBoundsException: size==0
	at org.codehaus.groovy.classgen.asm.OperandStack.doConvertAndCast(OperandStack.java:311)
	at org.codehaus.groovy.classgen.asm.OperandStack.doGroovyCast(OperandStack.java:296)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:592)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:505)
	at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:783)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1024)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:562)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:540)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:517)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:496)
	at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
	at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:213)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:146)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:176)
	at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:160)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)

1 error
{quote}",melix,shivawu,Critical,Closed,Fixed,07/Sep/12 03:14,11/Sep/12 01:15
Bug,GROOVY-5691,12816513,TimeoutException immediately thrown when using both TimedInterrupt and with {} closure on a Map,"The attached {{script.groovy}} has a very strange behaviour:
{code:title=Bar.java|borderStyle=solid}
class GroovyTimedInterruptAndWithTest {

  @groovy.transform.TimedInterrupt( 60L )
  def test() {
    
    def map = [:]
    
    map.with {
      key1 = 'foo'
      key2 = 42
    }
    
    map
  }
}

new GroovyTimedInterruptAndWithTest().test()
{code}

At runtime, the script *immediately* throws the TimeoutException (whereas the timeout's value given to the {{@TimedInterrupt}} annotation is 60 seconds) :

{code} 
java.util.concurrent.TimeoutException: Execution timed out after 60 units. Start time: null
	at GroovyTimedInterruptAndWithTest$_test_closure1.doCall(script.groovy)
	at GroovyTimedInterruptAndWithTest.test(script.groovy:8)
	at GroovyTimedInterruptAndWithTest$test.call(Unknown Source)
	at script.run(script.groovy:17)
{code} 

The {{Start time}} displayed in this stacktrace, which is {{null}}, is another strange fact.

Experienced on {{1.8.6}} but might also occur on all versions including {{@TimedInterrupt}} ({{1.8.0}}+).


This is not very critical as it's of course possible to write instead:
{code}
map['key1']='foo'
map['key2']=42
{code} 

but the thing is that the script runs fine when commenting the {{@TimedInterrupt}} annotation ; so this bug might hide another strange behaviours when using {{@TimedInterrupt}} annotation.",paulk,guillaume.cernier,Major,Closed,Fixed,07/Sep/12 04:20,11/Sep/12 01:15
Bug,GROOVY-5692,12818442,TypeChecked not checking generics placeholder types across arguments when a method has multiple arguments,"{code}
import groovy.transform.TypeChecked
main()
@TypeChecked main() { printEqual(1, 'foo') }
@TypeChecked <T> void printEqual(T arg1, T arg2) { println arg1 == arg2 }
{code}",melix,paulk,Major,Closed,Fixed,07/Sep/12 05:37,12/Sep/22 21:53
Bug,GROOVY-5693,12816475,Unexpected behavior of CompileStatic when assigning a Closure to a new block code,"{noformat}
@groovy.transform.CompileStatic
class Foo{
    def run(){
        Closure a ={
            int i ->
            println ""First closure ""+ i
        }
        Closure b ={
            String s ->
            println ""Second closure ""+ s
        }
        a=b
        a(""Testing!"")
    }
}
Foo f = new Foo()
f.run()
{noformat}
With CS applied the code will fail to compile with the following error:
{noformat}
[Static type checking] - Closure argument types: [int] do not match with parameter types: [java.lang.String]
at line: 13, column: 11
{noformat}
The compiler is expecting an integer but since the variable 'a' is re-assigned to a new piece of code that takes a String as a parameter this code should work",melix,carlosapc,Minor,Closed,Fixed,07/Sep/12 14:48,21/Sep/12 16:58
Bug,GROOVY-5695,12816476,jarsigner fails because two duplicate objects exists in groovy-2.0.1.jar,"two duplicate objects exists:
{code}
org/codehaus/groovy/antlib.xml
groovy/grape/defaultGrapeConfig.xml
{code}",paulk,lwolter,Major,Closed,Fixed,09/Sep/12 08:34,22/Dec/12 01:10
Bug,GROOVY-5697,12816529,Wiki snapshot (pdf) is outdated,"http://git.codehaus.org/gitweb.cgi?p=groovy-git.git;a=history;f=src/wiki-snapshot.pdf;h=5c133e0a719969a1c0f77adbf2ad127802ebe985;hb=refs/heads/GROOVY_2_0_X

Latest entry dates back to Groovy 1.7 
",guillaume,dd11,Trivial,Closed,Fixed,10/Sep/12 14:28,10/Jul/13 04:42
Bug,GROOVY-5698,12813417,STC: problem w/ map based constructors,"{code}
class CustomServletOutputStream extends OutputStream {
    OutputStream out

    void write(int i) {
        out.write(i)
    }

    void write(byte[] bytes) {
        out.write(bytes)
    }

    void write(byte[] bytes, int offset, int length) {
        out.write(bytes, offset, length)
    }

    void flush() {
        out.flush()
    }

    void close() {
        out.close()
    }
}

@groovy.transform.TypeChecked
class Test {
    static void test() {
        def csos = new CustomServletOutputStream(out: new ByteArrayOutputStream())
    }   
}

Test.test()
{code}

This fails with:
{code}
[Static type checking] - Cannot assign value of type java.io.ByteArrayOutputStream to variable of type java.io.OutputStream
{code}

However, the following spanning two lines work:
{code}
        def csos = new CustomServletOutputStream()
        csos.out = new ByteArrayOutputStream()
{code}",melix,guillaume,Major,Closed,Fixed,11/Sep/12 03:32,21/Sep/12 16:58
Bug,GROOVY-5699,12816388,STC: Property inference issue with IntRange properties,"The following example fails:
{code}
@groovy.transform.TypeChecked
class Test {
    static void test() {
        def range = 1..10
        int i = range.fromInt
    }   
}

Test.test()
{code}
With:
{code}
[Static type checking] - Cannot assign value of type java.lang.Object to variable of type int
{code}
Although using the getter method directly {{getFromInt()}} and the type checker is happy.",melix,guillaume,Major,Closed,Fixed,11/Sep/12 04:18,21/Sep/12 16:58
Bug,GROOVY-5700,12816525,STC: different behavior w/ map subscript and property access when using elvis,"There's a difference in behavior with static type checking between property and subscript access to map elements.

{code}
@groovy.transform.TypeChecked
class Test {
    static void test() {
        def m = [retries: 10]
        // passes
        int r1 = m['retries'] ?: 1
        // fails
        int r2 = m.retries ?: 1
    }   
}

Test.test()
{code}

Fails with:
{code}
[Static type checking] - Cannot assign value of type java.lang.Object to variable of type int
{code}",melix,guillaume,Major,Closed,Fixed,11/Sep/12 04:38,22/Aug/22 14:48
Bug,GROOVY-5702,12816508,STC: checker confused by overridden method from sub interface,"{code}

interface MyCloseable {
    void close()
}

interface OtherCloseable extends MyCloseable {
    void close()
}

class MyCloseableChannel implements OtherCloseable {}

@groovy.transform.TypeChecked
class Test {
    static void test() {
        def mc = new MyCloseableChannel()
        mc.close()
    }   
}

Test.test()
{code}

Here, {{OtherCloseable}} redefines {{close()}} from {{MyCloseable}}, and it seems to confuse the type checker, as it complains with:

{code}
[Static type checking] - Cannot find matching method MyCloseableChannel#close(). Please check if the declared type is right and if the method exists.
{code}

I found the case with {{java.nio.Channel}} which redefines the {{close()}} method from parent interface {{java.io.Closeable}}.",melix,guillaume,Major,Closed,Fixed,11/Sep/12 09:34,21/Sep/12 16:58
Bug,GROOVY-5703,12816429,CompileStatic - Calling vargarg parameter String is converted to array of strings,"Assume we have this method:

{code}
static void printMsgs(String ... msgs) {
    for(String s : msgs) { println s;}
}
{code}

Calling it from @CompileStatic:
{code}
@CompileStatic
  static void testCS() {
    //Print:
    // H
    // e
    // l
    // l
    // o
    printMsgs(""Hello"");
  }
{code}

The string 'hello' is converted to array of strings.

Calling it from non CompileStatic works fines.

The complete test case:
{code}
public class CompileStaticVarArg {

  static void main(String[] args) {


    test();

    println '-' * 20

    testCS()

  }


  static void test() {
    // Print 'hello'
    printMsgs(""Hello"");
  }

  @CompileStatic
  static void testCS() {
    //Print:
    // H
    // e
    // l
    // l
    // o
    printMsgs(""Hello"");
  }

  static void printMsgs(String ... msgs) {

    for(String s : msgs) {
      println s;
    }

  }

}
{code}



",melix,boaznahum,Blocker,Closed,Fixed,12/Sep/12 00:46,21/Sep/12 16:58
Bug,GROOVY-5704,12816546,CompileStatic conues field access with property access,"When accessing field 'x' that have the same name as property 'getX'
compiler try to access the property instead of field.

If you remove the property accessor 'getX' then error disappears

I don't know if it just compile time or also run-time.

The complete test case below demonstrates it:

{code}
class CSFieldConfuseProperty {

  private AtomicBoolean x;


  @CompileStatic
  public boolean getX() {
    return x.get();
  }

  /**
   * This one gives error:
   * Groovyc: [Static type checking] - Cannot find matching method boolean#set(boolean).
   * Please check if the declared type is right and if the method exists.
   */
  @CompileStatic
  public void setX1(boolean flag) {
    this.x.set(flag);
  }

  /**
   * This cone is compiled OK
   */
  public void setX2(boolean flag) {
    this.x.set(flag);
  }

}
{code}",melix,boaznahum,Major,Closed,Fixed,12/Sep/12 01:11,21/Sep/12 16:58
Bug,GROOVY-5705,12818178,STC: calling a closure stored in a property yields an NPE,"When you run the following example:

{code}
import groovy.transform.*

class Test {
    Closure c = { it }
    
    @TypeChecked
    void test() {
        c(""123"")
    }
}

new Test().test()
{code}

An NPE will be thrown:
{code}
java.lang.NullPointerException
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallExpression(StaticTypeCheckingVisitor.java:1717)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:193)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:163)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:1224)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:1465)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:1439)
	at org.codehaus.groovy.transform.StaticTypesTransformation.visit(StaticTypesTransformation.java:78)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:132)
	at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:176)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:613)
	at groovy.lang.GroovyShell.run(GroovyShell.java:480)
	at groovy.lang.GroovyShell.run(GroovyShell.java:163)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:951)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor242.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:415)
	at groovy.lang.Closure.call(Closure.java:409)
	at groovy.lang.Closure.run(Closure.java:493)
	at java.lang.Thread.run(Thread.java:680)
{code}",guillaume,guillaume,Major,Closed,Fixed,12/Sep/12 05:21,21/Sep/12 16:58
Bug,GROOVY-5706,12818189,JSR 223 invokeFunction broken in 2.0.2 - regression,"The below script works in Groovy 2.0.1 and earlier but stopped working in Groovy 2.0.2.  I invoke a global glosure named ""helloWorld"".  I use two different strings named ""helloWorld"".  Sometimes this works, sometimes this fails.

I suspect the problem is in GroovyScriptEngineImpl.  The globalClosures map was changed in 2.0.2.  It is no longer a java.util.Map but is some other map (ManagedConcurrentMap).  I suspect it may be a problem with the map using System.identityHashCode() is its mechanism for looking up Closures.  Obviously, any string named ""helloWorld"" should work here.

SEVERITY:
This problem causes breakage in real applications that use JSR 223 API for Groovy and other script engines.  A user would need to roll back to 2.0.1.


EXAMPLE:
{code}
import javax.script.Compilable;
import javax.script.CompiledScript;
import javax.script.Invocable;
import javax.script.ScriptEngine;
import javax.script.ScriptEngineManager;

public class HelloWorld {
    public static void main(String[] args) throws Exception {
        String script = ""void helloWorld() { println \""Hello World\"" }"";
        ScriptEngineManager factory = new ScriptEngineManager();
        ScriptEngine engine = factory.getEngineByName(""groovy"");
        CompiledScript compiled = ((Compilable) engine).compile(script);
        compiled.eval();
        String name = ""helloWorld"";
        String name2 = new String(name.toCharArray());
        if (!name.equals(name2)) {
            System.out.println(""should be equal!"");
        }
        ((Invocable) engine).invokeFunction(name);   // works
        ((Invocable) engine).invokeFunction(name);   // still works
        ((Invocable) engine).invokeFunction(name2);  // fails since Groovy 2.0.2
    }
}
{code}",blackdrag,benken_parasoft,Critical,Closed,Fixed,13/Sep/12 19:01,27/Nov/12 03:54
Bug,GROOVY-5707,12816448,"If superclass defines a readonly property, subclass may not use a private field of the same name","The following code fails because the type checker things ""testClass"" is a readonly property:

{code}
class Top {
    Class getTestClass() {}
}

@groovy.transform.TypeChecked
class Bottom extends Top {
    private Class testClass
    Bottom(Class foo) {
        this.testClass = foo
    }
}
{code}",melix,melix,Major,Closed,Fixed,14/Sep/12 08:30,21/Sep/12 16:58
Bug,GROOVY-5710,12816537,Stub generator should not use raw types when casting default return values,"Currently, for methods like:
{code}
List<BuildListener> getBuildListeners() { ... }
{code}
The stub generator generates:
{code}
public List<BuildListener> getBuildListeners() { return (List)null; }
{code}
Instead of returning:
{code}
(List<BuildListeners>)null
{code}",guillaume,guillaume,Minor,Closed,Fixed,18/Sep/12 02:04,21/Sep/12 16:58
Bug,GROOVY-5712,12816543,"@TypeChecked and ""Charset ISO_8859_1_CHARSET = Charset.forName(""ISO-8859-1"")"": Cannot assign value of type java.lang.Class to variable of type java.nio.charset.Charset","{code}
package example

import groovy.transform.TypeChecked

import java.nio.charset.Charset

@TypeChecked
class PotentialTypeCheckerIssueExample {
        private static Charset ISO_8859_1_CHARSET = Charset.forName(""ISO-8859-1"")

        static void main(String[] args) {
                new PotentialTypeCheckerIssueExample().printCharset()
        }

        def printCharset(){
                println ISO_8859_1_CHARSET
        }
}
{code}
The type checker reports
{code}
Groovy:[Static type checking] - Cannot assign value of type
java.lang.Class to variable of type
java.nio.charset.Charset        PotentialTypeCheckerIssueExample.groovy /GroovyTests/src/example        line
9       Java Problem
{code}
The code runs fine without @TypeChecked, the IDE shows the right
return type (Charset).

What's wrong here?",melix,peti,Major,Closed,Fixed,19/Sep/12 03:06,22/Dec/12 01:10
Bug,GROOVY-5713,12816437,timing problem with GroovyScriptEngine recompilation,"while GROOVY-4975 fixed several things the implementation still had a flaw that was not easily showing in the tests attached to GROOVY-4975. Because of the truncation of the last modification time of the URL it was very possible to get quite random times within a 1000ms range. Partially to that, partially to other issues isSourceNewer gave wrong results. For example very new entries, that just had been compiled, return true if depending on how long after that isSourceNewer is called. This leads to dependencies not picked up right",blackdrag,blackdrag,Major,Closed,Fixed,20/Sep/12 01:56,21/Sep/12 16:58
Bug,GROOVY-5720,12816553,Static Typechecking of Comparison Operations (e. g. '<') Not Strict Enough,"For classes implementing Comparable (e. g. String) calls to 'compareTo()' with single either directly or through operator like '<' are not detected as incorrect if the argument is on the wrong type (e. g. 'ab' < 1 is accepted).

Note: Attached test source is modification of standard test delivered with Groovy 2.0.1",melix,fplass,Major,Closed,Fixed,21/Sep/12 06:46,22/Dec/12 01:10
Bug,GROOVY-5721,12816438,Groovyc assumes wrong return type when concrete class derived from generic,"When a concrete class (MyList) extends  generic one (ArrayList<T>), Groovyc behave like base class is RAW type (T is summed to be obejct)

Example:

{code}
public class MyList extends ArrayList<Double> {}
{code}

{code}
@CompileStatic
  public static void f() {

    List<Double> list1 = new ArrayList<Double>();

    // OK
    Double d1 = list1.get(0);

    //---------------------------

    List<Double> list2 = new MyList<>();

    //Groovyc: [Static type checking] - Cannot assign value of type java.lang.Object to variable of type java.lang.Double
    Double d2 = list2.get(0);

    //---------------------------

    MyList list3 = new MyList();

    //Groovyc: [Static type checking] - Cannot assign value of type java.lang.Object to variable of type java.lang.Double
    Double d3 = list3.get(0);
  }
{code}",melix,boaznahum,Major,Closed,Fixed,24/Sep/12 04:56,22/Dec/12 01:10
Bug,GROOVY-5723,12816509,Problem with cached calls with GroovyServlet,"A couple users reported problems since Groovy 2.0.2 with the GroovyServlet:
https://github.com/glaforge/gaelyk/issues/163

When using Gaelyk on Google App Engine, they noticed that the first hit for a certain groovlet would run fine, while all subsequent calls would fail.

This problem started appearing after the changes operated on GROOVY-5125 which fixed an issue when using scripts stored in a JAR with GroovyScriptEngine.

This problem can be fixed by amending AbstractHttpServlet.",guillaume,guillaume,Critical,Closed,Fixed,24/Sep/12 09:56,04/Oct/12 17:02
Bug,GROOVY-5724,12818191,"@TypeChecked and JUnit Test using assertThat(myTestResult, notNullValue()","This worked with Groovy 2.0.0, with 2.0.4 it doesn't 
(where is your test coverage?) ;-)

{code}
package groovy.bugs.typechecker

import static org.hamcrest.CoreMatchers.*
import static org.junit.Assert.*
import groovy.transform.TypeChecked

@TypeChecked
class AssertNotNullTypecheckerBugTest {

	@org.junit.Test
	public void sometest() throws Exception {
		// act
		String result = '12345'.substring(2)
		// assert
		assertThat(result, notNullValue())
		assertEquals('345', result)
	}
}
{code}

I get the following error in Eclipse
{code}
Description	Resource	Path	Location	Type
Groovy:[Static type checking] - Cannot call org.junit.Assert#assertThat(java.lang.Object <T extends java.lang.Object>, org.hamcrest.Matcher <T extends java.lang.Object>) with arguments [java.lang.String, org.hamcrest.Matcher <T extends java.lang.Object>] 	AssertNotNullTypecheckerBugTest.groovy	/GroovyBugs/src/groovy/bugs/typechecker	line 15	Java Problem
{code}

-> The eclipse project containing this sample is attached.
",melix,peti,Major,Closed,Fixed,25/Sep/12 02:55,22/Dec/12 01:10
Bug,GROOVY-5725,12816577,Groovy classes with @CompileStatic cannot use constants declared on an interface,"Compile the class below and you'll get the following compilation error:

[Static type checking] - The variable [myStringConstant] is undeclared.


import groovy.transform.CompileStatic

interface MyInterface {
	String myStringConstant = 'STRING'

	String myMethod()
}

@CompileStatic
class MyClass implements MyInterface {
	@Override String myMethod() {
		myStringConstant
	}
}
",melix,bborchardt,Minor,Closed,Fixed,25/Sep/12 11:55,22/Dec/12 01:10
Bug,GROOVY-5726,12818193,Source locations of command chain expressions,"Source locations of command chain expressions are incorrect.  Consider the  following snippet (no whitespace before or after expression):

{code}foo bar baz{code}

Type that into the groovyConsole and inspect the AST. Expand a few sections and see that the {{this.foo(bar)}} method call has a {{columnNumber}} of 5, whic is incorrect.  Expand one more level and see that the {{foo}} constant expression has a {{columnNumber}} of 1, which is correct.

In Groovy-Eclipse, we have put a bit of a kludgy patch in to make it work.  At the end of the {{methodCallExpression(AST)}} method, immediately before the return statement, we added this text:

{code}
        // in the case of Groovy 1.8 command expressions, the slocs are incorrect for the start of the method
        if (!implicitThis && methodCallNode.getText().equals(""<command>"")) {
            ret.setStart(objectExpression.getStart());
            ret.setLineNumber(objectExpression.getLineNumber());
            ret.setColumnNumber(objectExpression.getColumnNumber());
            ret.setEnd(arguments.getEnd());
            ret.setLastLineNumber(arguments.getLastLineNumber());
            ret.setLastColumnNumber(arguments.getLastColumnNumber());
        }
{code}

I'm not particularly happy with this solution, but it works for us.  Perhaps you can come up with something better.",emilles,werdna,Major,Closed,Fixed,25/Sep/12 16:23,23/Jul/22 16:17
Bug,GROOVY-5728,12818437,Accessing private constructor from a static factory,"The following code works fine from Java but fails in Groovy:
{code}
public abstract class FooMain {
    private FooMain() {}
    public abstract String bar();
    public static FooMain factory() {
      return new FooMain() {
        public String bar() { return ""xxx""; }
      };
    }
    public static void main(String[] args) {
        System.out.println(factory().bar());
    }
}
// => java.lang.IllegalAccessError: tried to access method FooMain.<init>()V from class FooMain$1
{code}
",emilles,paulk,Minor,Closed,Fixed,28/Sep/12 04:23,31/Jan/22 18:01
Bug,GROOVY-5729,12811960,Delegation to an  interface with deprecated methods shows a compile error,"Given an interface:
{code}
public
interface DeprecatedMethods {

    @Deprecated
    public void setHeight(float height);

}
{code}
and a delegating class:
{code}
class DelegateDeprecated {
	
    @Delegate	
    private DeprecatedMethods delegate
	
}
{code}
then the compiler shows a compilation error:
{panel}
$ groovyc DelegateDeprecated.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
DelegateDeprecated.groovy: 15: Can't have an abstract method in a non-abstract class. The class 'DelegateDeprecated' must be declared abstract or the method 'void setHeight(float)' must be implemented.
 @ line 15, column 1.
   class DelegateDeprecated {
   ^

1 error
{panel}
This happens from the command line and Eclipse, and can be made to go away by removing the @Deprecated annotation.

",paulk,prajnainc,Major,Closed,Fixed,28/Sep/12 07:36,04/Oct/12 17:02
Bug,GROOVY-5730,12816521,array given spreaded to doCall method if EMC is enabled,"{code:Java}
ExpandoMetaClass.enableGlobally()
Object[] item = [1]
def cl = {assert it.class == Object[]}
cl(item){code}
The code above fails, the item array is given to the doCall method as if it was spread. The reason is that if ClosureMetaClass is active we invoke the doCall metho, but EMC uses call(Object[]) here. Since call(Object[]) basically just makes a call to doCall with the arguments spread, we get the wrong result above. Without EMC it is working of course.

This issue is strongly related to http://jira.grails.org/browse/GRAILS-9443. The issue there surfaced because of the introduction of BooleanClosureWrapper, but the issue described here is the underlaying problem to the other issue.",blackdrag,blackdrag,Blocker,Closed,Fixed,28/Sep/12 07:44,04/Oct/12 17:02
Bug,GROOVY-5732,12816558,Delegating to an abstract class hierarchy doesn't implement interfaces at a higher level,"Given:
{code}
public interface Field {	
	public void aFieldMethod();
}
{code}
and the classes:
{code}
public abstract class AbstractBaseClass implements Field {
}

public abstract class DelegatedClass extends AbstractBaseClass {
}

class Delegator {
	@Delegate private DelegatedClass delegate
}
{code}
then
{code}
assert Field.isAssignableFrom(Delegator)
{code}
fails. If Delegator is defined as
{code}
class Delegator {
	@Delegate private AbstractBaseClass delegate
}
{code}
the test succeeds
",paulk,prajnainc,Major,Closed,Fixed,28/Sep/12 11:27,04/Oct/12 17:02
Bug,GROOVY-5733,12816552,"JsonBuilder toPrettyString gen ""\""\"""" for emtyString it should be """"","{code}
      def builder = new groovy.json.JsonBuilder()
        def root = builder.people {
            firstName 'Guillame'
            lastName 'Laforge'
            married true
            conferences 'JavaOne', 'Gr8conf'
            blank ''
        }
        println builder.toString()
        println builder.toPrettyString()

        String s = '""""'
        println ""$s""
        println ""${s[1..-2]}""
        println ""${StringEscapeUtils.escapeJava(s[1..-2])}""
{code}

output
{code}
{""people"":{""firstName"":""Guillame"",""lastName"":""Laforge"",""married"":true,""conferences"":[""JavaOne"",""Gr8conf""],""blank"":""""}}
{
    ""people"": {
        ""firstName"": ""Guillame"",
        ""lastName"": ""Laforge"",
        ""married"": true,
        ""conferences"": [
            ""JavaOne"",
            ""Gr8conf""
        ],
        ""blank"": ""\""\""""
    }
}
""""
""""
\""\""
{code}


I think there is a bug on file ""D:\groovy-src-2.0.4\groovy-2.0.4\subprojects\groovy-json\src\main\groovy\groovy\json\JsonOutput.groovy""

line 190-192
{code}
     } else if (token.type == STRING) {
                output.append('""' + StringEscapeUtils.escapeJava(token.text[1..-2]) + '""')
{code}
",paulk,chokchai,Major,Closed,Fixed,01/Oct/12 05:54,18/Jan/13 16:06
Bug,GROOVY-5734,12816540,Ternary operator returning null fails with static type checking,"If I define:

{code}
@groovy.transform.CompileStatic
Integer someMethod() {
    (false) ? null : 8
}
{code}

this throws the following error:

[Static type checking] - Cannot return value of type java.lang.Object on method returning type java.lang.Integer -> java.lang.Integer ",melix,rcruzjo,Major,Closed,Fixed,01/Oct/12 08:13,17/Jul/15 07:13
Bug,GROOVY-5735,12816560,"Static type checking error when calling a method with generics that receives a Class<T> and a T object, and the instantiation of T is a parametric type","If I have the following code:

{code}
@groovy.transform.CompileStatic
class MyClass {

   public <T> void someMethod (java.lang.Class<T> clazz, T object) {}

   void method() {
      List<String> list = null
      someMethod(java.util.List.class, list)  
   }
}

new MyClass().method()
}
{code}

it throws this error [Static type checking] - Cannot call MyClass#someMethod(java.lang.Class <T>, java.lang.Object <T>) with arguments [java.lang.Class <java.util.List>, java.util.List <String>]


The issue here is that T is being inferred as List<String>, therefore it expects to have a Class<List<String>> (which cannot be constructed).
If instead T becomes just List, a raw type will be used in the second argument, and it should work.

As an additional note, this example used to work in Groovy 2.0.0",melix,rcruzjo,Major,Closed,Fixed,01/Oct/12 09:17,22/Dec/12 01:10
Bug,GROOVY-5736,12811923,"Log4j, Commons, Slf4j ast transforms are broken in Groovy-Eclipse","Since Groovy-Eclipse moved to using the 2.0.4 compiler, the @Log4j and other log ast transformations are crashing the compiler.  This is because in GROOVY-5574, there was a move to call {{Class.forName()}} in the Log AST transformation.  In Groovy-Eclipse, you cannot use Class.forName since the classpath of the project is not on the classpath of the compiler.

The workaround is to change {{org.codehaus.groovy.transform.LogASTTransformation.AbstractLoggingStrategy.classNode(String)}}.  Instead of throwing a {{GroovyRuntimeException}}, you could simply call and return {{ClassHelper.make(name);}}. 

I know this adds code that was removed to fix GROOVY-5574, but for Groovy-Eclipse this is safe since {{ClassHelper.make(name);}} will correctly use the project's class path to generate the ClassNode and classloader issues will not happen.

If you think that this solution is not a good one because of problems that could occur outside of eclipse, then we could talk about a different solution.",emilles,werdna,Major,Closed,Fixed,01/Oct/12 15:04,03/Feb/22 22:29
Bug,GROOVY-5737,12816469,"@Log won't compile with @CompileStatic if ""log"" is used in a Closure","The log variable injected by the @Log annotation is not compiled if it's used inside a Closure.

{code}
import groovy.transform.*
import groovy.util.logging.*

@Log
@CompileStatic
class GreetingActor {

  def receive = {
    log.info ""test""
  }

}
{code}",melix,chanwit,Major,Closed,Fixed,02/Oct/12 09:34,22/Dec/12 01:10
Bug,GROOVY-5738,12816542,The if(instanceof) block is not inferring type in a Closure,"I expect these classes to be compiled but it won't.

{code}
import groovy.transform.*

@CompileStatic
class Greeting implements Serializable {
    String who
}
{code}

{code}
import groovy.transform.*

@CompileStatic
class GreetingActor {

  def receive = {
    if(it instanceof Greeting) {
        println ""Hello ${it.who}""
    }
  }

}
{code}

The type of variable ""it"" in the Closure receive should be inferred as type Greeting. To reproduce call

$ groovyc Greeting.groovy GreetingActor.groovy

Thank you.",melix,chanwit,Major,Closed,Fixed,02/Oct/12 09:45,22/Dec/12 01:10
Bug,GROOVY-5739,12818444,performance problem in DefaultGroovyMethods.removeAll(),"There is a performance problem in ""DefaultGroovyMethods.removeAll"".
It appears in version 2.0.4 and also in the latest revision
(f8c7f25..).  I attached a JUnit test that exposes this problem and a
one-line patch (patchRemoveAll.diff) that fixes it.  On my machine,
for this test, the patch provides a 37X speedup.

Running the JUnit test for the un-patched version is, the output is:
Time is 2040

The output for the patched version is:
Time is 54

The problem is that the current implementation of ""removeAll"" calls:

""self.removeAll(Arrays.asList(items));""

which typically (e.g., java.util.AbstractCollection and thus most
collections) calls ""contains"" on the parameter (a list) for each
element in ""self"".  ""contains"" on a list has linear complexity, which
is very slow when called many times.

The patch uses a HashSet as a parameter, which has constant complexity
for ""contains"".

""DefaultGroovyMethods.retainAll"" has a similar problem.  I attached a
second patch for it (patchRetainAll.diff)",paulk,adriannistor,Major,Closed,Fixed,02/Oct/12 13:33,05/Apr/15 14:44
Bug,GROOVY-5740,12816524,Cannot use ASTTransformationCustomizer with TimedInterrupt,"The following code will fail:
{code}
def program = ""println 'hello world'; while(true) {;}""
def config = new CompilerConfiguration()
config.addCompilationCustomizers(new ASTTransformationCustomizer([value: 1L, unit: TimeUnit.MILLISECONDS],TimedInterrupt))
GroovyShell sh = new GroovyShell(config)
Script script = sh.parse(program)
System.out.println(script.run()) // fail here
{code}
With the exception:  
{noformat}
org.codehaus.groovy.classgen.ClassGeneratorException: Cannot generate bytecode for constant: MILLISECONDS of type: java.util.concurrent.TimeUnit$3
{noformat}
Per blackdrag, this is apparently a bug in line 197 of TimedInterruptableASTTransformation, which uses ConstantExpression when it should have used PropertyExpression.

As a result of this bug, the only TimeUnit available when using TimedInterrupt is TimeUnit.SECONDS, which is the default.  This is probably not too big of an issue for most uses, which is why I've set the priority to Minor.
",melix,driscoll,Minor,Closed,Fixed,02/Oct/12 18:33,15/Oct/15 18:19
Bug,GROOVY-5741,12811616,Compile Ok but strange Exception at runtime using @CompileStatic,"I just compile the following code with Groovy 2.0.4 and OK but when running it I got an Exception.
If I remove ""@CompileStatic"" than all goes right at compile and run time.
Thanks for the time.

{code:title=groovyXPath.groovy|borderStyle=solid}
package groovy.test.xpath

import groovy.transform.CompileStatic

import javax.xml.xpath.XPath
import javax.xml.xpath.XPathConstants
import javax.xml.xpath.XPathFactory

@CompileStatic
class groovyXPath {

	static main(args) {
		XPath lXPath = XPathFactory.newInstance().newXPath()
		println(""Hello XPath: ${lXPath}"")
	}

}
{code}

Exception
""""""
Exception in thread ""main"" java.lang.IllegalAccessException: Class groovy.test.xpath.groovyXPath can not access a member of class javax.xml.xpath.XPathFactory with modifiers ""protected""
	at sun.reflect.Reflection.ensureMemberAccess(Unknown Source)
	at java.lang.Class.newInstance0(Unknown Source)
	at java.lang.Class.newInstance(Unknown Source)
	at groovy.test.xpath.groovyXPath.main(groovyXPath.groovy:13)
""""""",melix,lguzzon,Blocker,Closed,Fixed,03/Oct/12 16:30,04/Mar/14 16:31
Bug,GROOVY-5742,12818513,CompileStatic goes into infinite loop when handling self-referential generics,"When using @CompileStatic or @TypeChecked the StaticTypeCheckingVisitor recursively calls itself on the following example:
{code}
abstract class Base<A extends Base<A>> {}
class Done extends Base<Done> { }
class Next<H, T extends Base<T>> extends Base<Next<H, T>> {
  H head; T tail
  static Next<H, T> next(H h, T t) { new Next<H, T>(head:h, tail:t) }
  String toString() { ""Next($head, ${tail.toString()})"" }
}
import static Next.*

@groovy.transform.CompileStatic
def foo() {
  Next<Integer, Next<String, Done>> x = next(3, next(""foo"", new Done()))
}

println foo() // => Next(3, Next(foo, Done@9b3ec2))
{code}
My real example is when using Functional Java's heterogeneous list classes but the above is a cut-down equivalent. The stack trace is:
{noformat}
java.lang.StackOverflowError
...
org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolve(StaticTypeCheckingVisitor.java:2836)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolveType(StaticTypeCheckingVisitor.java:2873)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolve(StaticTypeCheckingVisitor.java:2840)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolveType(StaticTypeCheckingVisitor.java:2873)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolve(StaticTypeCheckingVisitor.java:2840)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolveType(StaticTypeCheckingVisitor.java:2873)
{noformat}
",blackdrag,paulk,Major,Closed,Fixed,03/Oct/12 21:51,07/Apr/15 19:07
Bug,GROOVY-5743,12816496,Typed closure as a default parameter requires cast when method is TypeChecked,"If you have a typed Closure as a default parameter of a method ie:

{code}
@groovy.transform.TypeChecked
Integer a( String s, Closure<Integer> b={ String it -> it.length() } ) {
  b( s )
}

a( 'tim' )
{code}

You get the exception:

{quote}
[Static type checking] - Cannot return value of type java.lang.Object on method returning type java.lang.Integer -> java.lang.Integer
{quote}

The workaround is to either cast the return:

{code}
  (Integer)b( s )
{code}

Or stop using the default parameter:

{code}
@groovy.transform.TypeChecked
Integer a( String s ) {
  Closure<Integer> b={ String it -> it.length() }
  b( s )
}

a( 'tim' )
{code}
",melix,tim_yates,Major,Closed,Fixed,04/Oct/12 11:11,22/Dec/12 01:10
Bug,GROOVY-5744,12816559,Multiple assignment from Iterator skips every other element,"If you try to assign multiple variables from an iterator Groovy will skip every other value.

Example:
{code}
final list = [1,2,3]
final iter = list.iterator()

final def (a,b,c) = list
final def (d,e,f) = iter

assert ""$a $b $c"" == ""$d $e $f""
{code}

Result:
{noformat}
Assertion failed: 

assert ""$a $b $c"" == ""$d $e $f""
         |  |  |  |    |  |  |
         1  2  3  |    1  3  null
                  false
{noformat}

The expected behavior is either that the assertion succeeds, or the assignment to (d,e,f) fails outright.

If this is fixed it would also be nice if any Iterable could be used on the RHS.",emilles,justin.piper@gmail.com,Minor,Resolved,Fixed,05/Oct/12 17:54,10/Oct/22 19:45
Bug,GROOVY-5746,12816568,Array assignment with operator does double call for index expression and array part,"{code:Java}
@groovy.transform.CompileStatic
class IndexTest {
    private int f = 0
    public int getIndex() {f++;0}
    def foo() {
        List<String> x = [""1"",""2""]
        x[index] += ""1""
        assert x[0] == ""11""
        assert f == 1
    }
}
def x = new IndexTest()
x.foo()
{code}
The program above shows code in which we use an operator in combination with array assignment and an index with a side effect. This code must run without calling getIndex() twice. Since the field f is incremented twice, this is not the case. Normal Groovy executed the code as wished. The probable implementation is x[index] = x[index] + ""1"", which not only evals index twice, but also x. x and index must be evaluated only once!",emilles,blackdrag,Major,Closed,Fixed,08/Oct/12 09:22,03/Feb/22 22:32
Bug,GROOVY-5747,12811979,First call to Groovlet succeeds but subsequent calls return 404,".groovy extension mapped in web.xml and .groovy script located in WEB-INF/groovy

First call to test.groovy works
http://localhost:8080/test/test.groovy

INFO: Groovy servlet initialized on groovy.util.GroovyScriptEngine@6f03de90.
Oct 8, 2012 1:52:39 PM org.apache.catalina.core.ApplicationContext log

All subsequent calls return 404
http://localhost:8080/test/test.groovy

INFO: Server startup in 773 ms
GroovyServlet Error:  script: '/test.groovy':  Script not found, sending 404.

Note the .groovy mapping works corrected in Groovy 1.8.4 through 2.0.1 but appears to have broken as 2.0.2  through 2.0.5.

Attached sample WAR removed the groovy jar which was originally packed as ""WEB-INF/lib/groovy-all-2.0.5.jar""

Related discussion can be found here:
http://stackoverflow.com/questions/8228873/groovyservlet-returning-404-on-all-requests-after-successfully-loading-once",blackdrag,gjmathews1,Major,Closed,Fixed,08/Oct/12 13:03,22/Dec/12 01:10
Bug,GROOVY-5748,12811681,glitch with TypeChecked and generics,"This code:
{code}
// adapted from:
// http://stackoverflow.com/questions/9383681/how-can-i-design-a-type-safe-stack-in-java-preventing-pops-from-an-empty-list

import groovy.transform.ASTTest
import groovy.transform.TypeChecked
import static org.codehaus.groovy.transform.stc.StaticTypesMarker.*
import static org.codehaus.groovy.ast.ClassHelper.*

interface IStack<T> {
    INonEmptyStack<T, ? extends IStack<T>> push(T x)
}

interface IEmptyStack<T> extends IStack<T> {
    @Override
    INonEmptyStack<T, IEmptyStack<T>> push(T x)
}

interface INonEmptyStack<T, TStackBeneath extends IStack<T>> extends IStack<T> {
    T getTop()

    TStackBeneath pop()

    @Override
    INonEmptyStack<T, INonEmptyStack<T, TStackBeneath>> push(T x)
}

class EmptyStack<T> implements IEmptyStack<T> {
    @Override
    INonEmptyStack<T, IEmptyStack<T>> push(T x) {
        new NonEmptyStack<T, IEmptyStack<T>>(x, this)
    }
}

class NonEmptyStack<T, TStackBeneath extends IStack<T>>
        implements INonEmptyStack<T, TStackBeneath> {
    private final TStackBeneath stackBeneathTop;
    private final T top

    NonEmptyStack(T top, TStackBeneath stackBeneathTop) {
        this.top = top
        this.stackBeneathTop = stackBeneathTop
    }

    @Override
    T getTop() {
        top
    }

    @Override
    TStackBeneath pop() {
        stackBeneathTop
    }

    @Override
    INonEmptyStack<T, INonEmptyStack<T, TStackBeneath>> push(T x) {
        new NonEmptyStack<T, INonEmptyStack<T, TStackBeneath>>(x, this)
    }
}

@TypeChecked
def main() {
    final IStack<Integer> stack = new EmptyStack<Integer>()

    @ASTTest(phase=INSTRUCTION_SELECTION, value={
        println 'oneInteger:' + node.getNodeMetaData(INFERRED_TYPE)
    })
    def oneInteger = stack.push(1)
    assert oneInteger.getTop() == 1

    @ASTTest(phase=INSTRUCTION_SELECTION, value={
        println 'twoIntegers:' + node.getNodeMetaData(INFERRED_TYPE)
    })
    def twoIntegers = stack.push(1).push(2)
    assert twoIntegers.getTop() == 2

    @ASTTest(phase=INSTRUCTION_SELECTION, value={
        println 'oneIntegerAgain:' + node.getNodeMetaData(INFERRED_TYPE)
    })
    def oneIntegerAgain = stack.push(1).push(2).pop()
    // assert oneIntegerAgain.getTop() == 1 // BOOM!!!!
}
{code}
Gives this output:
{noformat}
oneInteger:INonEmptyStack <java.lang.Integer, IEmptyStack> -> INonEmptyStack <T, TStackBeneath extends IStack <T> -> IStack <T>>
twoIntegers:INonEmptyStack <java.lang.Integer, INonEmptyStack> -> INonEmptyStack <T, TStackBeneath extends IStack <T> -> IStack <T>>
oneIntegerAgain:IStack <INonEmptyStack> -> IStack <T>
{noformat}
and gives this error when the last assert is uncommented:
{noformat}
1 compilation error:
[Static type checking] - Cannot find matching method IStack#getTop(). Please check if the declared type is right and if the method exists.
 at line: 79, column: 12
{noformat}
",melix,paulk,Major,Closed,Fixed,08/Oct/12 14:49,07/Apr/15 19:07
Bug,GROOVY-5750,12816522,MissingFieldException when accessing @Lazy property of outer class,"If an inner class tries to access a @Lazy property of its outer class it will cause a MissingFieldException unless it either explicitly calls the getter or specifies the outer {{this}}.

{code}

class Outer
{
    @Lazy lazy   = 'lazy'
    final normal = 'normal'
    final inner  = new Inner()
    
    class Inner
    {
        def f() { normal }
        def g() { Outer.this.lazy }
        def h() { getLazy() }
        def i() { lazy }
    }
}

final outer = new Outer()

assert outer.normal == outer.inner.f()
assert outer.lazy   == outer.inner.g()
assert outer.lazy   == outer.inner.h()
assert outer.lazy   == outer.inner.i()
{code}

The expected behavior is that {{#g}}, {{#h}} and {{#i}} are equivalent.",,justin.piper@gmail.com,Major,Closed,Fixed,10/Oct/12 22:56,17/Dec/14 13:25
Bug,GROOVY-5751,12816576,Adding milliseconds with TimeCategory gives incorrect toString,"From [this question|http://stackoverflow.com/questions/12833903/groovy-timecategory-bad-millseconds-addition] on StackOverflow, doing:

{code}
def a = use(groovy.time.TimeCategory) {
  800.millisecond + 300.millisecond
}
println a
{code}

Gives the output:

{code}
0.1100 seconds
{code}

When it should show either:

{code}
1100 milliseconds
{code}

or

{code}
1.1 seconds
{code}",paulk,tim_yates,Major,Closed,Fixed,11/Oct/12 03:25,22/Dec/12 01:10
Bug,GROOVY-5752,12812018,DelegateASTTransformation#addGetterIfNeeded doesn't take boolean isX accessors into account,"I'm using @Delegate on a field whose type includes boolean properties. The resulting byte code properly delegates non-overridden getters and setters, but not boolean isX() accessors. This is due to DelegateASTTransformation#addGetterIfNeeded not considering isX accessors. 

Right now I've fixed this like this, wrapping the existing method in a for loop iterating over both 'get' and 'is':

{code}
    private void addGetterIfNeeded(FieldNode fieldNode, ClassNode owner, PropertyNode prop, String name) {
    	for (String accessor: new String[] {""get"", ""is""}) {
    		String getterName = accessor + Verifier.capitalize(name);
    		if (owner.getGetterMethod(getterName) == null) {
    			owner.addMethod(getterName,
    					ACC_PUBLIC,
    					nonGeneric(prop.getType()),
    					Parameter.EMPTY_ARRAY,
    					null,
    					new ReturnStatement(
    							new PropertyExpression(
    									new VariableExpression(fieldNode),
    									name)));
    		}
    	}
    }
{code}

I could include a JUnit test, but the bug seems obvious enough looking at the DelegateASTTransformation code.",paulk,jkuipers,Major,Closed,Fixed,11/Oct/12 03:53,01/Feb/17 23:19
Bug,GROOVY-5753,12817461,Regression: @TypeChecked fails on ternary expression,"The following code fails with Groovy 2.0.5, but worked with 2.0.0:

{code}import groovy.transform.TypeChecked

@TypeChecked
void unboxedTest() {
    double nan = Double.NaN
    double test =  true ? 1.0d : nan         // works    
    double test2 = true ? 1.0d : Double.NaN  // doesn't work
}{code}

Error message: 

{code}org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/tmp/map.groovy: 7: [Static type checking] - Cannot assign value of type java.io.Serializable or java.lang.Comparable to variable of type double
 @ line 7, column 25.
       double test2 = true ? 1.0d : Double.NaN  // doesn't work
{code}",melix,perplex79,Major,Closed,Fixed,11/Oct/12 05:28,22/Dec/12 01:10
Bug,GROOVY-5755,12816584,MissingPropertyException thrown when referencing an externally declared constant from a switch case in an enum instance method ,"For illustration see the code below:

{code}
import javax.xml.stream.XMLStreamConstants

enum MyEnum {
     INSTANCE {
        String getName(int i) {
            switch (i) {
                case javax.xml.stream.XMLStreamConstants.START_ELEMENT:
                    return ""START_ELEMENT""
                case XMLStreamConstants.END_ELEMENT:
                    return ""END_ELEMENT""
            }
        }
    }
}

println MyEnum.INSTANCE.getName(XMLStreamConstants.START_ELEMENT)
println MyEnum.INSTANCE.getName(XMLStreamConstants.END_ELEMENT)
{code}

The first invokation of getName succeeds because the constant is referenced with its fully qualified name in the first case statement. The second however fails in spite of the fact that the referenced class is imported.

Exact output of the script is:
{noformat}
START_ELEMENT
Caught: groovy.lang.MissingPropertyException: No such property: XMLStreamConstants for class: MyEnum$1
groovy.lang.MissingPropertyException: No such property: XMLStreamConstants for class: MyEnum$1
	at MyEnum$1.getName(EnumMemberMethodWithSwitchTest.groovy:9)
	at MyEnum$1$getName.call(Unknown Source)
	at EnumMemberMethodWithSwitchTest.run(EnumMemberMethodWithSwitchTest.groovy:18)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
{noformat}",paulk,zagyvaibalazs,Major,Closed,Fixed,11/Oct/12 10:22,12/Apr/13 16:55
Bug,GROOVY-5756,12818194,error during class generation: cannot define closures inside methods of an enum instance,"The following code fails to run:

{code}
enum MyEnum {
    INSTANCE {
        String foo() {
            def clos = {
                ""foo""
            }
            clos.call()
        }
    }

    String foo() {
    }
}

assert ""foo"" == MyEnum.INSTANCE.foo()
{code}

The Groovy runtime throws the following exception when trying to execute it:
{noformat}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
General error during class generation: NPE while processing ClosureDefiningEnumInstanceMethodTest.groovy

groovy.lang.GroovyRuntimeException: NPE while processing ClosureDefiningEnumInstanceMethodTest.groovy
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:198)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:786)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:279)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:258)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:244)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:185)
	at groovy.lang.GroovyShell$2.run(GroovyShell.java:206)
	at groovy.lang.GroovyShell$2.run(GroovyShell.java:204)
	at java.security.AccessController.doPrivileged(Native Method)
	at groovy.lang.GroovyShell.run(GroovyShell.java:204)
	at groovy.lang.GroovyShell.run(GroovyShell.java:150)
	at groovy.ui.GroovyMain.processOnce(GroovyMain.java:557)
	at groovy.ui.GroovyMain.run(GroovyMain.java:344)
	at groovy.ui.GroovyMain.process(GroovyMain.java:330)
	at groovy.ui.GroovyMain.processArgs(GroovyMain.java:119)
	at groovy.ui.GroovyMain.main(GroovyMain.java:99)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)
Caused by: java.lang.NullPointerException
	at org.codehaus.groovy.classgen.asm.ClosureWriter.createClosureClass(ClosureWriter.java:184)
	at org.codehaus.groovy.classgen.asm.ClosureWriter.getOrAddClosureClass(ClosureWriter.java:150)
	at org.codehaus.groovy.classgen.asm.ClosureWriter.writeClosure(ClosureWriter.java:77)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClosureExpression(AsmClassGenerator.java:546)
	at org.codehaus.groovy.ast.expr.ClosureExpression.visit(ClosureExpression.java:43)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateEqual(BinaryExpressionHelper.java:295)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitDeclarationExpression(AsmClassGenerator.java:522)
	at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:87)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:354)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:509)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:155)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	... 25 more

1 error
ï©{noformat}

As the stacktrace suggests, the problem is caused by the closure definition in the enum instance specific foo() method.",paulk,zagyvaibalazs,Major,Closed,Fixed,11/Oct/12 18:38,12/Apr/13 16:55
Bug,GROOVY-5758,12816601,"@CompileStatic complains about ArrayList not being a String, but doesn't so about HashSet. Seems inconsistent.","{code}
@CompileStatic
class MyGroove {

    private String str = null;

    def static main(args)
    {
        MyGroove myGroove = new MyGroove();
        println myGroove.str
        myGroove.str = ""abc""
        println myGroove.str.getClass()
        myGroove.str = new HashSet<String>()    // compiles fine
        println myGroove.str    // prints ""[]"" to the console

        // does not compile: [Static type checking] - Cannot assign 
        // value of type java.util.ArrayList to variable of 
        // type java.lang.String
        myGroove.str = new ArrayList<String>()
    }

}
{code}",melix,oliver_plow,Major,Closed,Fixed,15/Oct/12 02:48,22/Dec/12 01:10
Bug,GROOVY-5762,12816532,Static compilation runtime error when using mapped constructor,"Take this class

{code}
package compilestatic

import groovy.transform.Canonical;
import groovy.transform.CompileStatic;

@CompileStatic
class WTF {
	public static void main(String[] args) {
		new Person(name:""First"")
		first(new Person(name:""First""))
	}

	static Person first(Person p) {
		p
	}

}

@Canonical
class Person {
	String name
}
{code}

The following exception is thrown:
{code}
Caught: java.lang.NoSuchMethodError: compilestatic.Person.<init>(Ljava/util/Map;)V
java.lang.NoSuchMethodError: compilestatic.Person.<init>(Ljava/util/Map;)V
	at compilestatic.WTF.main(wtf.groovy:10)
{code}

Note that this exception comes from the *second* statement in the main method.  Comment out that statement and there is no exception.",melix,werdna,Major,Closed,Fixed,17/Oct/12 08:32,22/Dec/12 01:10
Bug,GROOVY-5765,12816498,Enum with vararg argument to the constructor ignores everything except the first argument,"enum with vararg argument to the constructor ignores everything except the first argument.

Included are two tests that are reproducible with groovy at the command line. Works fine using varargs as an argument for a class constructor - sample also included.",,japettyjohn,Major,Closed,Fixed,20/Oct/12 17:41,28/Dec/14 07:52
Bug,GROOVY-5766,12816373,Changes in groovy.json.JsonOutput require extensive permissions that aren't compatible with some web frameworks.,"It seems beginning somewhere after 2.0.1 but most likely in 2.0.5 there were some changes in groovy.json.JsonOutput that make it use certain methods or libraries that are restricted in App Engine.  Perhaps it was made multi-threaded?

If Groovy is going to support App Engine, these changes will have to be rolled back or made more configurable by allowing the developer to specify wether they want to leverage multithreading or not (If multithreading is the issue).

https://github.com/glaforge/gaelyk/issues/167

",guillaume,sdmurphy,Major,Closed,Fixed,22/Oct/12 21:58,22/Dec/12 01:10
Bug,GROOVY-5769,12816570,NullObject leak,"{code:Java}
def b = null  
assert b == null  
def a = null.getClass().newInstance()  
println a.getClass() 
assert a == null
{code}
The program shows a leak of NullObject. newInstance will give an instance. This is problematic in multiple ways. NullObject is supposed to be almost an invisible object, as well as a singleton.
To make the program above pass we could either fix the compareEquals or we fix newInstance() to return null for NullObject. I prefer the later one.",paulk,blackdrag,Major,Resolved,Fixed,25/Oct/12 10:10,10/Jun/23 23:14
Bug,GROOVY-5770,12816470,CompileStatic ignores the extra parameters when a class is parametrized,"{code:title=ParamTest.groovy|borderStyle=solid}
@groovy.transform.CompileStatic
class ParamTest <T>{
    static main(String[] args) {
        ParamTest<String,Integer> pt = new ParamTest<String>()
        println ""finish""
    }
}
{code}

The code fails to throw a compilation error on line 4. It happens when the class is initialized with more parameters than it was supposed to.
The following declarations for example will not throw a compile error either:


        ParamTest<String> pt = new ParamTest<String,Integer>()
        ParamTest<String,String> pt = new ParamTest<String,Integer>()",melix,carlosapc,Minor,Closed,Fixed,26/Oct/12 08:27,31/Mar/15 06:08
Bug,GROOVY-5771,12816596,Project compilation fails when I put an enum in my project,"If I put the following enum class in my project source code, compilation fails for the project and a red mark is put on ""line 0"" of this enum class and on another class:

{noformat}
General error during class generation: Index: 0, Size: 0 java.lang.IndexOutOfBoundsException: Index: 0, Size: 0 at 
 java.util.ArrayList.rangeCheck(Unknown Source) at java.util.ArrayList.get(Unknown Source) at 
 org.codehaus.groovy.classgen.asm.InvocationWriter.writeAICCall(InvocationWriter.java:459) at 
 org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeConstructor(InvocationWriter.java:430) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorCallExpression(AsmClassGenerator.java:908) at 
 org.codehaus.groovy.ast.expr.ConstructorCallExpression.visit(ConstructorCallExpression.java:43) at 
 org.codehaus.groovy.classgen.asm.CallSiteWriter.makeCallSite(CallSiteWriter.java:301) at 
 org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:231) at 
 org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:76) at 
 org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:60) at 
 org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:336) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:647) at 
 org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67) at 
 org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:584) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:504) at 
 org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47) at 
 org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:454) at 
 org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69) at 
 org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:102) at 
 org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:113) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:318) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:275) at 
 org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:124) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:395) at 
 org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1174) at 
 org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:51) at 
 org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:179) at 
 org.codehaus.groovy.control.CompilationUnit$6.call(CompilationUnit.java:857) at 
 org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1191) at 
 org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:623) at 
 org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:601) at 
 org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:578) at 
 org.codehaus.jdt.groovy.internal.compiler.ast.GroovyCompilationUnitDeclaration.processToPhase(GroovyCompilati
 onUnitDeclaration.java:171) at 
 org.codehaus.jdt.groovy.internal.compiler.ast.GroovyCompilationUnitDeclaration.generateCode(GroovyCompilatio
 nUnitDeclaration.java:1534) at org.eclipse.jdt.internal.compiler.Compiler.resolve(Compiler.java:986) at 
 org.eclipse.jdt.internal.compiler.Compiler.resolve(Compiler.java:1025) at 
 org.eclipse.jdt.internal.core.CompilationUnitProblemFinder.process(CompilationUnitProblemFinder.java:209) at 
 org.eclipse.jdt.internal.core.CompilationUnitProblemFinder.process(CompilationUnitProblemFinder.java:275) at 
 org.codehaus.jdt.groovy.model.GroovyReconcileWorkingCopyOperation.makeConsistent(GroovyReconcileWorking
 CopyOperation.java:80) at 
 org.eclipse.jdt.internal.core.ReconcileWorkingCopyOperation.executeOperation(ReconcileWorkingCopyOperation.ja
 va:89) at org.eclipse.jdt.internal.core.JavaModelOperation.run(JavaModelOperation.java:728) at 
 org.eclipse.jdt.internal.core.JavaModelOperation.runOperation(JavaModelOperation.java:788) at 
 org.codehaus.jdt.groovy.model.GroovyCompilationUnit.reconcile(GroovyCompilationUnit.java:429) at 
 org.eclipse.jdt.internal.ui.text.java.JavaReconcilingStrategy.reconcile(JavaReconcilingStrategy.java:126) at 
 org.eclipse.jdt.internal.ui.text.java.JavaReconcilingStrategy.access$0(JavaReconcilingStrategy.java:108) at 
 org.eclipse.jdt.internal.ui.text.java.JavaReconcilingStrategy$1.run(JavaReconcilingStrategy.java:89) at 
 org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42) at 
 org.eclipse.jdt.internal.ui.text.java.JavaReconcilingStrategy.reconcile(JavaReconcilingStrategy.java:87) at 
 org.eclipse.jdt.internal.ui.text.java.JavaReconcilingStrategy.reconcile(JavaReconcilingStrategy.java:151) at 
 org.eclipse.jdt.internal.ui.text.CompositeReconcilingStrategy.reconcile(CompositeReconcilingStrategy.java:86) at 
 org.eclipse.jdt.internal.ui.text.JavaCompositeReconcilingStrategy.reconcile(JavaCompositeReconcilingStrategy.java:
 104) at org.eclipse.jface.text.reconciler.MonoReconciler.process(MonoReconciler.java:77) at 
 org.eclipse.jface.text.reconciler.AbstractReconciler$BackgroundThread.run(AbstractReconciler.java:206)
{noformat}

The enum class that is giving problem is the following:

{code}
package com.utopicmusic.app.web.seo

import java.sql.SQLException

import org.hibernate.HibernateException
import org.hibernate.Session
import org.springframework.orm.hibernate3.HibernateCallback
import org.springframework.orm.hibernate3.HibernateTemplate



/**
 * Type of sitemap that can be generated.
 * 
 * @author Mauro Molinari
 */
enum SitemapType {

	/** Sitemap index. */
	INDEX {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate) {
			final def result = [:]
			// index for artists
			// count
			//int c = ARTISTS.countEntries(hibernateTemplate)

			return result
		}

		@Override
		String getTemplatePath() {
			getClass().package.name.replace('.', '/') + '/sitemap-index.ftl'
		}
	},

	/** Sitemap for artist public pages. */
	ARTISTS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}

		@Override
		int countEntries(final HibernateTemplate hibernateTemplate) {
			hibernateTemplate.execute(new HibernateCallback<Integer>() {
						@Override
						public Integer doInHibernate(Session session) throws HibernateException, SQLException {
							return session.createQuery(""select count(artist) from Artist as artist join artist.owner as user where user.active and artist.active"").uniqueResult()
						}
					})
		}
	},

	/** Sitemap for club public pages. */
	CLUBS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	},

	/** Sitemap for listener public pages. */
	LISTENERS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	},

	/** Sitemap for post pages. */
	POSTS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	},

	/** Sitemap for song pages. */
	SONGS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	},

	/** Sitemap for album pages. */
	ALBUMS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	},

	/** Sitemap for playlist pages. */
	PLAYLISTS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	},

	/** Sitemap for geographic pages. */
	LOCATIONS {
		@Override
		Map buildModel(final int offset, final HibernateTemplate hibernateTemplate){
		}
	}

	/**
	 * Returns the path within the classpath of the template that can be used to
	 * generate a sitemap of this type.
	 *
	 * @return the full path within the classpath to retrieve the template
	 */
	String getTemplatePath() {
		getClass().package.name.replace('.', '/') + '/sitemap.ftl'
	}

	/**
	 * Builds the model and view for a sitemap of this type.
	 * <p>
	 * Actually, only a part of sitemap is built, containing up to 30,000 links at most.
	 *
	 * @param offset the offset for the contents to include in the requested sitemap
	 * @return the model and reference to the view name needed to generate this type of sitemap
	 */
	Map buildModel(final int offset, final HibernateTemplate hibernateTemplate) {
		throw new UnsupportedOperationException('requested sitemap unavailable')
	}

	int countEntries(final HibernateTemplate hibernateTemplate) {
		throw new UnsupportedOperationException('count not implemented for this sitemap type')
	}
}
{code}",paulk,mauromol,Critical,Closed,Fixed,27/Oct/12 11:58,12/Apr/13 16:55
Bug,GROOVY-5775,12818195,XmlTemplateEngine does not escape expression values (part 1),"I would expect the following script to run without problems, but it does not:
{code}
import groovy.text.*

def xmlEngine = new XmlTemplateEngine()
def xml = '''<?xml version=""1.0""?>
<users xmlns:gsp='http://groovy.codehaus.org/2005/gsp'>
    <gsp:scriptlet>users.each {</gsp:scriptlet>
        <user id=""${it.id}""><gsp:expression>it.name</gsp:expression></user>
        <foo1>'</foo1>
        <foo2>""</foo2>
    <gsp:scriptlet>}</gsp:scriptlet>
</users>'''
def xmlBinding = [users: [
    new Expando(id: 1, name: 'mr & "" haki'),
    new Expando(id: 2, name: ""Hub < > ' ert"")]
]
def xmlOutput = xmlEngine.createTemplate(xml).make(xmlBinding).toString()
println xmlOutput
def root = new XmlParser().parseText(xmlOutput)
{code}

Error:
{noformat}
[Fatal Error] :3:9: The entity name must immediately follow the '&' in the entity reference.

Reason: the strings from the expandos are not escaped by the XmlTemplateEngine
{noformat}",paulk,xn137,Minor,Closed,Fixed,31/Oct/12 05:12,13/Jan/13 04:02
Bug,GROOVY-5776,12818197,"Accessing static property in closure of sub-class creates a ""serious error""","Discovered in GRECLIPSE-1492.  Compile the following snippet in Groovy 2.0.4:

{code}
class TestappService extends ServiceBase {

	def foo = { params ->
		def param = params[TESTAPP_IDX]

		return ""Hello, ${param}""
	}
}
class ServiceBase {

	static final String TESTAPP_IDX = 'index'
}
{code}

You get this stack trace:

{quote}
>>> a serious error occurred: BUG! exception in phase 'class generation' in source unit 'foo.groovy' Trying to access private constant field [ServiceBase#TESTAPP_IDX] from inner class
>>> stacktrace:
BUG! exception in phase 'class generation' in source unit 'foot.groovy' Trying to access private constant field [ServiceBase#TESTAPP_IDX] from inner class
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAttributeOrProperty(AsmClassGenerator.java:966)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitPropertyExpression(AsmClassGenerator.java:1052)
	at org.codehaus.groovy.classgen.AsmClassGenerator.processClassVariable(AsmClassGenerator.java:1292)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitVariableExpression(AsmClassGenerator.java:1249)
	at org.codehaus.groovy.ast.expr.VariableExpression.visit(VariableExpression.java:70)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.visitBoxedArgument(CallSiteWriter.java:239)
	at org.codehaus.groovy.classgen.asm.CallSiteWriter.makeSingleArgumentCall(CallSiteWriter.java:252)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeSingleArgumentCall(InvocationWriter.java:482)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateBinaryExpression(BinaryExpressionHelper.java:504)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.eval(BinaryExpressionHelper.java:239)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:527)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateEqual(BinaryExpressionHelper.java:295)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitDeclarationExpression(AsmClassGenerator.java:522)
	at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:87)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:604)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:354)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:509)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:155)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:786)
	at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:803)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:498)
	at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
	at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:213)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:146)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:176)
	at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:160)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:106)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)
{quote}",,werdna,Major,Closed,Fixed,31/Oct/12 11:29,08/Mar/22 12:16
Bug,GROOVY-5779,12811694,Static type checking - Color.red recognized as int,"Code:

import groovy.transform.TypeChecked;
import java.awt.Color;

class ColorTest
{
    @TypeChecked
    static main(args)
    {
        Color black = Color.black; // ok
        Color red = Color.red;       // Groovy:[Static type checking] - Cannot assign value of type int to variable of type java.awt.Color
    }
}

This occurs only with colors: 'red', 'blue' and 'green'. ",melix,lvanek,Minor,Closed,Fixed,02/Nov/12 05:41,22/Dec/12 01:10
Bug,GROOVY-5780,12816527,CompileStatic - Calling vargarg parameter GString is converted to array of strings,"OOPS... It happens again, almost the same bug as  http://jira.codehaus.org/browse/GROOVY-5703
But now we call with GString instead of String

Assume we have this method:

{code}
static void printMsgs(String ... msgs) {
    for(String s : msgs) { println s;}
}
{code}

Calling it from @CompileStatic:
{code}
@CompileStatic
    static void testCS() {
        String n1=""hello""
        //Print:
        // H
        // e
        // l
        // l
        // o
        printMsgs(""${n1}"");
    }
{code}

The string 'hello' is converted to array of strings.

Calling it from non CompileStatic works fines.
Also if the passed parameter is String and not GString then it also works fine.

The complete test case:
{code}
import groovy.transform.CompileStatic

public class CompileStaticVarArg {

    static void main(String[] args) {

        println GroovySystem.version
        println '-' * 20

        test();

        println '-' * 20

        testCS()

    }


    static void test() {
        String n1=""hello""
        // Print 'hello'
        printMsgs(""${n1}"");
    }

    @CompileStatic
    static void testCS() {
        String n1=""hello""
        //Print:
        // H
        // e
        // l
        // l
        // o
        printMsgs(""${n1}"");
    }

    //@CompileStatic
    static void printMsgs(String... msgs) {

        for (String s : msgs) {
            println s;
        }

    }

}
{code}
",melix,boaznahum,Critical,Closed,Fixed,02/Nov/12 06:12,22/Dec/12 01:10
Bug,GROOVY-5786,12816619,methods defined by one eval are 'lost' over time,"Don't suppose it's possible that a bug has been inadvertently introduced by the fix provided in GROOVY-5187 (which GC's script classes)?

Have a scenario where an eval is done, creating some classes (basically akin to including code from a separate script).  A second eval is done which depends on classes created in the first (same execution engine, obviously).  A bunch of work is done in this second eval, using lots of memory, undoubtedly invoking GC.  At a later point a method that was supposedly created by the first eval is called and is not found:

MissingMethodException: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getTRTimedParm()

Note that it isn't even trying to associate it with a script class as it should, but with the engine itself.

We were previously at version 2.0.1 which did not seem to have this problem, but I can't say for certain without going back to that version and trying this particular test case.

We are working around the problem by re-evaling the first script right before the use of the 'missing' method, which is obviously not ideal.

Is there a way to tell the engine to GC on demand?

Thanks!",blackdrag,danno,Major,Closed,Fixed,07/Nov/12 10:40,28/Dec/12 15:25
Bug,GROOVY-5787,12811969,Indexing map with wrong type inside closure fails with @CompileStatic,"When a map typed with Java-style generics is indexed with the wrong type inside of a closure, it fails with {{@CompileStatic}}.  The following error is generated:

{noformat}
On receiver: someMap with message: getAt and arguments: 1
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY
BUG! exception in phase 'class generation' in source unit '/home/ataylor/dev/test/bug.groovy' At line 6 column 28
On receiver: someMap with message: getAt and arguments: 1
This method should not have been called. Please try to create a simple example reproducing this error and filea bug report at http://jira.codehaus.org/browse/GROOVY
{noformat}

Here's a minimal example that triggers the bug:

{code}
@groovy.transform.CompileStatic
class Bug {
    static void main(args) {
        Map<String, Integer> someMap = ['one': 1]
        def closure = { -> someMap[1] }
    }
}
{code}
",melix,ataylor284,Minor,Closed,Fixed,07/Nov/12 22:27,22/Dec/12 01:10
Bug,GROOVY-5789,12816600,@CompileStatic changes scope of for loop variable,"The script:

{code}
def execute() {
    for (def i = 0; i < 4; ++i) { println i }
}

execute()
{code}

works exactly as expected. However:

{code}
import groovy.transform.CompileStatic

@CompileStatic execute() {
    for (def i = 0; i < 4; ++i) { println i }
}

execute()
{code}

creates an infinite loop in which the value of i is always 0.",melix,russel,Blocker,Closed,Fixed,10/Nov/12 09:52,22/Dec/12 01:10
Bug,GROOVY-5791,12816586,Increment operators with @CompileStatic,"Assume we have the script:
{code}
import groovy.transform.CompileStatic
import groovy.transform.TypeChecked

@CompileStatic
//@TypeChecked
int incInt(int n) {
    def result = n
    ++result
    result++
    return result
}
assert  incInt(5) == 7,""Must be 7 but we have 5""
{code}

Increment operations not execute when @CompileStatic and a local variable is not explicitly typed.  ",melix,vns,Blocker,Closed,Fixed,13/Nov/12 02:24,22/Dec/12 01:10
Bug,GROOVY-5792,12816588,XmlNodePrinter uses quote variables for more than just attributes,XmlNodePrinter.printEscaped is not checking isAttributeValue when deciding to use the quote variable for single quotes and double quotes. This means than in the body of an XML element it'll avoid quoting single or double quote if the quote variable is respectively a single or double quote. The quote variable should ONLY apply to attributes. This is causing the output to not be quoted when it should be.,paulk,quidryan,Major,Closed,Fixed,13/Nov/12 19:43,22/Dec/12 01:10
Bug,GROOVY-5793,12816617,Compilation error with method receiving a java.lang.Byte,"With Static Compilation enabled, if I try to call a method declared in a class that receives a parameter of type java.lang.Byte, the compilation fails, returning ""[Static type checking] - Cannot find matching method"".
The same problem occurs when calling a method from inside another method, for instance:

{code:title=Test.gvy|borderStyle=solid}

@groovy.transform.CompileStatic
void testMethod(java.lang.Byte param){
println(param)
}

@groovy.transform.CompileStatic
void run (){
testMethod(java.lang.Byte.valueOf(""123""))
}

run()

{code}

The problem seems to be in the ""isAssignableTo"" method from ""org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.java"" class, where ClassHelper.Byte_TYPE doesn't match the type received as an argument, probably because you are not using the redirect() method before comparing with the type that matters.",melix,rcruzjo,Blocker,Closed,Fixed,15/Nov/12 11:40,22/Dec/12 01:10
Bug,GROOVY-5797,12816629,CompileStatic BUG with indexing map of maps,"The following code:

{code}
@groovy.transform.CompileStatic
def m( Map param ) {
  def map = [ tim:4 ]
  map[ param.key ]
}

m( [ key: 'tim' ] )
{code}

Throws

{code}
BUG! exception in phase 'class generation' in source unit 'ConsoleScript38' At line 4 column 3
On receiver: map with message: getAt and arguments: param.key
{code}
",melix,tim_yates,Major,Closed,Fixed,16/Nov/12 07:25,22/Dec/12 01:10
Bug,GROOVY-5798,12816625,CompileStatic cannot cast int to char,"{code}
@groovy.transform.CompileStatic
char m( int v ) {
  char c = (char)v
  c
}

println m( 65 )
{code}

Gives:

{code}
[Static type checking] - Inconvertible types: cannot cast int to char
{code}

Removing CompileStatic or just doing:

{code}
@groovy.transform.CompileStatic
char m( int v ) {
  v
}

println m( 65 )
{code}

Works",melix,tim_yates,Major,Closed,Fixed,16/Nov/12 07:32,22/Dec/12 01:10
Bug,GROOVY-5799,12816663,Compile static forces 'as' when trying to make array of nulls,"The following:

{code}
@groovy.transform.CompileStatic
Integer[] m() {
  Integer[] arr = [ null, null ]
}

println m()
{code}

Throws:

{code}
[Static type checking] - Cannot assign value of type java.lang.Object into array of type [Ljava.lang.Integer;
{code}

To get it to work, either remove CompileStatic, or change the function to:

{code}
@groovy.transform.CompileStatic
Integer[] m() {
  Integer[] arr = [ null, null ] as Integer[]
}

println m()
{code}",melix,tim_yates,Major,Closed,Fixed,16/Nov/12 07:45,22/Dec/12 01:10
Bug,GROOVY-5800,12816606,Can't get 'in' to work under static compilation,"Given:

{code}
@groovy.transform.CompileStatic
boolean m( Integer i ) {
  i in [ 1, 2, 3 ]
}

println m( 1 )
{code}

We get {{false}}.

Comment out the annotation, or change it to:

{code}
@groovy.transform.CompileStatic
boolean m( Integer i ) {
  [ 1, 2, 3 ].contains( i )
}
{code}

And we get true again.",melix,tim_yates,Major,Closed,Fixed,16/Nov/12 08:28,22/Dec/12 01:10
Bug,GROOVY-5801,12816599,sql.firstRow() fetches all rows of query,"We were trying to use ""sql.firstRow(...)"" but had to switch to the other sql calls because firstRow is so efficient. Looking at the code (https://github.com/groovy/groovy-core/blob/master/subprojects/groovy-sql/src/main/java/groovy/sql/Sql.java#L2109) implies it could be quickly improved:

current code:
{code}
public GroovyRowResult firstRow(String sql) throws SQLException {
    List<GroovyRowResult> rows = rows(sql);
    if (rows.isEmpty()) return null;
    return (rows.get(0));
}
{code}

suggested code:
{code}
public GroovyRowResult firstRow(String sql) throws SQLException {
    List<GroovyRowResult> rows = rows(sql, 1, 1, null);
    if (rows.isEmpty()) return null;
    return (rows.get(0));
}
{code}

This would avoid first loading all of the rows into memory, just to grab the first one and throw the rest away. For large tables, this causes us to run out of memory.",pschumacher,samp,Major,Closed,Fixed,16/Nov/12 08:57,15/Oct/15 18:16
Bug,GROOVY-5802,12816626,Exception thrown in groovyConsole,"Add the following snippet to an empty groovyConsole using Groovy 2.0.4:

{code}
package pkg1

import groovy.transform.TypeChecked


class ExtendedList extends ArrayList<String>{    
}

class SourceClass {
    @TypeChecked public static ExtendedList createList() { 
        return null; 
    }
}
{code}

Compile and then you get the following exception:

{code}
Exception thrown
Nov 16, 2012 9:21:19 AM org.codehaus.groovy.runtime.StackTraceUtils sanitize
WARNING: Sanitizing stacktrace:
java.lang.IllegalArgumentException: argument type mismatch
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.setProperty(MetaClassImpl.java:2402)
	at groovy.lang.MetaClassImpl.setProperty(MetaClassImpl.java:3330)
	at org.codehaus.groovy.runtime.InvokerHelper.setProperties(InvokerHelper.java:463)
	at org.codehaus.groovy.runtime.InvokerHelper.createScript(InvokerHelper.java:440)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:625)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:652)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:643)
	at groovy.lang.GroovyShell$parse.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at groovy.ui.Console$_compileScript_closure18.doCall(Console.groovy:997)
	at groovy.ui.Console$_compileScript_closure18.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
	at groovy.lang.Closure.call(Closure.java:415)
	at groovy.lang.Closure.call(Closure.java:409)
	at groovy.lang.Closure.run(Closure.java:493)
	at java.lang.Thread.run(Thread.java:680)

java.lang.IllegalArgumentException: argument type mismatch
{code}",paulk,werdna,Major,Closed,Fixed,16/Nov/12 11:23,10/Jul/13 04:42
Bug,GROOVY-5803,12816633,"wasted work in ""ClassNode.addMixin""","The problem appears in Groovy 2.0.5 and in revision 9c9cd23..  I
attached a one-line patch (addMixin.diff) that fixes it.

In method ""ClassNode.addMixin"", the loop over ""mixins"" should call
""break"" immediately after ""skip"" is set to ""true"".  All the iterations
after ""skip"" is set to ""true"" do not perform any useful work, at best
they just set ""skip"" again to ""true"".

""ClassNode.addInterface"" has a similar problem.  I attached a second
patch for it (addInterface.diff).",paulk,adriannistor,Minor,Closed,Fixed,16/Nov/12 14:42,05/Apr/15 14:44
Bug,GROOVY-5804,12818447,VerifyError when @CompileStatic and negate boolean with !,"In a CompileStatic block, when a boolean is negated with ! expression, I have the following error at runtime: 

Error:
{code}
java.lang.VerifyError: 
(class: fr/manu/impl/MaClass$_retournerItems_closure2, method: doCall signature: (Ljava/lang/Object;)Ljava/lang/Object;) 
Expecting to find integer on stack
{code}


Groovy code:
{code:title=Class.groovy}
boolean b = true
if (!b) {
...
}
{code}",melix,mboillod,Major,Closed,Fixed,18/Nov/12 12:55,22/Dec/12 01:10
Bug,GROOVY-5806,12816616,Groovy 2.0.5 breaks GMock when EMC is used,"Exception is 

{code}
groovy.lang.MissingMethodException: No signature of method: org.gmock.internal.metaclass.MockProxyMetaClass$4.doCall() 
{code}

See related Grails issue

http://jira.grails.org/browse/GRAILS-9550

Sample non-Grails gradle project:

http://jira.grails.org/secure/attachment/17963/withGMock-non-grails.zip",melix,graemerocher,Critical,Closed,Fixed,19/Nov/12 07:49,22/Dec/12 01:10
Bug,GROOVY-5809,12816627,.groovy files on the classpath don't work if the absolute path contains non-latin characters,"If you have a .groovy file on the classpath when running a groovy script, imports of the classpath located file from the main script (or other places) only work when the absolute path to the .groovy file being imported doesn't contain non-latin characters.

It doesn't seem to make any difference whether or not the file to be imported is in the default package (top level) or not.

This problem is easy to reproduce. Attached is zip file with a groovy script and a single dependency in a subdirectory called 'test+test'. If you unzip this and cd to the subdirectory and run:

groovy -cp . Script.groovy

... it will fail. If you cd up a level and change the name of the directory from 'test+test' to 'test' then cd back into test and re-run it, it will work.",blackdrag,jimfcarroll,Major,Closed,Fixed,20/Nov/12 08:28,22/Dec/12 01:10
Bug,GROOVY-5810,12816595,Calling static method from subclass using super creates StackOverFlow,"Don't know whether calling super from a static method is supposed to compile. But it does in IDEA IC-122.519 EAP using Groovy 2.0.5 and runs into an endless loop.
{code}
@CompileStatic
class A {
    public static foo() {
        print(""A"")
    }
}

@CompileStatic
class B extends A {
    public static foo() {
        A.foo() // compiles and prints ""AB""
        super.foo() // compiles and creates StackOverFlow
        print(""B"")
    }

    def static main(args)
    {
        B.foo()
    }
}
{code}",melix,oliver_plow,Major,Closed,Fixed,20/Nov/12 09:51,22/Dec/12 01:10
Bug,GROOVY-5811,12816563,InvokerInvocationException not being unwrapped,"When methods compiled static are mixed with methods not compiled static and calling the closure, InvokerInvocationException is not unwrapped. Code depending on original Exception won't work.


See attached Groovy class.

If compiled with 2.0.5, following output should be printed

First: WRONG
Second: WRONG

If all static compilations are disabled, everything is ok

First: OK
Second: OK",melix,musketyr,Major,Closed,Fixed,20/Nov/12 12:39,22/Dec/12 01:10
Bug,GROOVY-5812,12816631,Java code behaves wrong on 2.0.5 but not on 1.8.8,"The folowing Java code:

{code}
public class HelloWorld {
	private String[] names;
	public String[] getNames() { return names; }
	public void setNames(String... names) { this.names = names; }

	public String toString() {
		return ""Hello "" + join(getNames());
	}

	private String join(String[] strings) {
		StringBuilder sb = new StringBuilder();
		String delim = """";
		for(String string : strings) {
			sb.append(delim).append(string);
			delim = "", "";
		}
		return sb.toString();
	}

	public static void main(String[] args) {
		HelloWorld helloWorld = new HelloWorld();
		helloWorld.setNames(""Morten"", ""Riccardo"", ""Anders"");
		System.out.println(helloWorld.toString());
	}
}
{code}

produces the same output when compiled in Java or in Groovy 1.8.8 (the names joined with commas), but in Groovy 2.0.5 it produces: Hello [Ljava.lang.String;@4e5a67df

Found while trying to demonstrate how to simplify Java code in Groovy (not very cool...) :-)",blackdrag,sbglasius,Major,Closed,Fixed,20/Nov/12 14:02,12/Jan/13 20:40
Bug,GROOVY-5814,12811886,Problem with Immutable and CompileStatic,"If you try:

{code}
import groovy.transform.*

@CompileStatic
@Immutable
class Test {
  int a
  String b
}

new Test( 1, 'tim' )
{code}

You get:

{code}
4 compilation errors:

Access to java.lang.Object#a is forbidden at line: -1, column: -1

Access to java.lang.Object#a is forbidden at line: -1, column: -1

Access to java.lang.Object#b is forbidden at line: -1, column: -1

Access to java.lang.Object#b is forbidden at line: -1, column: -1
{code}

If the two are incompatible, is it possible for a better error message?",melix,tim_yates,Major,Closed,Fixed,21/Nov/12 07:04,22/Dec/12 01:10
Bug,GROOVY-5818,12816618,MapStyleConstructorCall breaks Groovy-Eclipse,"The {{MapStyleConstructorCall}} ast node is used in place of a {{ConstructorCallExpression}} ast node.  However, since {{MapStyleConstructorCall}} is a sub-class of {{ByteCodeExpression}}, it is not treated as a normal ast node and AST visitors are not visiting it correctly.

This has an effect on Groovy-Eclipse in that source code operations (eg- search, refactor, type inferencing, organize imports, navigation, etc) will not work on {{MapStyleConstructorCall}}s or any of their child nodes.  

Additionally, source locations are not correct for this AST node. ",melix,werdna,Major,Closed,Fixed,26/Nov/12 12:55,22/Dec/12 01:10
Bug,GROOVY-5820,12818198,[SwingBuilder] ListFactory does not use custom JList instance if items: is specified,"From the Griffon User list ->

Hi everyone.  I feel like the answer is staring me in the face on this one but I can't figure out what I'm doing wrong.

I have an object that extends JList:

{code}
class MyList extends JList {
     
     public MyList(){
         super()
     }

     ... custom methods ...
}
{code}

and I'm using it in my view as such:

{code}
list(new MyList(), id:'listId', items:[""item 1"",""item 2"",""etc""], valueChanged:controller.listSelectionChangedAction)
{code}

Then in my controller I have:

{code}
def listSelectionChangedAction = { evt ->
     def list = (MyList)evt.source
     ... do stuff with list ...
}
{code}

But I'm getting an error for the type casting:

{code}
2012-11-27 19:34:52,434 [pool-2-thread-2] ERROR griffon.util.GriffonExceptionHandler - Uncaught Exception
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'javax.swing.JList[listId,0,0,256x136,alignmentX=0.0,alignmentY=0.0,border=,flags=50331944,maximumSize=,minimumSize=,preferredSize=,fixedCellHeight=-1,fixedCellWidth=-1,horizontalScrollIncrement=-1,selectionBackground=com.apple.laf.AquaImageFactory$SystemColorProxy[r=42,g=91,b=213],selectionForeground=com.apple.laf.AquaImageFactory$SystemColorProxy[r=255,g=255,b=255],visibleRowCount=8,layoutOrientation=0]' with class 'javax.swing.JList' to class 'com.myproject.MyList'
{code}

Attempts to access the custom methods of my list class without the type casting also produce an error.  Anyone know what's wrong with this?  Thanks in advance. ",aalmiray,aalmiray,Major,Closed,Fixed,28/Nov/12 03:13,22/Dec/12 01:10
Bug,GROOVY-5823,12818457,"wasted work in ""GenericsType.GenericsTypeMatcher.compareGenericsWithBound""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch (match.diff) that fixes it.

In method ""GenericsType.GenericsTypeMatcher.compareGenericsWithBound"",
the loop over ""redirectBoundType.upperBounds"" should break immediately
after ""match"" is set to ""false"".  All the iterations after ""match"" is
set to ""false"" do not perform any useful work, at best they just set
""match"" again to ""false"".

Similarly, in the same method
""GenericsType.GenericsTypeMatcher.compareGenericsWithBound"", the loop
over ""interfaces"" should break immediately after ""success"" is set to
""false"".  I attached a separate patch (interfaces.diff) for this loop.",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 09:00,05/Apr/15 14:44
Bug,GROOVY-5824,12818439,"wasted work in ""ClassCompletionVerifier.checkOverloadingPrivateAndPublic""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch (patch.diff) that fixes it.

In method ""ClassCompletionVerifier.checkOverloadingPrivateAndPublic"",
the loop over ""currentClass.getMethods(...)"" should break immediately
after both ""hasPrivate"" and ""hasPublic"" are ""true"".  All the
iterations after ""hasPrivate"" and ""hasPublic"" are ""true"" do not
perform any useful work, at best they just set ""hasPrivate"" and
""hasPublic"" again to ""true"".

Method ""addMethods"" in class ""EnumVisitor"" has a similar loop (over
""methods""), and this loop breaks immediately after both ""hasNext"" and
""hasPrevious"" are set to ""true"", just like in the proposed patch.

Similarly, method ""rootLoader"" in class ""GroovyStarter"" has a loop
that breaks when all three ""hadMain"" and ""hadConf"" and ""hadCP"" are set
to ""true"".",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 09:25,05/Apr/15 14:44
Bug,GROOVY-5825,12818453,"wasted work in ""ClassCompletionVerifier.addErrorIfParamsAndReturnTypeEqual""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch (patch.diff) that fixes it.

In method
""ClassCompletionVerifier.addErrorIfParamsAndReturnTypeEqual"", the loop
over ""p1"" and ""p2"" should break immediately after ""isEqual"" becomes
""false"".  All the iterations after ""isEqual"" is ""false"" do not perform
any useful work.",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 09:50,05/Apr/15 14:44
Bug,GROOVY-5826,12811700,"wasted work in ""DomToGroovy.mixedContent""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch (patch.diff) that fixes it.

In method ""DomToGroovy.mixedContent"", the loop over ""list"" should
break immediately after ""hasText"" and ""hasElement"" are ""true"".  All
the iterations after ""hasText"" and ""hasElement"" are ""true"" do not
perform any useful work, at best they just set ""hasText"" and
""hasElement"" again to ""true"".

Method ""addMethods"" in class ""EnumVisitor"" has a similar loop (over
""methods""), and this loop breaks immediately after both ""hasNext"" and
""hasPrevious"" are set to ""true"", just like in the proposed patch.

Similarly, method ""rootLoader"" in class ""GroovyStarter"" has a loop
that breaks when all three ""hadMain"" and ""hadConf"" and ""hadCP"" are set
to ""true"", like in the proposed patch.",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 10:08,05/Apr/15 14:43
Bug,GROOVY-5827,12818448,"wasted work in ""StaticTypeCheckingVisitor.areCategoryMethodCalls""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch that fixes it.

In method ""StaticTypeCheckingVisitor.areCategoryMethodCalls"", the loop
over ""foundMethods"" should break immediately after ""category"" is set
to ""false"".  All the iterations after ""category"" is set to ""false"" do
not perform any useful work, at best they just set ""category"" again to
""false"".",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 10:28,05/Apr/15 14:43
Bug,GROOVY-5828,12818201,No way to tell @Immutable that you're using an Immutable List structure already,"Given this class:

{code}
@Grab( 'org.clojure:clojure:1.4.0' )
import clojure.lang.PersistentList
import clojure.lang.PersistentVector

//@groovy.transform.Immutable(knownImmutableClasses=[List])
class Broken {
  List list
  
  Broken( List list ) {
    this.list = list
  }
  
  Broken cons( Object v ) {
    new Broken( list.cons( v ) )
  }
  
  String toString() {
    ""$list""
  }
}

def a = new Broken( PersistentList.EMPTY.cons( 'tim' ) )
println a.cons( 'yates' )

def b = new Broken( PersistentVector.EMPTY.cons( 'tim' ) )
println b.cons( 'yates' )
{code}

We get the output:

{code}
[yates, tim]
[tim, yates]
{code}

However, if we annotate the class as Immutable (and get rid of the constructor and the toString as would be required)

{code}
@Grab( 'org.clojure:clojure:1.4.0' )
import clojure.lang.PersistentList
import clojure.lang.PersistentVector

@groovy.transform.Immutable
class Broken {
  List list
  
  Broken cons( Object v ) {
    new Broken( list.cons( v ) )
  }
}

def a = new Broken( PersistentList.EMPTY.cons( 'tim' ) )
println a.cons( 'yates' )

def b = new Broken( PersistentVector.EMPTY.cons( 'tim' ) )
println b.cons( 'yates' )
{code}

We now get the output:

{code}
Caught: groovy.lang.MissingMethodException: No signature of method: java.util.Collections$UnmodifiableList.cons() is applicable for argument types: (java.lang.String) values: [yates]
Possible solutions: join(java.lang.String), count(groovy.lang.Closure), count(java.lang.Object), sort(), find(), any()
groovy.lang.MissingMethodException: No signature of method: java.util.Collections$UnmodifiableList.cons() is applicable for argument types: (java.lang.String) values: [yates]
Possible solutions: join(java.lang.String), count(groovy.lang.Closure), count(java.lang.Object), sort(), find(), any()
	at Broken.cons(PList3.groovy:10)
	at Broken$cons.call(Unknown Source)
	at PList3.run(PList3.groovy:15)
{code}

So the transformation has changed our List field into a UnmodifiableList, and I can no longer call the PersistentVector or PersistentList methods on it.

Changing the transformation to:

{code}
@groovy.transform.Immutable(knownImmutableClasses=[List])
{code}

Makes no change :-(

Is there (or could there be added) a way of saying that you know that a given field is an Immutable type?

Cheers,

Tim
",paulk,tim_yates,Major,Closed,Fixed,29/Nov/12 10:35,22/Dec/12 01:10
Bug,GROOVY-5829,12818452,"wasted work in ""AnnotationVisitor.checkIfValidEnumConstsAreUsed""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch that fixes it.

In method ""AnnotationVisitor.checkIfValidEnumConstsAreUsed"", the loop
over ""attributes.entrySet()"" should break immediately after ""ok"" is
set to ""false"".  All the iterations after ""ok"" is set to ""false"" do
not perform any useful work, at best they just set ""ok"" again to
""false"".",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 10:41,05/Apr/15 14:43
Bug,GROOVY-5830,12818451,"wasted work in ""SecurityTestSupport.executeTest""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch that fixes it.

In method ""SecurityTestSupport.executeTest"", the loop over
""result.errors()"" should break immediately after ""otherFailure"" is set
to ""true"".  All the iterations after ""otherFailure"" is set to ""true""
do not perform any useful work, at best they just set ""otherFailure""
again to ""true"".",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 12:22,05/Apr/15 14:43
Bug,GROOVY-5831,12818454,"wasted work in ""GroovyDocToolTest.testPlainGroovyDocTool""","The problem appears in Groovy 2.0.5 and in revision 740ae7c..  I
attached a one-line patch that fixes it.

In method ""GroovyDocToolTest.testPlainGroovyDocTool"", the loop over
""clazz.methods()"" should break immediately after ""seenThisMethod"" is
set to ""true"".  All the iterations after ""seenThisMethod"" is set to
""true"" do not perform any useful work, at best they just set
""seenThisMethod"" again to ""true"".",blackdrag,adriannistor,Minor,Closed,Fixed,29/Nov/12 12:32,05/Apr/15 14:44
Bug,GROOVY-5834,12816624,@StaticCompile fails for regex literals,"Compiling the following code:

{code}
@CompileStatic
class Sample { def pattern = ~'foo|bar' }
{code}

results in the compiler throwing the following exception:

{code}
Groovyc: java.lang.NullPointerException
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBitwiseNegationExpression(StaticTypeCheckingVisitor.java:1174)
	at org.codehaus.groovy.ast.expr.BitwiseNegationExpression.visit(BitwiseNegationExpression.java:37)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitField(ClassCodeVisitorSupport.java:129)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitField(StaticTypeCheckingVisitor.java:1055)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1048)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:222)
	at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitClass(StaticCompilationVisitor.java:108)
	at org.codehaus.groovy.transform.sc.StaticCompileTransformation.visit(StaticCompileTransformation.java:58)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:132)
	at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:176)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)
	at org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:44)
	at org.jetbrains.groovy.compiler.rt.GroovycRunner.main(GroovycRunner.java:129)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:112)
{code}",melix,ddimitrov,Major,Closed,Fixed,02/Dec/12 20:27,22/Dec/12 01:10
Bug,GROOVY-5835,12818202,Few @CompileStatic bugs,"There are multiple issues illustrated by the code below:
* compiler can not infer the type when iterating with each() over vararg argument. Iteration using for-in works fine. 
* When using explicit return from a closure inside the method, the compiler checks the method return type, rather than the closure's declared return type
* Certain types of overloading blow the compiler's stack - compare transform(mappers) with filter(predicates)

{code}
import groovy.transform.CompileStatic

public class Test<T> {

    Test<T> transform(boolean passThroughNulls=true, Closure<T> mapper) { this }

    /*@CompileStatic*/  // blows the compiler stack
    Test<T> transform(boolean passThroughNulls=true, Closure<T>... mappers) {
        mappers.each { Closure<T> it -> transform passThroughNulls, it }
        return this
    }


    @CompileStatic
    Test<T> filter(Closure<Boolean> predicate) {
        transform { T it -> 
            def retval = predicate(it) ? it : null
            /*return*/ retval // compiler thinks 'return' is for the enclosing method
        }  
        return this
    }

    @CompileStatic
    Test<T> filter(Closure<Boolean>... predicates) {
        predicates.each { Closure<Boolean> it -> // compiler can not infer the type of 'it' even we are iterating over plain array 
            filter it 
        }
        return this
    }
}
{code}",melix,ddimitrov,Major,Closed,Fixed,03/Dec/12 20:27,22/Dec/12 01:10
Bug,GROOVY-5836,12816556,@CompileStatic can not resolve function,"This fails:
{code}
import groovy.transform.CompileStatic

@CompileStatic class Test<T> {
    void transform(boolean passThroughNulls, Closure<T> mapper) {}
    void transformAll(boolean passThroughNulls, Closure<T>... mappers) {
        for (m in mappers) {
            transform passThroughNulls, m
        }
    }
}
{code}

This passes:
{code}
import groovy.transform.CompileStatic

@CompileStatic class Test<T> {
    void transform(boolean passThroughNulls, Closure mapper) {}
    void transformAll(boolean passThroughNulls, Closure... mappers) {
        for (m in mappers) {
            transform passThroughNulls, m
        }
    }
}
{code}

The only difference is whether we declare the generic parameter of the closure type.",melix,ddimitrov,Major,Closed,Fixed,03/Dec/12 20:42,22/Dec/12 01:10
Bug,GROOVY-5837,12812009,Griffon bind using parentheses fails silently or with cryptic message,"If a Griffon user accidentally replaces curly braces {} with parentheses () in a call to bind, the error either fails silently at runtime or with a cryptic compiler error (depending on the widget type).

In the sample code (attached), the first textfield and button work correctly.  The 2nd textfield (error #1) fails silently (it is blank, no compiler warnings). If uncommented, the 2nd button (error #2) causes a cryptic runtime exception (java.lang.NullPointerException  -  see full stacktrace attached).

Sample Griffon code for project ""bug"":
{code}
-- BugView.groovy ----------------------------------------------
package bug

def makeText = { phrase ->
  textField( columns:10,    text: bind { phrase.insect  } )  // correct
  button(    ""Larvae"",   enabled: bind { phrase.enabled } )  // correct
  textField( columns:10,    text: bind ( phrase.insect  ) )  // error #1 - silent failure
  // button(    ""Larvae"",   enabled: bind ( phrase.enabled ) )  // error #2 - compiler error
}

def val = 1
application( title: 'bug', preferredSize: [320, 240], pack: true, location: [50,50],
             iconImage: imageIcon('/griffon-icon-48x48.png').image
           ) {
  borderLayout()
  panel( constraints: CENTER, border: titledBorder(title: 'Bug Test')) {
    widget( makeText( model.""data${val}"" ) )
  }
}

-- BugModel.groovy ----------------------------------------------
package bug
import groovy.beans.Bindable

@Bindable 
class BugModel {
  Map data1 = [ insect:""Buggy Text"", buggyFlag: true ] as ObservableMap
}
{code}",aalmiray,thompson2526,Minor,Closed,Fixed,30/Nov/12 15:25,22/Dec/12 01:10
Bug,GROOVY-5838,12816557,groovydoc not working (ClassNotFoundException),"I am not the alone with this problem on IRC#grails.

Getting error when I execute:
# groovydoc

java.lang.ClassNotFoundException: org.codehaus.groovy.tools.groovydoc.Main
	at org.codehaus.groovy.tools.RootLoader.findClass(RootLoader.java:175)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at org.codehaus.groovy.tools.RootLoader.loadClass(RootLoader.java:147)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:96)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:128)
",paulk,k2s,Major,Closed,Fixed,04/Dec/12 17:05,22/Dec/12 01:10
Bug,GROOVY-5839,12816647,Typechecker does not detect shadowed generic parameter,"The typechecker does not realize that the <T> of the method shadows the <T> of the class and compiles this code without warning. The same code does not typecheck in Java.

{code}
public class GoodCodeRed<T> {
    Collection<GoodCodeRed<T>> attached

    @groovy.transform.CompileStatic
    public <T> void attach(GoodCodeRed<T> toAttach) {
        attached.add(toAttach)
    }
}
{code}

See also http://youtrack.jetbrains.com/issue/IDEA-96606",blackdrag,ddimitrov,Major,Closed,Fixed,05/Dec/12 08:32,23/Dec/15 10:10
Bug,GROOVY-5841,12812010,Runtime error with nested classes that have parameterized type,"Running the code below created a runtime error:
{code}
@CompileStatic
public class InnerType<T> {
     public T value = """"
}

@CompileStatic
public class MyType<T> {

     public InnerType<T> inner = new InnerType<>()

     @CompileStatic
     public static main(args)
     {
         MyType<String> type = new MyType<>()
         // type.inner.value = ""123""
         // println(type.inner.value)
     }
}
{code}

Runtime error:
{noformat}
Groovyc: java.lang.ArrayIndexOutOfBoundsException: 0
     at 
org.codehaus.groovy.ast.GenericsType$GenericsTypeMatcher.compareGenericsWithBound(GenericsType.java:344)
     at 
org.codehaus.groovy.ast.GenericsType$GenericsTypeMatcher.checkGenerics(GenericsType.java:273)
     at 
org.codehaus.groovy.ast.GenericsType$GenericsTypeMatcher.matches(GenericsType.java:244)
     at 
org.codehaus.groovy.ast.GenericsType.isCompatibleWith(GenericsType.java:173)
     at 
org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.typeCheckAssignment(StaticTypeCheckingVisitor.java:708)
     at 
org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitField(StaticTypeCheckingVisitor.java:1065)
     at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1048)
     at 
org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
     at 
org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:222)
     at 
org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitClass(StaticCompilationVisitor.java:108)
     at 
org.codehaus.groovy.transform.sc.StaticCompileTransformation.visit(StaticCompileTransformation.java:58)
     at 
org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:132)
     at 
org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:176)
     at 
org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)
     at 
org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)
     at 
org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)
     at 
org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)
     at 
org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:44)
     at 
org.jetbrains.groovy.compiler.rt.GroovycRunner.main(GroovycRunner.java:129)
     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
     at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
     at java.lang.reflect.Method.invoke(Method.java:601)
     at 
com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:112)

{noformat}",melix,oliver_plow,Major,Closed,Fixed,07/Dec/12 03:46,17/Dec/14 13:25
Bug,GROOVY-5842,12818210,Runtimer error when types down match with Map and Map.Entry when using @CompileStatic,"When running the follow code a runtime error occurs:
{code}
@CompileStatic
class Scratch
{

    @CompileStatic
    public static void main(String[] args)
    {
        def Map<String, Set<String>> map = new HashMap<>()
        def set = new HashSet<String>()
        set.add(""foo"")
        map.put(""bar"", set)

        map.each {
            println(it.getKey()) // Groovyc: [Static type checking] - Cannot find matching method java.lang.Object#getKey().
            def List<String> list = it.getValue() // Groovyc: [Static type checking] - Cannot find matching method java.lang.Object#getKey().
        }

        // compiles fine and created runtime error
        // Cannot cast object '[foo]' with class 'java.util.HashSet' to class 'java.util.List' due to: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: java.util.List(java.lang.String)
        map.each { Map.Entry<String, List<String>> entry ->
            println(entry.getKey())
            def List<String> list = entry.getValue()
        }
    }
}
{code}

Reason is that the type parameters for Map<String, Set<String>> and Map.Entry<String, List<String>> don't match, but the compiler doesn't reject it, although @CompileStatic is defined",melix,oliver_plow,Major,Closed,Fixed,07/Dec/12 03:57,05/Apr/14 00:00
Bug,GROOVY-5844,12816574,Copy/paste error in EmptyBorderFactory,"{code}
if ((top == null) || (top == null) || (top == null) || (top == null) || attributes) {
{code}
should be
{code}
if ((top == null) || (left == null) || (bottom == null) || (right == null) || attributes) {
{code}
",paulk,burtbeckwith,Minor,Closed,Fixed,07/Dec/12 22:46,05/Apr/15 14:44
Bug,GROOVY-5846,12812013,stackoverflow with compilestatic,"Reported to groovy-eclipse as https://jira.codehaus.org/browse/GRECLIPSE-1514. This code:

{code}
@CompileStatic
class C {
  def xxx(List list) {
    list.unique().each { }
  }
}
{code}

will stackoverflow

{code}
java.lang.StackOverflowError
	at java.util.HashMap.getEntry(HashMap.java:344)
	at java.util.HashMap.containsKey(HashMap.java:335)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolve(StaticTypeCheckingVisitor.java:2834)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolveType(StaticTypeCheckingVisitor.java:2862)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolve(StaticTypeCheckingVisitor.java:2838)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolveType(StaticTypeCheckingVisitor.java:2862)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolve(StaticTypeCheckingVisitor.java:2838)
	at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.fullyResolveType(StaticTypeCheckingVisitor.java:2862)
...
{code}",melix,aclement,Minor,Closed,Fixed,14/Dec/12 16:14,22/Dec/12 01:10
Bug,GROOVY-5849,12811718,"wasted work in ""ClassCompletionVerifier.checkMethodForWeakerAccessPrivileges""","The problem appears in version 1.8.4 and in revision 4cf5263..  I
attached a one-line patch (patch.diff) that fixes it.  I will also
post (in the ""Comments"" section) the URL for the github pull request.

The entire work performed in method
""ClassCompletionVerifier.checkMethodForWeakerAccessPrivileges""
produces no result when the parameter ""MethodNode mn"" represents a
public method, i.e., when ""mn.isPublic()"" is ""true"".  This condition
can be evaluated right at the beginning of the
""checkMethodForWeakerAccessPrivileges"" method body, thus avoiding the
entire method execution.

We can have this fast exist because the method
""checkMethodForWeakerAccessPrivileges"" produces results only when this
condition (in the method code):

{code:java|borderStyle=solid}
if ((mn.isPrivate() && !superMethod.isPrivate()) ||
        (mn.isProtected() && superMethod.isPublic()))
{code} 

evaluates to ""true"", which does not happen when ""mn.isPublic()"" is
""true"" (i.e., when ""!mn.isPrivate() && !mn.isProtected()"" is ""true"").

I attached a second one-line patch (patchTwoCond.diff) where I use the
condition ""!mn.isPrivate() && !mn.isProtected()"" instead of
""mn.isPublic()"", in case you prefer the longer condition.",blackdrag,adriannistor,Minor,Closed,Fixed,17/Dec/12 15:51,05/Apr/15 14:43
Bug,GROOVY-5850,12811737,"wasted work in ""StaticTypeCheckingVisitor.inferReturnTypeGenerics""","The problem appears in version 2.0.5 and in revision 4cf5263..  I
attached a one-line patch that fixes it.  I will also post (in the
""Comments"" section) the URL for the github pull request.  

There is no Groovy test that touches this code location (the entire
""else"" branch that contains this code is not touched), but I think the
patch is correct.

In method ""StaticTypeCheckingVisitor.inferReturnTypeGenerics"", the
loop over ""interfaces"" should break immediately after ""actualType"" is
set.  All the iterations after ""actualType"" is set do not perform any
useful work.",pschumacher,adriannistor,Minor,Closed,Fixed,17/Dec/12 17:39,05/Apr/15 14:44
Bug,GROOVY-5851,12818455,"wasted work in ""SimpleGroovyClassDocAssembler.extractName""","The problem appears in version 2.0.5 and in revision 4cf5263..  I
attached a three-line patch (patch.diff) that fixes it.  I will also
post (in the ""Comments"" section) the URL for the github pull request.

In method ""SimpleGroovyClassDocAssembler.extractName"", the loop over
""importedClassesAndPackages"" keeps overriding ""typeName"" with ""name"".
Therefore, only the last written value is visible out of the loop and
all the other writes and iterations are not necessary.  The patch
iterates from the end of ""importedClassesAndPackages"" and breaks the
first time when ""typeName"" is set.

The above fix (in patch.diff) is certainly correct (it's easy to see
through code inspection), but I think we can have an even shorter
patch (one line, in patchShort.diff), described below.  There is no
Groovy test that touches this code location, so I am not 100% sure
this second patch (patchShort.diff) is correct, though it should be.

patchShort.diff is based on the fact that the condition 
""if (name.endsWith(slashName))"" (which decides if ""typeName"" is set or
not) is true at most once in the loop over
""importedClassesAndPackages"".  Even if it is true more than one time,
the ""name"" value is still legal.  Thus, the loop can just break
immediately after ""typeName"" is set.",paulk,adriannistor,Minor,Closed,Fixed,17/Dec/12 18:43,07/Apr/15 19:13
Bug,GROOVY-5853,12811742,"wasted work in ""DefaultGrailsDomainClassInjector.implementsMethod""","The problem appears in version 2.0.5 and in revision 4cf5263..  I
attached a one-line patch that fixes it.  I will also post (in the
""Comments"" section) the URL for the github pull request.

In method ""DefaultGrailsDomainClassInjector.implementsMethod"", in the
loop over ""methods"", the statement
""boolean methodMatch = mn.getName().equals(methodName) && isZeroArg""
should be
""boolean methodMatch = isZeroArg && mn.getName().equals(methodName)"".
The second statement avoids comparing strings when ""isZeroArg"" is
""false"".

The code can be made even shorter and faster by having a fast exit (by
pulling ""isZeroArg"" out of the loop) at the beginning of the method
body; this avoids the entire method computation.  While the resulting
code is more compact than the original, the patch itself is a bit
larger (deletes 3 lines, adds 2).  I attached this patch
(patchFull.diff).",blackdrag,adriannistor,Minor,Closed,Fixed,18/Dec/12 12:00,05/Apr/15 14:44
Bug,GROOVY-5854,12811685,"wasted work in the ""ClassNode"" constructor","The problem appears in version 2.0.5 and in revision 4cf5263..  This
problem is similar to the previously fixed GROOVY-5803, GROOVY-5823,
GROOVY-5825, and GROOVY-5827.  I attached a one-line patch that fixes
it.  I will also post (in the ""Comments"" section) the URL for the
github pull request.

In the ""ClassNode"" constructor (the constructor with 5 parameters),
the loop over ""interfaces"" should break immediately after
""usesGenerics"" is set to ""true"". All the iterations after
""usesGenerics"" is set to ""true"" do not perform any useful work, at
best they just set ""usesGenerics"" again to ""true"".",blackdrag,adriannistor,Minor,Closed,Fixed,18/Dec/12 12:33,05/Apr/15 14:43
Bug,GROOVY-5856,12818208,Annotation causes @CompileStatic to fail,"If I add @CompileStatic to a class that already has an @Entity gaelyk annotation. I get a compilation error. 
{noformat}
:compileGroovy
startup failed:
General error during class generation: size==0

java.lang.ArrayIndexOutOfBoundsException: size==0
	at org.codehaus.groovy.classgen.asm.OperandStack.doConvertAndCast(OperandStack.java:312)
	at org.codehaus.groovy.classgen.asm.OperandStack.doGroovyCast(OperandStack.java:296)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:592)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:505)
	at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)
	at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)


the EntityTransformation#visit(nodes, source) method runs well but StaticCompileTransformation#visit(nodes, source) fails
{noformat}",melix,sdmurphy,Blocker,Closed,Fixed,18/Dec/12 23:04,22/Dec/12 01:10
Bug,GROOVY-5858,12816682,"""string"".count('') fails","String.count() fails when passed an empty string. 

Using 2.0.2 at the moment so haven't tried with other versions.",paulk,james.frost,Minor,Closed,Fixed,19/Dec/12 08:14,18/Jan/13 16:06
Bug,GROOVY-5859,12812020,Invalid generic cast generated in super constructor call in a Java stub,"For this Groovy class:
{code}
class TaggedsMap extends TreeMap {

    TaggedsMap() { super() }

    TaggedsMap(Comparator comparator) { super(comparator)}

    TaggedsMap(Map m) {
        super()
        putAll( m)
    }

    TaggedsMap(SortedMap m) {
        super()
        putAll (m)
    }
}
{code}
The following stub is generated:
{code}
public class TaggedsMap
  extends java.util.TreeMap  implements
    groovy.lang.GroovyObject {
public TaggedsMap
() {
super ((java.util.SortedMap<K, java.lang.Object extends V>)null);
}
public TaggedsMap
(java.util.Comparator comparator) {
super ((java.util.SortedMap<K, java.lang.Object extends V>)null);
}
public TaggedsMap
(java.util.Map m) {
super ((java.util.SortedMap<K, java.lang.Object extends V>)null);
}
public TaggedsMap
(java.util.SortedMap m) {
super ((java.util.SortedMap<K, java.lang.Object extends V>)null);
}
}
{code}
which isn't a valid Java code.

Please refer to http://youtrack.jetbrains.com/issue/IDEA-97668 for the full project to reproduce.",melix,gromopetr,Major,Closed,Fixed,19/Dec/12 09:13,15/Oct/15 18:19
Bug,GROOVY-5861,12816613,Incorrect Error: type ? is not a valid substitute,"I marked this as in 2.0.5 but it also happens in my almost 2.0.6 build too. This code:

{code}
interface Field<T extends java.io.Serializable> {
}
class StructureBase {
    public Field<?> get(Object arg0) {
        return null;
    }
}
{code}

produces this error:

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Foo.groovy: 4: The type ? is not a valid substitute for the bounded parameter <T extends java.io.Serializable>
 @ line 4, column 9.
   	public Field<?> get(Object arg0) {
           ^

1 error
{code}

If the code is put into a .java file and compiled by javac it is fine.",,aclement,Major,Closed,Fixed,20/Dec/12 16:46,11/Nov/20 02:00
Bug,GROOVY-5863,12816634,ImportCustomizer for CompilerCustomizationBuilder is not added to configscript CompilerConfiguration ,"whenever the -configscript option is given, the referenced script is evaluated using a GroovyShell. 

The code in FileSystemCompiler creates an ImportCustomizer for CompilerCustomizationBuilder, but that customizer is not added to the CompilerConfiguration of the GroovyShell instance.",blackdrag,andre.steingress,Minor,Closed,Fixed,21/Dec/12 15:09,12/Jan/13 20:40
Bug,GROOVY-5864,12812046,@AutoExternalize doesn't create no-argument constructor,"@AutoExternalize doesn't create a no-argument constructor, which is required to use Externalizable.

{noformat}
>javap -classpath .;c:\Users\Johann\.m2\repository\org\codehaus\groovy\groovy\2.0.5\groovy-2.0.5.jar xyz.Stream | grep Stream
Compiled from ""Stream.groovy""
public final class xyz.Stream implements java.io.Externalizable,groovy.lang.GroovyObject {
  public xyz.Stream(java.util.HashMap);
  public xyz.Stream(java.lang.String, java.lang.String, java.lang.String, java.lang.String, java.lang.Int
eger, java.lang.Integer, java.lang.Long);
>
{noformat}

Consequently, attempts at deserializing fail:

{noformat}
classOrInstanceShouldBeSerializable[2](xyz.SerializationTest)  Time elapsed: 0.016 sec  <<< ERROR!
org.apache.commons.lang.SerializationException: java.io.InvalidClassException: xyz.Stream; no valid consuctor
        at org.apache.commons.lang.SerializationUtils.deserialize(SerializationUtils.java:168)
...
Caused by: java.io.InvalidClassException: xyz.Stream; no valid constructor
        at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:147)
        at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:755)
...
{noformat}",paulk,johann,Minor,Closed,Fixed,22/Dec/12 07:42,05/Apr/14 00:00
Bug,GROOVY-5865,12816685,getAt(EmptyRange) not called when passing an EmptyRange to getAt(Collection),"Basically, this is an inconsistency of List#getAt(Collection). When its parameters are integer indexes and non-empty ranges, it works as expected. But when an EmptyRange is passed to it, it doesn't behave like List#getAt(EmptyRange):

{code:groovy}
def list = [1, 2, 3]
assert list[3..<3] == []        // This works
assert list[0, 2..<3] == [1, 3] // This works too
assert list[0, 3..<3] == [1]    // But this throws IndexOutOfBoundsException: toIndex = 4
{code}",paulk,epidemian,Major,Closed,Fixed,24/Dec/12 04:36,18/Jan/13 16:06
Bug,GROOVY-5867,12816691,xml:lang attribute appears to break StreamingMarkupBuilder >= 2.0.6,"When using XmlSlurper to parse XML containing an xml:lang attribute and trying to write out this XML using StreamingMarkupBuilder, I get this error in 2.0.6 and 2.1.0-beta-1, which I didn't get in 2.0.5:
{noformat}
[Fatal Error] :1:19: Attribute name ""xml:"" associated with an element type ""tag0:grammar"" must be followed by the ' = ' character.
ERROR:  'Attribute name ""xml:"" associated with an element type ""tag0:grammar"" must be followed by the ' = ' character.'
Caught: groovy.lang.GroovyRuntimeException: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 19; Attribute name ""xml:"" associated with an element type ""tag0:grammar"" must be followed by the ' = ' character.
groovy.lang.GroovyRuntimeException: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 19; Attribute name ""xml:"" associated with an element type ""tag0:grammar"" must be followed by the ' = ' character.
	at broken-groovy-test$_run_closure2.doCall(broken-groovy-test.groovy:8)
	at broken-groovy-test.run(broken-groovy-test.groovy:8)
{noformat}

The groovy code in question is:

{code}
import groovy.xml.StreamingMarkupBuilder;
import groovy.xml.XmlUtil;

def grammar = new XmlSlurper().parse(new File(args[0]))
def builder = new StreamingMarkupBuilder()
def copier = builder.bind { mkp.yield(grammar) };
processedFile = new File(""test-out.xml"")
processedFile.withWriter{ it << XmlUtil.serialize(copier) } 
{code}

And the XML that makes it fail is:

{code:xml}
<?xml version=""1.0"" encoding=""UTF-8""?>
<grammar xml:lang=""en-us"" xmlns=""http://www.w3.org/2001/06/grammar"">
</grammar>
{code}
",paulk,dhdaines,Major,Closed,Fixed,28/Dec/12 17:10,12/Jan/13 20:40
Bug,GROOVY-5868,12816657,ConfigObject is unexpectedly no longer Cloneable,"The reimplementation of ConfigObject is no longer Cloneable. So if your code relied on being able to clone a ConfigObject, it now throws an exception.

Sample program which used to work and doesn't anymore:
{code}
ConfigSlurper slurper = new ConfigSlurper();
ConfigObject conf = slurper.parse(""""""
foo {
   bar = ""baz""
}
"""""");
println conf.foo.bar
ConfigObject conf2 = conf.clone()
conf2.foo.bar = ""quux""
println conf2.foo.bar
{code}
",pschumacher,dhdaines,Major,Closed,Fixed,28/Dec/12 20:50,15/Oct/15 18:19
Bug,CASSANDRA-3687,12536810,Local range scans are not run on the read stage,"Running directly on the client request/StorageProxy thread means we're now allowing one range scan per thrift thread instead of one per read stage thread [which may be more, or less, depending on thrift server mode], and it bypasses the ""drop hopeless requests"" overcapacity protection built in there. ",jbellis,jbellis,Low,Resolved,Fixed,02/Jan/12 14:53,16/Apr/19 09:32
Bug,CASSANDRA-3690,12536930,Streaming CommitLog backup,"Problems with the current SST backups
1) The current backup doesn't allow us to restore point in time (within a SST)
2) Current SST implementation needs the backup to read from the filesystem and hence additional IO during the normal operational Disks
3) in 1.0 we have removed the flush interval and size when the flush will be triggered per CF, 
          For some use cases where there is less writes it becomes increasingly difficult to time it right.
4) Use cases which needs BI which are external (Non cassandra), needs the data in regular intervals than waiting for longer or unpredictable intervals.

Disadvantages of the new solution
1) Over head in processing the mutations during the recover phase.
2) More complicated solution than just copying the file to the archive.

Additional advantages:
Online and offline restore.
Close to live incremental backup.

Note: If the listener agent gets restarted, it is the agents responsibility to Stream the files missed or incomplete.

There are 3 Options in the initial implementation:
1) Backup -> Once a socket is connected we will switch the commit log and send new updates via the socket.
2) Stream -> will take the absolute path of the file and will read the file and send the updates via the socket.
3) Restore -> this will get the serialized bytes and apply's the mutation.

Side NOTE: (Not related to this patch as such) The agent which will take incremental backup is planned to be open sourced soon (Name: Priam).",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,03/Jan/12 00:39,16/Apr/19 09:32
Bug,CASSANDRA-3691,12537079,LeveledCompactionStrategy is broken because of generation pre-allocation in LeveledManifest.,"LeveledManifest constructor has the following code:

{code}
for (int i = 0; i < generations.length; i++)
{
    generations[i] = new ArrayList<SSTableReader>();
    lastCompactedKeys[i] = new DecoratedKey(cfs.partitioner.getMinimumToken(), null);
}
{code}

But in the DecoratedKey constructor we have:

{code}
assert token != null && key != null && key.remaining() > 0;
{code}

so when you tried to create a CF with LeveledCompressionStrategy that will result in 

{noformat}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:865)
	at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:953)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace.process(Cassandra.java:4103)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3078)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:857)
	... 7 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:770)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:209)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:300)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:281)
	at org.apache.cassandra.db.Table.initCf(Table.java:339)
	at org.apache.cassandra.db.Table.<init>(Table.java:288)
	at org.apache.cassandra.db.Table.open(Table.java:117)
	at org.apache.cassandra.db.migration.AddKeyspace.applyModels(AddKeyspace.java:72)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
	at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:850)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:752)
	... 14 more
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
	at org.apache.cassandra.db.compaction.LeveledManifest.<init>(LeveledManifest.java:79)
	at org.apache.cassandra.db.compaction.LeveledManifest.create(LeveledManifest.java:85)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.<init>(LeveledCompactionStrategy.java:74)
	... 19 more
ERROR 19:52:44,029 Fatal exception in thread Thread[MigrationStage:1,5,main]
{noformat}",slebresne,xedin,Normal,Resolved,Fixed,03/Jan/12 17:01,16/Apr/19 09:32
Bug,CASSANDRA-3693,12537183,strange values of pending tasks with compactionstats (below 0),"during scrub:

Every 2.0s: for i in 1 2 3; do nodetool -h 192.168.2.$i compactionstats; done                                                                                                                                     Wed Jan  4 13:48:13 2012

pending tasks: 2147483646
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     28034971475     72393139120    38.73%
pending tasks: -2147483647
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     24575687282     72385305067    33.95%
pending tasks: 0

",jbellis,zenek_kraweznik0,Low,Resolved,Fixed,04/Jan/12 12:53,16/Apr/19 09:32
Bug,CASSANDRA-3694,12537198,ClassCastException during hinted handoff,"{noformat}
ERROR 08:51:00,200 Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.ClassCastException: org.apache.cassandra.dht.BigIntegerToken cannot be cast to org.apache.cassandra.db.RowPosition
        at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1286)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1356)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:351)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",slebresne,brandon.williams,Normal,Resolved,Fixed,04/Jan/12 15:15,16/Apr/19 09:32
Bug,CASSANDRA-3696,12537228,Adding another datacenter's node results in 0 rows returned on first datacenter,"On Cassandra-1.0.5:
1. Create a node in C* with a fresh installation and create a keyspace on that node with one column family -

CREATE KEYSPACE test 
WITH placement_strategy = 'SimpleStrategy' 
and strategy_options={replication_factor:1};

use test; 
create column family cf1;

2. Insert values into cf1 -

set cf1[ascii('k')][ascii('c')] = ascii('v');

get cf1[ascii('k')]; 
=> (column=63, value=v, timestamp=1325689630397000) 
Returned 1 results.

3. update the strategy options from simple to networktopology with {Cassandra:1, Backup:1} 
4. read from cf1 to make sure the options change doesn't affect anything -

consistencylevel as LOCAL_QUORUM; 
get cf1[ascii('k')]; 
=> (column=63, value=v, timestamp=1325689630397000) 
Returned 1 results.

5. start a second node in the Backup datacenter 
6. read from cf1 again (on the first node) -

consistencylevel as LOCAL_QUORUM; 
get cf1[ascii('k')]; 
Returned 0 results.

After about 60 seconds, ""get cf1[ascii('k')]"" started to return results again. 

Also, when running at a CL of ONE on 1.0's head, we were able to see issues as well.

But, if more than one node was added to the second datacenter, then replication_strategy is changed, it seems okay.",jbellis,j.casares,Normal,Resolved,Fixed,04/Jan/12 19:02,16/Apr/19 09:32
Bug,CASSANDRA-3700,12537365,SelectStatement start/end key are not set correctly when a key alias is involved,"start/end key are set by antlr in WhereClause, but this depends on the ""KEY"" keyword.",xedin,jbellis,Normal,Resolved,Fixed,05/Jan/12 17:32,16/Apr/19 09:32
Bug,CASSANDRA-3711,12537700,Unsustainable Thread Accumulation in ParallelCompactionIterable.Reducer ThreadPoolExecutor,"With multithreaded compaction enabled, it looks like Reducer creates a new thread pool for every compaction.  These pools seem to just sit around - i.e. ""executor.shutdown()"" never gets called and the Threads live forever waiting for tasks that will never come.  For instance...


Name: CompactionReducer:1
State: TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@72938aea
Total blocked: 0  Total waited: 1

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1043)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1103)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
java.lang.Thread.run(Thread.java:722)
",jbellis,maedhroz,Low,Resolved,Fixed,08/Jan/12 23:46,16/Apr/19 09:32
Bug,CASSANDRA-3712,12537765,Can't cleanup after I moved a token.,"Before cleanup failed, I moved one node's token.
My cluster had 10GB data on 2 nodes. Data repartition was bad, tokens were 165[...] and 155[...].
I moved 155 to 075[...], then adjusted to 076[...]. The moves were correctly processed, with no exception.
But then, when I wanted to cleanup, it failed and keeps failing, on both nodes.

Other maintenance procedures like repair, compact or scrub work.
All the data is in the URLs CF.

Example session log:
nodetool cleanup fails:
$ ./nodetool --host cnode1 cleanup
Error occured during cleanup
java.util.concurrent.ExecutionException: java.lang.AssertionError
 at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
 at java.util.concurrent.FutureTask.get(FutureTask.java:83)
 at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
 at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:237)
 at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:958)
 at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1604)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
 at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
 at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
 at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
 at sun.rmi.transport.Transport$1.run(Transport.java:159)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError
 at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
 at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
 at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
 at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
 at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
 at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
 at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
 at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 ... 3 more


The server log looks like this:
 INFO [CompactionExecutor:260] 2012-01-09 14:08:41,716 CompactionManager.java (line 702) Cleaning up SSTableReader(path='/ke/cassandra/data/kev3/URLs-hc-457-Data.db')
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,220 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 156787206 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,226 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:47,236 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [pool-1-thread-1] 2012-01-09 14:08:51,003 Memtable.java (line 180) CFS(Keyspace='kev3', ColumnFamily='URLs.URLs_1_idx') liveRatio is 7.692510757866615 (just-counted was 4.512127842861816).  calculation took 8648ms for 97329 columns
 INFO [FlushWriter:23] 2012-01-09 14:08:54,360 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-143-Data.db (26375495 bytes)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:55,566 GCInspector.java (line 123) GC for ParNew: 206 ms for 1 collections, 934108624 used; max is 2034237440
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,289 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 188842513 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,297 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:57,297 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:57,619 GCInspector.java (line 123) GC for ParNew: 402 ms for 2 collections, 981893424 used; max is 2034237440
 INFO [FlushWriter:23] 2012-01-09 14:09:05,944 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-144-Data.db (31755390 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 174605041 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
 INFO [FlushWriter:23] 2012-01-09 14:09:06,447 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
ERROR [CompactionExecutor:260] 2012-01-09 14:09:06,448 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:260,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
	at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
	at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
	at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)




",yukim,hnicol,Normal,Resolved,Fixed,09/Jan/12 14:37,16/Apr/19 09:32
Bug,CASSANDRA-3714,12537832,Show Schema; inserts an extra comma in column_metadata,"create column family inode
  with column_type = 'Standard'
  and comparator = 'DynamicCompositeType(t=>org.apache.cassandra.db.marshal.TimeUUIDType,s=>org.apache.cassandra.db.marshal.UTF8Type,b=>org.apache.cassandra.db.marshal.BytesType)'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 1000000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 60
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and comment = 'Stores file meta data'
  and column_metadata = [
    {column_name : 'b@70617468',
    validation_class : BytesType,
    index_name : 'path',
    index_type : 0,
},
    {column_name : 'b@73656e74696e656c',
    validation_class : BytesType,
    index_name : 'sentinel',
    index_type : 0,
},
    {column_name : 'b@706172656e745f70617468',
    validation_class : BytesType,
    index_name : 'parent_path',
    index_type : 0,
}];

That's that was outputted when I ran show schema. When I tried it on a new cluster, it failed since the commas after 'index_type: 0' were present.

Proposed fixes:
1. Allow trailing commas
2. Do not output trailing commas",yukim,j.casares,Low,Resolved,Fixed,09/Jan/12 23:43,16/Apr/19 09:32
Bug,CASSANDRA-3718,12537933,cqlsh missing help for INSERT,this must have been overlooked.,thepaul,thepaul,Low,Resolved,Fixed,10/Jan/12 18:03,16/Apr/19 09:32
Bug,CASSANDRA-3726,12538024,cqlsh and cassandra-cli show keys differently for data created via stress tool,"{code}

// Run: stress --operation=INSERT --num-keys=5  --columns=2 --consistency-level=QUORUM --column-size=1 --threads=1 --replication-factor=1 --nodes=localhost

// cqlsh
cqlsh:Keyspace1> select * from Standard1;
 KEY,3 | C0,c | C1,c | 
 KEY,0 | 
 KEY,2 | C0,c | C1,c | 
 KEY,1 | C0,c | C1,c | 
 KEY,4 | C0,c | C1,c | 

cqlsh:Keyspace1> describe columnfamily Standard1;

CREATE COLUMNFAMILY Standard1 (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=ascii AND
  row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
  key_cache_size=200000.000000 AND
  row_cache_size=0.000000 AND
  read_repair_chance=1.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  row_cache_save_period_in_seconds=0 AND
  key_cache_save_period_in_seconds=14400 AND
  replicate_on_write=True;


// cassandra-cli
[default@Keyspace1] list Standard1;    
Using default limit of 100
-------------------
RowKey: 33
=> (column=C0, value=63, timestamp=1326259960705)
=> (column=C1, value=63, timestamp=1326259960705)
-------------------
RowKey: 30
-------------------
RowKey: 32
=> (column=C0, value=63, timestamp=1326259960704)
=> (column=C1, value=63, timestamp=1326259960704)
-------------------
RowKey: 31
=> (column=C0, value=63, timestamp=1326259960704)
=> (column=C1, value=63, timestamp=1326259960704)
-------------------
RowKey: 34
=> (column=C0, value=63, timestamp=1326259960705)
=> (column=C1, value=63, timestamp=1326259960705)

[default@Keyspace1] describe Standard1;
    ColumnFamily: Standard1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
{code}
",thepaul,cdaw,Low,Resolved,Fixed,11/Jan/12 07:39,16/Apr/19 09:32
Bug,CASSANDRA-3727,12538033,Fix unit tests failure,"On current 1.0 branch (and on my machine: Linux), I have the following unit test failures:
* CliTest and EmbeddedCassandraTest: they both first kind of pass (JUnit first prints a message with no failures in it), then hang until JUnit timeout and fails with a 'Timeout occurred'. In other word, the tests themselves are passing, but something they do prevents the process to exit cleanly leading to a JUnit timeout. I don't want to discard that as not a problem, because if something can make the process not exit cleanly, this can be a pain for restarts (and in particular upgrades) and hence would be basically a regression. I'm marking the ticket as blocker (for the release of 1.0.7) mostly because of this one.
* SystemTableTest: throws an assertionError. I haven't checked yet, so that could be an easy one to fix.
* RemoveTest: it fails, saying that '/127.0.0.1:7010 is in use by another process' (consistently). But I have no other process running on port 7010. It's likely just of problem of the test, but it's new and in the meantime removes are not tested.
* I also see a bunch of stack trace with errors like:
{noformat}
    [junit] ERROR 10:01:59,007 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Index.db to /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Index.db (errno 17)
{noformat}
(with SSTableReaderTest). This does not make the tests fail, but it is still worth investigating. It may be due to CASSANDRA-3101.",,slebresne,Urgent,Resolved,Fixed,11/Jan/12 09:39,16/Apr/19 09:32
Bug,CASSANDRA-3732,12538192,Update POM generation after migration to git,,stephenc,slebresne,Low,Resolved,Fixed,12/Jan/12 13:26,16/Apr/19 09:32
Bug,CASSANDRA-3733,12538223,"Once a host has been hinted to, log messages for it repeat every 10 mins even if no hints are delivered","{noformat}
 INFO 15:36:03,977 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:36:03,978 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 15:46:31,248 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:46:31,249 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 15:56:29,448 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:56:29,449 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 16:06:09,949 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 16:06:09,950 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 16:16:21,529 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 16:16:21,530 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
{noformat}

Introduced by CASSANDRA-3554.  The problem is that until a compaction on hints occurs, tombstones are present causing the isEmpty() check to be false.",brandon.williams,brandon.williams,Low,Resolved,Fixed,12/Jan/12 16:39,16/Apr/19 09:32
Bug,CASSANDRA-3735,12538275,"Fix ""Unable to create hard link"" SSTableReaderTest error messages","Sample failure (on Windows):

{noformat}
    [junit] java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\data\Keyspace1\backups\Standard1-hc-1-Index.db c:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\data\Keyspace1\Standard1-hc-1-Index.db,command error Code: 1, command output: Cannot create a file when that file already exists.
    [junit]
    [junit]     at org.apache.cassandra.utils.CLibrary.exec(CLibrary.java:213)
    [junit]     at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188)
    [junit]     at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)

    [junit]     at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 17:10:17,111 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,12/Jan/12 23:11,16/Apr/19 09:32
Bug,CASSANDRA-3736,12538279,-Dreplace_token leaves old node (IP) in the gossip with the token.,"https://issues.apache.org/jira/browse/CASSANDRA-957 introduce a -Dreplace_token,

however, the replaced IP keeps on showing up in the Gossiper when starting the replacement node:

{noformat}
 INFO [Thread-2] 2012-01-12 23:59:35,162 CassandraDaemon.java (line 213) Listening for thrift clients...
 INFO [GossipStage:1] 2012-01-12 23:59:35,173 Gossiper.java (line 836) Node /50.56.59.68 has restarted, now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,174 Gossiper.java (line 804) InetAddress /50.56.59.68 is now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,175 StorageService.java (line 988) Node /50.56.59.68 state jump to normal
 INFO [GossipStage:1] 2012-01-12 23:59:35,176 Gossiper.java (line 836) Node /50.56.58.55 has restarted, now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,176 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,177 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-12 23:59:45,048 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-13 00:00:06,062 Gossiper.java (line 632) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-01-13 00:01:06,320 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-13 00:01:06,320 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-13 00:01:06,321 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-13 00:01:16,106 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-13 00:01:37,121 Gossiper.java (line 632) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-01-13 00:02:37,352 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-13 00:02:37,353 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-13 00:02:37,353 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-13 00:02:47,158 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipStage:1] 2012-01-13 00:02:50,162 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipStage:1] 2012-01-13 00:02:50,163 StorageService.java (line 1156) Removing token 122029383590318827259508597176866581733 for /50.56.58.55
{noformat}

in the above, /50.56.58.55 was the replaced IP.

tried adding the ""Gossiper.instance.removeEndpoint(endpoint);"" in the StorageService.java where the message 'Nodes %s and %s have the same token %s.  Ignoring %s"",' seems only have fixed this temporary. Here is a ring output:

{noformat}
riptano@action-quick:~/work/cassandra$ ./bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
50.56.59.68     datacenter1 rack1       Up     Normal  6.67 KB         85.56%  60502102442797279294142560823234402248      
50.56.31.186    datacenter1 rack1       Up     Normal  11.12 KB        14.44%  85070591730234615865843651857942052864 
{noformat}

gossipinfo:
{noformat}
$ ./bin/nodetool -h localhost gossipinfo
/50.56.58.55
  LOAD:6835.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.58.55
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT
/50.56.59.68
  LOAD:6835.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,60502102442797279294142560823234402248
  RELEASE_VERSION:1.0.7-SNAPSHOT
action-quick2/50.56.31.186
  LOAD:11387.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT
{noformat}

Note that at 1 point earlier it seems to have been removed:

$ ./bin/nodetool -h localhost gossipinfo
/50.56.59.68
  LOAD:13815.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,60502102442797279294142560823234402248
  RELEASE_VERSION:1.0.7-SNAPSHOT
action-quick2/50.56.31.186
  LOAD:13725.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT

riptano@action-quick2:~/work/cassandra$  INFO [GossipStage:1] 2012-01-13 01:03:30,073 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster

 INFO [GossipStage:1] 2012-01-13 01:03:30,073 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP

 INFO [GossipStage:1] 2012-01-13 01:03:30,074 StorageService.java (line 1017) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55",vijay2win@yahoo.com,cywjackson,Normal,Resolved,Fixed,13/Jan/12 01:13,16/Apr/19 09:32
Bug,CASSANDRA-3737,12538297,Its impossible to removetoken joining down node,"We have a node that incidentaly started to join cluster. Admins made it down quicky, so now it looks :
10.112.0.234    datacenter1 rack1       Down   Joining 46.83 GB        2,90%   15893087653239874101909022095979644640  
And I can't removetoken such a node:

 nodetool -h tap9600 removetoken 15893087653239874101909022095979644640
Exception in thread ""main"" java.lang.UnsupportedOperationException: Token not found.
	at org.apache.cassandra.service.StorageService.removeToken(StorageService.java:2376)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
",brandon.williams,tivv,Normal,Resolved,Fixed,13/Jan/12 09:32,16/Apr/19 09:32
Bug,CASSANDRA-3738,12538304,sstable2json doesn't work for secondary index sstable due to partitioner mismatch,"sstable2json doesn't work for secondary index sstable in 1.0.6 while it worked in version 0.8.x.


$ bin/sstable2json $DATA/data/Keyspace1/users-hc-1-Data.db 
{
""1111"": [[""birth_year"",""1973"",1326450301786000], [""full_name"",""Patrick Rothfuss"",1326450301782000]],
""1020"": [[""birth_year"",""1975"",1326450301776000], [""full_name"",""Brandon Sanderson"",1326450301716000]]
}

$ bin/sstable2json $DATA/data/Keyspace1/users.users_birth_year_idx-hc-1-Data.db 
Exception in thread ""main"" java.lang.RuntimeException: Cannot open data/Keyspace1/users.users_birth_year_idx-hc-1 because partitioner does not match org.apache.cassandra.dht.RandomPartitioner
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:145)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:123)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:118)
	at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:360)
	at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:373)
	at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:431)


I tested with following sample data via cli:

create keyspace Keyspace1;
use Keyspace1;
create column family users with comparator=UTF8Type and
 column_metadata=[{column_name: full_name, validation_class: UTF8Type},
{column_name: email, validation_class: UTF8Type},
  {column_name: birth_year, validation_class: LongType, index_type: KEYS},
  {column_name: state, validation_class:  UTF8Type, index_type: KEYS}];
set users[1020][full_name] = 'Brandon Sanderson';
set users[1020][birth_year] = 1975;  
set users[1111][full_name] = 'Patrick Rothfuss';     
set users[1111][birth_year] = 1973;
get users where birth_year = 1973;
",yukim,skamio,Normal,Resolved,Fixed,13/Jan/12 10:35,16/Apr/19 09:32
Bug,CASSANDRA-3740,12538311,While using BulkOutputFormat  unneccessarily look for the cassandra.yaml file.,"I am trying to use BulkOutputFormat to stream the data from map of Hadoop job. I have set the cassandra related configuration using ConfigHelper ,Also have looked into Cassandra code seems Cassandra has taken care that it should not look for the cassandra.yaml file.
But still when I run the job i get the following error:

{
12/01/13 11:30:04 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
12/01/13 11:30:04 INFO input.FileInputFormat: Total input paths to process : 1
12/01/13 11:30:04 INFO mapred.JobClient: Running job: job_201201130910_0015
12/01/13 11:30:05 INFO mapred.JobClient:  map 0% reduce 0%
12/01/13 11:30:23 INFO mapred.JobClient: Task Id : attempt_201201130910_0015_m_000000_0, Status : FAILED
java.lang.Throwable: Child Error
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)

attempt_201201130910_0015_m_000000_0: Cannot locate cassandra.yaml
attempt_201201130910_0015_m_000000_0: Fatal configuration error; unable to start server.
}

Also let me know how can i make this cassandra.yaml file available to Hadoop mapreduce job?",brandon.williams,samarthg1986,Normal,Resolved,Fixed,13/Jan/12 12:14,16/Apr/19 09:32
Bug,CASSANDRA-3741,12538312,OOMs because delete operations are not accounted,"Currently we are moving to new data format where new format is written into new CFs and old one is deleted key-by-key. 
I have started getting OOMs and found out that delete operations are not accounted and so, column families are not flushed (changed == 0 with delete only operations) by storage manager.

This is pull request that fixed this problem for me: https://github.com/apache/cassandra/pull/5",akolyadenko,tivv,Normal,Resolved,Fixed,13/Jan/12 12:33,16/Apr/19 09:32
Bug,CASSANDRA-3744,12538403,Nodetool.bat double quotes classpath,Windows sucks and double quoting things breaks stuff.,nickmbailey,nickmbailey,Low,Resolved,Fixed,14/Jan/12 16:30,16/Apr/19 09:32
Bug,CASSANDRA-3751,12538739,Possible livelock during commit log playback,"In CommitLog.recover, there seems to be the possibility of concurrent inserts to tablesRecovered (a HashSet) in the Runnables instantiated a bit below (line 323 in 1.0.7). This apparently happened during a commit log playback during startup of a node that had not shut down cleanly (the cluster was under heavy load previously and there were several gigabytes of commit logs), resulting in two threads running in perpetuity (2 cores were at 100% from running these threads), preventing the node from coming up. The relevant portion of the stack trace is:

{noformat}
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:25"" prio=10 tid=0x00002aaad01e0800 nid=0x6f62 runnable [0x0000000044d54000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:21"" prio=10 tid=0x00002aaad00a2800 nid=0x6f5e runnable [0x0000000044950000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)

{noformat}

The most recently modified file in the commit log directory was this entry:
{noformat}
-rw-r----- 1 <redacted> <redacted>    0 Jan 16 16:03 CommitLog-1326758622599.log
{noformat}
though I'm not sure if this was related or not. ",jbellis,jchakerian,Normal,Resolved,Fixed,18/Jan/12 05:19,16/Apr/19 09:32
Bug,CASSANDRA-3752,12538787,bulk loader no longer finds sstables,"It looks like CASSANDRA-2749 broke it:

{noformat}
 WARN 13:02:20,107 Invalid file 'Standard1' in data directory /var/lib/cassandra/data/Keyspace1.
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,18/Jan/12 13:06,16/Apr/19 09:32
Bug,CASSANDRA-3755,12539021,NPE on invalid CQL DELETE command,"The CQL command {{delete from k where key='bar';}} causes Cassandra to hit a NullPointerException when the ""k"" column family does not exist, and it subsequently closes the Thrift connection instead of reporting an IRE or whatever. This is probably wrong.",dbrosius,thepaul,Low,Resolved,Fixed,19/Jan/12 19:31,16/Apr/19 09:32
Bug,CASSANDRA-3764,12539273,cqlsh doesn't error out immediately for use of invalid keyspace,"{noformat}
cqlsh> use wordcoun;
cqlsh:wordcoun> select * from input_words;
Bad Request: Keyspace wordcoun does not exist
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,21/Jan/12 05:54,16/Apr/19 09:32
Bug,CASSANDRA-3765,12539274,hadoop word count example is unable to output to cassandra with default settings,"{noformat}
12/01/21 06:03:16 WARN mapred.LocalJobRunner: job_local_0001
java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:407)
        at org.apache.cassandra.hadoop.ConfigHelper.getOutputPartitioner(ConfigHelper.java:384)
        at org.apache.cassandra.client.RingCache.<init>(RingCache.java:58)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:99)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:93)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:62)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:553)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
{noformat}

(Output to filesystem still works.)",jbellis,jbellis,Low,Resolved,Fixed,21/Jan/12 06:05,16/Apr/19 09:32
Bug,CASSANDRA-3766,12539280,KeysIndex is broken by CASSANDRA-1600,"CASSANDRA-1600 introduces bug which cause KeySearch throws an Exception during search

KeySearcher:163
logger.debug(""Skipping {}"", baseCfs.getComparator().getString(firstColumn.name()));

Here firstColumn is a column from *index* row, so column name is a *key* in the baseCf. 

So the correct version would be:
logger.debug(""Skipping {}"", baseCfs.metadata.getKeyValidator().getString(firstColumn.name()));

Right now it is not possible to query KeysIndex. ",philip.andronov,philip.andronov,Low,Resolved,Fixed,21/Jan/12 12:41,16/Apr/19 09:32
Bug,CASSANDRA-3767,12539326,cqlsh is missing cqlshlib (ImportError: No module named cqlshlib),"After a clean install of Cassandra, when running cqlsh I get the following error:

{code}>cqlsh
Traceback (most recent call last):
  File ""/usr/local/bin/cqlsh"", line 54, in <module>
    from cqlshlib import cqlhandling, pylexotron
ImportError: No module named cqlshlib
{code}
",thepaul,mattman,Normal,Resolved,Fixed,22/Jan/12 01:21,16/Apr/19 09:32
Bug,CASSANDRA-3784,12539806,RangeTest.java compilation error on Range.rangeSet in Eclipse (not Ant or IntelliJ),"When building from trunk:

{noformat}
The method rangeSet(Range<T>...) in the type Range is not applicable for the arguments (Range[])
RangeTest.java	/cassandra/test/unit/org/apache/cassandra/dht	line 184
{noformat}
",kirktrue,kirktrue,Low,Resolved,Fixed,25/Jan/12 18:50,16/Apr/19 09:32
Bug,CASSANDRA-3786,12539854,[patch] fix bad comparison of IColumn to ByteBuffer,"Code does

firstColumn.equals(startKey)

changed to 

firstColumn.name().equals(startKey)",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,26/Jan/12 02:36,16/Apr/19 09:32
Bug,CASSANDRA-3787,12539856,[patch] fix bad comparison of column name against * or 1,"code does 

(!selectClause.get(0).equals(""*"") && !selectClause.get(0).equals(""1"")))

which is a ColumnDefinition against a string 

changed to

String columnName = selectClause.get(0).toString();
if (!columnName.equals(""*"") && !columnName.equals(""1""))",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,26/Jan/12 02:46,16/Apr/19 09:32
Bug,CASSANDRA-3788,12539857,[patch] fix bad comparison in hadoop cf recorder reader,"code does

rows.get(0).columns.get(0).column.equals(startColumn)

which is a Column against a ByteBuffer

changed to 

rows.get(0).columns.get(0).column.name.equals(startColumn)",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,26/Jan/12 02:53,16/Apr/19 09:32
Bug,CASSANDRA-3789,12539858,[patch] fix bad validator lookup (bad key type),"code looks up an entry in a map by a byte[] even tho the map is keyed by ByteBuffer, add a ByteBuffer.wrap call to the key.",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,26/Jan/12 02:59,16/Apr/19 09:32
Bug,CASSANDRA-3796,12540000,post-2392 trunk does not build with java 7,"See below, on a fresh clone. Builds w/ java 6.

{code}
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:419: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]         int index = Collections.binarySearch(indexSummary.getKeys(), key);
    [javac]                                ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:509: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]             int left = Collections.binarySearch(samples, leftPosition);
    [javac]                                   ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:521: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]                       : Collections.binarySearch(samples, rightPosition);
    [javac]                                    ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
{code}
",scode,scode,Low,Resolved,Fixed,27/Jan/12 05:31,16/Apr/19 09:32
Bug,CASSANDRA-3797,12540003,StorageProxy static initialization not triggered until thrift requests come in,"While plugging in the metrics library for CASSANDRA-3671 I realized (because the metrics library was trying to add a shutdown hook on metric creation) that starting cassandra and simply shutting it down, causes StorageProxy to not be initialized until the drain shutdown hook.

Effects:

* StorageProxy mbean missing in visualvm/jconsole after initial startup (seriously, I thought I was going nuts ;))
* And in general anything that makes assumptions about running early, or at least not during JVM shutdown, such as the metrics library, will be problematic
",scode,scode,Low,Resolved,Fixed,27/Jan/12 06:37,16/Apr/19 09:32
Bug,CASSANDRA-3803,12540203,snapshot-before-compaction snapshots entire keyspace,Should only snapshot the CF being compacted,jbellis,jbellis,Low,Resolved,Fixed,27/Jan/12 22:04,16/Apr/19 09:32
Bug,CASSANDRA-3804,12540242,upgrade problems from 1.0 to trunk,"A 3-node cluster is on version 0.8.9, 1.0.6, or 1.0.7 and then one and only one node is taken down, upgraded to trunk, and started again. An rpc timeout exception happens if counter-add operations are done. It usually takes between 1 and 500 add operations before the failure occurs. The failure seems to happen sooner if the coordinator node is NOT the one that was upgraded. Here is the error: 

{code}

======================================================================
ERROR: counter_upgrade_test.TestCounterUpgrade.counter_upgrade_test
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/tahooie/cassandra-dtest/counter_upgrade_test.py"", line 50, in counter_upgrade_test
    cursor.execute(""UPDATE counters SET row = row+1 where key='a'"")
  File ""/usr/local/lib/python2.7/dist-packages/cql/cursor.py"", line 96, in execute
    raise cql.OperationalError(""Request did not complete within rpc_timeout."")
OperationalError: Request did not complete within rpc_timeout.

{code}

",xedin,tpatterson,Normal,Resolved,Fixed,28/Jan/12 04:01,16/Apr/19 09:32
Bug,CASSANDRA-3806,12540249,merge from 1.0 (aa20c7206cdc1efc1983466de05c224eccac1084) breaks build,"{code}
build-project:
     [echo] apache-cassandra: /tmp/cas/cassandra/build.xml
    [javac] Compiling 40 source files to /tmp/cas/cassandra/build/classes/thrift
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 296 source files to /tmp/cas/cassandra/build/classes/main
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]     ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                   ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                     ^
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                          ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                           ^
    [javac] 6 errors
{code}
",scode,scode,Normal,Resolved,Fixed,28/Jan/12 06:05,16/Apr/19 09:32
Bug,CASSANDRA-3812,12540413,Can't start a node with row_cache_size_in_mb=1,"I consistently get the following error when trying to run 'bin/cassandra':

{code}
ERROR 12:20:28,144 Fatal exception during initialization
org.apache.cassandra.config.ConfigurationException: Found system table files, but they couldn't be loaded!
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:279)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:174)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:367)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
{code}",yukim,tpatterson,Normal,Resolved,Fixed,30/Jan/12 19:25,16/Apr/19 09:32
Bug,CASSANDRA-3818,12540484,disabling m-a-t for fun and profit (and other ant stuff),"It should be possible to disable maven-ant-tasks for environments with more rigid dependency control, or where network access isn't available.

Patches to follow.",dbrosius,urandom,Low,Resolved,Fixed,31/Jan/12 02:32,16/Apr/19 09:32
Bug,CASSANDRA-3820,12540649,Columns missing after upgrade from 0.8.5 to 1.0.7.,"After an upgrade, one of our CFs had a lot of rows with missing columns. I've been able to reproduce in test conditions. Working on getting the tables to DataStax(data is private).


0.8 results:

{code}
[default@reddit] get CommentVote[36353467625f63333837336f32];
=> (column=date, value=313332333932323930392e3531, timestamp=1323922909506508)
=> (column=ip, value=REDACTED, timestamp=1327048432717348, ttl=2592000)
=> (column=name, value=31, timestamp=1327048433000740)
=> (column=REDACTED, value=30, timestamp=1323922909506432)
=> (column=thing1_id, value=REDACTED, timestamp=1323922909506475)
=> (column=thing2_id, value=REDACTED, timestamp=1323922909506486)
=> (column=REDACTED, value=31, timestamp=1323922909506518)
=> (column=REDACTED, value=30, timestamp=1323922909506497)
{code}


1.0 results:

{code}
[default@reddit] get CommentVote[36353467625f63333837336f32];
=> (column=ip, value=REDACTED, timestamp=1327048432717348, ttl=2592000)
=> (column=name, value=31, timestamp=1327048433000740)
{code}



A few notes:

* The rows with missing data were fully restored after scrubbing the sstables.
* The row which I reproduced on happened to be split across multiple sstables.
* When I copied the first sstable I found the row on, I was able to 'list' rows from the sstable, but any and all 'get' calls failed.
* These SStables were natively created on 0.8.5; they did not come from any previous upgrade.",,alienth,Normal,Resolved,Fixed,31/Jan/12 23:25,16/Apr/19 09:32
Bug,CASSANDRA-3821,12540677,Counters in super columns don't preserve correct values after cluster restart,"Set up a 3-node cluster with rf=3. Create a counter super column family and increment a bunch of subcolumns 100 times each, with cf=QUORUM. Then wait a few second, restart the cluster, and read the values back. They almost all come back different (and higher) then they are supposed to be.

Here are some extra things I've noticed:
 - Reading back the values before the restart always produces correct results.
 - Doing a nodetool flush before killing the cluster greatly improves the results, though sometimes a value will still be incorrect. You might have to run the test several times to see an incorrect value after a flush.
 - This problem doesn't happen on C* 1.0.7, unless you don't sleep between doing the increments and killing the cluster. Then it sometimes happens to a lesser degree.

A dtest has been added to demonstrate this issue. It is called ""super_counter_test.py"".",slebresne,tpatterson,Normal,Resolved,Fixed,01/Feb/12 05:37,16/Apr/19 09:32
Bug,CASSANDRA-3822,12540678,JdbcDate.getString(ByteBuffer) appears to not be multithreaded safe,"JdbcDate.getString(ByteBuffer) makes use of a static SimpleDateFormat

static final SimpleDateFormat FORMATTER = new SimpleDateFormat(DEFAULT_FORMAT);

SimpleDateFormat is not thread safe, as it uses a field from parent class DateFormat

    protected Calendar calendar;

to convert dates to calendars.

",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,01/Feb/12 05:38,16/Apr/19 09:32
Bug,CASSANDRA-3824,12540681,[patch] add missing break in nodecmd's command dispatching for SETSTREAMTHROUGHPUT,code falls thru SETSTREAMTHROUGHPUT into REBUILD case.,dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,01/Feb/12 06:05,16/Apr/19 09:32
Bug,CASSANDRA-3826,12540773,Pig cannot use output formats other than CFOF,Pig has ColumnFamilyOutputFormat hard coded.,brandon.williams,brandon.williams,Normal,Resolved,Fixed,01/Feb/12 20:18,16/Apr/19 09:32
Bug,CASSANDRA-3827,12540778,nosetests / system tests fail,"CQL Driver version used: 1.0.8.

{code}
EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_bad_batch_calls
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 381, in setUp
    try_run(self.inst, ('setup', 'setUp'))
  File ""/usr/local/lib/python2.7/site-packages/nose/util.py"", line 478, in try_run
    return func()
  File ""/var/lib/jenkins/jobs/Cassandra/workspace/test/system/__init__.py"", line 113, in setUp
    self.define_schema()
  File ""/var/lib/jenkins/jobs/Cassandra/workspace/test/system/__init__.py"", line 158, in define_schema
    Cassandra.CfDef('Keyspace1', 'Super1', column_type='Super', subcomparator_type='LongType', row_cache_size=1000, key_cache_size=0),
TypeError: __init__() got an unexpected keyword argument 'key_cache_size'
{code}
",xedin,mallen,Normal,Resolved,Fixed,01/Feb/12 21:06,16/Apr/19 09:32
Bug,CASSANDRA-3828,12540780,BulkOutputFormat shouldn't need flags to specify the output type,"BOF currently requires the IS_SUPER boolean to be set to determine if the output CF is going to be a super or not, and would similarly use a flag to indicate counters (if there was support for that yet.)  Instead, it should be able to introspect the mutations to determine what kind of columns to write.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,01/Feb/12 21:13,16/Apr/19 09:32
Bug,CASSANDRA-3831,12540841,scaling to large clusters in GossipStage impossible due to calculatePendingRanges ,"(most observations below are from 0.8, but I just now tested on
trunk and I can trigger this problem *just* by bootstrapping a ~180
nod cluster concurrently, presumably due to the number of nodes that
are simultaneously in bootstrap state)

It turns out that:

* (1) calculatePendingRanges is not just expensive, it's computationally complex - cubic or worse
* (2) it gets called *NOT* just once per node being bootstrapped/leaving etc, but is called repeatedly *while* nodes are in these states

As a result, clusters start exploding when you start reading 100-300
nodes. The GossipStage will get backed up because a single
calculdatePenginRanges takes seconds, and depending on what the
average heartbeat interval is in relation to this, this can lead to
*massive* cluster-wide flapping.

This all started because we hit this in production; several nodes
would start flapping several other nodes as down, with many nodes
seeing the entire cluster, or a large portion of it, as down. Logging
in to some of these nodes you would see that they would be constantly
flapping up/down for minutes at a time until one became lucky and it
stabilized.

In the end we had to perform an emergency full-cluster restart with
gossip patched to force-forget certain nodes in bootstrapping state.

I can't go into all details here from the post-mortem (just the
write-up would take a day), but in short:

* We graphed the number of hosts in the cluster that had more than 5
  Down (in a cluster that should have 0 down) on a minutely timeline.
* We also graphed the number of hosts in the cluster that had GossipStage backed up.
* The two graphs correlated *extremely* well
* jstack sampling showed it being CPU bound doing mostly sorting under calculatePendingRanges
* We were never able to exactly reproduce it with normal RING_DELAY and gossip intervals, even on a 184 node cluster (the production cluster is around 180).
* Dropping RING_DELAY and in particular dropping gossip interval to 10 ms instead of 1000 ms, we were able to observe all of the behavior we saw in production.

So our steps to reproduce are:

* Launch 184 node cluster w/ gossip interval at 10ms and RING_DELAY at 1 second.
* Do something like: {{while [ 1 ] ; do date ; echo decom ; nodetool decommission ; date ; echo done leaving decommed for a while ; sleep 3 ; date ; echo done restarting; sudo rm -rf /data/disk1/commitlog/* ; sudo rm -rf /data/diskarray/tables/* ; sudo monit restart cassandra ;date ; echo restarted waiting for a while ; sleep 40; done}} (or just do a manual decom/bootstrap once, it triggers every time)
* Watch all nodes flap massively and not recover at all, or maybe after a *long* time.

I observed the flapping using a python script that every 5 second
(randomly spread out) asked for unreachable nodes from *all* nodes in
the cluster, and printed any nodes and their counts when they had
unreachables > 5. The cluster can be observed instantly going into
massive flapping when leaving/bootstrap is initiated. Script needs
Cassandra running with Jolokia enabled for http/json access to
JMX. Can provide scrit if needed after cleanup.

The phi conviction, based on logging I added, was legitimate. Using
the 10 ms interval the average heartbeat interval ends up being like 25
ms or something like that. As a result, a single ~ 2 second delay in
gossip stage is huge in comparison to those 25 ms, and so we go past
the phi conviction threshold. This is much more sensitive than in
production, but it's the *same* effect, even if it triggers less
easily for real.

The best work around currently internally is to memoize
calculatePendingRanges so that we don't re-calculate if token meta
data, list of moving, list of bootstrapping and list of leaving are
all the same as on prior calculation. It's not entirely clear at this
point whether there is a clean fix to avoid executing
calculatePendingRanges more than once per unique node in this state.

It should be noted though that even if that is fixed, it is not
acceptable to spend several seconds doing these calculations on a ~
200 node cluster and it needs to be made fundamentally more efficient.

Here is a dump of thoughts by me in an internal JIRA ticket (not
exhaustive, I just went as far as to show that there is an issue;
there might be worse things I missed, but worse than cubic is bad
enough that I stopped):

(Comment uses 0.8 source.)

{quote}
Okay, so let's break down the computational complexity here.

Suppose ring size is {{n}} and number of bootstrapping/leaving tokens is {{m}}.  One of two places that take time (by measurement) is this part of calculatePendingRanges():

{code}
       // At this stage pendingRanges has been updated according to leave operations. We can
        // now continue the calculation by checking bootstrapping nodes.

        // For each of the bootstrapping nodes, simply add and remove them one by one to
        // allLeftMetadata and check in between what their ranges would be.
        for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
        {
            InetAddress endpoint = entry.getValue();

            allLeftMetadata.updateNormalToken(entry.getKey(), endpoint);
            for (Range range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
                pendingRanges.put(range, endpoint);
            allLeftMetadata.removeEndpoint(endpoint);
        }
{code}

I'll ignore stuff that's log(n) or better.

The outer loops is {{O(m)}}. The inner loop is {{O(n)}}, making aggregate so far {{O(nm)}}.

We have a call in there to updateNormalTokens() which implies a sorting, which his {{O(n log(n))}}. So now we're at {{O(n log(n) m)}}.

Next up we call {{getAddressRanges()}} which immediately does another {{O(n log(n)}} sort. we're still at {{O(n log(n) m}}. It then iterates (linear) and:

* calls {{getPrimaryRangeFor()}} for each.
* calls {{calculateNaturalEndpoints}} for each.

The former ends up sorting again, so now we're at {{O(n log(n) n log(n) m}} (worse than quadratic).

{{NTS.calculateNaturalEndpoints}} starts by collecting token meta data for nodes in the DC, by using {{updateNormalToken}}, which *implies sorting*. Woha woha. Now we're at {{O(n log(n) n log (n) n log(n) m)}}.

I might have missed things that are even worse, but this is bad enough to warrant this ticket. To put into perspective, 168 ^ 3 is 4.7 million.
{quote}
",scode,scode,Normal,Resolved,Fixed,01/Feb/12 23:44,16/Apr/19 09:32
Bug,CASSANDRA-3832,12540842,gossip stage backed up due to migration manager future de-ref,"This is just bootstrapping a ~ 180 trunk cluster. After a while, a
node I was on was stuck with thinking all nodes are down, because
gossip stage was backed up, because it was spending a long time
(multiple seconds or more, I suppose RPC timeout maybe) doing the
following. Cluster-wide restart -> back to normal. I have not
investigated further.

{code}
""GossipStage:1"" daemon prio=10 tid=0x00007f9d5847a800 nid=0xa6fc waiting on condition [0x000000004345f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000005029ad1c0> (a java.util.concurrent.FutureTask$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:364)
	at org.apache.cassandra.service.MigrationManager.rectifySchema(MigrationManager.java:132)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:75)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:802)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:918)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
{code}
",scode,scode,Urgent,Resolved,Fixed,01/Feb/12 23:45,16/Apr/19 09:32
Bug,CASSANDRA-3835,12540857,FB.broadcastAddress fixes and Soft reset on Ec2MultiRegionSnitch.reconnect,"looks like OutboundTcpConnectionPool.reset will clear the queue which might not be ideal for Ec2Multiregion snitch.
there is additional cleanup needed for FB.broadCastAddress.",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,02/Feb/12 00:41,16/Apr/19 09:32
Bug,CASSANDRA-3838,12540877,Repair Streaming hangs between multiple regions,"Streaming hangs between datacenters, though there might be multiple reasons for this, a simple fix will be to add the Socket timeout so the session can retry.

The following is the netstat of the affected node (the below output remains this way for a very long period).
[test_abrepairtest@test_abrepair--euwest1c-i-1adfb753 ~]$ nt netstats
Mode: NORMAL
Streaming to: /50.17.92.159
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1523325354/2475291786 - 61%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%



""Streaming:1"" daemon prio=10 tid=0x00002aaac2060800 nid=0x1676 runnable [0x000000006be85000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at com.sun.net.ssl.internal.ssl.OutputRecord.writeBuffer(OutputRecord.java:297)
        at com.sun.net.ssl.internal.ssl.OutputRecord.write(OutputRecord.java:286)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:743)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:731)
        at com.sun.net.ssl.internal.ssl.AppOutputStream.write(AppOutputStream.java:59)
        - locked <0x00000006afea1bd8> (a com.sun.net.ssl.internal.ssl.AppOutputStream)
        at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
        at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
        at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:117)
        at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:152)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Streaming from: /46.51.141.51
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2241-Data.db sections=7231 progress=0/1548922508 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2231-Data.db sections=4730 progress=0/296474156 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2244-Data.db sections=7650 progress=0/1580417610 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2217-Data.db sections=7682 progress=0/196689250 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2220-Data.db sections=7149 progress=0/478695185 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2171-Data.db sections=443 progress=0/78417320 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2222-Data.db sections=4590 progress=0/1310718798 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2213-Data.db sections=7876 progress=0/3308781588 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2216-Data.db sections=7386 progress=0/2868167170 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2254-Data.db sections=4618 progress=0/215989758 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1542191546/2475291786 - 62%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2210-Data.db sections=6698 progress=0/2304563183 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2229-Data.db sections=7386 progress=0/1324787539 - 0%


""Thread-198896"" prio=10 tid=0x00002aaac0e00800 nid=0x4710 runnable [0x000000004251b000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
        at com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)
        at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:798)
        - locked <0x00000005e220a170> (a java.lang.Object)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:755)
        at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
        - locked <0x00000005e220a1b8> (a com.sun.net.ssl.internal.ssl.AppInputStream)
        at com.ning.compress.lzf.LZFDecoder.readFully(LZFDecoder.java:392)
        at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:190)
        at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)
        at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:399)
        at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:115)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:119)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:37)
        at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:244)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:148)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:90)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:185)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
",jasobrown,vijay2win@yahoo.com,Low,Resolved,Fixed,02/Feb/12 06:10,16/Apr/19 09:32
Bug,CASSANDRA-3839,12540923,BulkOutputFormat binds to wrong client address when client is Dual-stack and server is IPv6,"Trying to run a map/reduce job with BulkOutputFormat, in an environment where the Hadoop nodes have Dual-stack (IPv4+IPv6) and the Cassandra servers are IPv6-only, it seems like the TCP connection setup for streaming is explicitly setting the source address to the IPv4 address of the Hadoop node, even though the destination address is IPv6. 

I'm seeing connection attempts where source address is an IPv4-represented-in-IPv6 address and destination is IPv6 of cassandra node. 

In the log output from the Hadoop M/R job, I see:

{noformat}
2012-02-01 16:49:19,909 WARN org.apache.cassandra.streaming.FileStreamTask: Failed attempt 1 to connect to /2001:4c28:a030:30:72f3:95ff:fe02:2936 to stream /var/lib/hadoop/mapred/local/taskTracker/forsberg/jobcache/job_201201120812_0204/attempt_201201120812_0204_m_000000_0/test/Histograms/test-Histograms-hc-1-Data.db sections=1 progress=0/749048 - 0%. Retrying in 4000 ms. (java.net.ConnectException: Connection timed out)
{noformat}

So, digging a bit down the code, I see that org.apache.cassandra.hadoop.BulkRecordWriter successfully creates a Thrift connection to my Cassandra cluster, over IPv6. It successfully retrieves tokenrange information.

Later on, in org.apache.cassandra.streaming.FileStreamTask, it fails to connect to the destination cassandra node. It seems to me that the problem is that org.apache.cassandra.net.OutboundTcpConnectionPool is asking FBUtilities.getLocalAddress for the address to bind to, and getLocalAddress is returning an IPv4 address when DatabaseDescriptor has not been initialized. And DatabaseDescriptor has not been initialized, becase in BulkOutputFormat we're not reading cassandra.yaml. 

I actually have a workaround for this which involves not applying patch that removes need to read cassandra.yaml, then point to a cassandra.yaml generated specifically for the purpose on each hadoop node, with listen_address set to the IPv6 address of the node. 

This is with net.ipv6.bindv6only=0 in Linux sysctl - something you must have for Hadoop to run. 

Also tried -D mapred.child.java.opts=""-Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true"", i.e. setting properties to prefer IPv6 stack to M/R job, but didn't help.

In this case, we would probably be better of not explicitly binding to any address - the OS would do that for us. I understand binding explicitly makes sense when this code is running inside Cassandra server.",brandon.williams,forsberg,Normal,Resolved,Fixed,02/Feb/12 13:46,16/Apr/19 09:32
Bug,CASSANDRA-3841,12540963,long-test timing out,"    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.LongCompactionSpeedTest:BeforeFirstTest:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongCompactionSpeedTest FAILED (timeout)
    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 64.536 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 41.104 sec
    [junit] 

BUILD FAILED
/Users/mallen/dstax/repos/git/cassandra/build.xml:1113: The following error occurred while executing this line:
/Users/mallen/dstax/repos/git/cassandra/build.xml:1036: Some long test(s) failed.

Total time: 63 minutes 9 seconds
",jasobrown,mallen,Low,Resolved,Fixed,02/Feb/12 18:05,16/Apr/19 09:32
Bug,CASSANDRA-3843,12541058,Unnecessary  ReadRepair request during RangeScan,"During reading with Quorum level and replication factor greater then 2, Cassandra sends at least one ReadRepair, even if there is no need to do that. 

With the fact that read requests await until ReadRepair will finish it slows down requsts a lot, up to the Timeout :(

It seems that the problem has been introduced by the CASSANDRA-2494, unfortunately I have no enought knowledge of Cassandra internals to fix the problem and do not broke CASSANDRA-2494 functionality, so my report without a patch.

Code explanations:
{code:title=RangeSliceResponseResolver.java|borderStyle=solid}
class RangeSliceResponseResolver {
    // ....
    private class Reducer extends MergeIterator.Reducer<Pair<Row,InetAddress>, Row>
    {
    // ....

        protected Row getReduced()
        {
            ColumnFamily resolved = versions.size() > 1
                                  ? RowRepairResolver.resolveSuperset(versions)
                                  : versions.get(0);
            if (versions.size() < sources.size())
            {
                for (InetAddress source : sources)
                {
                    if (!versionSources.contains(source))
                    {
                          
                        // [PA] Here we are adding null ColumnFamily.
                        // later it will be compared with the ""desired""
                        // version and will give us ""fake"" difference which
                        // forces Cassandra to send ReadRepair to a given source
                        versions.add(null);
                        versionSources.add(source);
                    }
                }
            }
            // ....
            if (resolved != null)
                repairResults.addAll(RowRepairResolver.scheduleRepairs(resolved, table, key, versions, versionSources));
            // ....
        }
    }
}
{code}


{code:title=RowRepairResolver.java|borderStyle=solid}
public class RowRepairResolver extends AbstractRowResolver {
    // ....
    public static List<IAsyncResult> scheduleRepairs(ColumnFamily resolved, String table, DecoratedKey<?> key, List<ColumnFamily> versions, List<InetAddress> endpoints)
    {
        List<IAsyncResult> results = new ArrayList<IAsyncResult>(versions.size());

        for (int i = 0; i < versions.size(); i++)
        {
            // On some iteration we have to compare null and resolved which are obviously
            // not equals, so it will fire a ReadRequest, however it is not needed here
            ColumnFamily diffCf = ColumnFamily.diff(versions.get(i), resolved);
            if (diffCf == null)
                continue;
        // .... 
{code}

Imagine the following situation:
NodeA has X.1 // row X with the version 1
NodeB has X.2 
NodeC has X.? // Unknown version, but because write was with Quorum it is 1 or 2

During the Quorum read from nodes A and B, Cassandra creates version 12 and send ReadRepair, so now nodes has the following content:
NodeA has X.12
NodeB has X.12

which is correct, however Cassandra also will fire ReadRepair to NodeC. There is no need to do that, the next consistent read have a chance to be served by nodes {A, B} (no ReadRepair) or by pair {?, C} and in that case ReadRepair will be fired and brings nodeC to the consistent state

Right now we are reading from the Index a lot and starting from some point in time we are getting TimeOutException because cluster is overloaded by the ReadRepairRequests *even* if all nodes has the same data :(",jbellis,philip.andronov,Normal,Resolved,Fixed,03/Feb/12 11:40,16/Apr/19 09:32
Bug,CASSANDRA-3844,12541078,Truncate leaves behind non-CFS backed secondary indexes,"If you setup a CF with a non-cfs backed secondary index then trucate it, nothing happens to the secondary index. we need a hook for CFStore to clean these up.",xedin,tjake,Low,Resolved,Fixed,03/Feb/12 14:28,16/Apr/19 09:32
Bug,CASSANDRA-3846,12541134,"cqlsh can't show data under python2.5, python2.6","Kris Hahn discovered a python2.6-ism in recent cqlsh changes:

{code}
        bval = escapedval.encode(output_encoding, errors='backslashreplace')
{code}

before python2.7, str.encode() didn't accept a keyword argument for the second parameter. the semantics are the same without naming the parameter, though, so removing the ""errors="" bit should suffice to make it run right.

does not affect any released version, but does affect HEAD of cassandra-1.0, cassandra-1.1, and trunk.",thepaul,thepaul,Low,Resolved,Fixed,03/Feb/12 20:52,16/Apr/19 09:32
Bug,CASSANDRA-3847,12541137,Pig should throw a useful error when the destination CF doesn't exist,"When trying to store data to nonexistent CF, no good error is returned.

Instead you get a message like:

{noformat}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
{noformat}

Which, if you follow its advice, will eventually lead you to an NPE in initSchema.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,03/Feb/12 21:30,16/Apr/19 09:32
Bug,CASSANDRA-3849,12541166,Saved CF row cache breaks when upgrading to 1.1,"Enabled row and key caching. Used stress to insert some data. ran nodetool flush, then nodetool compact. Then read the data back to populate the cache. Turned row_cache_save_period and key_cache_save_period really low to force saving the cache data. I verified that the row and key cache files existed in /var/lib/cassandra/saved_caches/.

I then killed cassandra, checked out branch cassandra-1.1, compiled and tried to start the node. The node failed to start, and I got this error:
{code}
 INFO 01:33:30,893 reading saved cache /var/lib/cassandra/saved_caches/Keyspace1-Standard1-RowCache
ERROR 01:33:31,009 Exception encountered during startup
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: Row cache is not enabled on column family [Standard1]
{code}",xedin,tpatterson,Normal,Resolved,Fixed,04/Feb/12 02:03,16/Apr/19 09:32
Bug,CASSANDRA-3850,12541180,get_indexed_slices losts index expressions,"in trunk 
CassandraServer.get_indexed_slices(ColumnParent , IndexClause , SlicePredicate , ConsistencyLevel)
 looses  index_clause.expressions when calling  constructing RangeSliceCommand by using wrong constructor.

This makes examples on http://wiki.apache.org/cassandra/CassandraCli produce wrong output as well as any get involving ""where"" check.
Patch to fix this issue http://pastebin.com/QQT0Tfpc",,philip.andronov,Normal,Resolved,Fixed,04/Feb/12 10:11,16/Apr/19 09:32
Bug,CASSANDRA-3855,12541341,RemoveDeleted dominates compaction time for large sstable counts,"With very large numbers of sstables (2000+ generated by a `bin/stress -n 100,000,000` run with LeveledCompactionStrategy), PrecompactedRow.removeDeletedAndOldShards dominates compaction runtime, such that commenting it out takes compaction throughput from 200KB/s to 12MB/s.

Stack attached.",yukim,stuhood,Normal,Resolved,Fixed,05/Feb/12 21:52,16/Apr/19 09:32
Bug,CASSANDRA-3862,12541448,RowCache misses Updates,"While performing stress tests to find any race problems for CASSANDRA-2864 I guess I (re-)found one for the standard on-heap row cache.

During my stress test I hava lots of threads running with some of them only reading other writing and re-reading the value.

This seems to happen:

- Reader tries to read row A for the first time doing a getTopLevelColumns
- Row A which is not in the cache yet is updated by Writer. The row is not eagerly read during write (because we want fast writes) so the writer cannot perform a cache update
- Reader puts the row in the cache which is now missing the update

I already asked this some time ago on the mailing list but unfortunately didn't dig after I got no answer since I assumed that I just missed something. In a way I still do but haven't found any locking mechanism that makes sure that this should not happen.

The problem can be reproduced with every run of my stress test. When I restart the server the expected column is there. It's just missing from the cache.

To test I have created a patch that merges memtables with the row cache. With the patch the problem is gone.

I can also reproduce in 0.8. Haven't checked 1.1 but I haven't found any relevant change their either so I assume the same aplies there.",slebresne,doubleday,Normal,Resolved,Fixed,06/Feb/12 17:17,16/Apr/19 09:32
Bug,CASSANDRA-3863,12541457,Nodetool ring output not sorted by token order,"Prior to 1.1 the output of nodetool ring was sorted in token order.  It looks like StorageService.getTokenToEndpointMap has been changed to return Map<String,String> in place of the previously used Map<Token, String>.",yukim,mallen,Low,Resolved,Fixed,06/Feb/12 18:38,16/Apr/19 09:32
Bug,CASSANDRA-3864,12541465,Unit tests failures in 1.1,"On the current 1.1 branch I get the following errors:
# SSTableImportTest:
{noformat}
[junit] Testcase: testImportSimpleCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
[junit] java.lang.Integer cannot be cast to java.lang.Long
[junit] java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[junit] 	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:132)
[junit] 	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:191)
[junit] 	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:174)
[junit] 	at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:290)
[junit] 	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:255)
[junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSimpleCf(SSTableImportTest.java:60)
{noformat}
# CompositeTypeTest:
{noformat}
[junit] Testcase: testCompatibility(org.apache.cassandra.db.marshal.CompositeTypeTest):	Caused an ERROR
[junit] Invalid comparator class org.apache.cassandra.db.marshal.CompositeType: must define a public static instance field or a public static method getInstance(TypeParser).
[junit] org.apache.cassandra.config.ConfigurationException: Invalid comparator class org.apache.cassandra.db.marshal.CompositeType: must define a public static instance field or a public static method getInstance(TypeParser).
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:294)
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.getAbstractType(TypeParser.java:268)
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:81)
[junit] 	at org.apache.cassandra.db.marshal.CompositeTypeTest.testCompatibility(CompositeTypeTest.java:216)
{noformat}
# DefsTest:
{noformat}
[junit] Testcase: testUpdateColumnFamilyNoIndexes(org.apache.cassandra.db.DefsTest):	FAILED
[junit] Should have blown up when you used a different comparator.
[junit] junit.framework.AssertionFailedError: Should have blown up when you used a different comparator.
[junit] 	at org.apache.cassandra.db.DefsTest.testUpdateColumnFamilyNoIndexes(DefsTest.java:539)
{noformat}
# CompactSerializerTest:
{noformat}
[junit] null
[junit] java.lang.ExceptionInInitializerError
[junit] 	at org.apache.cassandra.db.SystemTable.getCurrentLocalNodeId(SystemTable.java:437)
[junit] 	at org.apache.cassandra.utils.NodeId$LocalNodeIdHistory.<init>(NodeId.java:195)
[junit] 	at org.apache.cassandra.utils.NodeId$LocalIds.<clinit>(NodeId.java:43)
[junit] 	at java.lang.Class.forName0(Native Method)
[junit] 	at java.lang.Class.forName(Class.java:169)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:96)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest.scanClasspath(CompactSerializerTest.java:129)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:574)
[junit] 	at org.apache.cassandra.db.Table.<clinit>(Table.java:82)
{noformat}

There is also some error RemoveSubColumnTest and RemoveSubColumnTest but I'll open a separate ticket for those as they may require a bit more discussion.",brandon.williams,slebresne,Normal,Resolved,Fixed,06/Feb/12 19:09,16/Apr/19 09:32
Bug,CASSANDRA-3865,12541467,Cassandra-cli returns 'command not found' instead of syntax error,"When creating a column family from the output of 'show schema' with an index, there is a trailing comma after ""index_type: 0,""  The return from this is a 'command not found'  This is misleading because the command is found, there is just a syntax error.

'Command not found: `create column family $cfname ...`

",dbrosius,elubow,Low,Resolved,Fixed,06/Feb/12 19:32,16/Apr/19 09:32
Bug,CASSANDRA-3867,12541499,Disablethrift and Enablethrift can leaves behind zombie connections on THSHA server,"While doing nodetool disable thrift we disable selecting threads and close them... but the connections are still active...
Enable thrift creates a new Selector threads because we create new ThriftServer() which will cause the old connections to be zombies.

I think the right fix will be to call server.interrupt(); and then close the connections when they are done selecting.",vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,06/Feb/12 20:43,16/Apr/19 09:32
Bug,CASSANDRA-3870,12541595,Internal error processing batch_mutate: java.util.ConcurrentModificationException on CounterColumn,"Cassandra throws an exception below while performing batch_mutate with counter column insertion mutation to increment column with 1:

ERROR [Thrift:134] 2012-02-03 15:51:02,800 Cassandra.java (line 3462) Internal error processing batch_mutate
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:532)
        at org.apache.cassandra.service.AbstractWriteResponseHandler.waitForHints(AbstractWriteResponseHandler.java:89)
        at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:58)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:201)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639)
        at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Column family definition:

create column family CountersColumnFamily1
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 1000000.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 0.0
  and key_cache_save_period = 14400
  and read_repair_chance = 0.1
  and gc_grace = 43200
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'SerializingCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';",amorton,vjevdokimov,Normal,Resolved,Fixed,07/Feb/12 10:30,16/Apr/19 09:32
Bug,CASSANDRA-3872,12541652,Sub-columns removal is broken in 1.1,"CASSANDRA-3716 actually broke sub-columns deletion. The reason is that in QueryFilter.isRelevant, we've switched in checking getLocalDeletionTime() only (without looking for isMarkedForDelete). But for columns containers (in this case SuperColumn), the default local deletion time when not deleted is Integer.MIN_VALUE. In other words, a SC with only non-gcable tombstones will be considered as not relevant (while it should).

This is caught by two unit tests (RemoveSuperColumnTest and RemoveSubColumnTest) that are failing currently.",slebresne,slebresne,Normal,Resolved,Fixed,07/Feb/12 16:56,16/Apr/19 09:32
Bug,CASSANDRA-3874,12541721,cqlsh: handle situation where data can't be deserialized as expected,"When cqlsh tries to deserialize data which doesn't match the expected type (either because the validation type for the column/key alias was changed, or ASSUME has been used), it just fails completely and in most cases won't show any results at all. When there is only one misbehaving value out of a large number, this can be frustrating.

cqlsh should either show some failure marker in place of the bad value, or simply show the bytes along with some indicator of a failed deserialization.",thepaul,thepaul,Low,Resolved,Fixed,08/Feb/12 00:36,16/Apr/19 09:32
Bug,CASSANDRA-3875,12541791,SuperColumn may ignore relevant tombstones,"QueryFilter.isRelevant() consider that a super column that is gc-able but contains only non-gcable tombstone is irrelevant (if the tombstone timestmap is greater than the super column timestamp, which is almost implied by the fact that the tombstone are non-gcable while the SC is).",slebresne,slebresne,Normal,Resolved,Fixed,08/Feb/12 11:39,16/Apr/19 09:32
Bug,CASSANDRA-3876,12541794,nodetool removetoken force causes an inconsistent state,"Steps to reproduce (tested on 1.0.7 and trunk):
* Create a cluster of 3 nodes
* Insert some data
* stop one of the nodes
* Call removetoken on the token of the stopped node
* Immediately after, do removetoken force 
  - this will cause the original removetoken to fail with an error after 30s since the generation changed for the leaving node, but this is a convenient way of simulating the case where a removetoken hangs at streaming since the cleanup logic at the end of StorageService.removeToken is never executed.
  - if you want a more realistic reproduction then get a removetoken to hang in streaming, then do removetoken force

Effects:
* ""removetoken status"" now throws an exception because StorageService.removingNode is not cleared, but the endpoint is no longer a member of the ring:

$ nodetool -h localhost removetoken status
{noformat}
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:304)
	at org.apache.cassandra.service.StorageService.getRemovalStatus(StorageService.java:2369)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:683)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:672)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:619)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}

* truncate no longer works in the cli because the removed endpoint is not removed from Gossiper.unreachableEndpoints. 
The cli errors immediately with:
{noformat}
[default@ks1] truncate cf1;
null
UnavailableException()
	at org.apache.cassandra.thrift.Cassandra$truncate_result.read(Cassandra.java:20978)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_truncate(Cassandra.java:942)
	at org.apache.cassandra.thrift.Cassandra$Client.truncate(Cassandra.java:929)
	at org.apache.cassandra.cli.CliClient.executeTruncate(CliClient.java:1417)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:270)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
{noformat}

The logs show:
{noformat}
INFO [Thrift:11] 2012-02-08 11:55:50,135 StorageProxy.java (line 1172) Cannot perform truncate, some hosts are down
{noformat}

* there are probably other schema related things that fail for the same reason although this wasn't tested

Workaround:
* Restart the affected node.

Fix:
It looks like StorageService.forceRemoveCompletion is missing some cleanup logic which is present at the end of StorageService.removeToken. Adding this cleanup logic to forceRemoveCompletion fixes the above issues (see attached).",soverton,soverton,Normal,Resolved,Fixed,08/Feb/12 12:41,16/Apr/19 09:32
Bug,CASSANDRA-3881,12541955,reduce computational complexity of processing topology changes,"This constitutes follow-up work from CASSANDRA-3831 where a partial improvement was committed, but the fundamental issue was not fixed. The maximum ""practical"" cluster size was significantly improved, but further work is expected to be necessary as cluster sizes grow.

_Edit0: Appended patch information._

h3. Patches
||Compare||Raw diff||Description||
|[00_snitch_topology|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology]|[00_snitch_topology.patch|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology.diff]|Adds some functionality to TokenMetadata to track which endpoints and racks exist in a DC.|
|[01_calc_natural_endpoints|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints]|[01_calc_natural_endpoints.patch|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints.diff]|Rewritten O(logN) implementation of calculateNaturalEndpoints using the topology information from the tokenMetadata.|

----

_Note: These are branches managed with TopGit. If you are applying the patch output manually, you will either need to filter the TopGit metadata files (i.e. {{wget -O - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1}}), or remove them afterward ({{rm .topmsg .topdeps}})._",soverton,scode,Normal,Resolved,Fixed,09/Feb/12 10:31,16/Apr/19 09:32
Bug,CASSANDRA-3882,12541956,avoid distributed deadlock in migration stage,"This is follow-up work for the remainders of CASSANDRA-3832 which was only a partial fix. The deadlock in the migration stage needs to be fixed, as it can cause bootstrap (at least) to take potentially a very very long time to complete, and might also cause a lack of schema propagation until otherwise ""poked"".",richardlow,scode,Normal,Resolved,Fixed,09/Feb/12 10:36,16/Apr/19 09:32
Bug,CASSANDRA-3883,12541989,CFIF WideRowIterator only returns batch size columns,"Most evident with the word count, where there are 1250 'word1' items in two rows (1000 in one, 250 in another) and it counts 198 with the batch size set to 99.",jbellis,brandon.williams,Normal,Resolved,Fixed,09/Feb/12 16:15,16/Apr/19 09:32
Bug,CASSANDRA-3884,12542015,Intermittent SchemaDisagreementException,"Set up a cluster of two nodes (on cassandra-1.1), create some keyspaces and column families, and then make several schema changes. Everything is being done through only one of the nodes.  About once every 10 times (on my setup) I get a SchemaDisagreementException when creating and dropping keyspaces. 

There is a dtest for this: schema_changes_test.py. If your environment behaves like mine, you might need to run it 10 times to get the error.",xedin,tpatterson,Normal,Resolved,Fixed,09/Feb/12 19:51,16/Apr/19 09:32
Bug,CASSANDRA-3886,12542147,Pig can't store some types after loading them,"In CASSANDRA-2810, we removed the decompose methods in putNext instead relying on objToBB, however it cannot sufficiently handle all types.  For instance, if longs are loaded and then an attempt to store them is made, this causes a cast exception: java.io.IOException: java.io.IOException: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.pig.data.DataByteArray Output must be (key, {(column,value)...}) for ColumnFamily or (key, {supercolumn:{(column,value)...}...}) for SuperColumnFamily
",brandon.williams,brandon.williams,Normal,Resolved,Fixed,10/Feb/12 15:59,16/Apr/19 09:32
Bug,CASSANDRA-3888,12542173,remove no-longer-valid values from ColumnFamilyArgument enum,"{code}
[default@churnkeyspace] update column family churncf with keys_cached=100;
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.cli.CliClient.updateCfDefAttributes(CliClient.java:1244)
	at org.apache.cassandra.cli.CliClient.executeUpdateColumnFamily(CliClient.java:1091)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
{code}
",scode,scode,Normal,Resolved,Fixed,10/Feb/12 18:52,16/Apr/19 09:32
Bug,CASSANDRA-3903,12542433,Intermittent unexpected errors: possibly race condition around CQL parser?,"When running multiple simultaneous instances of the test_cql.py piece of the python-cql test suite, I can reliably reproduce intermittent and unpredictable errors in the tests.

The failures often occur at the point of keyspace creation during test setup, with a CQL statement of the form:

{code}
        CREATE KEYSPACE 'asnvzpot' WITH strategy_class = SimpleStrategy
            AND strategy_options:replication_factor = 1
    
{code}

An InvalidRequestException is returned to the cql driver, which re-raises it as a cql.ProgrammingError. The message:

{code}
ProgrammingError: Bad Request: line 2:24 no viable alternative at input 'asnvzpot'
{code}

In a few cases, Cassandra threw an ArrayIndexOutOfBoundsException and this traceback, closing the thrift connection:

{code}
ERROR [Thrift:244] 2012-02-10 15:51:46,815 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1520)
        at org.apache.cassandra.thrift.ThriftValidation.validateCfDef(ThriftValidation.java:634)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:744)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:898)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1245)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

Sometimes I see an ArrayOutOfBoundsError with no traceback:

{code}
ERROR [Thrift:858] 2012-02-13 12:04:01,537 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException
{code}

Sometimes I get this:

{code}
ERROR [MigrationStage:1] 2012-02-13 12:04:46,077 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.IllegalArgumentException: value already present: 1558
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:111)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:84)
        at org.apache.cassandra.config.Schema.load(Schema.java:392)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:284)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:209)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyImpl(AddColumnFamily.java:49)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:66)
        at org.apache.cassandra.cql.QueryProcessor$1.call(QueryProcessor.java:334)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Again, around 99% of the instances of this {{CREATE KEYSPACE}} statement work fine, so it's a little hard to git bisect out, but I guess I'll see what I can do.",slebresne,thepaul,Normal,Resolved,Fixed,13/Feb/12 19:29,16/Apr/19 09:32
Bug,CASSANDRA-3905,12542446,fix typo in nodetool help for repair,It says to use {{-rp}} instead of {{-pr}}.,scode,scode,Low,Resolved,Fixed,13/Feb/12 21:44,16/Apr/19 09:32
Bug,CASSANDRA-3906,12542474,BulkRecordWriter throws NPE for counter columns,"Using BulkRecordWriter, fails with counters due to an NPE (we used column instead of counter_column). I also noticed this broke for super columns too.",lenn0x,lenn0x,Normal,Resolved,Fixed,14/Feb/12 02:48,16/Apr/19 09:32
Bug,CASSANDRA-3907,12542482,Support compression using BulkWriter,Currently there is no way to enable compression using BulkWriter. ,lenn0x,lenn0x,Normal,Resolved,Fixed,14/Feb/12 05:18,16/Apr/19 09:32
Bug,CASSANDRA-3909,12542535,Pig should handle wide rows,Pig should be able to use the wide row support in CFIF.,brandon.williams,brandon.williams,Normal,Resolved,Fixed,14/Feb/12 14:29,16/Apr/19 09:32
Bug,CASSANDRA-3913,12542670,Incorrect InetAddress equality test,CASSANDRA-3485 introduced some InetAddress checks using == instead of .equals.,brandon.williams,brandon.williams,Low,Resolved,Fixed,15/Feb/12 12:19,16/Apr/19 09:32
Bug,CASSANDRA-3915,12542697,Fix LazilyCompactedRowTest,"LazilyCompactedRowTest.testTwoRowSuperColumn has never really worked. It uses LazilyCompactedRowTest.assertBytes() that assumes standard columns, even though that test is for super columns. For some reason, the deserialization of the super columns as columns was not breaking stuff and so the test was ""working"", but CASSANDRA-3872 changed that and LazilyCompactedRowTest.testTwoRowSuperColumn fails on current cassandra-1.1 branch (but it's not CASSANDRA-3872 fault, just the test that is buggy).",slebresne,slebresne,Low,Resolved,Fixed,15/Feb/12 15:19,16/Apr/19 09:32
Bug,CASSANDRA-3917,12542706,System test failures in 1.1,"On branch 1.1, I currently see two system test failures:
{noformat}
======================================================================
FAIL: system.test_thrift_server.TestMutations.test_get_range_slice_after_deletion
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/mcmanus/Git/cassandra/test/system/test_thrift_server.py"", line 1937, in test_get_range_slice_after_deletion
    assert len(result[0].columns) == 1
AssertionError
{noformat}
and
{noformat}
======================================================================
FAIL: Test that column ttled expires from KEYS index
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/mcmanus/Git/cassandra/test/system/test_thrift_server.py"", line 1908, in test_index_scan_expiring
    assert len(result) == 1, result
AssertionError: []

----------------------------------------------------------------------
{noformat}",slebresne,slebresne,Normal,Resolved,Fixed,15/Feb/12 16:14,16/Apr/19 09:32
Bug,CASSANDRA-3921,12542796,Compaction doesn't clear out expired tombstones from SerializingCache,"Compaction calls removeDeletedInCache, which looks like this:

{code}
.   public void removeDeletedInCache(DecoratedKey key)
    {
        ColumnFamily cachedRow = cfs.getRawCachedRow(key);
        if (cachedRow != null)
            ColumnFamilyStore.removeDeleted(cachedRow, gcBefore);
    }
{code}

For the SerializingCache, this means it calls removeDeleted on a temporary, deserialized copy, which leaves the cache contents unaffected.",jbellis,jbellis,Low,Resolved,Fixed,15/Feb/12 23:56,16/Apr/19 09:32
Bug,CASSANDRA-3922,12542825,streaming from all (not one) neighbors during rebuild/bootstrap,"The last round of changes that happened in CASSANDRA-3483 before it went in actually changed behavior - we now stream from *ALL* neighbors that have a range, rather than just one. This leads to data size explosion.

Attaching patch to revert to intended behavior.",scode,scode,Urgent,Resolved,Fixed,16/Feb/12 06:17,16/Apr/19 09:32
Bug,CASSANDRA-3926,12542950,all column validator options are not represented in cli help,"The options added to column validators from CASSANDRA-2530 are not shown as options in the CLI help.  I was going to create a column family with a float validator and double checked the help and it wasn't shown.  So I just had to double check that I could.  Would be nice to have those added to those docs, even though CQL is the way forward.",kirktrue,jeromatron,Low,Resolved,Fixed,16/Feb/12 22:10,16/Apr/19 09:32
Bug,CASSANDRA-3931,12543184,gossipers notion of schema differs from reality as reported by the nodes in question,"On a 1.1 cluster we happened to notice that {{nodetool gossipinfo | grep SCHEMA}} reported disagreement:

{code}
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
{code}

However, the result of a thrift {{describe_ring}} on the cluster claims they all agree and that {{b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d}} is the schema they have.

The schemas seem to ""actually"" propagate; e.g. dropping a keyspace actually drops the keyspace.",brandon.williams,scode,Normal,Resolved,Fixed,19/Feb/12 04:09,16/Apr/19 09:32
Bug,CASSANDRA-3933,12543356,./bin/cqlsh `describe keyspace <keyspace>` command doesn't work when ColumnFamily row_cache_provider wasn't specified.,"I have created Keyspace and ColumnFamily using CLI

{noformat}
/bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to Cassandra CLI version 1.0.7-SNAPSHOT

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace ks;
f89384f0-5bd3-11e1-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use ks;
Authenticated to keyspace: ks
[default@ks] create column family cf;
fc807690-5bd3-11e1-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] quit;
{noformat}

and then I have tried to describe keyspace using CQLsh

{noformat}
./bin/cqlsh                     
Connected to Test Cluster at localhost:9160.
[cqlsh 2.0.0 | Cassandra 1.0.7-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.20.0]
Use HELP for help.
cqlsh> describe keyspace ks;

CREATE KEYSPACE ks WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:datacenter1 = '1';

USE ks;

CREATE COLUMNFAMILY cf (
CfDef instance has no attribute 'row_cache_provider'
  KEY blob PRIMARY KEYcqlsh> 
{noformat}",thepaul,xedin,Low,Resolved,Fixed,20/Feb/12 15:08,16/Apr/19 09:32
Bug,CASSANDRA-3934,12543390,Short read protection is broken,"When a read needs to do more than one retry (due to short reads), the originalCount is not preserved by the retry leading to returning more than the requested number of columns.
Moreover, when a retried read checks whether more retry is needed, it doesn't compare the number of live column retrieved against the original number of columns requested by the user, but against the number of columns requested during the retry, making it much more likely to actually do one more retry.

This catch by the two tests 'short_read_test' and 'short_read_reversed_test' at https://github.com/riptano/cassandra-dtest/blob/master/consistency_test.py that are failing intermittently.",slebresne,slebresne,Low,Resolved,Fixed,20/Feb/12 18:44,16/Apr/19 09:32
Bug,CASSANDRA-3939,12543558,occasional failure of CliTest,"{{CliTest}} will occasionally fail with an NPE.

{noformat}
[junit] Testcase: testCli(org.apache.cassandra.cli.CliTest):	Caused an ERROR
[junit] java.lang.NullPointerException
[junit] java.lang.RuntimeException: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1039)
[junit] 	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:228)
[junit] 	at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:213)
[junit] 	at org.apache.cassandra.cli.CliTest.testCli(CliTest.java:241)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.validateSchemaIsSettled(CliClient.java:2855)
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1030)
{noformat}

This occurs because no default for {{schema_mwt}} is applied unless {{main()}} is invoked.

(Trivial )patch to follow.",urandom,urandom,Low,Resolved,Fixed,21/Feb/12 20:25,16/Apr/19 09:32
Bug,CASSANDRA-3940,12543576,mergeShardsChance deprecated; remove from thrift?,Or at least it should be marked deprecated somehow.,slebresne,thepaul,Low,Resolved,Fixed,21/Feb/12 22:00,16/Apr/19 09:32
Bug,CASSANDRA-3942,12543611,ColumnFamilyRecordReader can report progress > 100%,CFRR.getProgress() can return a value > 1.0 since the totalRowCount is a estimate.,brandon.williams,tjake,Low,Resolved,Fixed,22/Feb/12 02:53,16/Apr/19 09:32
Bug,CASSANDRA-3944,12543634,BulkRecordWriter will throw NullExceptions if no data is sent with the reducer,"In the BulkRecordWriter, in the close() method if no actual output is sent to the reducer, which can be caused by having too many reducers and not enough map data, we throw null exceptions and the job can fail.",lenn0x,lenn0x,Normal,Resolved,Fixed,22/Feb/12 07:55,16/Apr/19 09:32
Bug,CASSANDRA-3946,12543638,BulkRecordWriter shouldn't stream any empty data/index files that might be created at end of flush,"If by chance, we flush sstables during BulkRecordWriter (we have seen it happen), I want to make sure we don't try to stream them.",yukim,lenn0x,Low,Resolved,Fixed,22/Feb/12 08:15,16/Apr/19 09:32
Bug,CASSANDRA-3948,12543773,rename RandomAccessReader.MAX_BYTES_IN_PAGE_CACHE,"This should make the fadvising useless (mostly). See CASSANDRA-1470 for why, including links to kernel source. I have not investigated the history of when this broke or whether it was like from the beginning.

For the record I have not confirmed this by testing, only by code inspection. I happened to notice it working on other things, so there is some chance that I'm just mis-reading the code.
",xedin,scode,Normal,Resolved,Fixed,23/Feb/12 00:35,16/Apr/19 09:32
Bug,CASSANDRA-3954,12543997,Exceptions during start up after schema disagreement,"Hi,
i`ve got schema disaggreement after dropping down keyspace,
i`ve switched off one nodes in cluster, after starting i`ve got bunch of these exceptions:
{code}
ERROR [SSTableBatchOpen:1] 2012-02-24 14:21:00,759 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
        at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
        at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
        at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
        at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
        at java.util.TreeMap.getEntry(TreeMap.java:328)
        at java.util.TreeMap.containsKey(TreeMap.java:209)
        at java.util.TreeSet.contains(TreeSet.java:217)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:393)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:189)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:227)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
and this one on the end of start up:
{code}
ERROR [MigrationStage:1] 2012-02-24 14:37:22,750 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:282)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:216)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:330)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:240)
        at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:124)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
Any ideas why they`ve appeared?",xedin,mdymarek,Normal,Resolved,Fixed,24/Feb/12 14:49,16/Apr/19 09:32
Bug,CASSANDRA-3955,12544002,"HintedHandoff won't compact a single sstable, resulting in repeated log messages","First introduced by CASSANDRA-3554, and then mostly solved in CASSANDRA-3733, there is still one special case where the HH log message will repeat every 10 mins for 0 rows: when there have previously been hints delivered to the node, but now only a single sstable exists.  Because the we refused to compact a single sstable, and it contains tombstones for the hints, the message repeats.",brandon.williams,brandon.williams,Low,Resolved,Fixed,24/Feb/12 15:33,16/Apr/19 09:32
Bug,CASSANDRA-3957,12544061,Supercolumn serialization assertion failure,"As reported at http://mail-archives.apache.org/mod_mbox/cassandra-user/201202.mbox/%3CCADJL=w5kH5TEQXOwhTn5Jm3cmR4Rj=NfjcqLryXV7pLyASi95A@mail.gmail.com%3E,

{noformat}
ERROR 10:51:44,282 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 4690 to accomodate data size
of 2347 (predicted 2344) for RowMutation(keyspace='Player',
key='36336138643338652d366162302d343334392d383466302d356166643863353133356465',
modifications=[ColumnFamily(PlayerCity [SuperColumn(owneditem_1019
[]),SuperColumn(owneditem_1024 []),SuperColumn(owneditem_1026
[]),SuperColumn(owneditem_1074 []),SuperColumn(owneditem_1077
[]),SuperColumn(owneditem_1084 []),SuperColumn(owneditem_1094
[]),SuperColumn(owneditem_1130 []),SuperColumn(owneditem_1136
[]),SuperColumn(owneditem_1141 []),SuperColumn(owneditem_1142
[]),SuperColumn(owneditem_1145 []),SuperColumn(owneditem_1218
[636f6e6e6563746564:false:5@1329648704269002
,63757272656e744865616c7468:false:3@1329648704269006
,656e64436f6e737472756374696f6e54696d65:false:13@1329648704269007
,6964:false:4@1329648704269000,6974656d4964:false:15@1329648704269001
,6c61737444657374726f79656454696d65:false:1@1329648704269008
,6c61737454696d65436f6c6c6563746564:false:13@1329648704269005
,736b696e4964:false:7@1329648704269009,78:false:4@1329648704269003
,79:false:3@1329648704269004,]),SuperColumn(owneditem_133
[]),SuperColumn(owneditem_134 []),SuperColumn(owneditem_135
[]),SuperColumn(owneditem_141 []),SuperColumn(owneditem_147
[]),SuperColumn(owneditem_154 []),SuperColumn(owneditem_159
[]),SuperColumn(owneditem_171 []),SuperColumn(owneditem_253
[]),SuperColumn(owneditem_422 []),SuperColumn(owneditem_438
[]),SuperColumn(owneditem_515 []),SuperColumn(owneditem_521
[]),SuperColumn(owneditem_523 []),SuperColumn(owneditem_525
[]),SuperColumn(owneditem_562 []),SuperColumn(owneditem_61
[]),SuperColumn(owneditem_634 []),SuperColumn(owneditem_636
[]),SuperColumn(owneditem_71 []),SuperColumn(owneditem_712
[]),SuperColumn(owneditem_720 []),SuperColumn(owneditem_728
[]),SuperColumn(owneditem_787 []),SuperColumn(owneditem_797
[]),SuperColumn(owneditem_798 []),SuperColumn(owneditem_838
[]),SuperColumn(owneditem_842 []),SuperColumn(owneditem_847
[]),SuperColumn(owneditem_849 []),SuperColumn(owneditem_851
[]),SuperColumn(owneditem_852 []),SuperColumn(owneditem_853
[]),SuperColumn(owneditem_854 []),SuperColumn(owneditem_857
[]),SuperColumn(owneditem_858 []),SuperColumn(owneditem_874
[]),SuperColumn(owneditem_884 []),SuperColumn(owneditem_886
[]),SuperColumn(owneditem_908 []),SuperColumn(owneditem_91
[]),SuperColumn(owneditem_911 []),SuperColumn(owneditem_930
[]),SuperColumn(owneditem_934 []),SuperColumn(owneditem_937
[]),SuperColumn(owneditem_944 []),SuperColumn(owneditem_945
[]),SuperColumn(owneditem_962 []),SuperColumn(owneditem_963
[]),SuperColumn(owneditem_964 []),])])
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
",slebresne,jbellis,Normal,Resolved,Fixed,24/Feb/12 21:51,16/Apr/19 09:32
Bug,CASSANDRA-3962,12544197,CassandraStorage can't cast fields from a CF correctly,"Included scripts demonstrate the problem.  Regardless of whether the key is cast as a chararray or not, the Pig scripts fail with 

{code}
java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.lang.String
	at org.apache.pig.backend.hadoop.HDataType.getWritableComparableTypes(HDataType.java:72)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:117)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:269)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
{code}

",brandon.williams,jalkanen,Normal,Resolved,Fixed,26/Feb/12 20:51,16/Apr/19 09:32
Bug,CASSANDRA-3963,12544234,Exception durint start up after updating cassandra,"Hi,
i`ve updated cassandra on two-nodes cluster and i`ve got this exception during start up:
{code}
INFO 09:06:06,520 Opening /cassandra/system/HintsColumnFamily/system-HintsColumnFamily-hc-1 (178828054 bytes)
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:384)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:332)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:156)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:514)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3
{code}
we`re running 2 node cluster on snapshots from cassandra-1.1 branch:
* old snapshot: 5f5e00bc9a83bfd701723cbc7bf2307ea53da342 + patches from CASSANDRA-3740
* new snapshot: d65590ac8a5a3cfbe1529ef77346e84d6961db7d",xedin,mdymarek,Normal,Resolved,Fixed,27/Feb/12 09:42,16/Apr/19 09:32
Bug,CASSANDRA-3965,12544339,"Cassandra code base has files with import statements having ""*"" causing compilation failure","I tried to download a jar as part of the new unit test I am writing. I ran my unit test successfully but later if I run ""ant"" without ant clean, I run into the following compilation issue

{code}
build-project:
     [echo] apache-cassandra: /Users/harish/workspace/cassandra/build.xml
    [javac] Compiling 40 source files to /Users/harish/workspace/cassandra/build/classes/thrift
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 568 source files to /Users/harish/workspace/cassandra/build/classes/main
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1607: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Iterable<DecoratedKey<?>> keySamples(Range<Token> range)
    [javac]                                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:196: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Collection<Range<Token>> getLocalRanges(String table)
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:201: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getLocalPrimaryRange()
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:912: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Map<Range<Token>, List<InetAddress>> getRangeToAddressMap(String keyspace)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1554: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<Range<Token>, InetAddress> getChangedRangesForLeaving(String table, InetAddress endpoint)
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1975: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public AntiEntropyService.RepairFuture forceTableRepair(final Range<Token> range, final String tableName, final String... columnFamilies) throws IOException
    [javac]                                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2017: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getPrimaryRangeForEndpoint(InetAddress ep)
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2027: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     Collection<Range<Token>> getRangesForEndpoint(String table, InetAddress ep)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2038: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Range<Token>> getAllRanges(List<Token> sortedTokens)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2128: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Token> getSplits(String table, String cfName, Range<Token> range, int keysPerSplit)
    [javac]                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2152: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private List<DecoratedKey> keySamples(Iterable<ColumnFamilyStore> cfses, Range<Token> range)
    [javac]                                                                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2762: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch streamRanges(final Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable)
    [javac]                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2816: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch requestRanges(final Map<String, Multimap<InetAddress, Range<Token>>> ranges)
    [javac]                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                     ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1607: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Iterable<DecoratedKey<?>> keySamples(Range<Token> range)
    [javac]                                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:196: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Collection<Range<Token>> getLocalRanges(String table)
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:201: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getLocalPrimaryRange()
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:912: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Map<Range<Token>, List<InetAddress>> getRangeToAddressMap(String keyspace)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1554: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<Range<Token>, InetAddress> getChangedRangesForLeaving(String table, InetAddress endpoint)
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1975: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public AntiEntropyService.RepairFuture forceTableRepair(final Range<Token> range, final String tableName, final String... columnFamilies) throws IOException
    [javac]                                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2017: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getPrimaryRangeForEndpoint(InetAddress ep)
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2027: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     Collection<Range<Token>> getRangesForEndpoint(String table, InetAddress ep)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2038: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Range<Token>> getAllRanges(List<Token> sortedTokens)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2128: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Token> getSplits(String table, String cfName, Range<Token> range, int keysPerSplit)
    [javac]                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2152: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private List<DecoratedKey> keySamples(Iterable<ColumnFamilyStore> cfses, Range<Token> range)
    [javac]                                                                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2762: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch streamRanges(final Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable)
    [javac]                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2816: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch requestRanges(final Map<String, Multimap<InetAddress, Range<Token>>> ranges)
    [javac]                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                     ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:393: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Range range = new Range<Token>(token, token);
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:393: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Range range = new Range<Token>(token, token);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1328: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         assert !(range instanceof Range) || !((Range)range).isWrapAround() || range.right.isMinimum() : range;
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1328: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         assert !(range instanceof Range) || !((Range)range).isWrapAround() || range.right.isMinimum() : range;
    [javac]                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:853: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>,List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:884: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:904: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata_.getPendingRanges(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:919: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = getAllRanges(tokenMetadata_.sortedTokens());
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:959: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:961: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range range = entry.getKey();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1011: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> rangeToEndpointMap = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1011: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> rangeToEndpointMap = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]                                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1012: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1356: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> pendingRanges = HashMultimap.create();
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1368: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();
    [javac]                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1374: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1374: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();
    [javac]                                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1380: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : affectedRanges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1399: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1417: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1441: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> rangeAddresses = Table.open(table).getReplicationStrategy().getRangeAddresses(tokenMetadata_);
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1442: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<InetAddress, Range<Token>> sourceRanges = HashMultimap.create();
    [javac]                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1446: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1507: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<String, Map.Entry<InetAddress, Collection<Range<Token>>>> rangesToFetch = HashMultimap.create();
    [javac]                                                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1513: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> changedRanges = getChangedRangesForLeaving(table, endpoint);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1514: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Set<Range<Token>> myNewRanges = new HashSet<Range<Token>>();
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1514: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Set<Range<Token>> myNewRanges = new HashSet<Range<Token>>();
    [javac]                                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1515: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<Range<Token>, InetAddress> entry : changedRanges.entries())
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1520: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> sourceRanges = getNewSourceRanges(table, myNewRanges);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1521: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : sourceRanges.asMap().entrySet())
    [javac]                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1530: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : rangesToFetch.get(table))
    [javac]                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1533: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Collection<Range<Token>> ranges = entry.getValue();
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1557: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Collection<Range<Token>> ranges = getRangesForEndpoint(table, endpoint);
    [javac]                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1562: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> currentReplicaEndpoints = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1562: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> currentReplicaEndpoints = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1565: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1575: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> changedRanges = HashMultimap.create();
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1582: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1916: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Collection<Range<Token>> ranges = getLocalRanges(tableName);
    [javac]                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1921: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2045: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = new ArrayList<Range<Token>>();
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2045: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = new ArrayList<Range<Token>>();
    [javac]                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2049: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range<Token> range = new Range<Token>(sortedTokens.get(i - 1), sortedTokens.get(i));
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2049: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range<Token> range = new Range<Token>(sortedTokens.get(i - 1), sortedTokens.get(i));
    [javac]                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2052: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = new Range<Token>(sortedTokens.get(size - 1), sortedTokens.get(0));
    [javac]         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2052: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = new Range<Token>(sortedTokens.get(size - 1), sortedTokens.get(0));
    [javac]                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2164: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = getLocalPrimaryRange();
    [javac]         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2253: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStream = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2253: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStream = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                                                                                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2257: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesMM = getChangedRangesForLeaving(table, FBUtilities.getBroadcastAddress());
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2323: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<InetAddress, Range<Token>>> rangesToFetch = new HashMap<String, Multimap<InetAddress, Range<Token>>>();
    [javac]                                           ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2323: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<InetAddress, Range<Token>>> rangesToFetch = new HashMap<String, Multimap<InetAddress, Range<Token>>>();
    [javac]                                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2324: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2324: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                                                                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2336: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Collection<Range<Token>> currentRanges = getRangesForEndpoint(table, localAddress);
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2338: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Collection<Range<Token>> updatedRanges = strategy.getPendingAddressRanges(tokenMetadata_, newToken, localAddress);
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2342: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangeAddresses = strategy.getRangeAddresses(tokenMetadata_);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2345: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Pair<Set<Range<Token>>, Set<Range<Token>>> rangesPerTable = calculateStreamAndFetchRanges(currentRanges, updatedRanges);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2345: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Pair<Set<Range<Token>>, Set<Range<Token>>> rangesPerTable = calculateStreamAndFetchRanges(currentRanges, updatedRanges);
    [javac]                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2351: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesToFetchWithPreferredEndpoints = ArrayListMultimap.create();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2352: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> toFetch : rangesPerTable.right)
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2354: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 for (Range<Token> range : rangeAddresses.keySet())
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2367: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangeWithEndpoints = HashMultimap.create();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2369: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> toStream : rangesPerTable.left)
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2379: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> workMap = RangeStreamer.getWorkMap(rangesToFetchWithPreferredEndpoints);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2507: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> changedRanges = getChangedRangesForLeaving(table, endpoint);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2765: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<String, Multimap<Range<Token>, InetAddress>> entry : rangesToStreamByTable.entrySet())
    [javac]                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2767: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesWithEndpoints = entry.getValue();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2777: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             final Set<Map.Entry<Range<Token>, InetAddress>> pending = new HashSet<Map.Entry<Range<Token>, InetAddress>>(rangesWithEndpoints.entries());
    [javac]                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2777: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             final Set<Map.Entry<Range<Token>, InetAddress>> pending = new HashSet<Map.Entry<Range<Token>, InetAddress>>(rangesWithEndpoints.entries());
    [javac]                                                                                             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2779: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (final Map.Entry<Range<Token>, InetAddress> endPointEntry : rangesWithEndpoints.entries())
    [javac]                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2781: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 final Range<Token> range = endPointEntry.getKey();
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2819: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<String, Multimap<InetAddress, Range<Token>>> entry : ranges.entrySet())
    [javac]                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2821: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> endpointWithRanges = entry.getValue();
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2835: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Collection<Range<Token>> toFetch = endpointWithRanges.get(source);
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2868: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toStream = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2868: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toStream = new HashSet<Range<Token>>();
    [javac]                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2869: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toFetch  = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2869: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toFetch  = new HashSet<Range<Token>>();
    [javac]                                                  ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 100 errors

BUILD FAILED
/Users/harish/workspace/cassandra/build.xml:704: Compile failed; see the compiler error output for details.


{code}

I think the jar that I imported as part of unit tests has further some dependencies. After looking into some of the sources files I see that some files like ColumnFamilyStore.java have statements like 

{code}
import com.google.common.collect.*;
import org.apache.cassandra.dht.*;
{code}

It seems like if instead of importing ""*"" if we import the exact class, we will not run into this compilation failure. I would be happy to patch this if somebody could approve this?",,harishd,Normal,Resolved,Fixed,27/Feb/12 23:22,16/Apr/19 09:32
Bug,CASSANDRA-3967,12544364,Test fixes for Windows,"- SchemaLoader initializes CommitLog, creating empty segments; CleanupHelper then errors out trying to delete them (because they are still open)
- CFS.clearUnsafe resets the sstable generation but does not remove the sstables involved, so Windows will error out trying to rename over the existing ones",jbellis,jbellis,Low,Resolved,Fixed,28/Feb/12 04:42,16/Apr/19 09:32
Bug,CASSANDRA-3972,12544412,HintedHandoff fails to deliver any hints,"Summary says it all.  Whether in a memtable or sstable, no hints are delivered.",brandon.williams,brandon.williams,Urgent,Resolved,Fixed,28/Feb/12 12:35,16/Apr/19 09:32
Bug,CASSANDRA-3973,12544428,Pig CounterColumnFamily support,"Included a patch to the current test script to demonstrate that CassandraStorage does not support counter columns, as well as a patch to fix the issue.

The core reason is that CassandraStorage assumes that counters can be unpacked using CounterColumnType, when in fact the column value is already a Long.",jalkanen,jalkanen,Normal,Resolved,Fixed,28/Feb/12 15:08,16/Apr/19 09:32
Bug,CASSANDRA-3975,12544447,Hints Should Be Dropped When Missing CFid Implies Deleted ColumnFamily,"If hints have accumulated for a CF that has been deleted, Hinted Handoff repeatedly fails until manual intervention removes those hints. For 1.0.7, UnserializableColumnFamilyException is thrown only when a CFid is unknown on the sending node. As discussed on #cassandra-dev, if the schema is in agreement, the affected hint(s) should be deleted to avoid indefinite repeat failures.",jbellis,cherro,Normal,Resolved,Fixed,28/Feb/12 16:51,16/Apr/19 09:32
Bug,CASSANDRA-3981,12544643,Don't include original exception class name in CQL message,"In CreateColumnFamilyStatement, we do
{noformat}
catch (ConfigurationException e)
{
    throw new InvalidRequestException(e.toString());
}
{noformat}

This result in having the exception message looking like:
{noformat}
java.sql.SQLSyntaxErrorException: org.apache.cassandra.config.ConfigurationException: Cf1 already exists in keyspace Keyspace1
{noformat}",slebresne,slebresne,Low,Resolved,Fixed,29/Feb/12 16:33,16/Apr/19 09:32
Bug,CASSANDRA-3984,12544699,missing documents for caching in 1.1,add row cache and key cache setting documentation in CliHelp.yaml,vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,01/Mar/12 00:26,16/Apr/19 09:32
Bug,CASSANDRA-3985,12544758,Ensure a directory is selected for Compaction,"From http://www.mail-archive.com/user@cassandra.apache.org/msg20757.html

CompactionTask.execute() checks if there is a valid compactionFileLocation only if partialCompactionsAcceptable() . upgradesstables results in a CompactionTask with userdefined set, so the valid location check is not performed. 

The result is a NPE, partial stack 

{code:java}
$ nodetool -h localhost upgradesstables
Error occured while upgrading the sstables for keyspace MyKeySpace
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
        at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
        at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:995)
        at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1648)
<snip>
Caused by: java.lang.NullPointerException
        at java.io.File.<init>(File.java:222)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:641)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:652)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1888)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
{code}

(night time here, will fix tomorrow, anyone else feel free to fix it.)",jbellis,amorton,Low,Resolved,Fixed,01/Mar/12 10:38,16/Apr/19 09:32
Bug,CASSANDRA-3986,12544801,Cli shouldn't call FBU.getBroadcastAddress needlessly,"The cli is calling this, which causes yaml to be loaded, but the broadcast address isn't needed.

{code}
            // adding default data center from SimpleSnitch
            if (currentStrategyOptions == null || currentStrategyOptions.isEmpty())
            {
                SimpleSnitch snitch = new SimpleSnitch();
                Map<String, String> options = new HashMap<String, String>();
                options.put(snitch.getDatacenter(FBUtilities.getBroadcastAddress()), ""1"");

                ksDef.setStrategy_options(options);
            }
{code}

because SimpleSnitch always returns 'datacenter1'",brandon.williams,brandon.williams,Normal,Resolved,Fixed,01/Mar/12 16:19,16/Apr/19 09:32
Bug,CASSANDRA-3988,12544851,NullPointerException in org.apache.cassandra.service.AntiEntropyService when repair finds a keyspace with no CFs,"2012-03-01 21:38:09,039 [RMI TCP Connection(142)-10.253.106.21] INFO  StorageService - Starting repair command #15, repairing 3 ranges.
2012-03-01 21:38:09,039 [AntiEntropySessions:14] INFO  AntiEntropyService - [repair #d68369f0-63e6-11e1-0000-8add8b9398fd] new session: will sync /10.253.106.21, /10.253.106.248, /10.253.106.247 on range (85070591730234615865843651857942052864,106338239662793269832304564822427566080] for PersonalizationDataService2.[]
2012-03-01 21:38:09,039 [AntiEntropySessions:14] ERROR AbstractCassandraDaemon - Fatal exception in thread Thread[AntiEntropySessions:14,5,RMI Runtime]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:691)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",slebresne,wdhathaway,Low,Resolved,Fixed,01/Mar/12 22:32,16/Apr/19 09:32
Bug,CASSANDRA-3989,12544881,nodetool cleanup/scrub/upgradesstables promotes all sstables to next level (LeveledCompaction),"1.0.7 + LeveledCompactionStrategy
If you run nodetool cleanup, scrub, or upgradesstables, Cassandra execute compaction for each sstable. During the compaction, it put the new sstable to next level of the original sstable. If you run cleanup many times, sstables will reached to the highest level, and CASSANDRA-3608 will happens at next cleanup.

Reproduce procedure:
# create column family CF1 with compaction_strategy=LeveledCompactionStrategy and compaction_strategy_options={sstable_size_in_mb: 5};
# Insert some data into CF1.
# nodetool flush
# Verify the sstable is created at L1 in CF1.json
# nodetool cleanup
# Verify sstable in L1 is removed and new sstable is created at L2 in CF1.json
# repeat nodetool cleanup some times",slebresne,makiw,Low,Resolved,Fixed,02/Mar/12 03:21,16/Apr/19 09:32
Bug,CASSANDRA-3993,12545084,JavaDoc fix for org.apache.cassandra.db.filter.QueryFilter,@param should be on separate line,darkdimius,darkdimius,Low,Resolved,Fixed,04/Mar/12 10:21,16/Apr/19 09:32
Bug,CASSANDRA-3996,12545088,Keys index skips results,"While scanning results page if range index meets result already seen in previous result set it decreases columnsRead that causes next iteration to treat columsRead<rowsPerQuery as if last page was not full and scan is done.
",xedin,darkdimius,Normal,Resolved,Fixed,04/Mar/12 11:03,16/Apr/19 09:32
Bug,CASSANDRA-4000,12545192,Division by zero on get_slice,"We have a column family with String row keys and Long column keys.

Our WideEntityService is trying to get the first column in the range from 0 to Long.MAX. It's a batch operation performed for every row in the CF (rows count is approximately tens of thousands and each row contains from 0 to 1000 columns). 

After processing each row we are removing some of the columns we have queried. Also, at the same time we are writing in this CF in another threads but somewhat less intensive.

An error rises approximately for a one of 100 rows.


Exception itself:
[05-Mar-2012 18:47:25,247] ERROR [http-8095-1 WideEntityServiceImpl.java:142] - get: key1 - {type=RANGE, start=0, end=9223372036854775807, orderDesc=false, limit=1}
me.prettyprint.hector.api.exceptions.HCassandraInternalException: Cassandra encountered an internal error processing this request: TApplicationError type: 6 message:Internal error processing get_slice
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:31)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:285)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:268)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:233)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:131)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getSlice(KeyspaceServiceImpl.java:289)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery$1.doInKeyspace(ThriftSliceQuery.java:53)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery$1.doInKeyspace(ThriftSliceQuery.java:49)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery.execute(ThriftSliceQuery.java:48)",slebresne,vanger,Normal,Resolved,Fixed,05/Mar/12 15:45,16/Apr/19 09:32
Bug,CASSANDRA-4003,12545279,cqlsh still failing to handle decode errors in some column names,"Columns which are expected to be text, but which are not valid utf8, cause cqlsh to display an error and not show any output:

{noformat}
cqlsh:ks> CREATE COLUMNFAMILY test (a text PRIMARY KEY) WITH comparator = timestamp;
cqlsh:ks> INSERT INTO test (a, '2012-03-05') VALUES ('val1', 'val2');
cqlsh:ks> ASSUME test NAMES ARE text;
cqlsh:ks> select * from test;
'utf8' codec can't decode byte 0xe1 in position 4: invalid continuation byte
{noformat}

the traceback with cqlsh --debug:

{noformat}
Traceback (most recent call last):
  File ""bin/cqlsh"", line 581, in onecmd
    self.handle_statement(st)
  File ""bin/cqlsh"", line 606, in handle_statement
    return custom_handler(parsed)
  File ""bin/cqlsh"", line 663, in do_select
    self.perform_statement_as_tokens(parsed.matched, decoder=decoder)
  File ""bin/cqlsh"", line 666, in perform_statement_as_tokens
    return self.perform_statement(cqlhandling.cql_detokenize(tokens), decoder=decoder)
  File ""bin/cqlsh"", line 693, in perform_statement
    self.print_result(self.cursor)
  File ""bin/cqlsh"", line 728, in print_result
    self.print_static_result(cursor)
  File ""bin/cqlsh"", line 742, in print_static_result
    formatted_names = map(self.myformat_colname, colnames)
  File ""bin/cqlsh"", line 413, in myformat_colname
    wcwidth.wcswidth(name.decode(self.output_codec.name)))
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xe1 in position 4: invalid continuation byte
{noformat}",thepaul,thepaul,Low,Resolved,Fixed,06/Mar/12 04:55,16/Apr/19 09:32
Bug,CASSANDRA-4013,12545613,WARN No appenders could be found for logger (org.apache.cassandra.confi,"i'm installed apache-cassandra-1.1.0-beta1 version.
so, modify window7Config to Basic Linux Config 
and i play cmd mode....
write ""cassandra-cli"" and entered.
so i write ""create keyspace keyspace1;""
but not created. output is error....
what the... help...me...

and.. where is keyspace config?
different 0.6.8version to 1.1.0version
xml....to yaml?
 please send to mail ..
xyzxes@nate.com...
C:\apache-cassandra-1.1.0-beta1\bin>cassandra-cli
Starting Cassandra Client
Connected to: ""Test Cluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 1.1.0-beta1

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace keyspace1;
log4j:WARN No appenders could be found for logger (org.apache.cassandra.confi
atabaseDescriptor).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more
fo.
Cannot locate cassandra.yaml
Fatal configuration error; unable to start serv",,leekh,Normal,Resolved,Fixed,08/Mar/12 06:05,16/Apr/19 09:32
Bug,CASSANDRA-4019,12545635,java.util.ConcurrentModificationException in Gossiper,"I have never seen this one before. Might be triggered by a race condition under heavy load. This error was triggered on 0.8.9


ERROR [GossipTasks:1] 2012-03-05 04:16:55,263 Gossiper.java (line 162) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.utils.AbstractStatsDeque.sum(AbstractStatsDeque.java:37)
        at org.apache.cassandra.utils.AbstractStatsDeque.mean(AbstractStatsDeque.java:60)
        at org.apache.cassandra.gms.ArrivalWindow.mean(FailureDetector.java:259)
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:282)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:155)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:538)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:157)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [GossipStage:1] 2012-03-05 04:16:55,263 Gossiper.java (line 737) Node /192.168.3.18 has restarted, now UP again
 INFO [GossipStage:1] 2012-03-05 04:16:55,264 Gossiper.java (line 705) InetAddress /192.168.3.18 is now UP
",brandon.williams,tbritz,Low,Resolved,Fixed,08/Mar/12 10:46,16/Apr/19 09:32
Bug,CASSANDRA-4021,12545682,CFS.scrubDataDirectories tries to delete nonexistent orphans,"The check only looks for a missing data file, then deletes all other components, however it's possible for the data file and another component to be missing, causing an error:

{noformat}

 WARN 17:19:28,765 Removing orphans for /var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-24492: [Index.db, Filter.db, Digest.sha1, Statistics.db, Data.db]
ERROR 17:19:28,766 Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:357)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:352)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:105)
java.lang.AssertionError: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:357)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:352)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:105)
Exception encountered during startup: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
{noformat}",yulinyen,brandon.williams,Low,Resolved,Fixed,08/Mar/12 17:22,16/Apr/19 09:32
Bug,CASSANDRA-4022,12545687,Compaction of hints can get stuck in a loop,"Not exactly sure how I caused this as I was working on something else in trunk, but:

{noformat}
 INFO 17:41:35,682 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-339-Data.db')]
 INFO 17:41:36,430 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-340-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 5.912220MB/s.  Time: 748ms.
 INFO 17:41:36,431 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-340-Data.db')]
 INFO 17:41:37,238 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-341-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 5.479976MB/s.  Time: 807ms.
 INFO 17:41:37,239 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-341-Data.db')]
 INFO 17:41:38,163 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-342-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.786083MB/s.  Time: 924ms.
 INFO 17:41:38,164 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-342-Data.db')]
 INFO 17:41:39,014 GC for ParNew: 274 ms for 1 collections, 541261288 used; max is 1024458752
 INFO 17:41:39,151 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-343-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.485132MB/s.  Time: 986ms.
 INFO 17:41:39,151 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-343-Data.db')]
 INFO 17:41:40,016 GC for ParNew: 308 ms for 1 collections, 585582200 used; max is 1024458752
 INFO 17:41:40,200 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-344-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.223821MB/s.  Time: 1,047ms.
 INFO 17:41:40,201 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-344-Data.db')]
 INFO 17:41:41,017 GC for ParNew: 252 ms for 1 collections, 617877904 used; max is 1024458752
 INFO 17:41:41,178 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-345-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.526449MB/s.  Time: 977ms.
 INFO 17:41:41,179 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-345-Data.db')]
 INFO 17:41:41,885 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-346-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 6.263938MB/s.  Time: 706ms.
 INFO 17:41:41,887 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-346-Data.db')]
 INFO 17:41:42,617 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-347-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 6.066311MB/s.  Time: 729ms.
 INFO 17:41:42,618 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-347-Data.db')]
 INFO 17:41:43,376 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-348-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 5.834222MB/s.  Time: 758ms.
 INFO 17:41:43,377 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-348-Data.db')]
 INFO 17:41:44,307 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-349-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 4.760323MB/s.  Time: 929ms.
 INFO 17:41:44,308 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-349-Data.db')]
 INFO 17:41:45,021 GC for ParNew: 245 ms for 1 collections, 731287832 used; max is 1024458752
 INFO 17:41:45,316 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-350-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 4.395965MB/s.  Time: 1,006ms.
 INFO 17:41:45,316 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-350-Data.db')]
 INFO 17:41:46,022 GC for ParNew: 353 ms for 1 collections, 757476872 used; max is 1024458752
 INFO 17:41:46,451 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-351-Data.db,].  4,637,160 to 4,637
{noformat}

I suspect we broke something subtle in CASSANDRA-3955.",yukim,brandon.williams,Urgent,Resolved,Fixed,08/Mar/12 17:46,16/Apr/19 09:32
Bug,CASSANDRA-4025,12545704,org.apache.cassandra.io.util.FileUtils.delete(List<String>) only deletes every second file in the list,The above mentioned function does not work as expected. I fixed it by iterating over the list in reverse.,,mschuetz,Low,Resolved,Fixed,08/Mar/12 19:48,16/Apr/19 09:32
Bug,CASSANDRA-4026,12545730,EC2 snitch incorrectly reports regions,"Currently the org.apache.cassandra.locator.Ec2Snitch reports ""us-west"" in both the oregon and the california data centers.  This is incorrect, since they are different regions.

California => us-west-1
Oregon     => us-west-2

wget http://169.254.169.254/latest/meta-data/placement/availability-zone returns the value ""us-west-2a""


After parsing this returns

DC = us-west Rack = 2a


What it should return

DC = us-west-2 Rack = a


This makes it possible to use multi region when both regions are in the west coast.
",vijay2win@yahoo.com,tnine,Normal,Resolved,Fixed,08/Mar/12 23:26,16/Apr/19 09:32
Bug,CASSANDRA-4031,12545840,Exceptions during inserting emtpy string as column value on indexed column,"Hi,
I`m running one node cluster(issue occurs also on other cluster(which has 2 nodes)) on snapshot from cassandra-1.1 branch(i used 449e037195c3c504d7aca5088e8bc7bd5a50e7d0 commit).
i have simple CF, definition of TestCF:
{noformat}
[default@test_keyspace] describe Test_CF;
    ColumnFamily: Test_CF
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: [Test_CF.Test_CF_test_index_idx]
      Column Metadata:
        Column Name: test_index
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: Test_CF_test_index_idx
          Index Type: KEYS
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor
{noformat}
I`m trying to add new row(log from cassandra-cli, note that there is index on test_index):
{noformat}
[default@test_keyspace] list Test_CF;              
Using default limit of 100

0 Row Returned.
Elapsed time: 31 msec(s).
[default@test_keyspace] set Test_CF[absdsad3][test_index]='';
null
TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15906)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:788)
	at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:772)
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:894)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:211)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
[default@test_keyspace] list Test_CF;                        
Using default limit of 100
-------------------
RowKey: absdsad3
=> (column=test_index, value=, timestamp=1331298173009000)

1 Row Returned.
Elapsed time: 7 msec(s).
{noformat}
Exception from system.log:
{noformat}
 INFO [FlushWriter:56] 2012-03-09 13:42:02,500 Memtable.java (line 291) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-3251-Data.db (2077 bytes)
ERROR [MutationStage:2291] 2012-03-09 13:42:22,232 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:2291,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
        at org.apache.cassandra.db.index.SecondaryIndexManager.getIndexKeyFor(SecondaryIndexManager.java:294)
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:490)
        at org.apache.cassandra.db.Table.apply(Table.java:441)
        at org.apache.cassandra.db.Table.apply(Table.java:366)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:275)
        at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:446)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1228)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",yukim,mdymarek,Normal,Resolved,Fixed,09/Mar/12 13:09,16/Apr/19 09:32
Bug,CASSANDRA-4032,12545857,"memtable.updateLiveRatio() is blocking, causing insane latencies for writes","Reproduce by just starting a fresh cassandra with a heap large enough for live ratio calculation (which is {{O(n)}}) to be insanely slow, and then running {{./bin/stress -d host -n100000000 -t10}}. With a large enough heap and default flushing behavior this is bad enough that stress gets timeouts.

Example (""blocked for"" is my debug log added around submit()):

{code}
 INFO [MemoryMeter:1] 2012-03-09 15:07:30,857 Memtable.java (line 198) CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 8.89014894083727 (just-counted was 8.89014894083727).  calculation took 28273ms for 1320245 columns
 WARN [MutationStage:8] 2012-03-09 15:07:30,857 Memtable.java (line 209) submit() blocked for: 231135
{code}

The calling code was written assuming a RejectedExecutionException is thrown, but it's not because {{DebuggableThreadPoolExecutor}} installs a blocking rejection handler.
",scode,scode,Normal,Resolved,Fixed,09/Mar/12 15:10,16/Apr/19 09:32
Bug,CASSANDRA-4033,12545883,cqlsh: double wide unicode chars cause incorrect padding in select output,"{code}
CREATE COLUMNFAMILY cf3 (KEY text primary key);
INSERT INTO cf3 (KEY, col1, col2) VALUES ('a', '1234 1234 1234 1234', 'abcd');
INSERT INTO cf3 (KEY, col1, col2) VALUES ('b', '愛愛愛愛 愛愛愛愛 愛愛愛愛 愛愛愛愛', 'abcd');
SELECT * FROM cf3 WHERE key in ('a', 'b');
{code}
produces this output:
{code}
 KEY | col1                                                | col2
-----+-----------------------------------------------------+------
   a |                                 1234 1234 1234 1234 | abcd
   b |                       愛愛愛愛 愛愛愛愛 愛愛愛愛 愛愛愛愛 | abcd
{code}
note the extra spaces before the ""love"" glyphs.",thepaul,tpatterson,Low,Resolved,Fixed,09/Mar/12 17:59,16/Apr/19 09:32
Bug,CASSANDRA-4042,12546270,"add ""caching"" to CQL CF options","""Caching"" option is missing from CQL ColumnFamily options.",slebresne,xedin,Low,Resolved,Fixed,13/Mar/12 18:09,16/Apr/19 09:32
Bug,CASSANDRA-4044,12546319,examples/simple_authenticator README doesn't mention JVM args,"To use SimpleAuthenticator/SimpleAuthority, you need to use the {{-Dpasswd.properties=conf/passwd.properties}} and {{-Daccess.properties=conf/access.properties}} java args.  The README doesn't mention this anywhere.",thobbs,thobbs,Low,Resolved,Fixed,13/Mar/12 22:25,16/Apr/19 09:32
Bug,CASSANDRA-4045,12546321,BOF fails when some nodes are down,"As the summary says, we should allow jobs to complete when some targets are unavailable.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,13/Mar/12 22:39,16/Apr/19 09:32
Bug,CASSANDRA-4046,12546323,"when creating keyspace with simple strategy, it should only acception ""replication_factor"" as an option","currently I could do this:

{panel}
[default@unknown] create keyspace test
...     with placement_strategy = 'SimpleStrategy'
...     and strategy_options = \{DC : testdc, replication_factor :1\};
ebc5f430-6d47-11e1-0000-edee3ea2cbff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] 
{panel}

while i don't think this creates any ""problem"" in terms of the actual replication being used for the CL , we probably should acknowledge to the user that ""DC : testdc"" is not an valid option for the SimpleStrategy.",dbrosius,cywjackson,Low,Resolved,Fixed,13/Mar/12 23:01,16/Apr/19 09:32
Bug,CASSANDRA-4050,12546471,Rewrite RandomAccessReader to use FileChannel / nio to address Windows file access violations,"On Windows w/older java I/O libraries the files are not opened with FILE_SHARE_DELETE.  This causes problems as hard-links cannot be deleted while the original file is opened - our snapshots are a big problem in particular.  The nio library and FileChannels open with FILE_SHARE_DELETE which should help remedy this problem.

Original text:
I'm using Cassandra 1.0.8, on Windows 7.  When I take a snapshot of the database, I find that I am unable to delete the snapshot directory (i.e., dir named ""{datadir}\{keyspacename}\snapshots\{snapshottag}"") while Cassandra is running:  ""The action can't be completed because the folder or a file in it is open in another program.  Close the folder or file and try again"" [in Windows Explorer].  If I terminate Cassandra, then I can delete the directory with no problem.

I expect to be able to move or delete the snapshotted files while Cassandra is running, as this should not affect the runtime operation of Cassandra.",JoshuaMcKenzie,jn,Low,Resolved,Fixed,14/Mar/12 20:22,16/Apr/19 09:32
Bug,CASSANDRA-4051,12546493,Stream sessions can only fail via the FailureDetector,"If for some reason, FileStreamTask itself fails more than the number of retry attempts but gossip continues to work, the stream session will never be closed.  This is unlikely to happen in practice since it requires blocking the storage port from new connections but keeping the existing ones, however for the bulk loader this is especially problematic since it doesn't have access to a failure detector and thus no way of knowing if a session failed.",yukim,brandon.williams,Normal,Resolved,Fixed,14/Mar/12 22:37,16/Apr/19 09:32
Bug,CASSANDRA-4053,12546556,IncomingTcpConnection can not be closed when the peer is brutaly terminated or switch is failed,"IncomingTcpConnection has no way to detect the peer is down when the peer meets power loss or the network infrastructure is failed, and the thread is leaked...

For safety, as least SO_KEEPALIVE should be set on those IncomingTcpConnections. The better way is to close the incoming connections when failure detector notifies the peer failure, but it requires some extra bookmarking.

Besides it, it would be better if IncomingTcpConnection and OutgoingTcpConnection is marked as daemon thread...

",marcuse,hanzhu,Normal,Resolved,Fixed,15/Mar/12 10:09,16/Apr/19 09:32
Bug,CASSANDRA-4054,12546566,SStableImport and SStableExport does not serialize row level deletion,SSTableImport and SSTableExport does not serialize/de-serialize the row-level deletion info to/from the json file. This brings back the deleted data after restore from the json file.,dr-alves,hanzhu,Low,Resolved,Fixed,15/Mar/12 10:47,16/Apr/19 09:32
Bug,CASSANDRA-4055,12546662,Hector RetryService drop host,,,daningaddr,Urgent,Resolved,Fixed,15/Mar/12 21:19,16/Apr/19 09:32
Bug,CASSANDRA-4056,12546699,[patch] guard against npe due to null sstable,SSTableIdentityIterator ctor can be called from sibling ctor with a null sstable. So catch block's markSuspect should be npe guarded.,dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,16/Mar/12 00:48,16/Apr/19 09:32
Bug,CASSANDRA-4061,12546908,Decommission should take a token,"Like removetoken, decom should take a token parameter.  This is a bit easier said than done because it changes gossip, but I've seen enough people burned by this (as I have myself.)  In the short term though *decommission still accepts a token parameter* which I thought we had fixed.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,17/Mar/12 17:52,16/Apr/19 09:32
Bug,CASSANDRA-4065,12547162,Bogus MemoryMeter liveRatio calculations,"I get strange cfs.liveRatios.

A couple of mem meter runs seem to calculate bogus results: 

{noformat}
Tue 09:14:48 dd@blnrzh045:~$ grep 'setting live ratio to maximum of 64 instead of' /var/log/cassandra/system.log
 WARN [MemoryMeter:1] 2012-03-20 08:08:07,253 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:09,160 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:13,274 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:22,032 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:12:41,057 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 67.11787351054079
 WARN [MemoryMeter:1] 2012-03-20 08:13:50,877 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 112.58547951925435
 WARN [MemoryMeter:1] 2012-03-20 08:15:29,021 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 193.36945063589877
 WARN [MemoryMeter:1] 2012-03-20 08:17:50,716 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 348.45008340969434
{noformat}

Because meter runs never decrease liveRatio in Memtable (Which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?):

{noformat}
cfs.liveRatio = Math.max(cfs.liveRatio, newRatio);
{noformat}

Memtables are flushed every couple of secs:

{noformat}
ColumnFamilyStore.java (line 712) Enqueuing flush of Memtable-BlobStore@935814661(1874540/149963200 serialized/live bytes, 202 ops)
{noformat}

Even though a saner liveRatio has been calculated after the bogus runs:

{noformat}
INFO [MemoryMeter:1] 2012-03-20 08:19:55,934 Memtable.java (line 198) CFS(Keyspace='SmeetBlob', ColumnFamily='BlobStore') 
   liveRatio is 64.0 (just-counted was 2.97165811895841).  calculation took 124ms for 58 columns
{noformat}",doubleday,doubleday,Low,Resolved,Fixed,20/Mar/12 08:39,16/Apr/19 09:32
Bug,CASSANDRA-4070,12547350,CFS.setMaxCompactionThreshold doesn't allow 0 unless min is also 0,"Thrift allows to set the max compaction threshold to 0 to disable compaction. However, CFS.setMaxCompactionThreshold throws an exception min > max even if max is 0.

Note that even if someone sets 0 for both the min and max thresholds, we'll can have a problem because SizeTieredCompaction calls CFS.setMaxCompactionThreshold before calling CFS.setMinCompactionThreshold and thus will trigger the RuntimeException when it shouldn't.",slebresne,slebresne,Low,Resolved,Fixed,21/Mar/12 08:41,16/Apr/19 09:32
Bug,CASSANDRA-4074,12547710,cqlsh: Tab completion should not suggest consistency level ANY for select statements,consistency level ANY should not be suggested in tab-completion for SELECT statements,thepaul,tpatterson,Low,Resolved,Fixed,22/Mar/12 23:30,16/Apr/19 09:32
Bug,CASSANDRA-4077,12547767,ScrubTest failing on current 1.1.0 branch,"I get the following error:
{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 09:53:05,979 Corrupt sstable build/test/cassandra/data/Keyspace1/Super5/Keyspace1-Super5-f-2; skipped
    [junit] java.io.EOFException
    [junit] 	at java.io.DataInputStream.readFully(DataInputStream.java:180)
    [junit] 	at java.io.DataInputStream.readLong(DataInputStream.java:399)
    [junit] 	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.deserialize(ReplayPosition.java:133)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableMetadata$SSTableMetadataSerializer.deserialize(SSTableMetadata.java:206)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableMetadata$SSTableMetadataSerializer.deserialize(SSTableMetadata.java:194)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:155)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:483)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:86)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
    [junit] 	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testScrubFile(org.apache.cassandra.db.ScrubTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:87)
{noformat}",slebresne,slebresne,Normal,Resolved,Fixed,23/Mar/12 09:03,16/Apr/19 09:32
Bug,CASSANDRA-4078,12547796,StackOverflowError when upgrading to 1.0.8 from 0.8.10,"Hello

I am trying to upgrade our 1-node setup from 0.8.10 to 1.0.8 and seeing the following exception when starting up 1.0.8.  We have been running 0.8.10 without any issues.
 
Attached is the entire log file during startup of 1.0.8.  There are 2 exceptions:

1. StackOverflowError (line 2599)
2. InstanceAlreadyExistsException (line 3632)

I tried ""run scrub"" under 0.8.10 first, it did not help.  Also, I tried dropping the column family which caused the exception, it just got the same exceptions from another column family.

Thanks
",thepaul,wenjun@openf.in,Low,Resolved,Fixed,23/Mar/12 13:44,16/Apr/19 09:32
Bug,CASSANDRA-4081,12547942,"Issue with cassandra-cli ""assume"" command and custom types","There seems to be an issue with cassandra-cli's assume command with a custom type. I get ""Syntax error at position 35: missing EOF at '.'""

To make sure the issue is not with my custom type, I tried it with the built-in BytesType and got the same error:

[default@test] assume UserDetails validator as org.apache.cassandra.db.marshal.BytesType;
Syntax error at position 35: missing EOF at '.'

I also tried it with single and double quotes with no success:
[default@test] assume UserDetails validator as 'org.apache.cassandra.db.marshal.BytesType';
Syntax error at position 32: mismatched input ''org.apache.cassandra.db.marshal.BytesType'' expecting Identifier


Based on the output of ""help assume"" I should be able to just pass a fqn of a class.

> It is also valid to specify the fully-qualified class name to a class that
> extends org.apache.Cassandra.db.marshal.AbstractType.
",xedin,drew_kutchar,Normal,Resolved,Fixed,24/Mar/12 07:05,16/Apr/19 09:32
Bug,CASSANDRA-4086,12548228,decom should shut thrift down,"If you decom a node an then try to use it, you get nothing but timeouts.  Instead let's just kill thrift so intelligent clients can move along.",brandon.williams,brandon.williams,Low,Resolved,Fixed,27/Mar/12 02:12,16/Apr/19 09:32
Bug,CASSANDRA-4089,12548263,Typo fix: key_valiation_class -> key_validation_class,There is a typo in the Cli help docs for the update column family command.,,ash211,Low,Resolved,Fixed,27/Mar/12 06:32,16/Apr/19 09:32
Bug,CASSANDRA-4090,12548273,cqlsh can't handle python being a python3,"cqlsh fails to run when {{python}} is a Python 3, with this error message:

{code}
andrew@spite:~/src/cassandra-trunk/bin $ ./cqlsh 
  File ""./cqlsh"", line 97
    except ImportError, e:
                      ^
SyntaxError: invalid syntax
andrew@spite:~/src/cassandra-trunk/bin $ 
{code}

The error occurs because the cqlsh script checks for a default installation of python that is older than a certain version, but not one newer that is incompatible (e.g. Python3).  To fix this, I update the logic to only run {{python}} if it's a version at least 2.5 but before 3.0  If this version of python is in that range then role with it, otherwise try python2.6, python2.7, then python2.5 (no change from before).

This is working on my installation, where {{python}} executes python 3.2.2 and doesn't break backwards compatibility to distributions that haven't made the jump to Python3 as default yet.",ash211,ash211,Low,Resolved,Fixed,27/Mar/12 07:34,16/Apr/19 09:32
Bug,CASSANDRA-4093,12548388,schema_* CFs do not respect column comparator which leads to CLI commands failure.,"ColumnDefinition.{ascii, utf8, bool, ...} static methods used to initialize schema_* CFs column_metadata do not respect CF comparator and use ByteBufferUtil.bytes(...) for column names which creates problems in CLI and probably in other places.

The CompositeType validator throws exception on first column

String columnName = columnNameValidator.getString(columnDef.name);

Because it appears the composite type length header is wrong (25455)

AbstractCompositeType.getWithShortLength

java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:247)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:50)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:59)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:139)
	at org.apache.cassandra.cli.CliClient.describeColumnFamily(CliClient.java:2046)
	at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1969)
	at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1574)

(seen in trunk)",slebresne,dbrosius@apache.org,Normal,Resolved,Fixed,27/Mar/12 20:59,16/Apr/19 09:32
Bug,CASSANDRA-4095,12548427,Internal error processing get_slice (NullPointerException),"I get this pretty regularly.  It seems to happen transiently on multiple nodes in my cluster, every so often, and goes away.


ERROR [Thrift:45] 2012-03-26 19:59:12,024 Cassandra.java (line 3041) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:76)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:724)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:283)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:365)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:326)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



The line in question is (I think) the one below, so it looks like the column family reference for a row can sometimes be null?

    int liveColumnsInRow = row != null ? row.cf.getLiveColumnCount() : 0;



Here is my column family (on 1.0.8):

    ColumnFamily: WorkQueue (Super)
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type/org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 0.0/0
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      Replicate on write: false
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
",jbellis,jlaban,Low,Resolved,Fixed,28/Mar/12 03:09,16/Apr/19 09:32
Bug,CASSANDRA-4096,12548428,mlockall() returned code is ignored w/o assertions,"We log that mlockall() was successful only based on the lack of an assertion failure, so for anyone running w/o {{-ea}} we are lying about mlockall() succeeding.",jbellis,scode,Low,Resolved,Fixed,28/Mar/12 03:13,16/Apr/19 09:32
Bug,CASSANDRA-4099,12548549,IncomingTCPConnection recognizes from by doing socket.getInetAddress() instead of BroadCastAddress,"change ""this.from = socket.getInetAddress()"" to understand the broad cast IP, but the problem is we dont know until the first packet is received, this ticket is to work around the problem until it reads the first packet.",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,28/Mar/12 20:54,16/Apr/19 09:32
Bug,CASSANDRA-4100,12548569,Make scrub and cleanup operations throttled,Looks like scrub and cleanup operations are not throttled and it will be nice to throttle else we are likely to run into IO issues while running it on live cluster.,vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,28/Mar/12 22:42,16/Apr/19 09:32
Bug,CASSANDRA-4106,12549178,http link is broken in cassandra-env.sh,"# jmx: metrics and administration interface
#
# add this if you're having trouble connecting:
# JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=<public name>""
#
# see
# http://blogs.sun.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole
# for more on configuring JMX through firewalls, etc. (Short version:
# get it working with no firewall first.)


link to https://blogs.sun.com leads to 404",,chipitsine,Low,Resolved,Fixed,02/Apr/12 16:21,16/Apr/19 09:32
Bug,CASSANDRA-4111,12549251,Serializing cache can cause Segfault in 1.1,"Rare but this can happen per sure, looks like this issue is after CASSANDRA-3862 hence affectes only 1.1

        FreeableMemory old = map.get(key);
        if (old == null)
            return false;

        // see if the old value matches the one we want to replace
        FreeableMemory mem = serialize(value);
        if (mem == null)
            return false; // out of memory.  never mind.
        V oldValue = deserialize(old);
        boolean success = oldValue.equals(oldToReplace) && map.replace(key, old, mem);

        if (success)
            old.unreference();
        else
            mem.unreference();
        return success;

in the above code block we deserialize(old) without taking reference to the old memory, this can case seg faults when the old is reclaimed (free is called)
Fix is to get the reference just for deserialization

        V oldValue;
        // reference old guy before de-serializing
        old.reference();
        try
        {
             oldValue = deserialize(old);
        }
        finally
        {
            old.unreference();
        }",vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,02/Apr/12 23:29,16/Apr/19 09:32
Bug,CASSANDRA-4112,12549262,nodetool cleanup giving exception,"We just recently started using version 1.0.9, previously we were using tiered compaction because of a bug in 1.0.8 (not letting us use leveled compaction) and now since moving to 1.0.9 we have started using leveled compaction.

Trying to do a cleanup we are getting the following exception:

root@test:~# nodetool -h localhost cleanup 
Error occured during cleanup
java.util.concurrent.ExecutionException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:204)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:240)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:988)
        at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1639)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.NoSuchElementException
        at java.util.ArrayList$Itr.next(ArrayList.java:757)
        at org.apache.cassandra.db.compaction.LeveledManifest.replace(LeveledManifest.java:196)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:147)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:495)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1010)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:802)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:244)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:183)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)

cheers,
Shoaib",jbellis,shoaibmir,Normal,Resolved,Fixed,03/Apr/12 00:18,16/Apr/19 09:32
Bug,CASSANDRA-4113,12549357,cqlsh does not work with piping,"When I run cqlsh under ant like this
	<target name=""create-test-db"">
		<exec executable=""../dsc-cassandra-1.0.8/bin/cqlsh"" input=""./resource/test/test.cql"">  
		</exec>
	</target>
I got: 
     [exec] Traceback (most recent call last):
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 1891, in <module>
     [exec]     main(*read_options(sys.argv[1:], os.environ))
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 1863, in main
     [exec]     completekey=options.completekey)
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 384, in __init__
     [exec]     self.output_codec = codecs.lookup(encoding)
     [exec] TypeError: must be string, not None
     [exec] Result: 1

I suggest replacement of:
        if encoding is None:
            encoding = sys.stdout.encoding
to: 
       if encoding is None:
           encoding=locale.getpreferredencoding()
",thepaul,pavel1,Normal,Resolved,Fixed,03/Apr/12 16:18,16/Apr/19 09:32
Bug,CASSANDRA-4114,12549455,Default read_repair_chance value is wrong,"The documents says that the default read_repair_chance value is 0.1, and it is also declared so in CFMetaDeta. However, when creating a column family with ""create column family foo"" via cli and checking with ""show keyspaces"" shows that the read_repair_chance=1.0. This also happens when creating the column family through Hector.

Going through the code, I find that in CfDef class, the constructor without any parameters sets the read_repair_chance to 1. Changing this value to 0.1 seems to create a column family with the 0.1 read_repair_chance. The best might be to remove it from the CfDef as the read_repair_chance is set to the default value in CFMetaDeta.",jbellis,mkmainali,Low,Resolved,Fixed,04/Apr/12 07:20,16/Apr/19 09:32
Bug,CASSANDRA-4115,12549543,UNREACHABLE schema after decommissioning a non-seed node,"decommission a non-seed node, sleep 30 seconds, then use thrift to check the schema. UNREACHABLE is listed:

{'75dc4c07-3c1a-3013-ad7d-11fb34208465': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}",brandon.williams,tpatterson,Urgent,Resolved,Fixed,04/Apr/12 17:43,16/Apr/19 09:32
Bug,CASSANDRA-4116,12549563,check most recent TS values in SSTables when a row tombstone has already been encountered,"once C* comes across a row tombstone, C* should check the TS on the tombstone against all SSTables.  If the most recent TS in an SST is older than the row tombstone, that entire SST (or the remainder of it) can be safely ignored.

There are two drivers for this.

* avoid checking column values that could not possibly be in the result set

* avoid OOMing because all the tombstones are temporarily kept in memory.",samt,mdennis,Normal,Resolved,Fixed,04/Apr/12 20:27,16/Apr/19 09:32
Bug,CASSANDRA-4128,12549798,stress tool hangs forever on timeout or error,"The stress tool hangs forever if it encounters a timeout or exception. CTRL-C will kill it if run from a terminal, but when running it from a script (like a dtest) it hangs the script forever. It would be great for scripting it if a reasonable error code was returned when things go wrong.

To duplicate, clear out /var/lib/cassandra and then run ""stress --operation=READ"".",xedin,tpatterson,Low,Resolved,Fixed,05/Apr/12 21:42,16/Apr/19 09:32
Bug,CASSANDRA-4129,12549881,Cannot create keyspace with specific keywords through cli,"Keyspaces cannot be create when the keyspace name which are used as keywords in the cli, such as 'keyspace', 'family' etc., through CLI. Even when surrounding the keyspace with quotation does not solve the problem. However, such keyspaces can be created through other client such as Hector.

This is similar to the issue CASSANDRA-3195, in which the column families could not be created. Similar to the solution of CASSANDRA-3195, using String keyspaceName = CliUtil.unescapeSQLString(statement.getChild(0).getText()) in executeAddKeySpace would solve the problem. ",xedin,mkmainali,Low,Resolved,Fixed,06/Apr/12 13:42,16/Apr/19 09:32
Bug,CASSANDRA-4136,12550441,get_paged_slices doesn't reset startColumn after first row,"As an example, consider the WordCount example (see CASSANDRA-3883).  WordCountSetup inserts 1000 rows, each with three columns: text3, text4, int1.  (Some other miscellaneous columns are inserted in a few rows, but we can ignore them here.)

Paging through with get_paged_slice calls with a count of 99, CFRecordReader will first retrieve 33 rows, the last of which we will call K.  Then it will attempt to fetch 99 more columns, starting with row K column text4.

The bug is that it will only fetch text4 for *each* subsequent row K+i, instead of returning (K, text4), (K+1, int1), (K+1, int3), (K+1, text4), etc.",slebresne,jbellis,Urgent,Resolved,Fixed,10/Apr/12 21:15,16/Apr/19 09:32
Bug,CASSANDRA-4141,12550607,Looks like Serializing cache broken in 1.1,"I get the following error while setting the row cache to be 1500 MB

INFO 23:27:25,416 Initializing row cache with capacity of 1500 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid26402.hprof ...

havent spend a lot of time looking into the issue but looks like SC constructor has 

.initialCapacity(capacity)
.maximumWeightedCapacity(capacity)

 which 1500Mb",vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,12/Apr/12 00:00,16/Apr/19 09:32
Bug,CASSANDRA-4145,12550785,NullPointerException when using sstableloader with PropertyFileSnitch configured,"I got a NullPointerException when using sstableloader on 1.0.6. The cluster is using PropertyFileSnitch. The same configuration file is used for sstableloader. 

The problem is if StorageService is initialized before DatabaseDescriptor, PropertyFileSnitch will try to access StorageService.instance before it finishes initialization.


{code}
 ERROR 01:14:05,601 Fatal configuration error
org.apache.cassandra.config.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.PropertyFileSnitch'.
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:607)
        at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:454)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:306)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:187)
        at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:190)
        at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:183)
        at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:106)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:62)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:589)
        ... 7 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.locator.PropertyFileSnitch.reloadConfiguration(PropertyFileSnitch.java:170)
        at org.apache.cassandra.locator.PropertyFileSnitch.<init>(PropertyFileSnitch.java:60)
        ... 12 more
Error instantiating snitch class 'org.apache.cassandra.locator.PropertyFileSnitch'.
Fatal configuration error; unable to start server.  See log for stacktrace.
{code}",jicheng,jicheng,Low,Resolved,Fixed,13/Apr/12 07:08,16/Apr/19 09:32
Bug,CASSANDRA-4150,12550874,Allow larger cache capacities than 2GB,"The problem is that capacity is a Integer which can maximum hold 2 GB,
I will post a fix to CLHM in the mean time we might want to remove the maximumWeightedCapacity code path (atleast for Serializing cache) and implement it in our code.",vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,13/Apr/12 20:08,16/Apr/19 09:32
Bug,CASSANDRA-4154,12551062,CFRR wide row iterator does not handle tombstones well,"If the last row is a tombstone, CFRR's wide row iterator will throw an exception:

{noformat}

java.util.NoSuchElementException
        at com.google.common.collect.Iterables.getLast(Iterables.java:663)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:441)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:467)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:413)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:137)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:188)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:140)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:199)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
{noformat}",jbellis,brandon.williams,Normal,Resolved,Fixed,16/Apr/12 13:35,16/Apr/19 09:32
Bug,CASSANDRA-4156,12551084,CQL should support CL.TWO,CL.TWO was overlooked in creating the CQL language spec. It should probably be added.,mdennis,thepaul,Low,Resolved,Fixed,16/Apr/12 16:44,16/Apr/19 09:32
Bug,CASSANDRA-4159,12551182,isReadyForBootstrap doesn't compare schema UUID by timestamp as it should,"CASSANDRA-3629 introduced a wait to be sure the node is up to date on the schema before starting bootstrap. However, the isReadyForBootsrap() method compares schema version using UUID.compareTo(), which doesn't compare UUID by timestamp, while the rest of the code does compare using timestamp (MigrationManager.updateHighestKnown).

During a test where lots of node were boostrapped simultaneously (and some schema change were done), we ended up having some node stuck in the isReadyForBoostrap loop. Restarting the node fixed it, so while I can't confirm it, I suspect this was the source of that problem.",slebresne,slebresne,Normal,Resolved,Fixed,17/Apr/12 08:02,16/Apr/19 09:32
Bug,CASSANDRA-4160,12551206,ORDER BY ... DESC reverses comparrison predicates in WHERE,"When issuing a cql select statement with an ORDER BY ... DESC clause the comparison predicates in the WHERE clause gets reversed. 

Example: (see also attached)

SELECT number FROM test WHERE number < 3 ORDER BY number DESC

returns the results expected of WHERE number > 3",slebresne,dohse,Normal,Resolved,Fixed,17/Apr/12 12:06,16/Apr/19 09:32
Bug,CASSANDRA-4161,12551208,CQL 3.0 does not work in cqlsh with uppercase SELECT,"Uppercase SELECT prevents usage of CQL 3.0 features like ORDER BY

Example:

select * from test ORDER BY number; # works
SELECT * from test ORDER BY number; # fails",dohse,dohse,Low,Resolved,Fixed,17/Apr/12 12:20,16/Apr/19 09:32
Bug,CASSANDRA-4163,12551325,CQL3 ALTER TABLE command causes NPE,"To reproduce the problem:

./cqlsh --cql3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.30.0]
Use HELP for help.

cqlsh> CREATE KEYSPACE test34 WITH strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor='1';

cqlsh> USE test34;

cqlsh:test34> CREATE TABLE users (
          ... password varchar,
          ... gender varchar,
          ... session_token varchar,
          ... state varchar,
          ... birth_year bigint,
          ... pk varchar,
          ... PRIMARY KEY (pk)
          ... );

cqlsh:test34> ALTER TABLE users ADD coupon_code varchar;
TSocket read 0 bytes
",thepaul,khahn,Normal,Resolved,Fixed,17/Apr/12 23:43,16/Apr/19 09:32
Bug,CASSANDRA-4164,12551347,Cqlsh should support DESCRIBE on cql3-style composite CFs,"There is a discrepancy between create column family commands and then the output of the describe command:

{noformat}
cqlsh:test> CREATE TABLE timeline (
        ...     user_id varchar,
        ...     tweet_id uuid,
        ...     author varchar,
        ...     body varchar,
        ...     PRIMARY KEY (user_id, tweet_id)
        ... );
cqlsh:test> describe columnfamily timeline;

CREATE COLUMNFAMILY timeline (
  user_id text PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.UUIDType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write=True AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='org.apache.cassandra.io.compress.SnappyCompressor';
{noformat}",thepaul,nickmbailey,Normal,Resolved,Fixed,18/Apr/12 03:50,16/Apr/19 09:32
Bug,CASSANDRA-4166,12551454,BulkRecordWriter constructor crashes on wrong default parameter in conf.get() which is letter instead of number,"I've pulled the newest code and my bulkloading started to crash on:

{noformat}2012-04-18 12:34:09,620 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.NumberFormatException: For input string: ""O""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48){noformat}

The problem is this line and default parameter of conf.get() which was accidentally set to ""O"" (letter O) instead of 0 (zero).

{noformat}maxFailures = Integer.valueOf(conf.get(MAX_FAILED_HOSTS, ""O""));{noformat}

",,michalm,Normal,Resolved,Fixed,18/Apr/12 11:42,16/Apr/19 09:32
Bug,CASSANDRA-4168,12551487,"""Setup"" section of tools/stress/README.txt needs update","The README.txt file states ""Run `ant` from the Cassandra source directory, then Run `ant` from the contrib/stress directory.""

The file needs to reflect the changes in the way stress is built.",brandon.williams,tpatterson,Low,Resolved,Fixed,18/Apr/12 16:14,16/Apr/19 09:32
Bug,CASSANDRA-4170,12551529,cql3 ALTER TABLE ALTER TYPE has no effect,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY, bar int);
ALTER TABLE test ALTER bar TYPE float;
{noformat}

does not actually change the column type of bar. It does under cql2.

Note that on the current cassandra-1.1.0 HEAD, this causes an NPE, fixed by CASSANDRA-4163. But even with that applied, the ALTER shown here has no effect.",slebresne,thepaul,Normal,Resolved,Fixed,18/Apr/12 20:57,16/Apr/19 09:32
Bug,CASSANDRA-4171,12551531,cql3 ALTER TABLE foo WITH default_validation=int has no effect,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY) WITH default_validation=timestamp;
ALTER TABLE test WITH default_validation=int;
{noformat}

does not actually change the default validation type of the CF. It does under cql2.

No error is thrown. Some properties *can* be successfully changed using ALTER WITH, such as comment and gc_grace_seconds, but I haven't tested all of them. It seems probable that default_validation is the only problematic one, since it's the only (changeable) property which accepts CQL typenames.",slebresne,thepaul,Low,Resolved,Fixed,18/Apr/12 21:01,16/Apr/19 09:32
Bug,CASSANDRA-4181,12552177,Hadoop on CF with ColumnCounter columns fails,"Accessing CounterColumn from Hadoop fails with an exception:

{noformat} 
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:456)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:462)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:409)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:184)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator$WideColumnIterator.computeNext(ColumnFamilyRecordReader.java:500)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator$WideColumnIterator.computeNext(ColumnFamilyRecordReader.java:472)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators$PeekingImpl.hasNext(Iterators.java:1080)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:449)
        ... 11 more
{noformat}",marcocova,marcocova,Low,Resolved,Fixed,23/Apr/12 18:22,16/Apr/19 09:32
Bug,CASSANDRA-4183,12552532,Fix dependency versions in generated pos,Some of the versions of dependencies have fallen out of sync,stephenc,stephenc,Normal,Resolved,Fixed,25/Apr/12 08:12,16/Apr/19 09:32
Bug,CASSANDRA-4185,12552645,Minor CQL3 fixes,"The goal of this ticket is to be the home for a number of minor fixes/improvements in CQL3 that I didn't felt warranted a ticket each. It includes 4 patches:
* The first one fixes the grammar for float constants, so as to not recognize 3.-3, but to actually allow 3. (i.e, with radix point but with the fractional part left blank)
* The second one correctly detect the (invalid) case where a table is created with COMPACT STORAGE but without any 'clustering keys'.
* The third one fixes COUNT, first by making sure both COUNT(*) and COUNT(1) are correctly recognized and also by ""processing"" the internal row before counting, are there isn't a 1-to-1 correspondence between internal rows and CQL rows in CQL3. The grammar change in this patch actually rely on CASSANDRA-4184
* The fourth and last patch disallows the counter type for keys (i.e. any column part of the PRIMARY KEY) as it is completely non-sensical and will only led to confusion.
",slebresne,slebresne,Low,Resolved,Fixed,25/Apr/12 13:08,16/Apr/19 09:32
Bug,CASSANDRA-4190,12552706,Apparent data loss using super columns and row cache via ConcurrentLinkedHashCacheProvider,"Tested on a vanilla single-node cassandra 1.0.9 installation.

When using super columns along with row caching via ConcurrentLinkedHashCacheProvider (default if no JNA available, or explicitly configured even if JNA available), there's what appears as transient data loss.

Given this script executed in cassandra-cli:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and rows_cached=75000 and row_cache_provider='ConcurrentLinkedHashCacheProvider';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The output from the 3 gets above is as follows:

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

It's clear that the second and third set commands (country, region) are missing in the returned results.

If the row cache is explicitly invalidated (in a second terminal, via `nodetool -h localhost invalidaterowcache Test Users`), the missing data springs to life on next 'get':
{quote}
[default@Test] get Users['mina'];
=> (super_column=attrs,
     (column=country, value=Canada, timestamp=1335377839592000)
     (column=name, value=Mina, timestamp=1335377788441000)
     (column=region, value=Quebec, timestamp=1335377871353000))
Returned 1 results.
{quote}

From cursory checks, this does not to appear to happen with regular columns, nor with JNA enabled + SerializingCacheProvider.

",slebresne,minaguib,Normal,Resolved,Fixed,25/Apr/12 19:01,16/Apr/19 09:32
Bug,CASSANDRA-4192,12552832,CQL3: fix index dropping and assign default name if none provided at index creation,"This ticket proposes to fix two problems of CQL3 index handling:
# DROP INDEX is broken (because the code forgot to clone the metadata before doing modification which break the schema update path)
# If an index is created with a name (which CREATE INDEX allow), there is no way to drop the index (note that we will internally assign a name to the index ColumnFamilyStore, but we don't assign a name in the ColumnDefinition object, which is the only one checked by DROP INDEX).",slebresne,slebresne,Normal,Resolved,Fixed,26/Apr/12 10:35,16/Apr/19 09:32
Bug,CASSANDRA-4193,12552933,cql delete does not delete,"tested in 1.1 and trunk branch on a single node:
{panel}
cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;
cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps

cqlsh:test> delete from testcf_old where username = 'abc' and id =2;
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps
{panel}

same also when not using compact:
{panel}
cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps

cqlsh:test> delete from testcf where username = 'abc' and id =2;
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps
{panel}",slebresne,cywjackson,Normal,Resolved,Fixed,26/Apr/12 19:11,16/Apr/19 09:32
Bug,CASSANDRA-4195,12553369,error in log when upgrading multi-node cluster to 1.1,"I upgraded a cluster from 1.0.9 to 1.1.0. The following message shows up in the logs for all but the first node.
{code}
ERROR [GossipStage:1] 2012-04-30 07:37:06,986 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID                  
    at java.util.UUID.timestamp(UUID.java:331)                                  
    at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
    at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
    at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
    at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)           
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)   
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
{code}

this dtest demonstrates the issue. It was added to the cassandra-dtest repository as upgrade_to_11_test:
{code}
from dtest import Tester, debug 
from tools import * 
 
class TestUpgradeTo1_1(Tester): 
 
    def upgrade_test(self): 
        self.num_rows = 0 
        cluster = self.cluster 
 
        # Forcing cluster version on purpose 
        cluster.set_cassandra_dir(cassandra_version='1.0.9') 
 
        cluster.populate(3).start() 
        time.sleep(1) 
 
        for node in cluster.nodelist():     
            node.flush() 
            time.sleep(.5) 
            node.stop(wait_other_notice=True) 
            node.set_cassandra_dir(cassandra_version='1.1.0') 
            node.start(wait_other_notice=True) 
            time.sleep(.5)
{code}",xedin,tpatterson,Normal,Resolved,Fixed,30/Apr/12 14:17,16/Apr/19 09:32
Bug,CASSANDRA-4200,12553413,Move error on 1 node cluster,"Attempting to move a node in a 1 node cluster with a keyspace using NTS produces an error:

{noformat}
bin/nodetool -h localhost move 0
Exception in thread ""main"" java.lang.IllegalStateException: unable to find sufficient sources for streaming range (0,129685538820263942208828358218513421652]
at org.apache.cassandra.dht.RangeStreamer.getRangeFetchMap(RangeStreamer.java:197)
	at org.apache.cassandra.dht.RangeStreamer.getWorkMap(RangeStreamer.java:205)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:2419)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:2327)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
{noformat}",slebresne,nickmbailey,Normal,Resolved,Fixed,30/Apr/12 20:27,16/Apr/19 09:32
Bug,CASSANDRA-4201,12553420,Preserve commitlog size cap when recycling segments at startup,"1. Create a single node cluster, use default configuration, use cassandra.bat to start the server:

2. run the following commands in cli:
{code}
create keyspace toto;
use toto;
create column family titi;
truncate titi;
{code}

3. the node dies with this error:
{code}
ERROR 23:23:02,118 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Map failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:202)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:159)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(Unknown Source)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:119)
        ... 5 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        ... 7 more
 INFO 23:23:02,122 Stop listening to thrift clients
 INFO 23:23:02,123 Waiting for messaging service to quiesce
 INFO 23:23:02,125 MessagingService shutting down server thread.
{code}",jbellis,pchalamet,Low,Resolved,Fixed,30/Apr/12 21:28,16/Apr/19 09:32
Bug,CASSANDRA-4202,12553427,"CQL 3.0 prepare_cql_query fails on ""BEGIN BATCH""","Preparing the following (contrived) statement with the C++ Thrift bindings 
throws a TTransportException (""No more data to read."" from TTransport.h:41)

q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";
client.prepare_cql_query(pr, q, Compression::NONE);

{code:title=crashtest.cpp}
#include <protocol/TBinaryProtocol.h>
#include <thrift/transport/TSocket.h>
#include <thrift/transport/TTransportUtils.h>
#include ""Cassandra.h""

using namespace std;
using namespace apache::thrift;
using namespace apache::thrift::protocol;
using namespace apache::thrift::transport;
using namespace org::apache::cassandra;
using namespace boost;

int main(int argc, char **argv) {
    shared_ptr<TTransport> socket(new TSocket(""127.0.0.1"", 9160));
    shared_ptr<TTransport> transport(new TFramedTransport(socket));
    shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));

    CassandraClient client(protocol);

    try {
        transport->open();
        client.set_keyspace(""test1"");
        client.set_cql_version(""3.0.0"");

        CqlResult cr;
        CqlPreparedResult pr;

        // In cqlsh: create table crashtest (id int primary key, val text);
        const char *q;
        // q = ""insert into crashtest (id, val) values (?, ?)""; // This works fine
        q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";

        client.prepare_cql_query(pr,  q, Compression::NONE);

        vector<string> vtypes = pr.variable_types;
        vector<string>::iterator it;

        for (it = vtypes.begin(); it != vtypes.end(); it++) {
            cout << *it << endl;
        }
    } catch (TException &tx) {
        cerr << ""TException ERROR: "" << tx.what() << endl;
    }
}
{code}

{code:title=backtrace}
#0  0x00007fff901800e9 in __cxa_throw ()
#1  0x0000000100009ab9 in apache::thrift::transport::readAll<apache::thrift::transport::TBufferBase> (trans=@0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:41
#2  0x0000000100009c1d in apache::thrift::transport::TBufferBase::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:82
#3  0x0000000100009c5b in apache::thrift::transport::TFramedTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:390
#4  0x0000000100004b45 in apache::thrift::transport::TVirtualTransport<apache::thrift::transport::TFramedTransport, apache::thrift::transport::TBufferBase>::readAll_virt (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TVirtualTransport.h:99
#5  0x00000001000034c1 in apache::thrift::transport::TTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:126
#6  0x0000000100009f4c in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readI32 (this=0x100401370, i32=@0x7fff5fbff020) at TBinaryProtocol.h:372
#7  0x000000010000b5bf in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TBinaryProtocol.h:203
#8  0x0000000100006b07 in apache::thrift::protocol::TVirtualProtocol<apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>, apache::thrift::protocol::TProtocolDefaults>::readMessageBegin_virt (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TVirtualProtocol.h:432
#9  0x00000001000abe78 in apache::thrift::protocol::TProtocol::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TProtocol.h:518
#10 0x0000000100069a98 in org::apache::cassandra::CassandraClient::recv_prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0) at Cassandra.cpp:10231
#11 0x000000010003bf3f in org::apache::cassandra::CassandraClient::prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0, query=@0x7fff5fbff6b0, compression=org::apache::cassandra::Compression::NONE) at Cassandra.cpp:10206
#12 0x00000001000020ea in main (argc=1, argv=0x7fff5fbff8c8) at crashtest.cpp:36
{code}

{code:title=server error message}

ERROR 17:13:55,089 Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.cassandra.cql3.statements.UpdateStatement.prepare(UpdateStatement.java:278)
	at org.apache.cassandra.cql3.statements.BatchStatement.prepare(BatchStatement.java:157)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:207)
	at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:158)
	at org.apache.cassandra.thrift.CassandraServer.prepare_cql_query(CassandraServer.java:1260)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3484)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3472)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{code}",slebresne,sbillig,Low,Resolved,Fixed,30/Apr/12 22:03,16/Apr/19 09:32
Bug,CASSANDRA-4203,12553435,Fix descriptor versioning for bloom filter changes,"CASSANDRA-2975 introduced changes to the Data component, breaking stream compatibility",vijay2win@yahoo.com,jbellis,Normal,Resolved,Fixed,30/Apr/12 22:33,16/Apr/19 09:32
Bug,CASSANDRA-4204,12553443,Pig does not work on DateType,"cqlsh:PigDemo> describe columnfamily test1897;

CREATE COLUMNFAMILY test1897 (
KEY text PRIMARY KEY,
testcol timestamp
) WITH
comment='' AND
comparator=text AND
row_cache_provider='SerializingCacheProvider' AND
key_cache_size=200000.000000 AND
row_cache_size=0.000000 AND
read_repair_chance=1.000000 AND
gc_grace_seconds=864000 AND
default_validation=blob AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
row_cache_save_period_in_seconds=0 AND
key_cache_save_period_in_seconds=14400 AND
replicate_on_write=True;

cqlsh:PigDemo> select * from test1897;
KEY | testcol
-----+-------------------------
akey | 2012-01-21 00:14:12+0000

$ cat test1897.pig
cassandra_data = LOAD 'cassandra://PigDemo/test1897' USING CassandraStorage() AS (name, columns: bag {T: tuple()});
dump cassandra_data;

there seems problem with the DateType. the above simple pig script fail with the attached err
",brandon.williams,cywjackson,Normal,Resolved,Fixed,01/May/12 00:20,16/Apr/19 09:32
Bug,CASSANDRA-4205,12553450,SSTables are not updated with max timestamp on upgradesstables/compaction leading to non-optimal performance.,"We upgraded from 0.7.9 to 1.0.7 on a cluster with a heavy update load. After converting all the reads to named column reads instead of get_slice calls, we noticed that we still weren't getting the performance improvements implemented in CASSANDRA-2498. A single named column read was still touching multiple SSTables according to nodetool cfhistograms. 

To verify whether or not this was a reporting issue or a real issue, we ran multiple tests with stress and noticed that it worked as expected. After changing stress so that it ran the read/write test directly in the CF having issues (3 times stress & flush), we noticed that stress also touched multiple SSTables (according to cfhistograms).

So, the root of the problem is ""something"" left over from our pre-1.0 days. All SSTables were upgraded with upgradesstables, and have been written and compacted many times since the upgrade (4 months ago). The usage pattern for this CF is that it is constantly read and updated (overwritten), but no deletes. 

After discussing the problem with Brandon Williams on #cassandra, it seems the problem might be because a max timestamp has never been written for the old SSTables that were upgraded from pre 1.0. They have only been compacted, and the max timestamp is not recorded during compactions. 

A suggested fix is to special case this in upgradesstables so that a max timestamp always exists for all SSTables. 

{panel}
06:08 < driftx> thorkild_: tx.  The thing is we don't record the max timestamp on compactions, but we can do it specially for upgradesstables.
06:08 < driftx> so, nothing in... nothing out.
06:10 < thorkild_> driftx: ah, so when you upgrade from before the metadata was written, and that data is only feed through upgradesstables and compactions -> never properly written?
06:10 < thorkild_> that makes sense.
06:11 < driftx> right, we never create it, we just reuse it :(
{panel}
",jbellis,thorkild,Urgent,Resolved,Fixed,01/May/12 04:26,16/Apr/19 09:32
Bug,CASSANDRA-4206,12553475,AssertionError: originally calculated column size of 629444349 but now it is 588008950,"I've 4 node cluster of Cassandra 1.0.9. There is a rfTest3 keyspace with RF=3 and one CF with two secondary indexes. I'm importing data into this CF using Hadoop Mapreduce job, each row has less than 10 colkumns. From JMX:
MaxRowSize:  1597
MeanRowSize: 369

And there are some tens of millions of rows.

It's write-heavy usage and there is a big pressure on each node, there are quite some dropped mutations on each node. After ~12 hours of inserting I see these assertion exceptiona on 3 out of four nodes:

{noformat}
ERROR 06:25:40,124 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of 629444349 but now it is 588008950
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:388)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:256)
       at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:84)
       at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:437)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
629444349 but now it is 588008950
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:384)
       ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size
of 629444349 but now it is 588008950
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:161)
       at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:380)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
{noformat}

Few lines regarding Hints from the output.log:

{noformat}
 INFO 06:21:26,202 Compacting large row system/HintsColumnFamily:70000000000000000000000000000000 (1712834057 bytes) incrementally
 INFO 06:22:52,610 Compacting large row system/HintsColumnFamily:10000000000000000000000000000000 (2616073981 bytes) incrementally
 INFO 06:22:59,111 flushing high-traffic column family CFS(Keyspace='system', ColumnFamily='HintsColumnFamily') (estimated 305147360 bytes)
 INFO 06:22:59,813 Enqueuing flush of Memtable-HintsColumnFamily@833933926(3814342/305147360 serialized/live bytes, 7452 ops)
 INFO 06:22:59,814 Writing Memtable-HintsColumnFamily@833933926(3814342/305147360 serialized/live bytes, 7452 ops)
{noformat}

I think the problem may be somehow connected to an IntegerType secondary index. I had a different problem with CF with two secondary indexes, the first UTF8Type, the second IntegerType. After a few hours of inserting data in the afternoon and midnight repair+compact, the next day I couldn't find any row using the IntegerType secondary index. The output was like this:

{noformat}
[default@rfTest3] get IndexTest where col1 = '3230727:http://zaskolak.cz/download.php';
-------------------
RowKey: 3230727:8383582:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348630332000)
=> (column=col2, value=8383582, timestamp=1335348630332000)
-------------------
RowKey: 3230727:8383583:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348449078000)
=> (column=col2, value=8383583, timestamp=1335348449078000)
-------------------
RowKey: 3230727:8383579:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348778577000)
=> (column=col2, value=8383579, timestamp=1335348778577000)

3 Rows Returned.
Elapsed time: 292 msec(s).

[default@rfTest3] get IndexTest where col2 = 8383583;

0 Row Returned.
Elapsed time: 7 msec(s
{noformat}

You can see there really is an 8383583 in col2 in on of the listed rows, but the search by secondary index returns nothing.

The Assert Exception also happend only on CF with the secondary index of IntegerType. There were also secondary indexes of UTF8Type and
LongType types. It's the first time I've tried secondary indexes of other type than UTF8Type.

Regards,
Patrik",,patrik.modesto,Normal,Resolved,Fixed,01/May/12 11:28,16/Apr/19 09:32
Bug,CASSANDRA-4213,12553700,DynamicEndpointSnitch calculates score incorrectly,"updateScore does double = long/long math which calculates the score wrong 1/3 becomes 0.0 not 0.3333


need 1 to be cast to double",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,03/May/12 02:59,16/Apr/19 09:32
Bug,CASSANDRA-4219,12553879,Problem with creating keyspace after drop,"Hi,

I'm doing testing and wanted to drop a keyspace (with a column family) to re-add it with a different strategy. So I ran in cqlsh:

DROP KEYSPACE PlayLog;

CREATE KEYSPACE PlayLog WITH strategy_class = 'SimpleStrategy'
 AND strategy_options:replication_factor = 2;

And everything seemed to be fine. I ran some inserts, which also seemed to go fine, but then selecting them gave me:

cqlsh:PlayLog> select count(*) from playlog;
TSocket read 0 bytes

I wasn't sure what was wrong, so I tried dropping and creating again, and now when I try to create I get:

cqlsh> CREATE KEYSPACE PlayLog WITH strategy_class = 'SimpleStrategy'
  ...   AND strategy_options:replication_factor = 2;
TSocket read 0 bytes

And the keyspace doesn't get created. In the log it shows:

ERROR [Thrift:4] 2012-05-03 18:23:05,124 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
       at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
       at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
       at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:129)
       at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:701)
       at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:875)
       at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1235)
       at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
       at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
       at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
       at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
       at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
       at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
       at java.util.concurrent.FutureTask.get(Unknown Source)
       at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
       ... 13 more
Caused by: java.lang.AssertionError
       at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:441)
       at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:339)
       at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:269)
       at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       ... 3 more
ERROR [MigrationStage:1] 2012-05-03 18:23:05,124 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:441)
       at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:339)
       at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:269)
       at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)

Any ideas how I can recover from this?

I am running version 1.1.0 and have tried nodetool repair, cleanup, compact. I can create other keyspaces, but still can't create a keyspace called PlayLog even though it is not listed anywhere.

Jeff",xedin,jeffwilliams,Normal,Resolved,Fixed,04/May/12 07:29,16/Apr/19 09:32
Bug,CASSANDRA-4221,12553932,Error while deleting a columnfamily that is being compacted.,"The following dtest command produces an error:
{code}export CASSANDRA_VERSION=git:cassandra-1.1; nosetests --nocapture --nologcapture concurrent_schema_changes_test.py:TestConcurrentSchemaChanges.load_test{code}

Here is the error:
{code}
Error occured during compaction
java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:239)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1580)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1770)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:61)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:839)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:851)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:142)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:148)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:121)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:264)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
Caused by: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:102)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:985)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
	... 13 more
{code}

For reference, here is the dtest function that causes the failure. The error happens on the line near the bottom that drops the columnfamily:
{code}
    def load_test(self):                                                        
        """"""                                                                     
        apply schema changes while the cluster is under load.                   
        """"""                                                                     
        debug(""load_test()"")                                                    
                                                                                
        cluster = self.cluster                                                  
        cluster.populate(1).start()                                             
        node1 = cluster.nodelist()[0]                                           
        wait(2)                                                                 
        cursor = self.cql_connection(node1).cursor()                            
                                                                                
        def stress(args=[]):                                                    
            debug(""Stressing"")                                                  
            node1.stress(args)                                                  
            debug(""Done Stressing"")                                             
                                                                                
        def compact():                                                          
            debug(""Compacting..."")                                              
            node1.nodetool('compact')                                           
            debug(""Done Compacting."")                                           
                                                                                
        # put some data into the cluster                                        
        stress(['--num-keys=1000000'])                                          
                                                                                
        # now start compacting...                   
        tcompact = Thread(target=compact)                                       
        tcompact.start()                                                        
        wait(1)                                                                 
                                                                                
        # now the cluster is under a lot of load. Make some schema changes.     
        cursor.execute(""USE Keyspace1"")                                         
        wait(1)                                                                 
        cursor.execute(""DROP COLUMNFAMILY Standard1"")                           
                                                                                
        wait(3)                                                                 
                                                                                
        cursor.execute(""CREATE COLUMNFAMILY Standard1 (KEY text PRIMARY KEY)"")  
                                                                                
        tcompact.join()                                                         
 
{code}
Again, the error happens on cassandra-1.1, but not on cassandra-1.0.",xedin,tpatterson,Normal,Resolved,Fixed,04/May/12 13:23,16/Apr/19 09:32
Bug,CASSANDRA-4223,12554104,Non Unique Streaming session ID's,"I have observed repair processes failing due to duplicate Streaming session ID's. In this installation it is preventing rebalance from completing. I believe it has also prevented repair from completing in the past. 

The attached streaming-logs.txt file contains log messages and an explanation of what was happening during a repair operation. it has the evidence for duplicate session ID's.

The duplicate session id's were generated on the repairing node and sent to the streaming node. The streaming source replaced the first session with the second which resulted in both sessions failing when the first FILE_COMPLETE message was received. 

The errors were:

{code:java}
DEBUG [MiscStage:1] 2012-05-03 21:40:33,997 StreamReplyVerbHandler.java (line 47) Received StreamReply StreamReply(sessionId=26132848816442266, file='/var/lib/cassandra/data/FMM_Studio/PartsData-hc-1-Data.db', action=FILE_FINISHED)
ERROR [MiscStage:1] 2012-05-03 21:40:34,027 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/lib/cassandra/data/FMM_Studio/PartsData-hc-1-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:195)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code}

and

{code:java}
DEBUG [MiscStage:2] 2012-05-03 21:40:36,497 StreamReplyVerbHandler.java (line 47) Received StreamReply StreamReply(sessionId=26132848816442266, file='/var/lib/cassandra/data/OpsCenter/rollups7200-hc-3-Data.db', action=FILE_FINISHED)
ERROR [MiscStage:2] 2012-05-03 21:40:36,497 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MiscStage:2,5,main]
java.lang.IllegalStateException: target reports current file is /var/lib/cassandra/data/OpsCenter/rollups7200-hc-3-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:195)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code}


I think this is because System.nanoTime() is used for the session ID when creating the StreamInSession objects (driven from StorageService.requestRanges()) . 

From the documentation (http://docs.oracle.com/javase/6/docs/api/java/lang/System.html#nanoTime()) 

{quote}
This method provides nanosecond precision, but not necessarily nanosecond accuracy. No guarantees are made about how frequently values change. 
{quote}

Also some info here on clocks and timers https://blogs.oracle.com/dholmes/entry/inside_the_hotspot_vm_clocks

The hypervisor may be at fault here. But it seems like we cannot rely on successive calls to nanoTime() to return different values. 

To avoid message/interface changes on the StreamHeader it would be good to keep the session ID a long. The simplest approach may be to make successive calls to nanoTime until the result changes. We could fail if a certain number of milliseconds have passed. 

Hashing the file names and ranges is also a possibility, but more involved. 

(We may also want to drop latency times that are 0 nano seconds.)
",amorton,amorton,Normal,Resolved,Fixed,06/May/12 23:10,16/Apr/19 09:32
Bug,CASSANDRA-4227,12554426,StorageProxy throws NPEs for when there's no hostids for a target,"On trunk...

if there is no host id due to an old node, an info log is generated, but the code continues to use the null host id causing NPEs in decompose... Should this bypass this code, or perhaps can the plain ip address be used in this case? don't know.

as follows...



                    UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target);
                    if ((hostId == null) && (Gossiper.instance.getVersion(target) < MessagingService.VERSION_12))
                        logger.info(""Unable to store hint for host with missing ID, {} (old node?)"", target.toString());
                    RowMutation hintedMutation = RowMutation.hintFor(mutation, ByteBuffer.wrap(UUIDGen.decompose(hostId)));
                    hintedMutation.apply();
",urandom,dbrosius@apache.org,Low,Resolved,Fixed,09/May/12 04:04,16/Apr/19 09:32
Bug,CASSANDRA-4230,12554460,Deleting a CF always produces an error and that CF remains in an unknown state,"From the CLI perspective:

[default@Disco] drop column family client; 
null
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_column_family(Cassandra.java:1222)
	at org.apache.cassandra.thrift.Cassandra$Client.system_drop_column_family(Cassandra.java:1209)
	at org.apache.cassandra.cli.CliClient.executeDelColumnFamily(CliClient.java:1301)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

Log:

 INFO [MigrationStage:1] 2012-05-09 11:25:35,686 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,687 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,748 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-34-Data.db (1041 bytes)
 INFO [MigrationStage:1] 2012-05-09 11:25:35,749 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,750 Memtable.java (line 266) Writing Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,812 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db (649 bytes)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,814 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-27-Data.db'), SSTableReader
(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-26-Data.db'), SSTableReader(path
='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db')]
 INFO [MigrationStage:1] 2012-05-09 11:25:35,918 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,919 Memtable.java (line 266) Writing Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,945 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-29-Data.db,].  22,486 to 20,621 (~91% of orig
inal) bytes for 2 keys at 0.150120MB/s.  Time: 131ms.
 INFO [FlushWriter:3] 2012-05-09 11:25:36,013 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db (407 bytes)
ERROR [MigrationStage:1] 2012-05-09 11:25:36,043 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
        at org.apache.cassandra.utils.CLibrary.link(Native Method)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:150)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

ERROR [Thrift:3] 2012-05-09 11:25:36,048 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyDrop(MigrationManager.java:182)
        at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:948)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3348)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3336)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
        ... 11 more
Caused by: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
ERROR [MigrationStage:1] 2012-05-09 11:25:36,051 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)

        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,052 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-4-Data.db')]
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,187 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-6-Data.db,].  728 to 458 (~62% of original) bytes for 8 keys at 0.003235MB/s.  Time: 135ms.

Schema:

CREATE COLUMN FAMILY Client WITH
       key_validation_class = UUIDType AND
       comparator = UTF8Type AND
       column_metadata = [ { column_name: key, validation_class: BytesType }
                           { column_name: name, validation_class: UTF8Type }
                           { column_name: userid, validation_class: UUIDType, index_type: KEYS }
                         ] AND
       compression_options = { sstable_compression:SnappyCompressor, chunk_length_kb:64 } AND
       compaction_strategy = LeveledCompactionStrategy AND
       compaction_strategy_options = { sstable_size_in_mb: 10 } AND
       gc_grace = 432000;

State of data dir after deletion attempt:

# ls -lah /var/lib/cassandra/data/Disco/Client/ 
total 76K
drwxr-xr-x  3 cassandra cassandra 4.0K May  9 11:25 .
drwxr-xr-x 17 cassandra cassandra 4.0K May  3 12:34 ..
-rw-r--r--  2 cassandra cassandra  420 May  9 11:25 Client-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx.json
-rw-r--r--  1 cassandra cassandra  418 May  9 11:25 Client.json
-rw-r--r--  1 cassandra cassandra   46 May  9 11:25 Disco-Client-hc-6-CompressionInfo.db
-rw-r--r--  1 cassandra cassandra  458 May  9 11:25 Disco-Client-hc-6-Data.db
-rw-r--r--  1 cassandra cassandra  976 May  9 11:25 Disco-Client-hc-6-Filter.db
-rw-r--r--  1 cassandra cassandra  208 May  9 11:25 Disco-Client-hc-6-Index.db
-rw-r--r--  1 cassandra cassandra 4.3K May  9 11:25 Disco-Client-hc-6-Statistics.db
-rw-r--r--  4 cassandra cassandra   46 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-CompressionInfo.db
-rw-r--r--  4 cassandra cassandra   92 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Data.db
-rw-r--r--  4 cassandra cassandra  496 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Filter.db
-rw-r--r--  4 cassandra cassandra   26 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Index.db
-rw-r--r--  4 cassandra cassandra 4.3K May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Statistics.db
drwxr-xr-x  6 cassandra cassandra 4.0K May  9 11:25 snapshots
",xedin,edevil,Normal,Resolved,Fixed,09/May/12 10:28,16/Apr/19 09:32
Bug,CASSANDRA-4233,12554549,overlapping sstables in leveled compaction strategy,"CASSANDRA-4142 introduces test failures, that are caused by overlapping tables within a level, which Shouldn't Happen.",slebresne,jbellis,Normal,Resolved,Fixed,09/May/12 19:53,16/Apr/19 09:32
Bug,CASSANDRA-4246,12555858,cql3 ORDER BY not ordering,"Creating the simplest composite-key cql3 table I can think of, populating it with a few rows of data, then trying to do a query with an ORDER BY does not yield ordered results.

Here's a cql script:

{noformat}
create keyspace test with strategy_class = 'SimpleStrategy'
   and strategy_options:replication_factor = 1;
use test;
create table moo (a int, b int, c int, primary key (a, b));

insert into moo (a, b, c) values (123, 12, 3400);
insert into moo (a, b, c) values (122, 13, 3500);
insert into moo (a, b, c) values (124, 10, 3600);
insert into moo (a, b, c) values (121, 11, 3700);

select * from moo;
select * from moo order by b;
{noformat}

Here is the output of those two queries:

{noformat}
 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400

 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400
{noformat}

I also tried these using the bare thrift interface, to make sure it wasn't python-cql or cqlsh doing something stupid. Same results. Am I totally missing something important here about how this is supposed to work?",slebresne,thepaul,Normal,Resolved,Fixed,15/May/12 19:05,16/Apr/19 09:32
Bug,CASSANDRA-4249,12555985,LOGGING: Info log is not displaying number of rows read from saved cache at startup,"As part of commit with revision c9270f4e info logging for number of rows read from saved cache is not working. 
This is happening because we are not incrementing the counter cachedRowsRead in ColumnFamilyStore.initRowCache().",kohlisankalp,kohlisankalp,Low,Resolved,Fixed,16/May/12 08:33,16/Apr/19 09:32
Bug,CASSANDRA-4250,12555995,Missing arrayOffset in FBUtilities.hash,"In CASSANDRA-3869, FBUtilities.hash was optimised to use the backing byte array if there is one.  However, there is a missing +arrayOffset in the offset parameter.

This can cause incorrect hashes resulting in data going to the wrong place, etc..  I haven't observed any errors directly attributable to this so maybe we are lucky and all backing arrays start at 0 but this could cause data loss in the worst case.",richardlow,richardlow,Urgent,Resolved,Fixed,16/May/12 09:26,16/Apr/19 09:32
Bug,CASSANDRA-4252,12556136,Set operation mode to MOVING earlier,Right now when moving a node we set the OperationMode only once we've calculated the necessary ranges to transfer and if there actually are ranges to transfer. Due to the sleep for ring settling this means there are 30 seconds where the node is moving but the operation mode isn't set accordingly. Additionally if it turns out no data needs to be transferred then the move will complete without ever switching the OperationMode to moving.,jbellis,nickmbailey,Low,Resolved,Fixed,16/May/12 22:15,16/Apr/19 09:32
Bug,CASSANDRA-4255,12556268,concurrent modif ex when repair is run on LCS,"came across this, will try to figure a way to systematically reprod this. But the problem is the sstable list in the manifest is changing as the repair is triggered:

{panel}
Exception in thread ""main"" java.util.ConcurrentModificationException 
 at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
 at java.util.AbstractList$Itr.next(Unknown Source)
 at org.apache.cassandra.io.sstable.SSTable.getTotalBytes(SSTable.java:250)
 at org.apache.cassandra.db.compaction.LeveledManifest.getEstimatedTasks(LeveledManifest.java:435)
 at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getEstimatedRemainingTasks(LeveledCompactionStrategy.java:128)
 at org.apache.cassandra.db.compaction.CompactionManager.getPendingTasks(CompactionManager.java:1063)
 at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(Unknown Source)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(Unknown Source)
 at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
 at sun.rmi.transport.Transport$1.run(Unknown Source)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
 at java.lang.Thread.run(Unknown Source)
{panel}

maybe we could change the list to a copyOnArrayList? just a suggestion, haven't investigated much yet:

{code:title=LeveledManifest.java}
generations[i] = new ArrayList<SSTableReader>();
{code}",jbellis,cywjackson,Low,Resolved,Fixed,17/May/12 18:29,16/Apr/19 09:32
Bug,CASSANDRA-4256,12556269,Include stress tool in debian packaging,"The stress tool isn't included in the debian packaging. We need to update that to grab the stress shell script as well as put the stress.jar file in lib.

Also the stress shell script needs to be updated to include looking in /usr/share/cassandra... when searching for the stress jar so it will run in packaged installations.",nickmbailey,nickmbailey,Low,Resolved,Fixed,17/May/12 18:40,16/Apr/19 09:32
Bug,CASSANDRA-4257,12556274,CQL3 range query with secondary index fails,"This query fails:
select * from indextest where setid = 0 and row < 1;
when there's a secondary index on 'setid'; row isn't the primary key.

{code:title=CQL3}
bin$ ./cqlsh --cql3
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> create table indextest (id int primary key, row int, setid int);
cqlsh:warehouse1> create index indextest_setid_idx on indextest (setid);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (0, 0, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (1, 1, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (2, 2, 0);
cqlsh:warehouse1> select * from indextest where setid = 0;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
  2 |   2 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row = 1;
 id | row | setid
----+-----+-------
  1 |   1 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
TSocket read 0 bytes
{code}

{code:title=Error message}
ERROR 13:36:23,544 Error occurred during processing of message.
java.lang.NullPointerException
  at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:546)
  at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:253)
  at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
  at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
  at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
  at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
  at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
{code}

Works fine in CQL2:
{code:title=CQL2}
bin$ ./cqlsh_uuid --cql2
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
 id | row | setid
----+-----+-------
  0 |   0 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 2;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
{code}",slebresne,sbillig,Low,Resolved,Fixed,17/May/12 19:06,16/Apr/19 09:32
Bug,CASSANDRA-4259,12556422,Bug in SSTableReader.getSampleIndexesForRanges(...) causes uneven InputSplits generation for Hadoop mappers,"Running a simple mapreduce job on cassandra column family results in creating multiple small mappers for one half of the ring and one big mapper for the other half. Upper part (85... - 0) is cut into smaller slices. Lower part (0 - 85...) generates one big input slice. One mapper processing half of the ring causes huge inefficiency. Also the progress meter for this mapper is incorrect - it goes to 100% in a couple of seconds, than stays at 100% for an hour or two.

I've investigated the problem a bit. I think it is related to incorrect output of 'nodetool rangekeysample'. On the node resposible for part (0 - 85...) the output is empty! On the other node it works fine.

I think the bug is in SSTableReader.getSampleIndexesForRanges(...). These two lines:

   RowPosition leftPosition = range.left.maxKeyBound();
   RowPosition rightPosition = range.left.maxKeyBound();

should be changed to:

   RowPosition leftPosition = range.left.maxKeyBound();
   RowPosition rightPosition = range.right.maxKeyBound();

After that fix the output of nodetool is correct and the whole ring is split into small mappers.

The other half of the ring works fine because of extra 'if' in the code:

   int right = Range.isWrapAround(range.left, range.right)...

This causes that the bug does not show up in one-node cluster or in the ""last"" ring partition in muli-node clusters.

Can anyone look at it and verify my thoughts? I'm rather new to Cassandra.
",br1985,br1985,Normal,Resolved,Fixed,18/May/12 17:07,16/Apr/19 09:32
Bug,CASSANDRA-4262,12556550,1.1 does not preserve compatibility w/ index queries against 1.0 nodes,1.1 merged index + seq scan paths into RangeSliceCommand.  1.1 StorageProxy always sends a RSC for either scan type.  But 1.0 RSVH only does seq scans.,slebresne,jbellis,Urgent,Resolved,Fixed,19/May/12 22:09,16/Apr/19 09:32
Bug,CASSANDRA-4263,12556574,LeveledManifest.maxBytesForLevel calculates wrong for sstable_size_in_mb larger than 512m,"need to use long math

        if (level == 0)
            return 4 * maxSSTableSizeInMB * 1024 * 1024;",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,20/May/12 05:01,16/Apr/19 09:32
Bug,CASSANDRA-4266,12556747,Validate compression parameters,compression_parameters doesn't warn when unknown options are specified; see  http://ac31004.blogspot.co.uk/2012/05/snappy-compression-fails-for-apache.html,slebresne,jbellis,Low,Resolved,Fixed,21/May/12 20:32,16/Apr/19 09:32
Bug,CASSANDRA-4269,12556760,Setting column metadata for non-string comparator CFs breaks,"For example, use a comparator of LongType and try to create an index on a column named 2 (0x0000000000000002).  You'll get a stracktrace in the logs similar to this:

{noformat}
java.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 2
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:50)
	at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:115)
	at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1278)
	at org.apache.cassandra.config.CFMetaData.columnMetadata(CFMetaData.java:225)
	at org.apache.cassandra.config.CFMetaData.fromThrift(CFMetaData.java:636)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1061)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3436)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3424)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.nio.charset.MalformedInputException: Input length = 2
	at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
	at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:163)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:120)
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:46)
	... 13 more
{noformat}

This works in Cassandra 0.8 and 1.0.",slebresne,thobbs,Low,Resolved,Fixed,21/May/12 21:39,16/Apr/19 09:32
Bug,CASSANDRA-4270,12556774,long-test broken due to incorrect config option,"the long-test fails:
{code}
BUILD FAILED
/home/tahooie/datastax/cassandra/build.xml:1125: Problem: failed to create task or type jvmarg
Cause: The name is undefined.
{code}
The problem is that the build.xml file has the jvmarg outside the <testmacro> tag instead of inside it. A patch is forthcoming.",tpatterson,tpatterson,Low,Resolved,Fixed,21/May/12 22:46,16/Apr/19 09:32
Bug,CASSANDRA-4271,12556789,Exit status of bin/cassandra without -f is wrong,"The launch_service() function returns {{$?}} after exec'ing java, and the script then exits with that same status.

The problem is that we do a {{[ ! -z ""$pidpath""] && ...}} conditional statment after exec'ing when the foreground flag isn't set.  The value of {{$?}} then depends on that conditional and the statement, typically returning 1, because {{$pidpath}} isn't set.  So, even if everything appears to execute normally, you will get an exit status of 1 for the whole script.

I suspect the right thing to do is just return 0 when backgrounding.",thepaul,thobbs,Low,Resolved,Fixed,22/May/12 01:27,16/Apr/19 09:32
Bug,CASSANDRA-4275,12557011,Oracle Java 1.7 u4 does not allow Xss128k,"Problem: This happens when you try to start it with default Xss setting of 128k
=======
The stack size specified is too small, Specify at least 160k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Solution
=======
Set -Xss to 256k

Problem: This happens when you try to start it with Xss = 160k
========
ERROR [Thrift:14] 2012-05-22 14:42:40,479 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thrift:14,5,main]
java.lang.StackOverflowError

Solution
=======
Set -Xss to 256k",slebresne,appodictic,Normal,Resolved,Fixed,23/May/12 14:38,16/Apr/19 09:32
Bug,CASSANDRA-4276,12557021,Multiple SLF4J bindings warning when running stress,"{noformat}
> ./tools/bin/stress
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/path/apache-cassandra-1.1.0/tools/lib/stress.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/path/apache-cassandra-1.1.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
{noformat}",nickmbailey,mbulman,Normal,Resolved,Fixed,23/May/12 15:38,16/Apr/19 09:32
Bug,CASSANDRA-4277,12557047,"hsha default thread limits make no sense, and yaml comments look confused","The cassandra.yaml states with respect to {{rpc_max_threads}}:

{code}
# For the Hsha server, the min and max both default to quadruple the number of
# CPU cores.
{code}

The code seems to indeed do this. But this makes, as far as I can tell, no sense what-so-ever since the number of concurrent RPC threads you need is a function of the throughput and the average latency of requests (that includes synchronously waiting on network traffic).

Defaulting to anything having to do with CPU cores seems inherently wrong. If a default is non-static, a closer guess might be to look at thread stack size and heap size and infer what ""might"" be reasonable.

*NOTE*: The effect of having this too low, is ""strange"" (if you don't know what's going on) latencies observed form the client on all thrift requests (*any* thrift request, including e.g. {{describe_ring()}}), that isn't visible in any latency metric exposed by Cassandra. This is why I consider this ""major"", since unwitting users may be seeing detrimental performance for no good reason.

In addition, I read this about async:

{code}
# async -> Nonblocking server implementation with one thread to serve 
#          rpc connections.  This is not recommended for high throughput use
#          cases. Async has been tested to be about 50% slower than sync
#          or hsha and is deprecated: it will be removed in the next major release.
{code}

This makes even less sense. Running with *one* rpc thread limits you to a single concurrent request. How was that 50% number even attained? By single-node testing being completely CPU bound locally on a node? The actual effect should be ""stupidly slow"" in any real situation with lots of requests on a cluster of many nodes and network traffic (though I didn't test that) - especially in the event of any kind of hiccup like a node doing GC. I agree that if the above is true, async should *definitely* be deprecated, but the reasons seem *much* stronger than implied.

I may be missing something here, in which case I apologize,, but I specifically double-checked after I fixed this setting on on our our clusters after seeing exactly the expected side-effect of having it be too low. I always was under the impression that rpc_max_threads affects the number of RPC requests running concurrently, and code inspection (it being used for the worker thread limit) + the effects of client-observed latency is consistent with my understanding.

I suspect the setting was set strangely by someone because the phrasing of the comments in {{cassandra.yaml}} strongly suggest that this should be tied to CPU cores, hiding the fact that this really has to do with the number of requests that can be serviced concurrently regardless of implementation details of thrift/networking being sync/async/etc.

",scode,scode,Normal,Resolved,Fixed,23/May/12 18:38,16/Apr/19 09:32
Bug,CASSANDRA-4278,12557052,Can't specify certain keyspace properties in CQL,"A user using EC2MultiRegionSnitch, where the datacenter name has to match the AWS region names, will not be able to specify a keyspace's replica counts for those datacenters using CQL. AWS region names contain hyphens, which are not valid identifiers in CQL, and CQL keyspace/columnfamily properties must be identifiers or identifiers separated by colons.

Example:

{noformat}
CREATE KEYSPACE Foo
  WITH strategy_class = 'NetworkTopologyStrategy'
      AND strategy_options:""us-east""=1
      AND strategy_options:""us-west""=1;
{noformat}

(see http://mail-archives.apache.org/mod_mbox/cassandra-user/201205.mbox/browser for context)

..will not currently work, with or without the double quotes.

CQL should either allow hyphens in COMPIDENT, or allow quoted parts of a COMPIDENT token.",slebresne,thepaul,Low,Resolved,Fixed,23/May/12 19:24,16/Apr/19 09:32
Bug,CASSANDRA-4279,12557067,kick off background compaction when min/max changed,"When the threshold changes, we may be eligible for a compaction immediately (without waiting for a flush to trigger the eligibility check).",jbellis,jbellis,Low,Resolved,Fixed,23/May/12 20:51,16/Apr/19 09:32
Bug,CASSANDRA-4287,12558177,SizeTieredCompactionStrategy.getBuckets is quadradic in the number of sstables,"getBuckets first sorts the sstables by size (N log N) then adds each sstable to a bucket (N**2 in the worst case of all sstables the same size, because we use the bucket's contents as a hash key).",jbellis,jbellis,Low,Resolved,Fixed,25/May/12 19:47,16/Apr/19 09:32
Bug,CASSANDRA-4289,12558250,Secondary Indexes fail following a system restart,"Create a new cf with a secondary index, and queries with indexes predicates work fine until the server is restarted, after which they error and the following stacktrace is output to the log:

{code}
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:44)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:88)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:1)
	at org.apache.cassandra.utils.IntervalTree.comparePoints(IntervalTree.java:191)
	at org.apache.cassandra.utils.IntervalTree.contains(IntervalTree.java:203)
	at org.apache.cassandra.utils.IntervalTree.access$3(IntervalTree.java:201)
	at org.apache.cassandra.utils.IntervalTree$IntervalNode.searchInternal(IntervalTree.java:293)
	at org.apache.cassandra.utils.IntervalTree.search(IntervalTree.java:140)
	at org.apache.cassandra.utils.IntervalTree.search(IntervalTree.java:146)
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1259)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:229)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1300)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1174)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1104)
	at org.apache.cassandra.db.index.keys.KeysSearcher$1.computeNext(KeysSearcher.java:144)
	at org.apache.cassandra.db.index.keys.KeysSearcher$1.computeNext(KeysSearcher.java:1)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1409)
	at org.apache.cassandra.db.index.keys.KeysSearcher.search(KeysSearcher.java:88)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:595)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:47)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:870)
	at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:259)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:134)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1236)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:1)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

Tested with a single node setup & verified that this behaviour is only present in trunk, cassandra-1.0.10 works as expected.
",samt,samt,Normal,Resolved,Fixed,26/May/12 16:56,16/Apr/19 09:32
Bug,CASSANDRA-4291,12558264,Oversize integer in CQL throws NumberFormatException,"In CQL, the parser does not handle an oversize Integer, the client socket get closed and an exception is output in the log.

{noformat}cqlsh:TEST1> select count(*) from Items limit 10000000000000;
TSocket read 0 bytes
cqlsh:TEST1> select count(*) from Items limit 1;
TSocket read 0 bytes{noformat}

{noformat}
ERROR 02:51:28,600 Error occurred during processing of message.
java.lang.NumberFormatException: For input string: ""10000000000""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:461)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:631)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:221)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:951)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:873)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1234)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

The INTEGER type in Cql.g matches digits but not to any particular limit.",dbrosius,ssadler,Low,Resolved,Fixed,27/May/12 06:08,16/Apr/19 09:32
Bug,CASSANDRA-4294,12558481,Upgrading encounters: 'SimpleStrategy requires a replication_factor strategy option.' and refuses to start,"I've seen this reported quite a few times now:

{noformat}
ERROR [main] 2012-05-29 19:33:40,589 AbstractCassandraDaemon.java (line 370) Exception encountered during startup
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
  at org.apache.cassandra.db.Table.<init>(Table.java:275)
  at org.apache.cassandra.db.Table.open(Table.java:114)
  at org.apache.cassandra.db.Table.open(Table.java:97)
  at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
  at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
  at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
  at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:71)
  at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:218)
  at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:295)
  at org.apache.cassandra.db.Table.<init>(Table.java:271)
  ... 5 more
{noformat}

The common thread seems to be old lineage, from at least 0.7.  1.0.x works fine, but upgrading to 1.1 causes the problem.",slebresne,brandon.williams,Normal,Resolved,Fixed,29/May/12 19:48,16/Apr/19 09:32
Bug,CASSANDRA-4296,12558575,CQL3: create table don't always validate access to the right keyspace,"Create table allows (like other queries) to override the currently set keyspace ({{CREATE TABLE foo.bar ...}}). However, when we do that, the access check is done on the wrong keyspace. In particular if no keyspace was set, this end up in a NPE.",slebresne,slebresne,Low,Resolved,Fixed,30/May/12 10:41,16/Apr/19 09:32
Bug,CASSANDRA-4300,12558773,missing host ID results in NPE when delivering hints,"
In {{StorageService.handledStateNormal()}} the token-to-endpoint map is updated before the id-to-endpoint map, creating a small window where {{TokenMetadata.isMember()}} can return true before a host ID is available.

Trivial patch forthcoming.

{noformat}
[...]
 INFO [GossipStage:1] 2012-05-30 21:59:10,683 Gossiper.java (line 833) Node /10.2.131.32 has restarted, now UP
 INFO [GossipStage:1] 2012-05-30 21:59:10,684 Gossiper.java (line 799) InetAddress /10.2.131.32 is now UP
 INFO [HintedHandoff:1] 2012-05-30 21:59:10,697 HintedHandOffManager.java (line 304) Started hinted handoff for host: null with IP: /10.2.131.32
ERROR [HintedHandoff:1] 2012-05-30 21:59:10,698 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:112)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:305)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:265)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:86)
	at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:439)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 Events.java (line 130) Node 10.2.131.32 now available: true
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 StorageService.java (line 1218) Node /10.2.131.32 state jump to normal
 INFO [GossipStage:1] 2012-05-30 21:59:10,701 Events.java (line 120) Node 10.2.131.32 is now in state NORMAL
[...]
{noformat}",urandom,urandom,Normal,Resolved,Fixed,31/May/12 12:48,16/Apr/19 09:32
Bug,CASSANDRA-4302,12558854,cassandra-stress scripts should be executable,Just need to {{chmod u+x tools/bin/cassandra-stress tools/bin/cassandra-stressd}}.,yukim,thobbs,Low,Resolved,Fixed,31/May/12 20:15,16/Apr/19 09:32
Bug,CASSANDRA-4307,12559327,isMarkedForDelete can return false if it is a few seconds in the future,"The patch in CASSANDRA-3716 causes some weird issues to arrise when server times don't exactly match up (and since the resolution is seconds, it is easy to be off by just enough to see it).

I am seeing a case where during schema propagation .isMarkedForDelete() is checked, but the timestamp is a few seconds in the future because the schema was sent from a different node. The code then happily tries to interpret the value of the column as a String, but it is actually the Int encoded deletion time.

Here is an example in the code that does this check and will do the wrong thing if the deletion timestamp is even just a few seconds in the future: https://github.com/apache/cassandra/blob/47f0cc5d38d272ec9f7d6179eb3ffa28c6f74107/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java#L607-609

To prove that this is a problem, here is a stack trace of a machine trying to interpret the ""localDeletionTime"" value of a DeletedColumn as UTF-8 because the .isMarkedForDeletion() check failed:

https://gist.github.com/deb064d4377d206368d3",slebresne,wadey,Normal,Resolved,Fixed,05/Jun/12 06:09,16/Apr/19 09:32
Bug,CASSANDRA-4309,12559451,"CQL3: cqlsh exception running ""describe schema""","{code}
cqlsh> describe schema;

CREATE KEYSPACE system WITH strategy_class = 'LocalStrategy';

USE system;

Traceback (most recent call last):
  File ""./cqlsh"", line 811, in onecmd
    self.handle_statement(st, statementtext)
  File ""./cqlsh"", line 839, in handle_statement
    return custom_handler(parsed)
  File ""./cqlsh"", line 1329, in do_describe
    self.describe_schema()
  File ""./cqlsh"", line 1264, in describe_schema
    self.print_recreate_keyspace(k, sys.stdout)
  File ""./cqlsh"", line 1091, in print_recreate_keyspace
    self.print_recreate_columnfamily(ksname, cf.name, out)
  File ""./cqlsh"", line 1114, in print_recreate_columnfamily
    layout = self.get_columnfamily_layout(ksname, cfname)
  File ""./cqlsh"", line 706, in get_columnfamily_layout
    layout = self.fetchdict()
  File ""./cqlsh"", line 605, in fetchdict
    return dict(zip([d[0] for d in desc], row))
TypeError: 'NoneType' object is not iterable
{code}",thepaul,cdaw,Low,Resolved,Fixed,05/Jun/12 21:45,16/Apr/19 09:32
Bug,CASSANDRA-4311,12559479,clean up messagingservice protocol limitations,"Weaknesses of the existing protocol:

- information asymmetry: node A can know what version node B expects, but not vice versa (see CASSANDRA-4101)
- delayed information: node A will often not know what version node B expects, until after first contacting node B -- forcing it to throw that first message away and retry for the next one
- protocol cannot handle both cross-dc forwarding and broadcast_address != socket address (see bottom of CASSANDRA-4099)
- version is partly global, partly per-connection, and partly per-message, resulting in some interesting hacks (CASSANDRA-3166) and difficulty layering more sophisticated OutputStreams on the socket (CASSANDRA-3127, CASSANDRA-4139)",jbellis,jbellis,Normal,Resolved,Fixed,06/Jun/12 02:18,16/Apr/19 09:32
Bug,CASSANDRA-4312,12559485,fix OOM with ReadMessageTest.testNoCommitLog,"this test can throw OOMs, because it uses a FileReader and readLine to read the commit log. However, some commit logs are fully allocated, but not initialized, (all 0s) so finding an EOL means reading 134M of data. Even for commit logs that have data they really aren't filereader-type streams.

changed to do simple byte finding in the streams instead.",dbrosius@apache.org,dbrosius@apache.org,Low,Resolved,Fixed,06/Jun/12 04:06,16/Apr/19 09:32
Bug,CASSANDRA-4313,12559572,CFS always try to load key cache,"Inside constructor, below condition is always evaluated to true:

{code}
if (caching != Caching.NONE || caching != Caching.ROWS_ONLY)
    CacheService.instance.keyCache.loadSaved(this);
{code}

should be

{code}
 if (caching == Caching.ALL || caching == Caching.KEYS_ONLY)
{code}",yukim,yukim,Low,Resolved,Fixed,06/Jun/12 17:42,16/Apr/19 09:32
Bug,CASSANDRA-4314,12559612,Index CF tombstones can cause OOM,"My database (now at 1.0.10) is in a state in which it goes out of memory with hardly any activity at all.  A key slice nothing more.

The logs attached are this including verbose gc in stdout.  I started up cassandra and waited a bit to ensure that it was unperturbed.

Then (about 15:46) I ran this slice (using Pelops), which in this case should return NO data.  My client times out and the database goes OOM.

                  ConsistencyLevel cl = ConsistencyLevel.TWO;//TWO nodes in my cluster
                  Selector s = new Selector(this.pool);
                  List<IndexExpression> indexExpressions = new ArrayList<IndexExpression>();
                  IndexExpression e = new IndexExpression(
                              ByteBuffer.wrap(""encryptionSettingsID"".getBytes(ASCII)), IndexOperator.EQ,
                              ByteBuffer.wrap(encryptionSettingsID.getBytes(Utils.ASCII)));
                  indexExpressions.add(e);
                  IndexClause indexClause = new IndexClause(indexExpressions,
                              ByteBuffer.wrap(EMPTY_BYTE_ARRAY), 1);
                  SlicePredicate predicate = new SlicePredicate();
                  predicate.setColumn_names(Arrays.asList(new ByteBuffer[]
                        { ByteBuffer.wrap(COL_PAN_ENC_BYTES) }));
                  List<KeySlice> slices = s.getKeySlices(CF_TOKEN, indexClause, predicate, cl);

Note that “encryptionSettingsID” is an indexed column.  When this is executed there should be no columns with the supplied value.

I suppose I may have some kind of blatant error in this query but it is not obvious to me.  I’m relatively new to cassandra.

My key space is defined as follows:

KsDef(name:TB_UNIT, strategy_class:org.apache.cassandra.locator.SimpleStrategy, strategy_options:{replication_factor=3}, 
cf_defs:[

CfDef(keyspace:TB_UNIT, name:token, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:70 61 6E 45 6E 63, validation_class:BytesType), ColumnDef(name:63 72 65 61 74 65 54 73, validation_class:DateType), ColumnDef(name:63 72 65 61 74 65 44 61 74 65, validation_class:DateType, index_type:KEYS, index_name:TokenCreateDate), ColumnDef(name:65 6E 63 72 79 70 74 69 6F 6E 53 65 74 74 69 6E 67 73 49 44, validation_class:UTF8Type, index_type:KEYS, index_name:EncryptionSettingsID)], caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:pan_d721fd40fd9443aa81cc6f59c8e047c6, column_type:Standard, comparator_type:BytesType, caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:counters, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:75 73 65 43 6F 75 6E 74, validation_class:CounterColumnType)], default_validation_class:CounterColumnType, caching:keys_only)

])


tpstats show pending tasks many minutes after time out:


[root@r610-lb6 bin]# ../cassandra/bin/nodetool -h 127.0.0.1 tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
ReadStage                         3         3            107         0                 0
RequestResponseStage              0         0             56         0                 0
MutationStage                     0         0              6         0                 0
ReadRepairStage                   0         0              0         0                 0
ReplicateOnWriteStage             0         0              0         0                 0
GossipStage                       0         0           2231         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
MemtablePostFlusher               0         0              3         0                 0
StreamStage                       0         0              0         0                 0
FlushWriter                       0         0              3         0                 0
MiscStage                         0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0              9         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
BINARY                       0
READ                         0
MUTATION                     0
REQUEST_RESPONSE             0

cfstats:

Keyspace: keyspace
        Read Count: 118
        Read Latency: 0.14722033898305084 ms.
        Write Count: 0
        Write Latency: NaN ms.
        Pending Tasks: 0
                Column Family: token
                SSTable count: 7
                Space used (live): 4745885584
                Space used (total): 4745885584
                Number of Keys (estimate): 18626048
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 118
                Read Latency: 0.147 ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 55058352
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 150
                Compacted row maximum size: 258
                Compacted row mean size: 201

                Column Family: pan_2fef6478b62242dd94aecaa049b9d7bb
                SSTable count: 7
                Space used (live): 1987147156
                Space used (total): 1987147156
                Number of Keys (estimate): 14955264
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 28056224
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 104
                Compacted row maximum size: 124
                Compacted row mean size: 124

                Column Family: counters
                SSTable count: 11
                Space used (live): 3433469364
                Space used (total): 3433469364
                Number of Keys (estimate): 21475328
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 40271696
                Key cache capacity: 4652
                Key cache size: 4652
                Key cache hit rate: NaN
                Row cache: disabled
                Compacted row minimum size: 125
                Compacted row maximum size: 179
                Compacted row mean size: 150

",jbellis,wpoziombka,Urgent,Resolved,Fixed,06/Jun/12 22:16,16/Apr/19 09:32
Bug,CASSANDRA-4317,12559719,AssertionError in handleStateNormal in a mixed cluster,"In a 3 node cluster with one seed on trunk, a member on trunk, and another member on a previous version, the following occurs only on the non-seed trunk member:

{noformat}

ERROR 16:44:18,708 Exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1072)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:995)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1568)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:819)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:897)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:57)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't repro if a non-trunk member is the seed, however upgrading the seed first should still be valid.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,07/Jun/12 16:51,16/Apr/19 09:32
Bug,CASSANDRA-4318,12559766,Nodetool compactionstats fails with NullPointerException,"Test uses Column family C defined as follows:

create column family C with caching = 'keys_only' and key_validation_class = 'LongType' and compression_options = { sstable_compression: SnappyCompressor, chunk_length_kb: 64 } and max_compaction_threshold=0; 

max_compaction_threshold is set to 0 to disable auto compaction.

SSTables are streamed via sstableloader, after which a major compaction is triggered using ""nodetool compact MyKeyspace C"".

Thereafter, attempts to request compaction stats via ""nodetool compactionstats"" fail with the following exception:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.compaction.CompactionInfo.asMap(CompactionInfo.java:103)
        at org.apache.cassandra.db.compaction.CompactionManager.getCompactions(CompactionManager.java:1115)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662) ",jbellis,sj.climber,Low,Resolved,Fixed,07/Jun/12 22:25,16/Apr/19 09:32
Bug,CASSANDRA-4320,12559780,Assertion error while delivering the hints.,"java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:351)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:269)
        at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:442)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Did some digging and looks like we just need to skip the deleted columns.",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,08/Jun/12 00:25,16/Apr/19 09:32
Bug,CASSANDRA-4321,12559783,stackoverflow building interval tree & possible sstable corruptions,"After upgrading to 1.1.1 (from 1.1.0) I have started experiencing StackOverflowError's resulting in compaction backlog and failure to restart. 

The ring currently consists of 6 DC's and 22 nodes using LCS & compression.  This issue was first noted on 2 nodes in one DC and then appears to have spread to various other nodes in the other DC's.  

When the first occurrence of this was found I restarted the instance but it failed to start so I cleared its data and treated it as a replacement node for the token it was previously responsible for.  This node successfully streamed all the relevant data back but failed again a number of hours later with the same StackOverflowError and again was unable to restart. 

The initial stack overflow error on a running instance looks like this:

ERROR [CompactionExecutor:314] 2012-06-07 09:59:43,017 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:314,1,main]
java.lang.StackOverflowError
        at java.util.Arrays.mergeSort(Arrays.java:1157)
        at java.util.Arrays.sort(Arrays.java:1092)
        at java.util.Collections.sort(Collections.java:134)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow.  Compactions stop from this point onwards]


I restarted this failing instance with DEBUG logging enabled and it throws the following exception part way through startup:

ERROR 11:37:51,046 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.db.Table$2.apply(Table.java:574)
        at org.apache.cassandra.db.Table$2.apply(Table.java:571)
        at com.google.common.collect.Iterators$8.next(Iterators.java:751)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG 11:37:51,052 Initializing ksU.cfS


And then finally fails with the following:

DEBUG 11:49:03,752 Creating IntervalNode from [Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b))]
java.lang.reflect.InvocationTargetException
DEBUG 11:49:03,753 Configured datacenter replicas are dc1:2, dc2:2, dc3:2, dc4:2, dc5:0, dc6:2, dc7:0, dc8:0, dc9:2
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3

Running with assertions enabled allows me to start the instance but when doing so I get errors such as:

ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)

and:

ERROR 01:27:58,946 Exception in thread Thread[CompactionExecutor:9,1,main]
java.lang.AssertionError: Last written key DecoratedKey(81958437188197992567937826278457419048, 4fa1aebad23f81e4321d344d) >= current key DecoratedKey(64546479828744423263742604083767363606, 4fcafc0f19f6a8092d4d4f94) writing into /var/lib/XX/data/cassandra/ks1/cf1/ks1-cf1-tmp-hd-657317-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Just like the initial errors compactions appear to stop occurring after this point.  

Given the above this looks like sstables are getting corrupted.  By restarting nodes I am able to identify several hundred sstables exhibiting the same problem and this appears to be growing.

I have tried scrubbing those affected nodes but the problem continues to occur.  If this is due to sstable corruptions is there another way of validating sstables for correctness?  Given that it has spread to various servers in other DC's it looks like this is directly related to the 1.1.1 upgrade recently performed on the ring.",slebresne,awinter,Normal,Resolved,Fixed,08/Jun/12 02:00,16/Apr/19 09:32
Bug,CASSANDRA-4322,12559863,Error in CLI when updating keyspace,"To repro:

1. Open the cli
2. Create a keyspace:
  create keyspace ks1 with placement_strategy = SimpleStrategy and strategy_options = {replication_factor:1};
3. Update the keyspace:
  update keyspace ks1 with strategy_options = {replication_factor:3};

The output is:

[default@unknown] create keyspace ks1 with placement_strategy = SimpleStrategy and strategy_options = {replication_factor:1};                               
8ecd5e16-e0f7-37e7-850e-38ee1a3a510e
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace ks1 with strategy_options = {replication_factor:3};                                        
857af387-6677-3e39-bdf6-e1132673c25b
Waiting for schema agreement...
... schemas agree across the cluster
org.apache.thrift.protocol.TProtocolException: Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
[default@unknown]

The problem is that the patch in CASSANDRA-4052 assumes the CLI is authenticated to a working keyspace.  getKSMetaData in executeUpdateKeySpace is called with keySpace, which is null.

Changing this to keyspaceName partially solves it, we now get:

[default@unknown] update keyspace ks1 with strategy_options = {replication_factor:3};
Not authenticated to a working keyspace.
18d750fc-19d9-30f0-b8b9-18b2e4a0a0d4
Waiting for schema agreement...
... schemas agree across the cluster
Not authenticated to a working keyspace.

This comes from replayAssumptions in getKSMetaData.

It seems that the refresh code needs to be reworked slightly to not assume the CLI is authenticated to a keyspace.",xedin,richardlow,Low,Resolved,Fixed,08/Jun/12 16:03,16/Apr/19 09:32
Bug,CASSANDRA-4328,12560155,CQL client timeout when inserting data after creating index,"After creating index on table inserts fails.
steps (from cqlsh -3)
create table myapp (pidh text, cn text, tn text, s text, m text, ts bigint, PRIMARY KEY (pidh, ts));
INSERT INTO myapp(pidh, cn, tn, s, m, ts) VALUES ('4274@localhost','Test.tests','main','text','bzzzzz',2231897614162493);
create index idx_cn on myapp(cn);

Next insert from cql client time outs without showing error.
Each insert in systemlog gives ERROR [MutationStage:xx] ....
from log file:

 INFO [MigrationStage:1] 2012-06-11 12:28:35,715 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,716 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,868 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db (1312 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [MigrationStage:1] 2012-06-11 12:28:35,869 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:35,869 CompactionTask.java (line 109) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-141-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-142-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-140-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db')]
 INFO [FlushWriter:4] 2012-06-11 12:28:35,869 Memtable.java (line 266) Writing Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,104 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-65-Data.db (325 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:36,130 CompactionTask.java (line 221) Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-144-Data.db,].  42,461 to 38,525 (~90% of original) bytes for 3 keys at 0.140767MB/s.  Time: 261ms.
 INFO [MigrationStage:1] 2012-06-11 12:28:36,140 SecondaryIndexManager.java (line 208) Creating new index : ColumnDefinition{name=636e, validator=org.apache.cassandra.db.marshal.UTF8Type, index_type=KEYS, index_name='idx_cn', component_index=1}
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,141 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,141 Memtable.java (line 266) Writing Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,255 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db (170 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,256 SecondaryIndex.java (line 159) Submitting index build of myapp.idx_cn for data in SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db')
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,258 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,258 Memtable.java (line 266) Writing Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,390 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/IndexInfo/system-IndexInfo-hd-14-Data.db (84 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8744)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,390 SecondaryIndex.java (line 200) Index build of myapp.idx_cn complete
ERROR [MutationStage:37] 2012-06-11 12:28:39,657 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:37,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more


ERROR [MutationStage:39] 2012-06-11 12:29:39,876 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:39,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more
",slebresne,evilezh,Normal,Resolved,Fixed,11/Jun/12 11:44,16/Apr/19 09:32
Bug,CASSANDRA-4331,12560299,sstable2json error,"/apache-cassandra-1.1.1/bin> ./sstable2json  /home/cassandra/data/pimda/CF_bookmark/pimda-CF_bookmark-hd-48-Data.db > test.json


ERROR 22:27:14,215 Error in ThreadPoolExecutor
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
	at java.util.TreeMap.getEntry(TreeMap.java:328)
	at java.util.TreeMap.containsKey(TreeMap.java:209)
	at java.util.TreeSet.contains(TreeSet.java:217)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:396)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
	at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:27:14,219 Error in ThreadPoolExecutor
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)",jbellis,gang0713,Normal,Resolved,Fixed,12/Jun/12 06:30,16/Apr/19 09:32
Bug,CASSANDRA-4334,12560374,"cqlsh tab completion error in ""CREATE KEYSPACE""","The following:
{code}
cqlsh> CREATE KEYSPACE test WITH strategy_class = 'S<TAB>
{code}
will tab complete like this:
{code}
cqlsh> CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy '
{code}
Note the extra space after SimpleStrategy. Not a big deal to remove, but it could be misleading to people.",aleksey,tpatterson,Low,Resolved,Fixed,12/Jun/12 16:25,16/Apr/19 09:32
Bug,CASSANDRA-4337,12560470,Data insertion fails because of commitlog rename failure,"h3. Configuration
Cassandra server configuration:
{noformat}heap size: 4 GB
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: ""xxx.xxx.xxx.10,xxx.xxx.xxx.11""
listen_address: xxx.xxx.xxx.10
rpc_address: 0.0.0.0
rpc_port: 9160
rpc_timeout_in_ms: 20000
endpoint_snitch: PropertyFileSnitch{noformat}

cassandra-topology.properties
{noformat}xxx.xxx.xxx.10=datacenter1:rack1
xxx.xxx.xxx.11=datacenter1:rack1
default=datacenter1:rack1{noformat}

Ring configuration:
{noformat}Address         DC          Rack        Status State   Load            Effective-Ownership Token
                                                                                           85070591730234615865843651857942052864
xxx.xxx.xxx.10  datacenter1 rack1       Up     Normal  23,11 kB        100,00%             0
xxx.xxx.xxx.11  datacenter1 rack1       Up     Normal  23,25 kB        100,00%             85070591730234615865843651857942052864{noformat}

h3.Problem
I have ctreated keyspace and column family using CLI commands:
{noformat}create keyspace testks with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options = {datacenter1:2};
use testks;
create column family testcf;{noformat}

Then I started my Java application, which inserts 50 000 000 rows to created column family using Hector client. Client is connected to node 1.
After about 30 seconds (160 000 rows were inserted) Cassandra server on node 1 throws an exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-13 10:26:38,393 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}
	
Then, few seconds later Cassandra server on node 2 throws the same exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-14 10:26:44,005 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}

After that, my application cannot insert any more data. Hector gets TimedOutException from Thrift:
{noformat}Thread-4 HConnectionManager.java 306 2012-06-14 10:26:56,034 HConnectionManager  operateWithFailover 	 WARN  	 %Could not fullfill request on this host CassandraClient<xxx.xxx.xxx.10:9160-10> 
Thread-4 HConnectionManager.java 307 2012-06-14 10:26:56,034 HConnectionManager operateWithFailover 	 WARN  	 %Exception:  
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:35)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)
	at patrycjusz.nosqltest.db.cassandra.CassandraHectorDbAdapter.commitTransaction(CassandraDbAdapter.java:63)
	at patrycjusz.nosqltest.DbTest.insertData(DbTest.java:459)
	at patrycjusz.nosqltest.gui.InsertPanel.executeTask(NePanel.java:154)
	at patrycjusz.nosqltest.gui.InsertPanel$1.run(NePanel.java:141)
	at java.lang.Thread.run(Unknown Source)
Caused by: TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20269)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:922)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:908)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:103)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:258)
	... 8 more{noformat}",jbellis,patrycjusz,Normal,Resolved,Fixed,13/Jun/12 09:34,16/Apr/19 09:32
Bug,CASSANDRA-4343,12560701,fix sstable blacklisting for LCS,,jbellis,jbellis,Low,Resolved,Fixed,14/Jun/12 23:56,16/Apr/19 09:32
Bug,CASSANDRA-4344,12560755,Windows tools don't work and litter the environment,"On Windows the tools either don't work at all (cassandra-stress) and/or litter the shell environment (cassandra-stress & sstablemetadata) by repeatedly appending the same information to variables, eventually running out of space.
",h2o,h2o,Normal,Resolved,Fixed,15/Jun/12 11:49,16/Apr/19 09:32
Bug,CASSANDRA-4349,12560856,PFS should give a friendlier error message when a node has not been configured,see CASSANDRA-4345,jbellis,jbellis,Low,Resolved,Fixed,16/Jun/12 03:06,16/Apr/19 09:32
Bug,CASSANDRA-4352,12594967,cqlsh: ASSUME functionality broken by CASSANDRA-4198 fix,"All uses of the {{ASSUME}} command in cqlsh now appear to be wholly ineffective at affecting subsequent value output.

This is due to a change in the grammar definition introduced by the fix for CASSANDRA-4198, upon which definition the ASSUME functionality relied.

All that's needed to fix is to update the token-binding names used.",thepaul,thepaul,Low,Resolved,Fixed,18/Jun/12 17:10,16/Apr/19 09:32
Bug,CASSANDRA-4353,12594970,no error propagated to client when updating a column family with an invalid column def,"CASSANDRA-3761 appears to have introduced a regression which is exposed by test_system_column_family_operations in test/system/test_thrift_server.py

The test fails with this stack trace:
{noformat}
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_system_column_family_operations
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 1469, in test_system_column_family_operations
    _expect_exception(fail_invalid_field, InvalidRequestException)
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 209, in _expect_exception
    r = fn()
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 1468, in fail_invalid_field
    client.system_update_column_family(modified_cf)
  File ""/usr/lib/python2.6/site-packages/cassandra/Cassandra.py"", line 1892, in system_update_column_family
    return self.recv_system_update_column_family()
  File ""/usr/lib/python2.6/site-packages/cassandra/Cassandra.py"", line 1903, in recv_system_update_column_family
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/usr/lib64/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""/usr/lib64/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 272, in read
    self.readFrame()
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 276, in readFrame
    buff = self.__trans.readAll(4)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TSocket.py"", line 108, in read
    raise TTransportException(type=TTransportException.END_OF_FILE, message='TSocket read 0 bytes')
TTransportException: TSocket read 0 bytes

----------------------------------------------------------------------

{noformat}

The logs have the following stack trace:

{noformat}
ERROR [Thrift:1] 2012-06-18 18:17:27,865 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:47)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:115)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1303)
        at org.apache.cassandra.config.CFMetaData.columnMetadata(CFMetaData.java:228)
        at org.apache.cassandra.config.CFMetaData.fromThrift(CFMetaData.java:648)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1061)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

",soverton,soverton,Low,Resolved,Fixed,18/Jun/12 17:31,16/Apr/19 09:32
Bug,CASSANDRA-4360,12595278,Fix broken streaming after CASSANDRA-4311,"CASSANDRA-4311 made change in message exchange, that the message will contain header only at the beginning of exchange. This causes FileStreamTask to fail since it expects message header in StreamReply message.",yukim,yukim,Low,Resolved,Fixed,20/Jun/12 15:38,16/Apr/19 09:32
Bug,CASSANDRA-4364,12595422,Compaction invalidates row cache,"Compactions invalidate row cache after CASSANDRA-3862

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionIterable.java#L87",jbellis,marcuse,Low,Resolved,Fixed,21/Jun/12 12:29,16/Apr/19 09:32
Bug,CASSANDRA-4365,12595431,Use CF comparator to sort indexed columns in SecondaryIndexManager,"SecondaryIndexManager is supposed to have it's internal map sorted according to the base CF comparator, but instead it sorts using the byte buffer natural ordering.

This order is carried along by the sorted set returned by getIndexedColumns(), which in turns end up in a NamesQueryFilter when reading indexed columns, so the order should really be the CF one.

I'll note that I don't think this is a bug because SSTableNamesIterator don't in fact rely on the actual ordering of the names. But it's worth fixing to avoid future problems.",slebresne,slebresne,Low,Resolved,Fixed,21/Jun/12 13:57,16/Apr/19 09:32
Bug,CASSANDRA-4368,12595636,bulkLoad() method in StorageService throws AssertionError,"Firstly, I apologize if this is a duplicate, as I cannot find a bug related to that.

We tried to stream some data to our Cassandra cluster by using JMX bulkLoad method. However, jmx reports a AssertionError since 1.1.0. I haven't really debugged into Cassandra, but by eyeballing the code it seems the AssertionError is thrown from SSTableReader.open() method with the line:
{code}
assert practitioner != null;
{code}
and tracing the code backwards, it seems the code in SSTableLoader.openSSTables() method has been changed to get the partitioner from the impl of inner class SSTableLoader.Client:
{code}
sstables.add(SSTableReader.open(desc, components, null, client.getPartitioner()));
{code}
This is different than 1.0.x codebase, when the partitioner is retrieved from StorageService:
{code}
sstables.add(SSTableReader.open(desc, components, null, StorageService.getPartitioner()));
{code}
The problem seems to me is when StorageService.bulkLoad instantiaties an impl of SSTableLoader.Client() it never does anything with the partitioner, resulting in the call 'client.getPartitioner()' returning null, thus the AssertionError.

(Note: this is me eyeballing the code only without debugging into it).
",brandon.williams,dinoscottie,Normal,Resolved,Fixed,22/Jun/12 21:19,16/Apr/19 09:32
Bug,CASSANDRA-4370,12595782,hsha server may stop responding and will not close selectors,"Cassandra launches several threads to listen on selectors. There can be CancelledKeyException and cassandra will log ""Unexpected exception"". In that case there two problems:
1) listener thread will be closed and cassandra will stop after all listener threads will stop.
2) selector will be not closed",kvaster,kvaster,Normal,Resolved,Fixed,25/Jun/12 12:20,16/Apr/19 09:32
Bug,CASSANDRA-4372,12595805,CQL3 Range Query contains unwanted results with composite columns,"Here is a CQL3 range query sample where I get wrong results (tested using cqlsh --cql3) from my perspective:

CREATE KEYSPACE testing WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;

USE testing;

CREATE TABLE bug_test (a int, b int, c int, d int, e int, f text, PRIMARY KEY (a, b, c, d, e) );

INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 2, '2');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 2, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 3, '3');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 5, '5');

----------

Normal select everything query:

SELECT * FROM bug_test;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 1 | 1
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Everything fine so far.

----------

Select with greater equal comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e >= 2;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

----------

Select with greater comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e > 2;

Results:
 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

The same issue is also present with between ranges (e >= 1 AND e <= 2)...",slebresne,grinser,Normal,Resolved,Fixed,25/Jun/12 16:33,16/Apr/19 09:32
Bug,CASSANDRA-4375,12595863,FD incorrectly using RPC timeout to ignore gossip heartbeats,"Short version: You can't run a cluster with short RPC timeouts because nodes just constantly flap up/down.

Long version:

CASSANDRA-3273 tried to fix a problem resulting from the way the failure detector works, but did so by introducing a much more sever bug: With low RPC timeouts, that are lower than the typical gossip propagation time, a cluster will just constantly have all nodes flapping other nodes up and down.

The cause is this:

{code}
+    // in the event of a long partition, never record an interval longer than the rpc timeout,
+    // since if a host is regularly experiencing connectivity problems lasting this long we'd
+    // rather mark it down quickly instead of adapting
+    private final double MAX_INTERVAL_IN_MS = DatabaseDescriptor.getRpcTimeout();
{code}

And then:

{code}
-        tLast_ = value;            
-        arrivalIntervals_.add(interArrivalTime);        
+        if (interArrivalTime <= MAX_INTERVAL_IN_MS)
+            arrivalIntervals_.add(interArrivalTime);
+        else
+            logger_.debug(""Ignoring interval time of {}"", interArrivalTime);
{code}

Using the RPC timeout to ignore unreasonably long intervals is not correct, as the RPC timeout is completely orthogonal to gossip propagation delay (see CASSANDRA-3927 for a quick description of how the FD works).

In practice, the propagation delay ends up being in the 0-3 second range on a cluster with good local latency. With a low RPC timeout of say 200 ms, very few heartbeat updates come in fast enough that it doesn't get ignored by the failure detector. This in turn means that the FD records a completely skewed average heartbeat interval, which in turn means that nodes almost always get flapped on interpret() unless they happen to *just* have had their heartbeat updated. Then they flap back up whenever the next heartbeat comes in (since it gets brought up immediately).

In our build, we are replacing the FD with an implementation that simply uses a fixed {{N}} second time to convict, because this is just one of many ways in which the current FD hurts, while we still haven't found a way it actually helps relative to the trivial fixed-second conviction policy.

For upstream, assuming people won't agree on changing it to a fixed timeout, I suggest, at minimum, never using a value lower than something like 10 seconds or something, when determining whether to ignore. Slightly better is to make it a config option.

(I should note that if propagation delays are significantly off from the expected level, other things than the FD already breaks - such as the whole concept of {{RING_DELAY}}, which assumes the propagation time is roughly constant with e.g. cluster size.)",brandon.williams,scode,Normal,Resolved,Fixed,26/Jun/12 04:41,16/Apr/19 09:32
Bug,CASSANDRA-4377,12595934,CQL3 column value validation bug,"{noformat}
cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' and strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> CREATE TABLE stats (
        ...   gid          blob,
        ...   period     int,
        ...   tid          blob, 
        ...   sum        int,
        ...   uniques           blob,
        ...   PRIMARY KEY(gid, period, tid)
        ... );
cqlsh:test> describe columnfamily stats;

CREATE TABLE stats (
  gid blob PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}

You can see in the above output that the stats cf is created with the column validator set to text, but neither of the non primary key columns defined are text. It should either be setting metadata for those columns or not setting a default validator or some combination of the two.",xedin,nickmbailey,Normal,Resolved,Fixed,26/Jun/12 16:44,16/Apr/19 09:32
Bug,CASSANDRA-4379,12595941,cleanup optimization can delete data but not corresponding index entries,introduced by CASSANDRA-4079,jbellis,jbellis,Normal,Resolved,Fixed,26/Jun/12 17:18,16/Apr/19 09:32
Bug,CASSANDRA-4380,12595967,CQLSH: describe command doesn't output valid CQL command.,"{noformat}
cqlsh:test> describe columnfamily stats;

CREATE TABLE stats (
  gid blob PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}

I can create a cf in cql3 and then use describe cf to get the above output. However trying to run that create statement says that all of the following are invalid options:

* default_validation
* min_compaction_threshold
* max_compaction_threshold
* comparator",thepaul,nickmbailey,Normal,Resolved,Fixed,26/Jun/12 19:20,16/Apr/19 09:32
Bug,CASSANDRA-4384,12596137,HintedHandoff can begin before SS knows the hostID,"Since HH fires from the FD, SS won't quite have the hostId yet:

{noformat}
 INFO 18:58:04,196 Started hinted handoff for host: null with IP: /10.179.65.102
 INFO 18:58:04,197 Node /10.179.65.102 state jump to normal
ERROR 18:58:04,197 Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:120)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:304)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:250)
        at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:87)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:433)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Simple solution seems to be getting the hostId from gossip instead.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,27/Jun/12 19:00,16/Apr/19 09:32
Bug,CASSANDRA-4385,12596170,bug when trying to describe a cf in a pre cql3 case sensitive keyspace,"I can't describe column families in my schema defined via cassandra-cli. Update also seems to fail for the same CF's.

CREATE KEYSPACE Hastur
  with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
  and strategy_options = {replication_factor:2};

CREATE COLUMN FAMILY LookupByKey
  with compaction_strategy = 'LeveledCompactionStrategy'
  and compression_options = null;

Then later, https://gist.github.com/3006886",thepaul,al@ooyala.com,Low,Resolved,Fixed,27/Jun/12 21:36,16/Apr/19 09:32
Bug,CASSANDRA-4389,12596244,HHOM.scheduleAllDeliveries still expects tokens as row keys,scheduleAllDeliveries needs updating to expect hostIds instead of tokens in the HINTS_CF,soverton,soverton,Low,Resolved,Fixed,28/Jun/12 11:44,16/Apr/19 09:32
Bug,CASSANDRA-4390,12596249,Query with WHERE statement delivers wrong results,"Data is written to cassandra via phpcassa ( https://github.com/thobbs/phpcassa v1.0).

Inspection the records via cqlsh,the query with a WHERE statement doesn't deliver all records.

https://issues.apache.org/jira/secure/attachment/12533818/Cassandra_Query_Issue.png

I'am working with a 2 node cluster and the effect is the same on both nodes. Furthermore a nodetool repair, nodetool rebuild has no positive effect. The data type for this column is varint. 


******

DESCRIBE COLUMNFAMILY rm_advertisements

CREATE TABLE rm_advertisements (
  KEY text PRIMARY KEY,
  css text,
  form text,
  custom_field2 text,
  vacancy_type text,
  custom_field3 text,
  active boolean,
  vacancy_responsibles text,
  job_site text,
  vacancy_email_notification text,
  custom_field4 text,
  vacancy_name text,
  startdate text,
  vacancy_id varint,
  custom_field5 text,
  html text,
  company_id varint,
  ctime text,
  title text,
  expires text,
  atime text,
  custom_field1 text,
  vacancy_language text,
  tags text,
  mtime text,
  mailapply boolean,
  vacancy_description text,
  vacancy_function text,
  shorthash text
) WITH
  comment='Advertisements' AND
  comparator=text AND
  read_repair_chance=1.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

CREATE INDEX rm_advertisements_active ON rm_advertisements (active);

CREATE INDEX rm_advertisements_job_site ON rm_advertisements (job_site);

CREATE INDEX rm_advertisements_vacancy_id ON rm_advertisements (vacancy_id);

CREATE INDEX rm_advertisements_company_id ON rm_advertisements (company_id);

CREATE INDEX rm_advertisements_title ON rm_advertisements (title);

CREATE INDEX rm_advertisements_expires ON rm_advertisements (expires);

CREATE INDEX rm_advertisements_vacancy_language ON rm_advertisements (vacancy_language);

CREATE INDEX rm_advertisements_mailapply ON rm_advertisements (mailapply);

CREATE INDEX rm_advertisements_shorthash ON rm_advertisements (shorthash);
",,bite,Normal,Resolved,Fixed,28/Jun/12 12:00,16/Apr/19 09:32
Bug,CASSANDRA-4395,12596377,SSTableNamesIterator misses some tombstones,The title says it all.,slebresne,slebresne,Normal,Resolved,Fixed,29/Jun/12 11:07,16/Apr/19 09:32
Bug,CASSANDRA-4396,12596424,Subcolumns not removed when compacting tombstoned super column,"When we compact a tombstone for a super column with the old data for that super column, we end up writing the deleted super column and all the subcolumn data that is now worthless to the new sstable. This is especially inefficient when reads need to scan tombstones during a slice.

Here is the output of a simple test I ran to confirm:

insert supercolumn, then flush
{noformat}
Nicks-MacBook-Pro:12:20:52 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-1-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": -9223372036854775808, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
{noformat}

delete supercolumn, flush again

{noformat}
[Nicks-MacBook-Pro:12:20:59 cassandra-1.0] cassandra$ bin/nodetool -h localhost flush
[Nicks-MacBook-Pro:12:22:41 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-2-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": []}}
}
{noformat}

compact and check resulting sstable

{noformat}
[Nicks-MacBook-Pro:12:22:55 cassandra-1.0] cassandra$ bin/nodetool -h localhost compact 
[Nicks-MacBook-Pro:12:23:09 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-3-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
[Nicks-MacBook-Pro:12:23:20 cassandra-1.0] cassandra$ 
{noformat}",jbellis,nickmbailey,Low,Resolved,Fixed,29/Jun/12 17:34,16/Apr/19 09:32
Bug,CASSANDRA-4399,12596527,use data size ratio in liveRatio instead of live size : serialized throughput,,jbellis,jbellis,Low,Resolved,Fixed,01/Jul/12 06:59,16/Apr/19 09:32
Bug,CASSANDRA-4400,12596570,Correctly catch exception when Snappy cannot be loaded,"From the mailing list, on C* 1.1.1:
{noformat}
INFO 14:22:07,600 Global memtable threshold is enabled at 35MB
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:317)
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:219)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1681)
        at java.lang.Runtime.loadLibrary0(Runtime.java:840)
        at java.lang.System.loadLibrary(System.java:1047)
        at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
        ... 17 more
ERROR 14:22:09,934 Exception encountered during startup
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
{noformat}",slebresne,slebresne,Low,Resolved,Fixed,02/Jul/12 07:08,16/Apr/19 09:32
Bug,CASSANDRA-4401,12596665,"If processor is missing from /proc/cpuinfo, cassandra will not start","cassandra.env.sh does an egrep on /proc/cpuinfo in order to find the number of processors on the system.  If /proc/cpuinfo does not contain a processor :# line then the script will fail because of a divide  by 0 error.  Changing the Linux section of cassandra.env.sh to:


Linux)
            system_memory_in_mb=`free -m | awk '/Mem:/ {print $2}'`
            system_cpu_cores=`egrep -c 'processor([[:space:]]+):.*' /proc/cpuinfo`
            if [ ""$system_cpu_cores"" -lt ""1"" ]
            then
               system_cpu_cores=""1""
            fi
is a possible fix",,acobley,Low,Resolved,Fixed,02/Jul/12 15:17,16/Apr/19 09:32
Bug,CASSANDRA-4402,12596739,Atomicity violation bugs because of misusing concurrent collections,"My name is Yu Lin. I'm a Ph.D. student in the CS department at
UIUC. I'm currently doing research on mining Java concurrent library
misusages. I found some misusages of ConcurrentHashMap in Cassandra
1.1.1, which may result in potential atomicity violation bugs or harm
the performance.

The code below is a snapshot of the code in file
src/java/org/apache/cassandra/db/Table.java from line 348 to 369

L348        if (columnFamilyStores.containsKey(cfId))
L349        {
L350            // this is the case when you reset local schema
L351            // just reload metadata
L352            ColumnFamilyStore cfs = columnFamilyStores.get(cfId);
L353            assert cfs.getColumnFamilyName().equals(cfName);
            ...
L364        }
L365        else
L366        {
L367            columnFamilyStores.put(cfId, ColumnFamilyStore.createColumnFamilyStore(this, cfName));
L368        }

In the code above, an atomicity violation may occur between line 348
and 352. Suppose thread T1 executes line 348 and finds that the
concurrent hashmap ""columnFamilyStores"" contains the key
""cfId"". Before thread T1 executes line 352, another thread T2 removes
the ""cfId"" key from ""columnFamilyStores"". Now thread T1 resumes
execution at line 352 and will get a null value for ""cfs"". Then the
next line will throw a NullPointerException when invoking the method
on ""cfs"".

Second, the snapshot above has another atomicity violation. Let's look
at lines 348 and 367. Suppose a thread T1 executes line 348 and finds
out the concurrent hashmap does not contain the key ""cfId"". Before it
gets to execute line 367, another thread T2 puts a pair <cfid, v> in
the concurrent hashmap ""columnFamilyStores"". Now thread T1 resumes
execution and it will overwrite the value written by thread T2. Thus,
the code no longer preserves the ""put-if-absent"" semantics.

I found some similar misusages in other files:

In src/java/org/apache/cassandra/gms/Gossiper.java, similar atomicity
violation may occur if thread T2 puts a value to map
""endpointStateMap"" between lines <1094 and 1099>, <1173 and 1178>. Another
atomicity violation may occur if thread T2 removes the value on key
""endpoint"" between lines <681 and 683>.
",jacklondongood,jacklondongood,Low,Resolved,Fixed,02/Jul/12 20:40,16/Apr/19 09:32
Bug,CASSANDRA-4403,12596836,cleanup uses global partitioner to estimate ranges in index sstables,"Introduced in CASSANDRA-1404, CleanupTest is showing this on trunk (on stderr, so test doesn't actually fail):

{noformat}
    [junit] java.lang.ClassCastException: org.apache.cassandra.dht.Token$KeyBound cannot be cast to org.apache.cassandra.dht.Token
    [junit]     at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:24)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:386)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:383)
    [junit]     at java.util.Arrays.mergeSort(Arrays.java:1270)
    [junit]     at java.util.Arrays.sort(Arrays.java:1210)
    [junit]     at java.util.Collections.sort(Collections.java:159)
    [junit]     at org.apache.cassandra.dht.Range.normalize(Range.java:382)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:570)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:549)
    [junit]     at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:111)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:136)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't happen on the 1.1 branch (less robust test?) but the problem is still there.",jbellis,jbellis,Low,Resolved,Fixed,02/Jul/12 22:19,16/Apr/19 09:32
Bug,CASSANDRA-4404,12596837,tombstone estimation needs to avoid using global partitioner against index sstables,,yukim,jbellis,Normal,Resolved,Fixed,02/Jul/12 22:26,16/Apr/19 09:32
Bug,CASSANDRA-4409,12597426,Only consider whole row tombstone in collation controller,"CollationController has that optimization that if it has already seen a row tombstone more recent that a sstable max timestamp, it skips the sstable.  However, this was not updated correctly while introducing range tombstone and currently the code might skip a sstable based on the timestamp of a tombstone that does not cover the full row.",slebresne,slebresne,Low,Resolved,Fixed,04/Jul/12 09:55,16/Apr/19 09:32
Bug,CASSANDRA-4411,12597524,Assertion with LCS compaction,"As instructed in CASSANDRA-4321 I have raised this issue as a continuation of that issue as it appears the problem still exists.

I have repeatedly run sstablescrub across all my nodes after the 1.1.2 upgrade until sstablescrub shows no errors.  The exceptions described in CASSANDRA-4321 do not occur as frequently now but the integrity check still throws exceptions on a number of nodes.  Once those exceptions occur compactionstats shows a large number of pending tasks with no progression afterwards.

{code}
ERROR [CompactionExecutor:150] 2012-07-05 04:26:15,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:150,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
",slebresne,awinter,Normal,Resolved,Fixed,05/Jul/12 04:52,16/Apr/19 09:32
Bug,CASSANDRA-4419,12597744,leveled compaction generates too small sstables,"When I set sstable_size_in_mb to 96 I end up with sstable data files no larger than 60M.

This in turn messes up the LeveledManifest calculation since it finds compaction candidates by summing up the size of sstables at a particular level and comparing it to the configured size multiplied by the desired number of sstables at a level, resulting in ~20 sstables in level 1 instead of the 10 that one would expect from looking at LeveledManifest.

Some additional logging here reveals that the position parameter passed to LeveledCompactionTask.newSSTableSegmentThresholdReached() is significantly higher than the size of the output file.",slebresne,noa,Low,Resolved,Fixed,06/Jul/12 10:55,16/Apr/19 09:32
Bug,CASSANDRA-4420,12597764,Possible schema corruption with cql 3.0,"Hi,

i've got some problems while creating schemas with cql 3.0. After that i can't even start cassandra anymore.

Following steps for reproduction were done on a new installation of cassandra:

1. simply create a keyspace test via ""cqlsh -3""

create keyspace test with strategy_class = 'SimpleStrategy' and strategy_options:replication_factor = 1;

2. add cf with composite columns via ""cqlsh -3""

create table test1 (
    a int,
    b int,
    c int,
    d int,
    primary key (a, b, c)
);

3. drop column family 

drop columnfamily test1;

So until now everything went fine. Now i'm trying to insert a slightly modified column family with the same name above.

4. create new cf via ""cqlsh -3""

create table test1 (
    a int,
    b int,
    c int,
    primary key (a, b)
);

This creation fails with following exception:


java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
        at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
        at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
        at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)


Now at this point you can't do anything anymore via cql or cli. Shutting down and starting cassandra again throws same exceptions:


ERROR 14:48:41,705 Exception encountered during startup
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2Exception encountered during startup: Index: 2, Size: 2

	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)


Actually it's the result of a slightly different problem in combination with composite columns, but i will describe this later.

I've got no idea, what the problem is, there might be some corruption in table schemas, even after dropping tables.

I have to delete cassandra data in order to get cassandra running again.

Best Regards 

Bert Passek",xedin,bertpassek,Normal,Resolved,Fixed,06/Jul/12 12:53,16/Apr/19 09:32
Bug,CASSANDRA-4427,12597963,Restarting a failed bootstrap instajoins the ring,"I think when we made auto_bootstrap = true the default, we broke the check for the bootstrap flag, creating a dangerous situation.",jbellis,brandon.williams,Normal,Resolved,Fixed,08/Jul/12 23:06,16/Apr/19 09:32
Bug,CASSANDRA-4429,12598029,nodetool ring throws java.lang.AssertionError in TokenMetadata.getTopology,"
{noformat}
$ bin/nodetool -h localhost ring
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getTopology(TokenMetadata.java:851)
        at org.apache.cassandra.service.StorageService.effectiveOwnership(StorageService.java:2781)
        at org.apache.cassandra.service.StorageService.effectiveOwnership(StorageService.java:70)
{noformat}

TokenMetadata.getTopology() can only be called on a clone of TokenMetadata, not the StorageService instance.
",soverton,soverton,Normal,Resolved,Fixed,09/Jul/12 13:04,16/Apr/19 09:32
Bug,CASSANDRA-4432,12598217,Change nanoTime() to currentTimeInMillis() in schema related code.,"From nanoTime() description:

""The value returned represents nanoseconds since some fixed but arbitrary time (perhaps in the future, so values may be negative). This method provides nanosecond precision, but not necessarily nanosecond accuracy. No guarantees are made about how frequently values change.""

Also see http://www.mail-archive.com/dev@cassandra.apache.org/msg04992.html
",xedin,xedin,Normal,Resolved,Fixed,10/Jul/12 16:51,16/Apr/19 09:32
Bug,CASSANDRA-4435,12598461,hints compaction loop over same sstable,"Noticed the following while testing something else:

{noformat}
INFO 22:14:48,496 Completed flushing /var/lib/cassandra/data/system/hints/system-hints-ia-1-Data.db (109645 bytes) for commitlog position ReplayPosition(segmentId=9372773011543415, position=30358488)
 INFO 22:14:48,498 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-1-Data.db')]
 INFO 22:14:48,500 SSTables for user defined compaction are already being compacted.
 INFO 22:14:48,500 Finished hinted handoff of 16893 rows to endpoint /10.179.64.227
 INFO 22:14:48,658 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-2-Data.db,].  109,645 to 899 (~0% of original) bytes for 1 keys at 0.005392MB/s.  Time: 159ms.
 INFO 22:14:48,660 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-2-Data.db')]
 INFO 22:14:48,668 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-3-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,669 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-3-Data.db')]
 INFO 22:14:48,679 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-4-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.095261MB/s.  Time: 9ms.
 INFO 22:14:48,680 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-4-Data.db')]
 INFO 22:14:48,697 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-5-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.050433MB/s.  Time: 17ms.
 INFO 22:14:48,698 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-5-Data.db')]
 INFO 22:14:48,714 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-6-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,715 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-6-Data.db')]
 INFO 22:14:48,722 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-7-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,723 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-7-Data.db')]
 INFO 22:14:48,736 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-8-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.065950MB/s.  Time: 13ms.
 INFO 22:14:48,737 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-8-Data.db')]
 INFO 22:14:48,744 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-9-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,745 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-9-Data.db')]
 INFO 22:14:48,753 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-10-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,754 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-10-Data.db')]
 INFO 22:14:48,761 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-11-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,762 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-11-Data.db')]
 INFO 22:14:48,775 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-12-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.065950MB/s.  Time: 13ms.
 INFO 22:14:48,776 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-12-Data.db')]
 INFO 22:14:48,783 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-13-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,784 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-13-Data.db')]
 INFO 22:14:48,792 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-14-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,793 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-14-Data.db')]
 INFO 22:14:48,800 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-15-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,802 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-15-Data.db')]
 INFO 22:14:48,809 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-16-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,810 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-16-Data.db')]
 INFO 22:14:48,826 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-17-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,827 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-17-Data.db')]
 INFO 22:14:48,834 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-18-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,835 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-18-Data.db')]
 INFO 22:14:48,842 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-19-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,842 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-19-Data.db')]
 INFO 22:14:48,850 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-20-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,850 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-20-Data.db')]
 INFO 22:14:48,867 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-21-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,868 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-21-Data.db')]
 INFO 22:14:48,876 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-22-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,877 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-22-Data.db')]
 INFO 22:14:48,884 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-23-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,885 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-23-Data.db')]
 INFO 22:14:48,893 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-24-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,895 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-24-Data.db')]
 INFO 22:14:48,901 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-25-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,902 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-25-Data.db')]
 INFO 22:14:48,910 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-26-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,910 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-26-Data.db')]
 INFO 22:14:48,926 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-27-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.057157MB/s.  Time: 15ms.
 INFO 22:14:48,930 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-27-Data.db')]
 INFO 22:14:48,938 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-28-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,943 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-28-Data.db')]
 INFO 22:14:48,949 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-29-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,950 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-29-Data.db')]
 INFO 22:14:48,966 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-30-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,967 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-30-Data.db')]
 INFO 22:14:48,974 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-31-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,974 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-31-Data.db')]
 INFO 22:14:48,980 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-32-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,981 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-32-Data.db')]
 INFO 22:14:48,988 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-33-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,989 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-33-Data.db')]
 INFO 22:14:48,995 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-34-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.171471MB/s.  Time: 5ms.
 INFO 22:14:48,995 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-34-Data.db')]
 INFO 22:14:49,002 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-35-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.171471MB/s.  Time: 5ms.
 INFO 22:14:49,002 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-35-Data.db')]
{noformat}

After this, the loop stopped.  It also did not occur on another member which delivered hints, so it may not be easy to replicate.  I suspect something related to CASSANDRA-3442 caused this, though the odd thing is there shouldn't even be a tombstone left.",yukim,brandon.williams,Normal,Resolved,Fixed,11/Jul/12 21:48,16/Apr/19 09:32
Bug,CASSANDRA-4436,12598552,Counters in columns don't preserve correct values after cluster restart,"Similar to #3821. but affecting normal columns. 


Set up a 2-node cluster with rf=2.
1. Create a counter column family and increment a 100 keys in loop 5000 times. 
2. Then make a rolling restart to cluster. 
3. Again increment another 5000 times.
4. Make a rolling restart to cluster.
5. Again increment another 5000 times.
6. Make a rolling restart to cluster.


After step 6 we were able to reproduce bug with bad counter values. 
Expected values were 15 000. Values returned from cluster are higher then 15000 + some random number.
Rolling restarts are done with nodetool drain. Always waiting until second node discover its down then kill java process. ",slebresne,pvelas,Normal,Resolved,Fixed,12/Jul/12 13:32,16/Apr/19 09:32
Bug,CASSANDRA-4439,12598764,"Updating column family using cassandra-cli results in ""Cannot modify index name""","Using cassandra-cli the following update to a column family worked in 1.1.0:
{code}
create keyspace testing;
use testing;

create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

After upgrading to 1.1.2, the update statement fails with the following exception in system.log:
{quote}
ERROR [Thrift:16] 2012-07-13 14:51:54,558 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:161)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1063)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
        ... 11 more
Caused by: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
ERROR [MigrationStage:1] 2012-07-13 14:51:54,561 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
{quote}


After further testing the following works in 1.1.2:
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

So it appears that if you did not specify an index_name when creating the column originally, you cannot update the column family anymore.",xedin,a.schultz,Low,Resolved,Fixed,13/Jul/12 22:01,16/Apr/19 09:32
Bug,CASSANDRA-4441,12598998,Fix validation of dates,"Our date validation (timestamp type) doesn't validate that the date is correct, i.e. it allows dates like 2011-42-42, because DateUtils.parseDate() doesn't do the validation (don't ask me how it can generate a timestamp from bogus date, apparently it can).

The easy fix is to use DateUtils.parseDateStrictly(), which does the validation. This does require to update commons-lang to >= 2.5 (we have 2.4).",slebresne,slebresne,Low,Resolved,Fixed,16/Jul/12 17:04,16/Apr/19 09:32
Bug,CASSANDRA-4446,12599347,nodetool drain sometimes doesn't mark commitlog fully flushed,"I recently wiped a customer's QA cluster. I drained each node and verified that they were drained. When I restarted the nodes, I saw the commitlog replay create a memtable and then flush it. I have attached a sanitized log snippet from a representative node at the time. 

It appears to show the following :
1) Drain begins
2) Drain triggers flush
3) Flush triggers compaction
4) StorageService logs DRAINED message
5) compaction thread excepts
6) on restart, same CF creates a memtable
7) and then flushes it [1]

The columnfamily involved in the replay in 7) is the CF for which the compaction thread excepted in 5). This seems to suggest a timing issue whereby the exception in 5) prevents the flush in 3) from marking all the segments flushed, causing them to replay after restart.

In case it might be relevant, I did an online change of compaction strategy from Leveled to SizeTiered during the uptime period preceding this drain.

[1] Isn't commitlog replay not supposed to automatically trigger a flush in modern cassandra?",jbellis,rcoli,Low,Resolved,Fixed,18/Jul/12 21:30,16/Apr/19 09:32
Bug,CASSANDRA-4452,12599522,remove RangeKeySample from attributes in jmx,"RangeKeySample in org.apache.cassandra.db:type=StorageService MBean can be really huge (over 200MB in our case). That's a problem for monitoring tools as they're not build for that. Recommended and often used mx4j may be killer in this situation.

It would be good enough to make RangeKeySample ""operation"" instead of ""attribute"" in jmx. Looking at how MBeanServer.registerMBean() works we can do one of the following:
a) add some dummy parameter to getRangeKeySample
b) name it differently - not like getter (next time somebody will rename it back)
c) implement MXBean instead of MBean (a lot of work)

Any of those work. All of them are ""hacks"". Any better idea?



BTW: It's blocker for some installations. Our update to 1.1.2 caused downtime, downgrade back to 1.0.x, repairs, etc.",jendap,jendap,Low,Resolved,Fixed,19/Jul/12 20:15,16/Apr/19 09:32
Bug,CASSANDRA-4455,12599710,Nodetool fail to setcompactionthreshold,"first change compaction threshold from 4/32 to 2/2
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 2 2
It successful
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 4 32
Exception in thread ""main"" java.lang.RuntimeException: The min_compaction_threshold cannot be larger than the max.
        at org.apache.cassandra.db.ColumnFamilyStore.setMinimumCompactionThreshold(ColumnFamilyStore.java:1697)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeSetter(MBeanIntrospector.java:238)
        at com.sun.jmx.mbeanserver.PerInterface.setAttribute(PerInterface.java:84)
        at com.sun.jmx.mbeanserver.MBeanSupport.setAttribute(MBeanSupport.java:240)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.setAttribute(DefaultMBeanServerInterceptor.java:762)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.setAttribute(JmxMBeanServer.java:699)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.setAttribute(RMIConnectionImpl.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

The tool first try to set min then max, so it failed, since orign max is smaller the new min.
The work around is:
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 2 32
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 4 32
",aleksey,jasont,Low,Resolved,Fixed,21/Jul/12 10:17,16/Apr/19 09:32
Bug,CASSANDRA-4456,12599726,AssertionError in ColumnFamilyStore.getOverlappingSSTables() during repair,"We have hit the following exception on several nodes while running repairs across our 1.1.2 ring. We've observed it happen on either the node executing the repair or a participating replica in the repair operation. The result in either case is that the repair hangs.


ERROR [ValidationExecutor:9] 2012-07-21 01:54:03,019 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:9,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:874)
        at org.apache.cassandra.db.compaction.CompactionController.<init>(CompactionController.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:834)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:698)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:68)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:438)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


In building this ring we migrated sstables from an identical 0.8.8 ring by:

 1. Creating the schema on our new 1.1.2 ring.
 2. Rsyncing over sstables from 0.8.8 ring.
 3. Renaming the sstables to match the directory and file naming structure of 1.1.x.
 4. Ran nodetool refresh <keyspace> <cf> for each CF across each node.
 5. Ran nodetool upgradesstables for each CF across each node.

When those steps had completed, we began rolling repairs. Not all of the repair operations have hit the exception -- some of the repairs have completed successfully.
",slebresne,mheffner,Normal,Resolved,Fixed,21/Jul/12 14:07,16/Apr/19 09:32
Bug,CASSANDRA-4459,12599883,pig driver casts ints as bytearray,"we seem to be auto-mapping C* int columns to bytearray in Pig, and farther down I can't seem to find a way to cast that to int and do an average.  

{code}

grunt> cassandra_users = LOAD 'cassandra://cqldb/users' USING CassandraStorage();
grunt> dump cassandra_users;
(bobhatter,(act,22),(fname,bob),(gender,m),(highSchool,Cal High),(lname,hatter),(sat,500),(state,CA),{})
(alicesmith,(act,27),(fname,alice),(gender,f),(highSchool,Tuscon High),(lname,smith),(sat,650),(state,AZ),{})
 
// notice sat and act columns are bytearray values 
grunt> describe cassandra_users;
cassandra_users: {key: chararray,act: (name: chararray,value: bytearray),fname: (name: chararray,value: chararray),
gender: (name: chararray,value: chararray),highSchool: (name: chararray,value: chararray),lname: (name: chararray,value: chararray),
sat: (name: chararray,value: bytearray),state: (name: chararray,value: chararray),columns: {(name: chararray,value: chararray)}}

grunt> users_by_state = GROUP cassandra_users BY state;
grunt> dump users_by_state;
((state,AX),{(aoakley,(highSchool,Phoenix High),(lname,Oakley),state,(act,22),(sat,500),(gender,m),(fname,Anne),{})})
((state,AZ),{(gjames,(highSchool,Tuscon High),(lname,James),state,(act,24),(sat,650),(gender,f),(fname,Geronomo),{})})
((state,CA),{(philton,(highSchool,Beverly High),(lname,Hilton),state,(act,37),(sat,220),(gender,m),(fname,Paris),{}),(jbrown,(highSchool,Cal High),(lname,Brown),state,(act,20),(sat,700),(gender,m),(fname,Jerry),{})})

// Error - use explicit cast
grunt> user_avg = FOREACH users_by_state GENERATE cassandra_users.state, AVG(cassandra_users.sat);
grunt> dump user_avg;
2012-07-22 17:15:04,361 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.AVG as multiple or none of them fit. Please use an explicit cast.

// Unable to cast as int
grunt> user_avg = FOREACH users_by_state GENERATE cassandra_users.state, AVG((int)cassandra_users.sat);
grunt> dump user_avg;
2012-07-22 17:07:39,217 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1052: Cannot cast bag with schema sat: bag({name: chararray,value: bytearray}) to int
{code}

*Seed data in CQL*
{code}
CREATE KEYSPACE cqldb with 
  strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' 
  and strategy_options:replication_factor=3;	


use cqldb;

CREATE COLUMNFAMILY users (
  KEY text PRIMARY KEY, 
  fname text, lname text, gender varchar, 
  act int, sat int, highSchool text, state varchar);

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (gjames, Geronomo, James, f, 24, 650, 'Tuscon High', 'AZ');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (aoakley, Anne, Oakley, m , 22, 500, 'Phoenix High', 'AX');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (jbrown, Jerry, Brown, m , 20, 700, 'Cal High', 'CA');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (philton, Paris, Hilton, m , 37, 220, 'Beverly High', 'CA');

select * from users;
{code}",brandon.williams,cdaw,Normal,Resolved,Fixed,23/Jul/12 18:51,16/Apr/19 09:32
Bug,CASSANDRA-4460,12599929,SystemTable.setBootstrapState always sets bootstrap state to true,"    public static void setBootstrapState(BootstrapState state)
    {
        String req = ""INSERT INTO system.%s (key, bootstrapped) VALUES ('%s', '%b')"";
        processInternal(String.format(req, LOCAL_CF, LOCAL_KEY, getBootstrapState()));
        forceBlockingFlush(LOCAL_CF);
    }

Third parameter %b is set from getBootstrapState() which returns an enum, thus %b collapses to null/non null checks. This would seem then to always set it to true.",brandon.williams,dbrosius@apache.org,Low,Resolved,Fixed,24/Jul/12 00:45,16/Apr/19 09:32
Bug,CASSANDRA-4462,12600069,upgradesstables strips active data from sstables,"From the discussion here: http://mail-archives.apache.org/mod_mbox/cassandra-user/201207.mbox/%3CCAOac0GCtyDqS6ocuHOuQqre4re5wKj3o-ZpUZGkGsjCHzDVbTA%40mail.gmail.com%3E

We are trying to migrate a 0.8.8 cluster to 1.1.2 by migrating the sstables from the 0.8.8 ring to a parallel 1.1.2 ring. However, every time we run the `nodetool upgradesstables` step we find it removes active data from our CFs -- leading to lost data in our application.

The steps we took were:


1. Bring up a 1.1.2 ring in the same AZ/data center configuration with
tokens matching the corresponding nodes in the 0.8.8 ring.
2. Create the same keyspace on 1.1.2.
3. Create each CF in the keyspace on 1.1.2.
4. Flush each node of the 0.8.8 ring.
5. Rsync each non-compacted sstable from 0.8.8 to the corresponding node in
1.1.2.
6. Move each 0.8.8 sstable into the 1.1.2 directory structure by renaming the file to the  /cassandra/data/<keyspace>/<cf>/<keyspace>-<cf>... format. For example, for the keyspace ""Metrics"" and CF ""epochs_60"" we get:
""cassandra/data/Metrics/epochs_60/Metrics-epochs_60-g-941-Data.db"".
7. On each 1.1.2 node run `nodetool -h localhost refresh Metrics <CF>` for each CF in the keyspace. We notice that storage load jumps accordingly.
8. On each 1.1.2 node run `nodetool -h localhost upgradesstables`.

Afterwards we would test the validity of the data by comparing it with data from the original 0.8.8 ring. After an upgradesstables command the data was always incorrect.

With further testing we found that we could successfully use scrub to convert our sstables without data loss. However, any invocation of upgradesstables causes active data to be culled from the sstables:

 INFO [CompactionExecutor:4] 2012-07-24 04:27:36,837 CompactionTask.java (line 109) Compacting [SSTableReader(path='/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-51-Data.db')]
 INFO [CompactionExecutor:4] 2012-07-24 04:27:51,090 CompactionTask.java (line 221) Compacted to [/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-58-Data.db,].  60,449,155 to 2,578,102 (~4% of original) bytes for 4,002 keys at 0.172562MB/s.  Time: 14,248ms.

These are the steps we've tried:

WORKS		refresh -> scrub
WORKS		refresh -> scrub -> major compaction
WORKS		refresh -> scrub -> cleanup
WORKS		refresh -> scrub -> repair

FAILS		refresh -> upgradesstables
FAILS		refresh -> scrub -> upgradesstables
FAILS		refresh -> scrub -> repair -> upgradesstables
FAILS		refresh -> scrub -> major compaction -> upgradesstables

We have fewer than 143 million row keys in the CFs we're testing and none
of the *-Filter.db files are > 10MB, so I don't believe this is our
problem: https://issues.apache.org/jira/browse/CASSANDRA-3820

The keyspace is defined as:

Keyspace: Metrics:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [us-east:3]

And the column family that we tested with is defined as:

    ColumnFamily: metrics_900
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.1
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor

All rows have a TTL of 30 days and a gc_grace=0 so it's possible that a small number of older columns would be removed during a compaction/scrub/upgradesstables step. However, the majority should still be kept as their TTL's have not expired yet.",slebresne,mheffner,Normal,Resolved,Fixed,24/Jul/12 21:23,16/Apr/19 09:32
Bug,CASSANDRA-4463,12600072,Nodes Don't Restart: Assertion Error on Serializing Cache provider,"I stopped Cassandra on one of our 1.1.2 nodes and I couldn't start it any more. System.log didn't have much useful info but output.log had this:

java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:43)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:39)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:116)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:174)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:45)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:430)
        at org.apache.cassandra.db.Table.open(Table.java:124)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        at com.netflix.priam.cassandra.NFThinCassandraDaemon.init(NFThinCassandraDaemon.java:41)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3


Deleting the stuff in saved_caches folder fixed the problem.",jbellis,arya,Low,Resolved,Fixed,24/Jul/12 21:47,16/Apr/19 09:32
Bug,CASSANDRA-4465,12600253,"Index fails to be created on all nodes in cluster, restart resolves","On a production cluster, under load, creating an index on a column resulted in the index being successfully created on 4 of 21 nodes. All nodes received the schema agreement and were in concert. There were no errors logged on any of the nodes that failed to build the index.

A rolling restart of the cluster resulted in the nodes which had previously failed to build the index doing so when coming back up from a restart.",,heffergm,Low,Resolved,Fixed,26/Jul/12 01:15,16/Apr/19 09:32
Bug,CASSANDRA-4466,12600449,ColumnFamilyRecordReader hadoop integration fails with ghost keys,"When running hadoop-cassandra jobs with range queries over ghost keys, the ColumnFamilyRecordReader throws an exception if the last key in a slice_range query is a ghost key. 

This seems to be related to changes made in CASSANDRA-2855 to prevent ghost keys appearing in a hadoop map. 

The call stack trace is attached.

I made a one-line change to ColumnFamilyRecordReader.java, which seems to solve this issue for us.
",jbellis,ndrummond@rim.com,Low,Resolved,Fixed,27/Jul/12 10:25,16/Apr/19 09:32
Bug,CASSANDRA-4469,12600569,Fix online help in cqlsh for COPY commands,"the HELP COPY output from cqlsh says:

{noformat}
COPY [cqlsh only]

  Imports CSV data into a Cassandra table.

COPY <table_name> [ ( column [, ...] ) ]
     FROM ( '<filename>' | STDIN )
     [ WITH <option>='value' [AND ...] ];
COPY <table_name> [ ( column [, ...] ) ]
     TO ( '<filename>' | STDOUT )
     [ WITH <option>='value' [AND ...] ];
{noformat}

It's confusing cause COPY is now for both export and import, since CASSANDRA-4434.",thepaul,thepaul,Low,Resolved,Fixed,28/Jul/12 18:55,16/Apr/19 09:32
Bug,CASSANDRA-4470,12600572,cqlsh COPY FROM without explicit column names is broken,"When trying to do a COPY FROM command in cqlsh without an explicit list of column names, an error results:

{noformat}
cqlsh:a> copy blah from stdin;
[Use \. on a line by itself to end input]
[copy] a,b,c

object of type 'NoneType' has no len()
{noformat}

Broken by the fix for CASSANDRA-4434.",thepaul,thepaul,Normal,Resolved,Fixed,28/Jul/12 20:32,16/Apr/19 09:32
Bug,CASSANDRA-4475,12600870,Using 'key' as primary key throws exception when using CQL2,"When I run following CQL on trunk, throws exception (only in CQL2). This statement used to work and I think something is broken after CASSANDRA-4179.

{code}
CREATE TABLE Standard1 (key ascii PRIMARY KEY, c0 ascii);
{code}

Exception is:

{code}
ERROR [Thrift:1] 2012-07-31 09:54:02,585 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:166)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:123)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:49)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:45)
        at org.apache.cassandra.cql3.CFDefinition.getKeyId(CFDefinition.java:167)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:81)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1382)
        at org.apache.cassandra.config.CFMetaData.keyAliases(CFMetaData.java:235)
        at org.apache.cassandra.cql.CreateColumnFamilyStatement.getCFMetaData(CreateColumnFamilyStatement.java:170)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:692)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:846)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}",slebresne,yukim,Low,Resolved,Fixed,31/Jul/12 15:01,16/Apr/19 09:32
Bug,CASSANDRA-4477,12600923,cql3: defining more than one pk should be invalid,"dtests caught this on trunk:

{noformat}
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 277, in create_invalid_test
    assert_invalid(cursor, ""CREATE TABLE test (key1 text PRIMARY KEY, key2 text PRIMARY KEY)"")
  File ""/var/lib/buildbot/cassandra-dtest/assertions.py"", line 31, in assert_invalid
    assert False, ""Expecting query to be invalid""
AssertionError: Expecting query to be invalid
{noformat}",slebresne,brandon.williams,Normal,Resolved,Fixed,31/Jul/12 22:17,16/Apr/19 09:32
Bug,CASSANDRA-4479,12601104,JMX attribute setters not consistent with cassandra.yaml,"If a setting is configurable both via cassandra.yaml and JMX, the two should be consistent. If that doesn't hold, then the JMX setter can't be trusted. 

Here I present the example of phi_convict_threshold.

I'm trying to set phi_convict_threshold via JMX, which sets FailureDetector.phiConvictThreshold_, but this doesn't update Config.phi_convict_threshold, which gets its value from cassandra.yaml when starting up.

Some places, such as FailureDetector.interpret(InetAddress), use FailureDetector.phiConvictThreshold_; others, such as AntiEntropyService.line 813 in cassandra-1.1.2, use Config.phi_convict_threshold:
{code}
            // We want a higher confidence in the failure detection than usual because failing a repair wrongly has a high cost.
            if (phi < 2 * DatabaseDescriptor.getPhiConvictThreshold())
                return;
{code}

where DatabaseDescriptor.getPhiConvictThreshold() returns Conf.phi_convict_threshold.


So, it looks like there are cases where a value is stored in multiple places, and setting the value via JMX doesn't set all of them. I'd say there should only be a single place where a configuration parameter is stored, and that single field:
* should read in the value from cassandra.yaml, optionally falling back to a sane default
* should be the field that the JMX attribute reads and sets, and
* any place that needs the current global setting should get it from that field. However, there could be cases where you read in a global value at the start of a task and keep that value locally until the end of the task.

Also, anything settable via JMX should be volatile or set via a synchronized setter, or else according to the Java memory model other threads may be stuck with the old setting.


So, I'm requesting the following:
* Setting up guidelines for how to expose a configuration parameter both via cassandra.yaml and JMX, based on what I've mentioned above
* Going through the list of configuration parameters and fixing any that don't match those guidelines


I'd also recommend logging any changes to configuration parameters.",cmerrill,edong,Low,Resolved,Fixed,01/Aug/12 18:51,16/Apr/19 09:32
Bug,CASSANDRA-4484,12601241,"Drain causes incorrect error messages: ""Stream took more than 24H to complete; skipping""","After calling drain on a node, there are a bunch of incorrect error messages in the cassandra log file: ""Stream took more than 24H to complete; skipping"".

The problem is in MessagingService.waitForStreaming. It is logging an error if ThreadPoolExecutor.awaitTermination returns true, but if a timeout happens it returns false. See http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#awaitTermination%28long,%20java.util.concurrent.TimeUnit%29",canadianveggie,canadianveggie,Low,Resolved,Fixed,02/Aug/12 16:41,16/Apr/19 09:32
Bug,CASSANDRA-4487,12601434,remove uses of SchemaDisagreementException,"Since we can handle concurrent schema changes now, there's no need to validateSchemaAgreement before modification now.",xedin,jbellis,Low,Resolved,Fixed,03/Aug/12 22:21,16/Apr/19 09:32
Bug,CASSANDRA-4491,12601482,cqlsh needs to use system.local instead of system.Versions,"Apparently the system.Versions table was removed as part of CASSANDRA-4018. cqlsh in 1.2 should use system.local preferentially, and fall back on system.Versions to keep backwards compatibility with older c*.

Also changed in 4018: all the system.schema_* CFs now use columns named ""keyspace_name"", ""columnfamily_name"", and ""column_name"" instead of ""keyspace"", ""columnfamily"", and ""column"".

While we're at it, let's update the cql3 table structure parsing and the DESCRIBE command for the recent Cassandra changes too.",thepaul,thepaul,Low,Resolved,Fixed,05/Aug/12 03:27,16/Apr/19 09:32
Bug,CASSANDRA-4492,12601486,HintsColumnFamily compactions hang when using multithreaded compaction,"Running into an issue on a 6 node ring running 1.0.11 where HintsColumnFamily compactions often hang indefinitely when using multithreaded compaction. Nothing of note in the logs. In some cases, the compaction hangs before a tmp sstable is even created.

I've wiped out every hints sstable and restarted several times. The issue always comes back rather quickly and predictably. The compactions sometimes complete if the hint sstables are very small. Disabling multithreaded compaction stops this issue from occurring.

Compactions of all other CFs seem to work just fine.

This ring was upgraded from 1.0.7. I didn't keep any hints from the upgrade.

I should note that the ring gets a huge amount of writes, and as a result the HintedHandoff rows get be quite wide. I didn't see any large-row compaction notices when the compaction was hanging (perhaps the bug was triggered by incremental compaction?). After disabling multithreaded compaction, several of the rows that were successfully compacted were over 1GB.

Here is the output I see from compactionstats where a compaction has hung. The 'bytes compacted' column never changes.

{code}
pending tasks: 1
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction          systemHintsColumnFamily          268082       464784758     0.06%
{code}


The hung thread stack is as follows: (full jstack attached, as well)

{code}
""CompactionExecutor:37"" daemon prio=10 tid=0x00000000063df800 nid=0x49d9 waiting on condition [0x00007eb8c6ffa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000050f2e0e58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Deserializer.computeNext(ParallelCompactionIterable.java:329)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Deserializer.computeNext(ParallelCompactionIterable.java:281)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:126)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:100)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:101)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:88)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:141)
        at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:395)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}",carlyeks,alienth,Low,Resolved,Fixed,05/Aug/12 04:12,16/Apr/19 09:32
Bug,CASSANDRA-4493,12601508,Fix update of CF comparator (including adding new collections),"Updating the comparator of a column family (which is authorized if the new comparator is ""compatible"" with the old one, and that includes adding a new component to a CompositeType, or adding a new collection to a CQL3 table) doesn't completely work. The problem is that even if we change the comparator in CFMetada, the old comparator will still be aliased by the current memtable. This means updates (that expect the new comparator) will fail until a new memtable is created.",slebresne,slebresne,Normal,Resolved,Fixed,05/Aug/12 19:22,16/Apr/19 09:32
Bug,CASSANDRA-4494,12601526,nodetool can't work at all !,"1. download cassandra 1.1.3 , then start with ""{cassandra}/bin/cassandra -pf &""
2. cd to bin , call nodetool as ""./nodetool -h localhost ring""
3. console returned : failed to connect to 'localhost:7199' : connection refused


BUT ,

at the same centos , all was ok before (1.1.2) .


PS: 

cassandra-cli/cqlsh works well (1.1.3)


--------------

update:

even if add the following in cassandra-env.sh , connection refused as well :
JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=10.10.30.11
",thepaul,sun74533,Urgent,Resolved,Fixed,06/Aug/12 03:28,16/Apr/19 09:32
Bug,CASSANDRA-4497,12602033,Update CQL pseudo-maps to real map syntax,"- compression_parameters
- replication_parameters (combine strategy + options like we did compression)
- anything else?",xedin,jbellis,Normal,Resolved,Fixed,07/Aug/12 16:23,16/Apr/19 09:32
Bug,CASSANDRA-4499,12602099,CassandraStorage allow_deletes doesn't work in Hadoop cluster,"When using CassandraStorage in a Pig script that runs in a Hadoop cluster, the environment variable configuration option for allow_deletes doesn't work.  I'd like to see allow_deletes added as a URL parameter.

For example, I'd like to do
STORE storable_events INTO 'cassandra://drlcrs/event?allow_deletes=true' USING org.apache.cassandra.hadoop.pig.CassandraStorage();",brandon.williams,mrtidy,Normal,Resolved,Fixed,07/Aug/12 23:21,16/Apr/19 09:32
Bug,CASSANDRA-4532,12603138,NPE when trying to select a slice from a composite table,"I posted this question on StackOverflow, because i need a solution. 

Created a table with :

{noformat}
create table compositetest(m_id ascii,i_id int,l_id ascii,body ascii, PRIMARY KEY(m_id,i_id,l_id));
{noformat}

wanted to slice the results returned, so did something like below, not sure if its the right way. The first one returns data perfectly as expected, second one to get the next 3 columns closes the transport of my cqlsh

{noformat}
cqlsh:testkeyspace1> select * from compositetest where i_id<=3 limit 3;
 m_id | i_id | l_id | body
------+------+------+------
   m1 |    1 |   l1 |   b1
   m1 |    2 |   l2 |   b2
   m2 |    1 |   l1 |   b1

cqlsh:testkeyspace1> Was trying to write something for slice range.

TSocket read 0 bytes
{noformat}

Is there a way to achieve what I am doing here, it would be good if some meaning ful error is sent back, instead of cqlsh closing the transport.

On the server side I see the following error.

{noformat}
ERROR [Thrift:3] 2012-08-12 15:15:24,414 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql3.statements.SelectStatement$Restriction.setBound(SelectStatement.java:1277)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1151)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1001)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:215)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

With ThriftClient I get :

{noformat}
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql_query(Cassandra.java:1402)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_cql_query(Cassandra.java:1388)
{noformat}",slebresne,basu76,Low,Resolved,Fixed,12/Aug/12 19:41,16/Apr/19 09:32
Bug,CASSANDRA-4533,12603184,Multithreaded cache saving can skip caches,"Cassandra flushes the key and row cache to disk periodically. It also uses a atomic flag in flushInProgress to enforce single cache writer at any time.

However, the cache saving task could be submitted to CompactionManager concurrently, as long as the number of worker thread in CompactionManager is larger than 1. 

Due to the effect of above atomic flag, only one cache will be written out to disk. Other writer are cancelled when the flag is true.

I observe the situation in Cassandra 1.0. If nothing is changed, the problem should remain in Cassandra 1.1, either.",yukim,hanzhu,Low,Resolved,Fixed,13/Aug/12 06:40,16/Apr/19 09:32
Bug,CASSANDRA-4534,12603250,old-style mapred interface doesn't set key limit correctly,"{{next(ByteBuffer key, SortedMap<ByteBuffer, IColumn> value)}} calls clear/put/rewind, but not flip or limit.",jbellis,jbellis,Low,Resolved,Fixed,13/Aug/12 14:54,16/Apr/19 09:32
Bug,CASSANDRA-4537,12603335,We should emit number of sstables in each level from JMX,"We should add methods to this Mbean org.apache.cassandra.db.ColumnFamilyStoreMBean

These metrics will be helpful to see how sstables are distributed in different levels and how they move to higher level with time. 
Currently we can see this by looking at the json file but with JMX, we can monitor the historic values over time using any monitoring tool.  ",yukim,kohlisankalp,Low,Resolved,Fixed,13/Aug/12 21:32,16/Apr/19 09:32
Bug,CASSANDRA-4546,12603535,cqlsh: handle when full cassandra type class names are given,"When a builtin Cassandra type was being used for data in previous versions of Cassandra, only the short name was sent: ""UTF8Type"", ""TimeUUIDType"", etc. Starting with 1.2, as of CASSANDRA-4453, the full class names are sent.

Cqlsh doesn't know how to handle this, and is currently treating all data as if it were an unknown type. This goes as far as to cause an exception when the type is actually a number, because the driver deserializes it right, and then cqlsh tries to use it as a string.

Here for googlage:

{noformat}
AttributeError: 'int' object has no attribute 'replace'
{noformat}

Fixeries are in order.",thepaul,thepaul,Normal,Resolved,Fixed,14/Aug/12 23:28,16/Apr/19 09:32
Bug,CASSANDRA-4551,12603783,"Nodetool getendpoints keys do not work with spaces, key_validation=ascii value of key => ""a test""  no delimiter","Nodetool getendpoints keys do not work with embedded spaces, key_validation=ascii value of key => ""a test""  no delimiter tried to escape key => ""a test"" with double and single quotes. It doesn't work. It just reiterates the format of the tool's command: getendpoints requires ks, cf and key args",gdeangel,mvaldez,Low,Resolved,Fixed,16/Aug/12 20:36,16/Apr/19 09:32
Bug,CASSANDRA-4553,12603795,NPE while loading Saved KeyCache,"WARN [main] 2012-08-16 15:31:13,896 AutoSavingCache.java (line 146) error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:140)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:251)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:354)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:326)
	at org.apache.cassandra.db.Table.initCf(Table.java:312)
	at org.apache.cassandra.db.Table.<init>(Table.java:252)
	at org.apache.cassandra.db.Table.open(Table.java:97)
	at org.apache.cassandra.db.Table.open(Table.java:75)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:168)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,16/Aug/12 22:35,16/Apr/19 09:32
Bug,CASSANDRA-4555,12603936,select statement with indexed column causes node to OOM,"After creating a keyspace, table and index on a clean ccm 3 node cluster based on trunk, when a select statement with an index expression is executed in cqlsh one of the nodes OOM's and goes down.

The steps to reproduce the problem are:

create a 3 node cluster from trunk (I used ccm)

execute the following statements in cqlsh:

{noformat}
CREATE KEYSPACE trace WITH strategy_class = 'SimpleStrategy'
  AND strategy_options:replication_factor = '1';

CREATE TABLE trace.trace_events(sessionId  timeuuid,
  coordinator       inet,
  eventId           timeuuid,
  description       text,
  duration          bigint,
  happened_at       timestamp,
  name              text,
  payload_types     map<text, text>,
  payload           map<text, blob>,
  source            inet,
  type              text,
  PRIMARY KEY (sessionId, coordinator, eventId));

CREATE INDEX idx_name ON trace.trace_events (name);
{noformat}

Executing the following statement causes node2 to OOM:

select * from trace_events where name = 'batch_mutate';

In my case node2 goes down with:

{noformat}
ERROR [Thread-9] 2012-08-17 19:42:55,741 CassandraDaemon.java (line 131) Exception in thread Thread[Thread-9,5,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.dht.Token$TokenSerializer.deserialize(Token.java:97)
	at org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer.deserialize(AbstractBounds.java:161)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:299)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:181)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:94)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:181)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:69)
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:55,746 ThriftServer.java (line 221) Stop listening to thrift clients
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:55,748 Gossiper.java (line 1054) Announcing shutdown
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:56,749 MessagingService.java (line 657) Waiting for messaging service to quiesce
 INFO [ACCEPT-/127.0.0.2] 2012-08-17 19:42:56,751 MessagingService.java (line 849) MessagingService shutting down server thread.
{noformat}
",jbellis,dr-alves,Normal,Resolved,Fixed,18/Aug/12 01:02,16/Apr/19 09:32
Bug,CASSANDRA-4561,12604193,update column family fails,"[default@test] show schema;
create column family Messages
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 2
  and max_compaction_threshold = 4
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compaction_strategy_options = {'sstable_size_in_mb' : '1024'}
  and compression_options = {'chunk_length_kb' : '64', 'sstable_compression' : 'org.apache.cassandra.io.compress.DeflateCompressor'};


[default@test] update column family Messages with min_compaction_threshold = 4 and  max_compaction_threshold = 32;
a5b7544e-1ef5-3bfd-8770-c09594e37ec2
Waiting for schema agreement...
... schemas agree across the cluster

[default@test] show schema;
create column family Messages
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 2
  and max_compaction_threshold = 4
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compaction_strategy_options = {'sstable_size_in_mb' : '1024'}
  and compression_options = {'chunk_length_kb' : '64', 'sstable_compression' : 'org.apache.cassandra.io.compress.DeflateCompressor'};",xedin,zenek_kraweznik0,Normal,Resolved,Fixed,21/Aug/12 09:24,16/Apr/19 09:32
Bug,CASSANDRA-4564,12604290,MoveTest madness,"I encountered what looks like bugs in {{o.a.c.service.MoveTest.newTestWriteEndpointsDuringMove()}} while doing something else; Here is a (poorly researched )ticket before I forget :)

* There are two loops over non-system tables, and the first is a NOOP
* In the second loop, a set exactly {{replicationFactor}} in size is compared against {{tmd.getWriteEndpoints()}}, which should produce greater than {{replicationFactor}} endpoints during a move (shouldn't it?); How does this pass?",swordheart,urandom,Low,Resolved,Fixed,21/Aug/12 22:17,16/Apr/19 09:32
Bug,CASSANDRA-4566,12604450,test-clientutil target is failing,"The {{test-clientutil}} target is failing (on trunk at least):

{noformat}
    [junit] java.lang.NoClassDefFoundError: org/apache/cassandra/io/IVersionedSerializer
    [junit] 	at java.lang.ClassLoader.defineClass1(Native Method)
    [junit] 	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
    [junit] 	at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    [junit] 	at org.apache.cassandra.utils.UUIDGen.<clinit>(UUIDGen.java:38)
    [junit] 	at org.apache.cassandra.cql.jdbc.ClientUtilsTest.test(ClientUtilsTest.java:59)
    [junit] Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.io.IVersionedSerializer
    [junit] 	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    [junit] 	at java.security.AccessController.doPrivileged(Native Method)
    [junit] 	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
{noformat}

This target is to ensure that the clientutil jar has no unsatisfied dependencies.  In this case it looks like static initialization is pulling in {{o.a.c.io.IVersionedSerializer}}, (probably transitively).",dbrosius@apache.org,urandom,Urgent,Resolved,Fixed,22/Aug/12 16:00,16/Apr/19 09:32
Bug,CASSANDRA-4567,12604460,Error in log related to Murmur3Partitioner,"Start a 2-node cluster on cassandra-1.1. Bring down one node, upgrade it to trunk, start it back up. The following error shows up in the log:
{code}
...
 INFO [main] 2012-08-22 10:44:40,012 CacheService.java (line 170) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 (148 bytes)
 INFO [SSTableBatchOpen:2] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 (226 bytes)
 INFO [SSTableBatchOpen:3] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 (89 bytes)
ERROR [SSTableBatchOpen:3] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:2] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:2,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:1] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [main] 2012-08-22 10:44:40,486 DatabaseDescriptor.java (line 522) Couldn't detect any schema definitions in local storage.
 INFO [main] 2012-08-22 10:44:40,487 DatabaseDescriptor.java (line 525) Found table data in data directories. Consider using the CLI to define your schema.
...
{code}

Note that the error does not happen when upgrading from cassandra-1.0 to cassandra-1.1, or when ""upgrading"" from trunk to trunk.

This is the exact dtest I used:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        node1 = cluster.nodelist()[0]
        node1.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
{code}",vijay2win@yahoo.com,tpatterson,Normal,Resolved,Fixed,22/Aug/12 17:41,16/Apr/19 09:32
Bug,CASSANDRA-4568,12604476,countPendingHints JMX operation is returning garbage for the key,"countPendingHints JMX operation should return a map from key: endpoint IP address to value: number of pending hints. It is returning garbage for the key (looks like binary data concerning the hint itself). The value looks correct.

Steps to reproduce:

1) Set up a two-node cluster. 

2) Disable gossip on the second node.  

`nodetool ring` output from node 1:

Address         DC          Rack        Status State   Load            Effective-Ownership Token                                       
                                                                                           85070591730234615865843651857942052864      
192.168.1.162   datacenter1 rack1       Up     Normal  21.46 KB        100.00%             0                                           
192.168.1.130   datacenter1 rack1       Down   Normal  6.67 KB         100.00%             85070591730234615865843651857942052864      


3) While the second node is still down, create a keyspace with RF=2 and a CF within this keyspace. Then insert two records into the CF:

Connected to Test Cluster at 192.168.1.162:9160.
[cqlsh 2.2.0 | Cassandra 1.1.2 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> create KEYSPACE demo WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 2;
cqlsh> use demo;
cqlsh:demo> create table users (username varchar primary key, password varchar);
cqlsh:demo> insert into users (username, password) values (scott, tiger);
cqlsh:demo> insert into users (username, password) values (root, password);

4) Use a JMX client to execute the countPendingHints operation:

jblangston:~ jblangston$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7199 org.apache.cassandra.db:type=HintedHandoffManager countPendingHints

08/22/2012 14:21:37 -0500 org.archive.jmx.Client countPendingHints: {@B^h ??	?[b??scottdemoscott?????password?ߞHtigerdemoF
?P??	?[b??rootdemoroot?????password?ߞ?Wpassworddemo=2}

5) Notice the output.  The value (2) is correct but the key is garbage instead of an endpoint IP address.",brandon.williams,jblangston@datastax.com,Low,Resolved,Fixed,22/Aug/12 19:29,16/Apr/19 09:32
Bug,CASSANDRA-4571,12604647,"Strange permament socket descriptors increasing leads to ""Too many open files""","On the two-node cluster there was found strange socket descriptors increasing. lsof -n | grep java shows many rows like""

java       8380 cassandra  113r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  114r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  115r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  116r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  117r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  118r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  119r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  120r     unix 0xffff8101a374a080            938348482 socket
"" And number of this rows constantly increasing. After about 24 hours this situation leads to error.
We use PHPCassa client. Load is not so high (aroud ~50kb/s on write). ",jbellis,sergshne,Urgent,Resolved,Fixed,23/Aug/12 17:20,16/Apr/19 09:32
Bug,CASSANDRA-4572,12604655,lost+found directory in the data dir causes problems again,"Looks like we've regressed from CASSANDRA-1547 and mounting a fs directly on the data dir is a problem again.

{noformat}
INFO [main] 2012-08-22 23:30:03,710 Directories.java (line 475) Upgrade from pre-1.1 version detected: migrating sstables to new directory layout ERROR [main] 2012-08-22 23:30:03,712 AbstractCassandraDaemon.java (line 370) Exception encountered during startup 
                java.lang.NullPointerException         at org.apache.cassandra.db.Directories.migrateSSTables(Directories.java:487)
{noformat}",yukim,brandon.williams,Normal,Resolved,Fixed,23/Aug/12 18:08,16/Apr/19 09:32
Bug,CASSANDRA-4573,12604662,HSHA doesn't handle large messages gracefully,"HSHA doesn't seem to enforce any kind of max message length, and when messages are too large, it doesn't fail gracefully.

With debug logs enabled, you'll see this:

{{DEBUG 13:13:31,805 Unexpected state 16}}

Which seems to mean that there's a SelectionKey that's valid, but isn't ready for reading, writing, or accepting.

Client-side, you'll get this thrift error (while trying to read a frame as part of {{recv_batch_mutate}}):

{{TTransportException: TSocket read 0 bytes}}",xedin,thobbs,Normal,Resolved,Fixed,23/Aug/12 18:44,16/Apr/19 09:32
Bug,CASSANDRA-4574,12604687,Missing String.format() in AntiEntropyService.java logs,"A String.format() is missing in AntiEntropyService.java (line 625 in 1.2). 
This is what is written to the logs: AntiEntropyService.java (line 625) \[repair #%s] No neighbors to repair with on range %s: session completed.",dbrosius@apache.org,julienlambert,Low,Resolved,Fixed,23/Aug/12 21:55,16/Apr/19 09:32
Bug,CASSANDRA-4576,12604898,Error in non-upgraded node's log when upgrading another node to trunk,"I'm upgrading a 2-node cluster from cassandra-1.1 to trunk. On node1 I flush, stop the node, upgrade it to trunk, and start it. The following error gets written once a second in the log for node2:

{code}
ERROR [GossipStage:10] 2012-08-24 11:03:36,293 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[GossipStage:10,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.UnknownHostException: addr is of illegal length
	at java.net.InetAddress.getByAddress(InetAddress.java:935)
	at java.net.InetAddress.getByAddress(InetAddress.java:1318)
	at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
	at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
	... 4 more
{code}


Here is the exact code I ran to produce the error:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.partitioner = 'org.apache.cassandra.dht.RandomPartitioner'
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        (node1, node2) = cluster.nodelist()
        node1.watch_log_for('Listening for thrift clients...')
        node2.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
        import pdb; pdb.set_trace()  # <-- pause here and tail -f the node2.logfilename()
{code}",xedin,tpatterson,Normal,Resolved,Fixed,24/Aug/12 18:17,16/Apr/19 09:32
Bug,CASSANDRA-4577,12604982,Allow preparing queries without parameters,"Currently it's not possible to prepare any queries that do not take any parameters using Cassandra's new native protocol because of an assertion error.

This makes client development rather difficult (you need to parse CQL queries to detect the number of parameters and skip the preparation of those) and there is probably no reason to handle queries with no parameters separately.",tux21b,tux21b,Normal,Resolved,Fixed,25/Aug/12 21:51,16/Apr/19 09:32
Bug,CASSANDRA-4578,12605040,Dead lock in mutation stage when many concurrent writes to few columns,"When I send many request to increment counters to few counter columns, sometimes mutation stage cause dead lock. When it happened, all of mutation threads are locked and do not accept updates any more.

{noformat}
""MutationStage:432"" - Thread t@1389
   java.lang.Thread.State: TIMED_WAITING
	at java.lang.Object.wait(Native Method)
	- waiting on <b90b45b> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:443)
	at java.util.concurrent.TimeUnit.timedWait(TimeUnit.java:292)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:54)
	at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:55)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
	- locked <4b1b0a6f> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
{noformat}",slebresne,suguru,Low,Resolved,Fixed,27/Aug/12 04:46,16/Apr/19 09:32
Bug,CASSANDRA-4579,12605133,CQL queries using LIMIT sometimes missing results,"In certain conditions, CQL queries using LIMIT clauses are not being given all of the expected results (whether unset column values or missing rows).

Here are the condition sets I've been able to identify:

First mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. Conditions:

 * Table has a multi-component primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

Second mode: result has fewer rows than it should, lower than both the LIMIT and the actual number of matching rows. Conditions:

 * Table has a single-column primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

It would make sense to me that this would have started with CASSANDRA-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing CASSANDRA-3647.

Test case for the first failure mode:

{noformat}
DROP KEYSPACE test;

CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int,
    b int,
    c int,
    d int,
    e int,
    PRIMARY KEY (a, b)
);

INSERT INTO testcf (a, b, c, d, e) VALUES (1, 11, 111, 1111, 11111);
INSERT INTO testcf (a, b, c, d, e) VALUES (2, 22, 222, 2222, 22222);
INSERT INTO testcf (a, b, c, d, e) VALUES (3, 33, 333, 3333, 33333);
INSERT INTO testcf (a, b, c, d, e) VALUES (4, 44, 444, 4444, 44444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- columns d and e in result row are null
SELECT * FROM testcf LIMIT 2; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 3; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 4; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 5; -- results are correct (4 rows returned)
{noformat}

Test case for the second failure mode:

{noformat}
CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int primary key,
    b int,
    c int,
);

INSERT INTO testcf (a, b, c) VALUES (1, 11, 111);
INSERT INTO testcf (a, b, c) VALUES (2, 22, 222);
INSERT INTO testcf (a, b, c) VALUES (3, 33, 333);
INSERT INTO testcf (a, b, c) VALUES (4, 44, 444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- gives 1 row
SELECT * FROM testcf LIMIT 2; -- gives 1 row
SELECT * FROM testcf LIMIT 3; -- gives 2 rows
SELECT * FROM testcf LIMIT 4; -- gives 2 rows
SELECT * FROM testcf LIMIT 5; -- gives 3 rows
{noformat}",slebresne,thepaul,Normal,Resolved,Fixed,27/Aug/12 20:51,16/Apr/19 09:32
Bug,CASSANDRA-4587,12605495,StackOverflowError in LeveledCompactionStrategy$LeveledScanner.computeNext,"while running nodetool repair, the following was logged in system.log:


ERROR [ValidationExecutor:2] 2012-08-30 10:58:19,490 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.StackOverflowError
        at sun.nio.cs.UTF_8.updatePositions(UTF_8.java:76)
        at sun.nio.cs.UTF_8$Encoder.encodeArrayLoop(UTF_8.java:411)
        at sun.nio.cs.UTF_8$Encoder.encodeLoop(UTF_8.java:466)
        at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
        at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:258)
        at java.lang.StringCoding.encode(StringCoding.java:290)
        at java.lang.String.getBytes(String.java:954)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
        at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
.

(about 900 lines deleted)
.


        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:202)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:90)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:47)
        at org.apache.cassandra.db.compaction.CompactionIterable.iterator(CompactionIterable.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:703)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:442)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",jbellis,schnidrig,Low,Resolved,Fixed,30/Aug/12 09:20,16/Apr/19 09:32
Bug,CASSANDRA-4590,12605562,"""The system cannot find the path specified"" when creating hard link on Windows","When upgrading from Cassandra 1.0.5 to 1.1.3, we have a test case (uses embedded Cassandra) that started failing as shown below. Other than the upgrade, no changes were made to the code or config. I believe this MAY be related to the change made in CASSANDRA-3101.

We verified that the file it is trying to create the hard link to does exist - so it is purely the creation of the link that is failing.

Here is the basic failure:

# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.


Here is a more complete log output:


# [11:30:59.975] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.976] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Digest.sha1]
# [11:30:59.977] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Index.db]
# [11:30:59.978] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Filter.db]
# [11:30:59.978] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Statistics.db]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.i.s.SSTable] [delete] [Deleted target\test\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-he-4]
# [11:30:59.981] [INFO ] [o.a.c.d.ColumnFamilyStore] [maybeSwitchMemtable] [Enqueuing flush of Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.981] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Writing Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.992] [DEBUG] [o.a.c.d.Directories] [getLocationWithMaximumAvailableSpace] [expected data files size is 134; largest free partition (target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts) has 82645161984 bytes free]
# [11:31:00.012] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Completed flushing target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db (123 bytes) for commitlog position ReplayPosition(segmentId=592725621297887, position=6701)]
# [11:31:00.013] [DEBUG] [o.a.c.u.I.IntervalNode] [<init>] [Creating IntervalNode from [Interval(DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334), DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334))]]
# [11:31:00.013] [DEBUG] [o.a.c.d.DataTracker] [addNewSSTablesSize] [adding target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1 to list of files tracked for RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [submitBackground] [Scheduling a background task check for RevKeyspace.PropertyProductDefaultInventoryCounts with SizeTieredCompactionStrategy]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [Checking RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [discard completed log segments for ReplayPosition(segmentId=592725621297887, position=6701), column family 1001]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.SizeTieredCompactionStrategy] [getNextBackgroundTask] [Compaction buckets are [[SSTableReader(path='target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db')]]]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [Not safe to delete commit log CommitLogSegment(target\test\cassandra\commitlog\CommitLog-592725621297887.log); dirty is Versions (7), ; hasNext: false]
# [11:31:00.015] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [No tasks available]
# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) [cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.308] [ERROR] [o.a.c.s.AbstractCassandraDaemon] [uncaughtException] [Exception in thread Thread[MigrationStage:1,5,main]]
java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [ERROR] [o.a.c.t.CustomTThreadPoolServer] [run] [Error occurred during processing of message.]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:170) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.CassandraServer.system_drop_keyspace(CassandraServer.java:1008) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3476) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3464) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.get(FutureTask.java:83) ~[na:1.6.0_33]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 11 common frames omitted
Caused by: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	... 3 common frames omitted
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [DEBUG] [o.a.c.s.ClientState] [logout] [logged out: #<User allow_all groups=[]>]
# [11:31:00.310] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-5>]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [MARK HOST AS DOWN TRIGGERED for host 127.0.0.1(127.0.0.1):9162]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [Pool state on shutdown: <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}; IsActive?: true; Active: 1; Blocked: 0; Idle: 15; NumBeforeExhausted: 49]
# [11:31:00.311] [INFO ] [m.p.c.c.ConcurrentHClientPool] [shutdown] [Shutdown triggered on <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-6>]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-15>]
# [11:31:00.311] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-14>]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-13>]
",jbellis,allenservedio,Low,Resolved,Fixed,30/Aug/12 18:44,16/Apr/19 09:32
Bug,CASSANDRA-4594,12605617,COPY TO and COPY FROM don't default to consistent ordering of columns,"Here is the input:
{code}                                                         
CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
USE test;                                                                       
                                                                                
CREATE TABLE airplanes (                                                        
                name text PRIMARY KEY,                                          
                manufacturer ascii,                                             
                year int,                                                       
                mach float                                                      
            );                                                                  
                                                                                
INSERT INTO airplanes (name, manufacturer, year, mach) VALUES ('P38-Lightning', 'Lockheed', 1937, '.7');
                                                                                
COPY airplanes TO 'temp.cfg' WITH HEADER=true;                                  
                                                                                
TRUNCATE airplanes;                                                                
                                                                                   
COPY airplanes FROM 'temp.cfg' WITH HEADER=true;                                
                                                                                   
SELECT * FROM airplanes;
{code}

Here is what happens when executed. Note how it tried to import the float into the int column:
{code}
cqlsh:test> DROP KEYSPACE test;                                                                
cqlsh:test> CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh:test> USE test;                                                                       
cqlsh:test>                                                                                    
cqlsh:test> CREATE TABLE airplanes (                                            
        ...                 name text PRIMARY KEY,                              
        ...                 manufacturer ascii,                                 
        ...                 year int,                                           
        ...                 mach float                                          
        ...             );                                                      
cqlsh:test>                                                                     
cqlsh:test> INSERT INTO airplanes (name, manufacturer, year, mach) VALUES ('P38-Lightning', 'Lockheed', 1937, '.7');
cqlsh:test>                                                                     
cqlsh:test> COPY airplanes TO 'temp.cfg' WITH HEADER=true;                      
1 rows exported in 0.003 seconds.                                               
cqlsh:test> TRUNCATE airplanes;                                                 
cqlsh:test>                                                                     
cqlsh:test> COPY airplanes FROM 'temp.cfg' WITH HEADER=true;                    
Bad Request: unable to make int from '0.7'                                      
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.002 seconds.
{code}",thepaul,tpatterson,Low,Resolved,Fixed,30/Aug/12 23:13,16/Apr/19 09:32
Bug,CASSANDRA-4597,12605651,Impossible to set LeveledCompactionStrategy to a column family.,"CFPropDefs.applyToCFMetadata() does not set the compaction class on CFM

When altering the compaction strategy of a column family to LeveledCompactionStrategy, the compaction strategy is not changed (the describe command shows that the SizeTieredCompactionStrategy is still set to the CF)
When creating a column family WITH compaction_strategy_class='LeveledCompactionStrategy', the compaction strategy class used is  SizeTieredCompactionStrategy

Ex : 
jal@jal-VirtualBox:~/cassandra/apache-cassandra-1.1.1/bin$ ./cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use test1;
cqlsh:test1> describe table pns_credentials;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

I want to set the LeveledCompaction strategy for this table, so I execute the following ALTER TABLE :

cqlsh:test1> alter table pns_credentials 
         ... WITH compaction_strategy_class='LeveledCompactionStrategy'
         ... AND compaction_strategy_options:sstable_size_in_mb=10;

In Cassandra logs, I see some informations :
 INFO 10:23:52,532 Enqueuing flush of Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,533 Writing Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,629 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-94-Data.db (1442 bytes) for commitlog position ReplayPosition(segmentId=3556583843054, position=1987)


However, when I look at the description of the table, the table is still with the SizeTieredCompactionStrategy
cqlsh:test1> describe table pns_credentials ;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
 
In the schema_columnfamilies table (in system keyspace), the table pns_credentials is still using the SizeTieredCompactionStrategy
cqlsh:test1> use system;
cqlsh:system> select * from schema_columnfamilies ;
...
         test1 |   pns_credentials |                   null | KEYS_ONLY |                        [] |         | org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy |                          {} |                                                                                                                                                                           org.apache.cassandra.db.marshal.UTF8Type | {""sstable_compression"":""org.apache.cassandra.io.compress.SnappyCompressor""} |          org.apache.cassandra.db.marshal.UTF8Type |           864000 | 1029 |       ise |     org.apache.cassandra.db.marshal.UTF8Type |                        0 |                       32 |                        4 |                0.1 |               True |          null | Standard |        null
... 


Same behaviour using cqlsh or command-cli.
",xedin,jal06,Normal,Resolved,Fixed,31/Aug/12 07:51,16/Apr/19 09:32
Bug,CASSANDRA-4598,12605780,describeOwnership() in Murmur3Partitioner.java doesn't work for close tokens,"On a 2 node-cluster, if the two tokens are close enough, the ownership information displayed will be 0.00% for each, instead of ~0% for one and ~100% for the other. The number of replicas displayed is then 0, even if you have more.

Reproduce:
- Create a 2-node cluster, using Murmur3Partitioner
- Move the tokens to two consecutive values
- Display ring with nodetool

Problem:
This line causing this problem is in {{describeOwnership()}} of {{Murmur3Partitioner.java}} (lines 117 and 123):

{{float age = ((ti - tim1 + ri) % ri) / ri;}}

If {{ti - tim1}} (the difference of the two consecutive tokens) is too small, then the precision of the float isn't enough to represent the exact numbers (because {{ri}}, the total range of the ring, is a very big number). 

For example, {{(float) (ri + 1) = (float) (ri - 1) = (float) ri = 9.223372E18}}, so that {{((ri+1)%ri)/ri = ((ri-1)%ri)/ri = (ri%ri)/ri = 0}}. Whereas with a correct precision, the exact value for {{(ri-1)%ri}} should be {{ri-1}} and {{(ri-1)/ri ~ 1.0 (100%)}} instead of 0%.

Also, as the number of replica is determined by NodeCmd using the ownership percentages, it is wrong too.

Solution:
We might want to use BigInteger or BigDecimal somewhere?",,julienlambert,Low,Resolved,Fixed,31/Aug/12 20:50,16/Apr/19 09:32
Bug,CASSANDRA-4599,12605804,Event tracing always records thread name as 'TracingStage',"Since LoggingEvent#getThreadName gets current thread name when accessed, name of tracing thread ('TracingStage') is always logged to events CF.",yukim,yukim,Low,Resolved,Fixed,31/Aug/12 22:19,16/Apr/19 09:32
Bug,CASSANDRA-4601,12605927,Ensure unique commit log file names,"The commit log segment name uses System.nanoTime() as part of the file name. There is no guarantee that successive calls to nanoTime() will return different values. And on less than optimal hypervisors this happens a lot. 

I observed the following in the wild:

{code:java}
ERROR [COMMIT-LOG-ALLOCATOR] 2012-08-31 15:56:49,815 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.lang.AssertionError: attempted to delete non-existing file CommitLog-13926764209796414.log
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.discard(CommitLogSegment.java:172)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$4.run(CommitLogAllocator.java:223)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
{code}

My _assumption_ is that it was because of duplicate file names. As this is on a hypervisor that is less than optimal. 
 
After a while (about 30 minutes) mutations stopped being processed and the pending count sky rocketed. I _think_ this was because log writing was blocked trying to get a new segment and writers could not submit to the commit log queue. The only way to stop the affected nodes was kill -9. 

Over about 24 hours this happened 5 times. I have deployed a patch that has been running for 12 hours without incident, will attach. 

The affected nodes could still read, and I'm checking logs to see how the other nodes handled the situation.",amorton,amorton,Urgent,Resolved,Fixed,02/Sep/12 22:37,16/Apr/19 09:32
Bug,CASSANDRA-4602,12605931,Stack Size on Sun JVM 1.6.0_33 must be at least 160k,"I started a fresh Cassandra 1.1.4 install with Sun JVM 1.6.35.

On startup I got this in output.log

{noformat}
The stack size specified is too small, Specify at least 160k
Cannot create Java VM
Service exit with a return value of 1
{noformat}

Remembering CASSANDRA-4275 I monkeyed around and started the JVM with -Xss160k the same as Java 7. I then got

{code:java}
ERROR [WRITE-/192.168.1.12] 2012-08-31 01:43:29,865 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[WRITE-/192.168.1.12,5,main]
java.lang.StackOverflowError
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
	at java.io.BufferedOutputStream.flush(Unknown Source)
	at java.io.DataOutputStream.flush(Unknown Source)
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:156)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:126)
{code}

Same as CASSANDRA-4442

At which point I dropped back to Java 6.33. 

CASSANDRA-4457 bumped the stack size to 180 for java 7, should we also do this for Java 6.33+ ?",jbellis,amorton,Normal,Resolved,Fixed,02/Sep/12 23:34,16/Apr/19 09:32
Bug,CASSANDRA-4612,12606123,cql error with ORDER BY when using IN,"{code}
            CREATE TABLE test(
                my_id varchar, 
                col1 int, 
                value varchar, 
                PRIMARY KEY (my_id, col1)
            );

INSERT INTO test(my_id, col1, value) VALUES ( 'key1', 1, 'a');
INSERT INTO test(my_id, col1, value) VALUES ( 'key2', 3, 'c');
INSERT INTO test(my_id, col1, value) VALUES ( 'key3', 2, 'b');
INSERT INTO test(my_id, col1, value) VALUES ( 'key4', 4, 'd');
SELECT col1 FROM test WHERE my_id in('key1', 'key2', 'key3') ORDER BY col1;
{code}

The following error results: TSocket read 0 bytes
The log gives a traceback:
{code}
ERROR [Thrift:8] 2012-09-04 12:02:15,894 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1356)
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1343)
	at java.util.Arrays.mergeSort(Arrays.java:1270)
	at java.util.Arrays.sort(Arrays.java:1210)
	at java.util.Collections.sort(Collections.java:159)
	at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:821)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:793)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:136)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:107)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:115)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3618)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3606)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}",xedin,tpatterson,Normal,Resolved,Fixed,04/Sep/12 18:20,16/Apr/19 09:32
Bug,CASSANDRA-4620,12606243,Avoid special characters which might yield to a build fail,on jdk7 ant will fail on StreamingHistogram.java due to the special characters used line 130 and 133.,,pyritschard,Normal,Resolved,Fixed,05/Sep/12 13:29,16/Apr/19 09:32
Bug,CASSANDRA-4621,12606245,AssertionError in StorageProxy.getRestrictedRange,"On a freshly built cassandra from trunk, I can create a column family with a composite row key using the syntax:

for instance a standard eventlog CF:

     CREATE TABLE events (
       facility text,
       prio int,
       message text,
       PRIMARY KEY ( (facility, prio) )
     );

A simple query will then generate exceptions:

SELECT * FROM events; will yield:

ERROR 15:33:40,383 Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: [min(0),max(-8021625467324731134)]
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41)
	at org.apache.cassandra.dht.Bounds.split(Bounds.java:59)
	at org.apache.cassandra.service.StorageProxy.getRestrictedRanges(StorageProxy.java:1073)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:879)
	at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:209)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:128)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:107)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:115)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3618)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3606)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

",slebresne,pyritschard,Normal,Resolved,Fixed,05/Sep/12 13:35,16/Apr/19 09:32
Bug,CASSANDRA-4622,12606257,corrupted saved caches,"I'm seeing this fairly frequently on trunk:

{noformat}

 INFO 05:15:23,805 reading saved cache /var/lib/cassandra/saved_caches/system-schema_columnfamilies-KeyCache-b.db
 WARN 05:15:23,808 error reading saved cache /var/lib/cassandra/saved_caches/system-schema_columnfamilies-KeyCache-b.db
java.lang.NullPointerException
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:151)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:247)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:362)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.Table.initCf(Table.java:333)
        at org.apache.cassandra.db.Table.<init>(Table.java:271)
        at org.apache.cassandra.db.Table.open(Table.java:101)
        at org.apache.cassandra.db.Table.open(Table.java:79)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:201)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:349)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:392)
 INFO 05:15:23,858 Opening /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ia-5 (171 bytes)
 INFO 05:15:23,870 Opening /var/lib/cassandra/data/system/peers/system-peers-ia-1 (7983 bytes)
 INFO 05:15:23,870 Opening /var/lib/cassandra/data/system/peers/system-peers-ia-2 (7876 bytes)
 INFO 05:15:23,884 Opening /var/lib/cassandra/data/system/local/system-local-ia-35 (4910 bytes)
 INFO 05:15:23,885 Opening /var/lib/cassandra/data/system/local/system-local-ia-33 (75 bytes)
 INFO 05:15:23,885 Opening /var/lib/cassandra/data/system/local/system-local-ia-34 (4676 bytes)
 INFO 05:15:23,912 reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
 WARN 05:15:23,912 error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:151)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:247)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:362)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.Table.initCf(Table.java:333)
        at org.apache.cassandra.db.Table.<init>(Table.java:271)
        at org.apache.cassandra.db.Table.open(Table.java:101)
        at org.apache.cassandra.db.Table.open(Table.java:79)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:201)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:349)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:392)
{noformat}",vijay2win@yahoo.com,brandon.williams,Normal,Resolved,Fixed,05/Sep/12 15:31,16/Apr/19 09:32
Bug,CASSANDRA-4624,12606412,ORDER BY validation is not restrictive enough,"We're not able to do order by on anything that is a key range. However, we only refuse queries that have an empty where clause, but that doesn't exclude all key ranges at all.",slebresne,slebresne,Low,Resolved,Fixed,06/Sep/12 14:51,16/Apr/19 09:32
Bug,CASSANDRA-4626,12606503,Multiple values for CurrentLocal Node ID,"From this email thread http://www.mail-archive.com/user@cassandra.apache.org/msg24677.html

There are multiple columns for the CurrentLocal row in NodeIdInfo:

{noformat}

[default@system] list NodeIdInfo ;
Using default limit of 100
...
-------------------
RowKey: 43757272656e744c6f63616c
=> (column=01efa5d0-e133-11e1-0000-51be601cd0ff, value=0a1020d2, timestamp=1344414498989)
=> (column=92109b80-ea0a-11e1-0000-51be601cd0af, value=0a1020d2, timestamp=1345386691897)
{noformat}

SystemTable.getCurrentLocalNodeId() throws an assertion that occurs when the static constructor for o.a.c.utils.NodeId is in the stack.

The impact is a java.lang.NoClassDefFoundError when accessing a particular CF (I assume on with counters) on a particular node.

Cannot see an obvious cause in the code. ",slebresne,amorton,Normal,Resolved,Fixed,06/Sep/12 23:25,16/Apr/19 09:32
Bug,CASSANDRA-4627,12606532,support inet data type,"CASSANDRA-4018 introduced a new cassandra data type with a cql name ""inet"", which is not yet supported in cqlsh. Add support for decoding and formatting.",thepaul,thepaul,Normal,Resolved,Fixed,07/Sep/12 05:12,16/Apr/19 09:32
Bug,CASSANDRA-4628,12606538,CQL/JDBC: date vs. timestamp issues,"Cassandra's datatypes only have one Date/Time type named timestamp containing both date and time. Calling the validator org.apache.cassandra.db.marshal.DateType might be OK in general but can be confusing in the jdbc context where there is a distinction between date, time and timestamp. In terms of jdbc there should be more datatypes for dates and times or the jdbc driver should take one of the following options:
- stick to timestamp
- check if the date has a time part and distinguish by the data between date and timestamp automatically
- use distinct datatypes according to the jdbc spec, the types would need to be in cassandra then too

Now back to my actual problem:
org.apache.cassandra.cql.jdbc.JdbcDate returns Types.DATE in getType(). Even if having inserted a complete date with time (making it a timestamp) the ResultSetMetaData.getColumnType() implementation still returns Types.DATE (source of this is in JdbcDate). If some other java code (where i don't have access to) uses the metadata to get the type and then getDate() to get the value the time is cut off the value and only the date is returned.

But the ResultSet.getObject() implementation returns a complete java.util.Date including the time.",mkrumpholz,mkrumpholz,Low,Resolved,Fixed,07/Sep/12 07:25,16/Apr/19 09:32
Bug,CASSANDRA-4631,12606609,minimum stack size for u34 and later is 180k,"We currently only set the stack to 180k for java 7, but it looks like java 6 u34 and later now need this too.  Let's just set them all to 180k.",brandon.williams,brandon.williams,Low,Resolved,Fixed,07/Sep/12 15:26,16/Apr/19 09:32
Bug,CASSANDRA-4633,12606649,cassandra-stress:  --enable-cql does not work with COUNTER_ADD,"When I remove --enable-cql the following runs successfully.
Note:  INSERT/READ are fine.

{code}
./cassandra-stress --operation=COUNTER_ADD --enable-cql --replication-factor=3 --consistency-level=ONE --num-keys=10000  --columns=20 

total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [1] retried 10 times - error incrementing key 0001 ((InvalidRequestException): cannot parse 'C58' as hex bytes)

Operation [0] retried 10 times - error incrementing key 0000 ((InvalidRequestException): cannot parse 'C58' as hex bytes)

0,0,0,NaN,0
FAILURE
{code}",aleksey,cdaw,Low,Resolved,Fixed,07/Sep/12 19:55,16/Apr/19 09:32
Bug,CASSANDRA-4634,12606735,cqlsh --color option doesn't allow you to disable colors,"There's no way to disable colors with cqlsh, despite it having a {{--color}} option, because that option can only enable color if present, not disable it, and the default is that color is enabled.

(Incidentally, if the {{--file}} option is used, it will disable color.)",thobbs,thobbs,Low,Resolved,Fixed,08/Sep/12 21:53,16/Apr/19 09:32
Bug,CASSANDRA-4640,12606864,Make CQL3 the default,"In 1.2, CQL3 will be final and thus I believe we should make it the default for CQL (and thus cqlsh).

Of course CQL2 will still be available, one will just have to call set_cql_version (for thrift) or 'cqlsh -2'.",slebresne,slebresne,Low,Resolved,Fixed,10/Sep/12 13:13,16/Apr/19 09:32
Bug,CASSANDRA-4645,12607078,(CQL3) Re-allow order by on non-selected columns,"CASSANDRA-4612 added a limitation to ORDER BY query in that it requires the columns part of the ORDER BY to be in the select clause, while this wasn't the case previously.

The reason for that is that for ORDER BY with IN queries, the sorting is done post-query, and by the time we do the ordering, we've already cut down the result set to the select clause, so if the column are not in the select clause we cannot sort on them.

We should remove that that limitation however as this is a regression from what we had before. As far as 1.2.0 is concerned, at the very least we should lift the limitation for EQ queries since we don't do any post-query sorting in that case and that was working correctly pre-CASSANDRA-4612. But we should also remove that limitation for IN query, even if it's in a second time.",slebresne,slebresne,Low,Resolved,Fixed,11/Sep/12 15:07,16/Apr/19 09:32
Bug,CASSANDRA-4648,12607116,Unable to start Cassandra with simple authentication enabled,"I followed the steps for enabling simple authentication as described here, http://www.datastax.com/docs/1.1/configuration/authentication. I tried starting Cassandra with, 

cassandra -f -Dpasswd.properties=conf/passwd.properties -Daccess.properties=conf/access.properties

Start up failed with this exception in my log:

ERROR [main] 2012-09-11 15:03:04,642 CassandraDaemon.java (line 403) Exception encountered during startup
java.lang.AssertionError: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:136)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:298)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:386)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:429)
Caused by: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.service.ClientState.validateLogin(ClientState.java:254)
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:235)
        at org.apache.cassandra.cql3.statements.SelectStatement.checkAccess(SelectStatement.java:105)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:106)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:124)
        ... 4 more",slebresne,jsanda,Normal,Resolved,Fixed,11/Sep/12 19:39,16/Apr/19 09:32
Bug,CASSANDRA-4649,12607145,"Cassandra 1.2 should not accept CQL version ""3.0.0-beta1""","During Cassandra 1.1's whole lifecycle, the CREATE KEYSPACE syntax was pretty dramatically and incompatibly different from what is there now for 1.2. That's ok, since we explicitly said there could be breaking changes in the syntax before 3.0.0 final, but at least we should make it clear that 3.0.0 is not compatible with the 3.0.0-beta1 syntax we had out for quite a while.

If we don't want to reject connections asking for 3.0.0-beta1, we should bump the version number to 3.0.1 or something.",slebresne,thepaul,Low,Resolved,Fixed,11/Sep/12 23:10,16/Apr/19 09:32
Bug,CASSANDRA-4655,12607241,Truncate operation doesn't delete rows from HintsColumnFamily.,"Steps to reproduce:
1. Start writing of data to some column family, let name it 'MyCF'
2. Stop 1 node
3. Wait some time (until some data will be collected in HintsColumnFamily)
4. Start node (HintedHandoff will be started automatically for 'MyCF')
5. Run 'truncate' command for 'MyCF' column family from command from cli
6. Wait until truncate will be finished
7. You will see that 'MyCF' is not empty because HintedHandoff is copying data 

So, I suggest to clean HintsColumnFamily (for truncated column family) before we had started to discard sstables. 
I think it should be done in CompactionManager#submitTrucate() method. I can try to create patch but I need to know right way of cleaning HintsColumnFamily. Could you clarify it?
",jbellis,azotcsit,Low,Resolved,Fixed,12/Sep/12 13:48,16/Apr/19 09:32
Bug,CASSANDRA-4657,12607257,cql version race condition with rpc_server_type: sync,"If clients connect to a cassandra cluster configured with rpc_server_type: sync with heterogeneous cql versions (2 and 3), the cql version used for execution on the server changes seemingly randomly.
It's due to the fact that CustomTThreadPoolServer.java does not set the remoteSocket anytime, or does not clear the cql version in the ThreadLocal clientState object.
When CassandraServer.java calls state() it gets the ThreadLocal object clientState, which has its cqlversion already changed by a previous socket that was using the same thread.


The easiest fix is probably to do a SocketSessionManagementService.instance.set when accepting a new client and SocketSessionManagementService.instance.remove when the client is closed, but if you really want to use the ThreadLocal clientState and not alloc/destroy a ClientState everytime, then you should clear this clientState on accept of a new client.

The problem can be reproduced with cqlsh -3 on one side and a client that does not set the cql version, expecting to get version 2 by default, but actually gettingv v2/v3 depending on which thread it connects to.

The problem does not happen with other rpc_server_types, nor with clients that set their cql version at connection.",jbellis,ecourreges,Low,Resolved,Fixed,12/Sep/12 15:10,16/Apr/19 09:32
Bug,CASSANDRA-4659,12607311,Fix consistency ALL parsing in CQL3,"CASSANDRA-4490 made some change to the parsing of ALL for consistency levels (introducing a specific token K_ALL). It's unclear why since that new token is not used (that is, except for the consistency level), probably some left over of a previous version of the patch.

In any case, this doesn't work. K_ALL and K_LEVEL being both tokens, the string 'ALL' will always generate K_ALL and never K_LEVEL and thus 'USING CONSISTENCY ALL' doesn't parse anymore.",slebresne,slebresne,Low,Resolved,Fixed,12/Sep/12 17:35,16/Apr/19 09:32
Bug,CASSANDRA-4669,12607713,Empty .cqlsh_history file causes cqlsh to crash on startup.,"Not sure how I got it, but I ended up with an empty .cqlsh_history file.  In that state, when starting cqlsh, you end up with:

bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 2588, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""bin/cqlsh"", line 2543, in main
    readline.read_history_file(HISTORY)
IOError: [Errno 22] Invalid argument

Its a simple fix to check for a non-empty history file.  I'll attach the patch.",boneill,boneill,Low,Resolved,Fixed,14/Sep/12 20:55,16/Apr/19 09:32
Bug,CASSANDRA-4672,12607934,_TRACE verb is not droppable which causes an AssertionError,When a big enough statement is traced (like select *) an assertion error is fired because the _TRACE verb is not droppable.,dr-alves,dr-alves,Low,Resolved,Fixed,17/Sep/12 18:49,16/Apr/19 09:32
Bug,CASSANDRA-4674,12607992,cqlsh COPY TO and COPY FROM don't work with cql3,cqlsh COPY TO and COPY FROM don't work with cql3 due to previous cql3 changes.,aleksey,aleksey,Normal,Resolved,Fixed,18/Sep/12 01:06,16/Apr/19 09:32
Bug,CASSANDRA-4675,12607993,NPE in NTS when using LQ against a node (DC) that doesn't have replica,"in a NetworkTopologyStrategy where there are 2 DC:

{panel}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
127.0.0.1       dc1         r1          Up     Normal  115.78 KB       50.00%  0                                           
127.0.0.2       dc2         r1          Up     Normal  129.3 KB        50.00%  85070591730234615865843651857942052864  
{panel}
I have a KS that has replica is 1 of the dc (dc1):

{panel}
[default@unknown] describe Keyspace3;                                                                                                                     
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

But if I connect to a node in dc2, using LOCAL_QUORUM, I get NPE in the Cassandra node's log:

{panel}
[default@unknown] consistencylevel as LOCAL_QUORUM;                       
Consistency level is set to 'LOCAL_QUORUM'.
[default@unknown] use Keyspace3;                                          
Authenticated to keyspace: Keyspace3
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                     
Internal error processing get
org.apache.thrift.TApplicationException: Internal error processing get
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:511)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}

node2's log:
{panel}
ERROR [Thrift:3] 2012-09-17 18:15:16,868 Cassandra.java (line 2999) Internal error processing get
java.lang.NullPointerException
        at org.apache.cassandra.locator.NetworkTopologyStrategy.getReplicationFactor(NetworkTopologyStrategy.java:142)
        at org.apache.cassandra.service.DatacenterReadCallback.determineBlockFor(DatacenterReadCallback.java:90)
        at org.apache.cassandra.service.ReadCallback.<init>(ReadCallback.java:67)
        at org.apache.cassandra.service.DatacenterReadCallback.<init>(DatacenterReadCallback.java:63)
        at org.apache.cassandra.service.StorageProxy.getReadCallback(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:609)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.internal_get(CassandraServer.java:383)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:401)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2989)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{panel}

I could workaround it by adding dc2:0 to the option:

{panel}
[default@Keyspace3] describe Keyspace3;                                           
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc2:0, dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

Now you get UA:

{panel}
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                           
null
UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6506)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:519)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}


On a side note, is there a thought on having a CL.LOCAL_ONE? Ie if local node (wrt the dc) does not have replica, on a LOCAL_ONE, it won't try to go across DC to try to get it. It would be similar to LOCAL_QUORUM.",jbellis,cywjackson,Low,Resolved,Fixed,18/Sep/12 01:23,16/Apr/19 09:32
Bug,CASSANDRA-4679,12608065,Fix binary protocol NEW_NODE event,"As discussed on CASSANDRA-4480, the NEW_NODE/REMOVED_NODE of the binary protocol are not correctly fired (NEW_NODE is fired on node UP basically). This ticket is to fix that.",slebresne,slebresne,Low,Resolved,Fixed,18/Sep/12 14:01,16/Apr/19 09:32
Bug,CASSANDRA-4685,12608233,Fix scrubbing of CQL3 created tables,"{noformat}
 INFO 12:20:42,822 Scrubbing SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ia-1-Data.db')
 WARN 12:20:42,826 Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Error validating row DecoratedKey(61935297886570031978600740763604084078, 4b6579737061636531)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:244)
        at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:111)
        at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:157)
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173)
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:495)
        at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:484)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:66)
        at org.apache.cassandra.db.compaction.CompactionManager$3.perform(CompactionManager.java:223)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:193)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
        at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:65)
        at org.apache.cassandra.db.Column.validateFields(Column.java:287)
        at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:378)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:240)
        ... 15 more
 WARN 12:20:42,826 Row at 19 is unreadable; skipping to next
 WARN 12:20:42,827 No valid rows found while scrubbing SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ia-1-Data.db'); it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot
{noformat}",slebresne,brandon.williams,Normal,Resolved,Fixed,19/Sep/12 13:00,16/Apr/19 09:32
Bug,CASSANDRA-4688,12608242,comments and error messages use wrong method names,comments and error messages in ColumnFamilyInputFormat do not reflect the actual method names in ConfigHelper,crotwell,crotwell,Low,Resolved,Fixed,19/Sep/12 13:50,16/Apr/19 09:32
Bug,CASSANDRA-4689,12608254,Log error when using IN together with ORDER BY,"{code}
$ bin/cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use ks;
cqlsh:ks> drop TABLE test;
cqlsh:ks> CREATE TABLE test (my_id varchar, time_id uuid, value int, PRIMARY KEY (my_id, time_id));
cqlsh:ks> INSERT INTO test (my_id, time_id, value) VALUES ('key1', 1, 1);
cqlsh:ks> INSERT INTO test (my_id, time_id, value) VALUES ('key2', 2, 2);
cqlsh:ks> select * from test where my_id in('key1', 'key2') order by time_id;
TSocket read 0 bytes
{code}

The log shows this:
{code}
ERROR [Thrift:5] 2012-09-19 08:44:59,859 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.IllegalArgumentException: Column time_id wasn't found in select clause.
	at org.apache.cassandra.cql3.statements.SelectStatement.getColumnPositionInSelect(SelectStatement.java:866)
	at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:836)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:807)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:137)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1242)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

NOTE: This issue appears similar to https://issues.apache.org/jira/browse/CASSANDRA-4612 from the user perspective, even though 4612 was verified as fixed.",xedin,tpatterson,Normal,Resolved,Fixed,19/Sep/12 14:57,16/Apr/19 09:32
Bug,CASSANDRA-4695,12608517,CompactionsTest fails with timeout,"{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.CompactionsTest:testStandardColumnCompactions:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsTest FAILED (timeout)
{code}",azotcsit,azotcsit,Low,Resolved,Fixed,20/Sep/12 19:05,16/Apr/19 09:32
Bug,CASSANDRA-4698,12608662,Keyspace disappears when upgrading node from cassandra-1.1.1 to cassandra-1.1.5,"Here is how I got the problem to happen:

1. Get this zipped data directory (about 33Mb):
  scp cass@50.57.69.32:/home/cass/cassandra.zip ./ (password cass)
2. Unzip it in /var/lib/
3. clone the cassandra git repo
4. git checkout cassandra-1.1.1; ant jar;
5. bin/cassandra 
6. Run cqlsh -3, then DESC COLUMNFAMILIES; Note the presence of Keyspace performance_tests
7. pkill -f cassandra; git checkout cassandra-1.1.5; ant realclean; ant jar;
8. bin/cassandra
9. Run cqlsh -3, then DESC COLUMNFAMILIES; Note that there is no performance_tests keyspace",xedin,tpatterson,Normal,Resolved,Fixed,21/Sep/12 16:21,16/Apr/19 09:32
Bug,CASSANDRA-4700,12608695,cql 2 counter validations need to use default consistencylevel if none is explicitly given,"i was trying to run cql 2 query

cqlsh:stats> UPDATE Minutewise_Product_Stats SET '2LX:OQ:XYZ.com:664230591:1:totalView'='2LX:SOQ:XYZ.com:664230591:1:totalView'+1, '2LX:OQ:XYZ.com:664230591:1:keywordClick'='2LX:SOQ:xyz.com:664230591:1:keywordClick'+1 WHERE KEY='2017:4' ;

WHEN I GOT  this error

ERROR 20:38:46,220 Error occurred during processing of message.
java.lang.NullPointerException
    at org.apache.cassandra.cql.UpdateStatement.prepareRowMutations(UpdateStatement.java:151)
    at org.apache.cassandra.cql.UpdateStatement.prepareRowMutations(UpdateStatement.java:128)
    at org.apache.cassandra.cql.QueryProcessor.batchUpdate(QueryProcessor.java:245)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:563)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:817)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1675)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:1)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)


attached is the patch file  to fix this bug

CQL2fix.patch

diff --git a/src/java/org/apache/cassandra/cql/UpdateStatement.java b/src/java/org/apache/cassandra/cql/UpdateStatement.java
index 3a47712..0caa61b 100644
--- a/src/java/org/apache/cassandra/cql/UpdateStatement.java
+++ b/src/java/org/apache/cassandra/cql/UpdateStatement.java
@@ -146,8 +146,11 @@ public class UpdateStatement extends AbstractModification
         }
 
         CFMetaData metadata = validateColumnFamily(keyspace, columnFamily, hasCommutativeOperation);
-        if (hasCommutativeOperation)
-            cLevel.validateCounterForWrite(metadata);
+        
+        if (hasCommutativeOperation){
+        	ConsistencyLevel currentCLevel = getConsistencyLevel();
+        	currentCLevel.validateCounterForWrite(metadata);
+        }
 
         QueryProcessor.validateKeyAlias(metadata, keyName);
 
",niteesh.kumar,niteesh.kumar,Low,Resolved,Fixed,21/Sep/12 21:05,16/Apr/19 09:32
Bug,CASSANDRA-4703,12608756,o.a.c.service.StorageProxy - compilation issue,"Running ant on master yields the following compilation error. 



build-project:
     [echo] apache-cassandra: /home/user/workspace/cassandra/build.xml
    [javac] Compiling 1 source file to /home/user/workspace/cassandra/build/classes/main
    [javac] /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java:803: local variable remotes is accessed from within inner class; needs to be declared final
    [javac]                             sendToHintedEndpoints(cm.makeReplicationMutation(), remotes, responseHandler, localDataCenter, consistency_level);
    [javac]                                                                                 ^
    [javac] Note: /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java uses unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error




",kaykay.unique,kaykay.unique,Normal,Resolved,Fixed,22/Sep/12 16:38,16/Apr/19 09:32
Bug,CASSANDRA-4706,12608789,CQL3 CREATE TABLE with set and counter causes java.lang.IllegalArgumentException,"Running a freshly compiled cassandra with no data, and a brand new keyspace (SimpleStrategy, replication_factor 1)


{noformat}
cqlsh:test> CREATE TABLE test (id bigint PRIMARY KEY, count counter, things set<text>);
TSocket read 0 bytes
{noformat}

{noformat}
ERROR 11:25:54,926 Error occurred during processing of message.
java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:247)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:50)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:59)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:143)
	at org.apache.cassandra.config.CFMetaData.validate(CFMetaData.java:1064)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:123)
	at org.apache.cassandra.cql3.statements.CreateColumnFamilyStatement.announceMigration(CreateColumnFamilyStatement.java:100)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:83)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:116)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1677)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}",slebresne,titanous,Low,Resolved,Fixed,23/Sep/12 15:40,16/Apr/19 09:32
Bug,CASSANDRA-4708,12608822,StorageProxy slow-down and memory leak,"I am consistently observing slow-downs in StorageProxy caused by the NonBlockingHashMap used indirectly by MessagingService via the callbacks ExpiringMap.

This seems do be due to NBHM having unbounded memory usage in the face of workloads with high key churn. As monotonically increasing integers are used as callback id's by MessagingService, the backing NBHM eventually ends up growing the backing store unboundedly. This causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. 

This behavior is especially noticable for high throughput workloads where the dataset is completely in ram and I'm doing up to a hundred thousand reads per second.

Replacing NBHM in ExpiringMap with the java standard library ConcurrentHashMap resolved the issue and allowed me to keep a consistent high throughput.

An open issue on NBHM can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362",danielnorberg,danielnorberg,Normal,Resolved,Fixed,24/Sep/12 07:09,16/Apr/19 09:32
Bug,CASSANDRA-4709,12608856,(CQL3) Missing validation for IN queries on column not part of the PK,"Copy-pasting from the original mail (http://mail-archives.apache.org/mod_mbox/cassandra-user/201209.mbox/%3C20120922185826.GO6205@pslp2%3E):
{noformat}
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> 
cqlsh> create keyspace xpl1 WITH strategy_class ='SimpleStrategy' and strategy_options:replication_factor=1;
cqlsh> use xpl1;
cqlsh:xpl1> create table t1 (pk varchar primary key, col1 varchar, col2 varchar);
cqlsh:xpl1> create index t1_c1 on t1(col1);
cqlsh:xpl1> create index t1_c2 on t1(col2);
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1a','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1b','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1c','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk2','foo2','bar2');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk3','foo3','bar3');
cqlsh:xpl1> select * from t1 where col2='bar1';
 pk   | col1 | col2
------+------+------
 pk1b | foo1 | bar1
  pk1 | foo1 | bar1
 pk1a | foo1 | bar1
 pk1c | foo1 | bar1

cqlsh:xpl1> select * from t1 where col2 in ('bar1', 'bar2') ;
cqlsh:xpl1> 
{noformat}

We should either make that last query work or refuse the query but returning nothing is wrong.",slebresne,slebresne,Low,Resolved,Fixed,24/Sep/12 12:12,16/Apr/19 09:32
Bug,CASSANDRA-4711,12608897,Duplicate column names with DynamicCompositeType,"I have a column family whose comparator is DynamicCompositeType and validation is CounterColumnType.  During automated testing, there have been occasions where a counter column is created twice, throwing off the query results for the column.

Doing a 'get' via the cli, I see the following output for the row:

=> (counter=s@language:b@00000001:s@pt_BR, value=198)
=> (counter=s@language:s@possible, value=200)
=> (counter=s@language:b@00000001:s@pt_BR, value=0)

If I print out the byte value of the column names along with their MD5 sum I see:

Name: [language, java.nio.HeapByteBuffer[pos=0 lim=4 cap=4], pt_BR]
Byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520
MD5: 2db353a9a72a0d7cb6cb277ac5125653
Name: [language, possible]
Byte array: ffffff8073086c616e67756167650ffffff807308706f737369626c650
MD5: 82cad9b6a65c794e97cf1d4613e2e367
Name: [language, java.nio.HeapByteBuffer[pos=0 lim=4 cap=4], pt_BR]
Byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520
MD5: 2db353a9a72a0d7cb6cb277ac5125653

Unfortunately, I have been unable to duplicate this manually or via a generic test script and our QA department can only duplicate ~25% of the time.",slebresne,thrykol,Normal,Resolved,Fixed,24/Sep/12 16:35,16/Apr/19 09:32
Bug,CASSANDRA-4716,12609060,CQL3 predicate logic is reversed when used on a reversed column,"Example:

{code}
cqlsh:test>
cqlsh:test> CREATE TABLE testrev (
        ... key text,
        ... rdate timestamp,
        ... num double,
        ... PRIMARY KEY(key,rdate)
        ... ) WITH COMPACT STORAGE
        ...   AND CLUSTERING ORDER BY(rdate DESC);
cqlsh:test> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:test> select key from testrev where rdate > '2012-01-02' ;
 key 
-----
 foo 

cqlsh:test> select key from testrev where rdate < '2012-01-02' ;
cqlsh:test>
{code}",slebresne,tjake,Normal,Resolved,Fixed,25/Sep/12 15:00,16/Apr/19 09:32
Bug,CASSANDRA-4717,12609084,cqlsh fails to format values of ReversedType-wrapped classes,"See the test case for CASSANDRA-4715, but run it on trunk. The ReversedType-wrapped column (rdate) will be displayed as a floating-point integer (it gets deserialized into a native type correctly, but cqlsh's format-according-to-type machinery doesn't know how to handle the cqltypes.ReversedType subclass.",slebresne,thepaul,Low,Resolved,Fixed,25/Sep/12 16:54,16/Apr/19 09:32
Bug,CASSANDRA-4719,12609115,"binary protocol: when an invalid event type is watched via a REGISTER message, the response message does not have an associated stream id","I tried sending a REGISTER message with an eventlist including the string ""STATUS_FOO"", in order to test error handling in the python driver for that eventuality. But the response from the server (a ""Server error"" with a message of ""java.lang.IllegalArgumentException: No enum const class org.apache.cassandra.transport.Event$Type.STATUS_FOO"") had a stream_id of 0, so the driver was not able to associate it with the request.",slebresne,thepaul,Low,Resolved,Fixed,25/Sep/12 20:25,16/Apr/19 09:32
Bug,CASSANDRA-4720,12609124,cqlsh fails to format timeuuid values,"If a user has a table with a timeuuid column, and the user tries to select some rows containing timeuuid values, a weird error results:

{noformat}
global name 'unix_time_from_uuid1' is not defined
{noformat}

Not very helpful. It should probably display the timeuuid somehow.",thepaul,thepaul,Normal,Resolved,Fixed,25/Sep/12 20:54,16/Apr/19 09:32
Bug,CASSANDRA-4721,12609127,Have Cassandra return the right error for keyspaces with dots,"cqlsh> CREATE KEYSPACE 'solr.test' WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
Bad Request: Invalid keyspace name: shouldn't be empty nor more than 48 characters long (got ""solr.test"")",jbellis,j.casares,Low,Resolved,Fixed,25/Sep/12 21:02,16/Apr/19 09:32
Bug,CASSANDRA-4724,12609250,Some operations of HintedHandOffManager bean have wrong output,"I suggest to change output of listEndpointsPendingHints and countPendingHints operations in HintedHandOffManager bean.

Current output:
  - listEndpointsPendingHints:
{code}
c�.@ÁM��JprV���c�.@ÁM��JprV���
{code}
  - countPendingHints:
{code}
116570217535704627=1170
{code}

Suggested output:
  - listEndpointsPendingHints:
{code}
localhost/127.0.0.1
{code}
  - countPendingHints:
{code}
localhost/127.0.0.1=1170
{code}

",azotcsit,azotcsit,Low,Resolved,Fixed,26/Sep/12 14:25,16/Apr/19 09:32
Bug,CASSANDRA-4727,12609344,Avoid ConcurrentModificationExceptions on relocateTokens,"code loops over a HashMap and deletes items from the hashmap without using the iterator.

will result in ConcurrentModificationExceptions... remove thru the iterator instead.",dbrosius@apache.org,dbrosius@apache.org,Low,Resolved,Fixed,27/Sep/12 04:47,16/Apr/19 09:32
Bug,CASSANDRA-4728,12609349,"NPE with some load of writes, but possible snitch setting issue for a cluster","The following errors are showing under height load

ERROR [MutationStage:8294] 2012-09-25 22:01:47,628 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:8294,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.locator.PropertyFileSnitch.getDatacenter(PropertyFileSnitch.java:104)
	at com.datastax.bdp.snitch.DseDelegateSnitch.getDatacenter(DseDelegateSnitch.java:69)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:93)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1984)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1972)
	at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:262)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:248)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:505)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:56)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


ERROR [MutationStage:13164] 2012-09-25 22:19:06,486 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13164,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [Thrift:12] 2012-09-25 22:19:07,433 Cassandra.java (line 3462) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [Thrift:16] 2012-09-25 22:19:07,437 Cassandra.java (line 2999) Internal error processing get


java.lang.NullPointerException
 INFO [GossipStage:280] 2012-09-26 00:15:15,371 Gossiper.java (line 818) InetAddress /172.16.233.208 is now dead.
ERROR [GossipStage:280] 2012-09-26 00:15:15,372 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:280,5,main]
j

ERROR [MutationStage:40529] 2012-09-26 00:15:21,527 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:40529,5,main]
java.lang.NullPointerException
 INFO [GossipStage:281] 2012-09-26 00:15:23,013 Gossiper.java (line 818) InetAddress /172.16.232.159 is now dead.
ERROR [GossipStage:281] 2012-09-26 00:15:23,014 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:281,5,main]
",jbellis,alexliu68,Low,Resolved,Fixed,27/Sep/12 05:11,16/Apr/19 09:32
Bug,CASSANDRA-4737,12609728,doc/native_protocol.txt isn't up to date,CASSANDRA-4449 seems to have changed the datatype of the query id returned by a RESULT-PREPARED message from an {{int}} to a {{short}} n followed by n bytes (representing a md5sum). The specification at doc/native_protocol.txt doesn't cover this change yet.,slebresne,tux21b,Low,Resolved,Fixed,01/Oct/12 09:22,16/Apr/19 09:32
Bug,CASSANDRA-4739,12609767,Prepared Statements don't support collections,"I'm putting a collection onto the wire in an EXECUTE request with exactly the same bytes that Cassandra encodes the same data in a response:

""Can't apply operation on column with org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type) type""

Here's the full trace log from cassandra:

{noformat}
DEBUG 19:24:15,414 Received: PREPARE INSERT INTO things (id, set_text) VALUES (?, ?);
TRACE 19:24:15,414 CQL QUERY: INSERT INTO things (id, set_text) VALUES (?, ?);
TRACE 19:24:15,415 Stored prepared statement #413587006 with 2 bind markers
DEBUG 19:24:15,415 Responding: RESULT PREPARED 413587006 [id(gocql_collections, things), org.apache.cassandra.db.marshal.TimeUUIDType][set_text(gocql_collections, things), org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)]
DEBUG 19:24:15,416 Received: EXECUTE 413587006 with 2 values
TRACE 19:24:15,416 [1] 'java.nio.HeapByteBuffer[pos=18 lim=34 cap=53]'
TRACE 19:24:15,416 [2] 'java.nio.HeapByteBuffer[pos=38 lim=51 cap=53]'
DEBUG 19:24:15,417 Responding: ERROR INVALID: Can't apply operation on column with org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type) type.
{noformat}


The prepared statement is:

{noformat}
INSERT INTO things (id, set_text) VALUES (?, ?);
{noformat}

and the collection value ({'asdf', 'sdf'}) is encoded as:

{noformat}
00 02 00 04 61 73 64 66 00 03 73 64 66
{noformat}

which is byte-for-byte exactly the same as what I get from Cassandra off the wire when I do a query for the same data.

I already have the driver working with other queries/inserts with all other types, so this is just a collection encoding problem.",slebresne,titanous,Normal,Resolved,Fixed,01/Oct/12 16:32,16/Apr/19 09:32
Bug,CASSANDRA-4746,12609853,cqlsh timestamp formatting is broken - displays wrong timezone info (at least on Ubuntu),"cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> create table ts (id int primary key, ts timestamp);
cqlsh:test> insert into ts (id, ts) values (1, '2012-05-14 07:53:20+0000');
cqlsh:test> select * from ts;
 id | ts
----+--------------------------
  1 | 2012-05-14 10:53:20+0000


Should've been 2012-05-14 10:53:20+0300.

cqlsh formats timestamps using '%Y-%m-%d %H:%M:%S%z' format-string and 'the %z escape that expands to the preferred hour/minute offset is not supported by all ANSI C libraries'. In this case it's just replaced with all zeroes.",aleksey,aleksey,Low,Resolved,Fixed,02/Oct/12 02:09,16/Apr/19 09:32
Bug,CASSANDRA-4749,12609917,Possible problem with widerow in Pig URI,"I don't have a good way to test this directly, but I'm concerned the Uri parsing for widerows isn't going to work.  setLocation 
1.) calls setLocationFromUri (which sets widerows to the Uri value)
2.) sets widerows to a static value (which is defined as false)
3.) sets widerows to the system setting if it exists.  
That doesn't seem right...

But setLocationFromUri also gets called from setStoreLocation, and I don't really know the difference between setLocation and setStoreLocation in terms of what is going on in terms of the integration between cassandra/pig/hadoop.",brandon.williams,oberman,Normal,Resolved,Fixed,02/Oct/12 14:30,16/Apr/19 09:32
Bug,CASSANDRA-4751,12610056,User rpc_address for binary protocol and change default port,"The events mechanism of the binary protocol require that we know the address on which other nodes can be joined (for the binary protocol). Hence CASSANDRA-4501. However, in 1.2 we've already burned all the padding in 1.1 gossip, so we can't gossip a new info (the binary protocol address), so CASSANDRA-4501 will have to move to 1.3.

But we do already gossip the rpc_address value, so an option is to make the binary protocol bind on the rpc_address (but a specific port) rather than having it's own setting. This ticket suggests to do that. Imo, there is little downside to do it: the thrift and binary transport are not meant to be used together except during the transition from one to the other, and even then having to use the same network interface is hardly a limitation (in other words, even for 1.3, we might want to hold on CASSANDRA-4501 until someone comes with a compelling use case for it).",slebresne,slebresne,Low,Resolved,Fixed,03/Oct/12 09:41,16/Apr/19 09:32
Bug,CASSANDRA-4752,12610062,Drop keyspace causes schema disagreement,"The fix for CASSANDRA-4698 introduced a bug whereby when drop keyspace is issued a schema disagreement immediately occurs. This seems to be because the 

{code}ColumnFamily cf = ColumnFamily.create(modification.getValue().metadata());{code}

in {{RowMutation.deserializeFixingTimestamps}} does not preserve deletion info for the cf in the modification. In most cases, this doesn't cause a problem, but for a drop keyspace modification, there are no columns in the cf, so the deletion info is effectively lost leading to an incorrect digest being created and ultimately a schema disagreement.

Replacing the {{create}} with {{cloneMeShallow}} does preserve the deletion info and avoids the schema disagreement issue.
",samt,samt,Normal,Resolved,Fixed,03/Oct/12 10:38,16/Apr/19 09:32
Bug,CASSANDRA-4753,12610089,Timeout reporter writes hints for the local node,"MessagingService.java:330 calls StorageProxy.scheduleLocalHint() without checking if the local node is the target. This causes StorageProxy.scheduleLocalHint to throw AssertionError sometimes.

Got this error when some batches are timed out. This can happen because even local batches now go through StorageProxy.sendMessages and aren't just rm.apply()'d.",jbellis,aleksey,Low,Resolved,Fixed,03/Oct/12 15:47,16/Apr/19 09:32
Bug,CASSANDRA-4754,12610091,There is an inconsistency between default configuration in cassandra.yaml and java code,Options max_hint_window_in_ms and in_memory_compaction_limit_in_mb have different values in cassandra.yaml and in java code. I suggest to lead java code values to cassandra.yaml values.,azotcsit,azotcsit,Low,Resolved,Fixed,03/Oct/12 15:57,16/Apr/19 09:32
Bug,CASSANDRA-4755,12610134,Bulk loader won't work with CQL3,"Currently the bulk loader uses thrift to get the schema and validate it before bulk loading a cf. Since we stopped returning cql3 cfs through describe_keyspaces, the bulk loader will be unable to load those cfs.

If we figure out/add a way to validate the schema over jmx, we could use that for getting token ranges as well and drop thrift completely from the bulk loader.

Another option might be querying system tables manually to validate things.",aleksey,nickmbailey,Normal,Resolved,Fixed,03/Oct/12 20:46,16/Apr/19 09:32
Bug,CASSANDRA-4759,12610142,CQL3 Predicate logic bug when using composite columns,"Looks like a predicate logic bug that only happens when you have > 2 primary keys and use COMPACT STORAGE (meaning its using composite columns under the hood)

First I'll show it works with just 2 
{code}
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate)
       ...          ) WITH COMPACT STORAGE
       ...            AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
 key | rdate                    | num
-----+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 10.5
{code}

Now we create with 3 parts to the PRIMARY KEY
{code}
cqlsh:dev> drop TABLE testrev ;
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          rdate2 timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate,rdate2)
       ...          ) WITH COMPACT STORAGE
       ...          AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,rdate2,num) VALUES ('foo','2012-01-01','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
 key | rdate                    | rdate2                   | num
-----+--------------------------+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 2012-01-01 00:00:00-0500 | 10.5

cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
{code}

The last query should return the row...
",slebresne,tjake,Normal,Resolved,Fixed,03/Oct/12 21:23,16/Apr/19 09:32
Bug,CASSANDRA-4764,12610366,Clean out STREAM_STAGE vestiges,Currently it appears as though bulk loading operations don't run in any stage. Seems like they should be running in STREAM_STAGE.,jbellis,nickmbailey,Low,Resolved,Fixed,04/Oct/12 17:46,16/Apr/19 09:32
Bug,CASSANDRA-4765,12610368,StackOverflowError in CompactionExecutor thread,"Seeing the following error:


Exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.StackOverflowError
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)

",jbellis,abashir,Normal,Resolved,Fixed,04/Oct/12 18:17,16/Apr/19 09:32
Bug,CASSANDRA-4770,12610588,(CQL3) data type not in lowercase are not handled correctly.,"Seems that we accept {{int}} but we don't accept {{INT}} (that is, the parser accepts it, but we fail later to recognize it).",slebresne,slebresne,Low,Resolved,Fixed,05/Oct/12 15:56,16/Apr/19 09:32
Bug,CASSANDRA-4771,12610619,Setting TTL to Integer.MAX causes columns to not be persisted.,"When inserting columns via batch mutation, we have an edge case where columns will be set to Integer.MAX.  When setting the column expiration time to Integer.MAX, the columns do not appear to be persisted.

Fails:

Integer.MAX_VALUE 
Integer.MAX_VALUE/2

Works:
Integer.MAX_VALUE/3

",dbrosius@apache.org,tnine,Normal,Resolved,Fixed,05/Oct/12 19:34,16/Apr/19 09:32
Bug,CASSANDRA-4772,12610635,HintedHandoff fails to deliver hints after first repaired node,"If some node has hints for a few nodes it will deliver hints only for the first one of them. After all hints delivery for the first node compaction process is started. After compaction all data from hints cf is removed.

target fix for 1.2 version:
{code}
diff --git a/src/java/org/apache/cassandra/db/HintedHandOffManager.java b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
index e5ff163..c02997e 100644
--- a/src/java/org/apache/cassandra/db/HintedHandOffManager.java
+++ b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
@@ -189,7 +189,7 @@ public class HintedHandOffManager implements HintedHandOffManagerMBean
         ArrayList<Descriptor> descriptors = new ArrayList<Descriptor>();
         for (SSTable sstable : hintStore.getSSTables())
             descriptors.add(sstable.descriptor);
-        return CompactionManager.instance.submitUserDefined(hintStore, descriptors, Integer.MAX_VALUE);
+        return CompactionManager.instance.submitUserDefined(hintStore, descriptors, (int) System.currentTimeMillis() / 1000);
     }

 
     private static boolean pagingFinished(ColumnFamily hintColumnFamily, ByteBuffer startColumn)
{code}

Can I expect to see that fix in 1.1.6 version?",azotcsit,azotcsit,Normal,Resolved,Fixed,05/Oct/12 21:01,16/Apr/19 09:32
Bug,CASSANDRA-4774,12610704,IndexOutOfBoundsException in org.apache.cassandra.gms.Gossiper.sendGossip,"ERROR [GossipTasks:1] 2012-10-06 10:47:48,390 Gossiper.java (line 169) Gossip error
java.lang.IndexOutOfBoundsException: Index: 13, Size: 5
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.gms.Gossiper.sendGossip(Gossiper.java:541)
	at org.apache.cassandra.gms.Gossiper.doGossipToUnreachableMember(Gossiper.java:575)
	at org.apache.cassandra.gms.Gossiper.access$300(Gossiper.java:59)
	at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:141)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",cnlwsu,bcoverston,Low,Resolved,Fixed,06/Oct/12 13:14,16/Apr/19 09:32
Bug,CASSANDRA-4776,12610780,Don't require quotes for true and false,"The docs at http://cassandra.apache.org/doc/cql3/CQL.html#identifiers describe using double quotes for an identifier that is a reserved word. The following works as expected,

cqlsh:test> select ""columnfamily"" from system.schema_columnfamilies;

I have a table with a boolean column. In order to insert a boolean value, I have to enclose it in single quotes. The table looks like,

CREATE TABLE bool_test (
  id int PRIMARY KEY,
  val boolean
);

Here is what happens when I try using double quotes,

cqlsh:rhq> insert into bool_test (id, val) values (4, ""false"");
Bad Request: line 1:43 no viable alternative at input 'false'


The use of single quotes here seems inconsistent with what is described in the docs, and makes things a bit confusing. It would be nice if single or double quotes could be used for identifiers that are reserved words. I also think it is a bit counter-intuitive to require quotes for true and false which are literal values.",slebresne,jsanda,Low,Resolved,Fixed,08/Oct/12 01:20,16/Apr/19 09:32
Bug,CASSANDRA-4778,12610898,leveled compaction does less work in L0 than intended,"We have this code in the candidate loop:

{code}
.               if (SSTable.getTotalBytes(candidates) > maxSSTableSizeInBytes)
                {
                    // add sstables from L1 that overlap candidates
                    candidates.addAll(overlapping(candidates, generations[1]));
                    break;
                }
{code}

thus, as soon as we have enough to compact to make one L1 sstable's worth of data, we stop collecting candidates.",jbellis,jbellis,Low,Resolved,Fixed,08/Oct/12 22:40,16/Apr/19 09:32
Bug,CASSANDRA-4781,12611005,Sometimes Cassandra starts compacting system-shema_columns cf repeatedly until the node is killed,"Cassandra starts flushing system-schema_columns cf in a seemingly infinite loop:

 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,804 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32107-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.202762MB/s.  Time: 18ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,804 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32107-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,824 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32108-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.182486MB/s.  Time: 20ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,825 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32108-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,864 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32109-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.096045MB/s.  Time: 38ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,864 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32109-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,894 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32110-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.121657MB/s.  Time: 30ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,894 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32110-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,914 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32111-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.202762MB/s.  Time: 18ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,914 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32111-Data.db')]
.........

Don't know what's causing it. Don't know a way to predictably trigger this behaviour. It just happens sometimes.",yukim,aleksey,Normal,Resolved,Fixed,09/Oct/12 15:39,16/Apr/19 09:32
Bug,CASSANDRA-4782,12611021,Commitlog not replayed after restart,"It seems that there are two corner cases where commitlog is not replayed after a restart :

 - After a reboot of a server + restart of cassandra (1.1.0 to 1.1.4)
 - After doing an upgrade from cassandra 1.1.X to cassandra 1.1.5

This is due to the fact that the commitlog segment id should always be an  incrementing number (see this condition : https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L247 )

But this assertion can be broken :
In the first case, it is generated by System.nanoTime() but it seems that System.nanoTime() is using the boot time as the base/reference (at least on java6 & linux), thus after a reboot, System.nanoTime() can return a lower number than before the reboot (and the javadoc says the reference is a relative point in time...)
In the second case, this was introduced by #4601 (which changes System.nanoTime() by System.currentTimeMillis() thus people starting with 1.1.5 are safe)

This could explain the following tickets : #4741 and #4481
",jbellis,frousseau,Urgent,Resolved,Fixed,09/Oct/12 16:51,16/Apr/19 09:32
Bug,CASSANDRA-4783,12611037,AE in cql3 select,"Caused by 'select * from foo where key='blah' and column in (...)

{noformat}
ERROR 18:35:46,169 Exception in thread Thread[Thrift:11,5,main]
java.lang.AssertionError
        at org.apache.cassandra.cql3.statements.SelectStatement.getRequestedColumns(SelectStatement.java:443)
        at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:312)
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:200)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:125)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1658)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Causes cqlsh to hang forever.",slebresne,brandon.williams,Normal,Resolved,Fixed,09/Oct/12 18:39,16/Apr/19 09:32
Bug,CASSANDRA-4786,12611102,NPE in migration stage after creating an index,"The dtests are generating this error after trying to create an index in cql2:

{noformat}

ERROR [MigrationStage:1] 2012-10-09 20:54:12,796 CassandraDaemon.java (line 132) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
ERROR [Thrift:1] 2012-10-09 20:54:12,797 CustomTThreadPoolServer.java (line 214) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:348)
    at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:238)
    at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:209)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:714)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:816)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1656)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:344)
    ... 13 more
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
{noformat}",slebresne,brandon.williams,Normal,Resolved,Fixed,10/Oct/12 01:58,16/Apr/19 09:32
Bug,CASSANDRA-4788,12611230,streaming can put files in the wrong location,"Some, but not all streaming incorrectly puts files in the top level data directory.  Easiest way to repro that I've seen is bootstrap where it happens 100% of the time, but other operations like move and repair seem to do the right thing.",yukim,brandon.williams,Normal,Resolved,Fixed,10/Oct/12 19:35,16/Apr/19 09:32
Bug,CASSANDRA-4789,12611242,CassandraStorage.getNextWide produces corrupt data,"This took me a while to track down.  I'm seeing the problem when the ""key changes"" case happens.  The intended behavior (as far as I can tell) when the key changes is the method returns the current tuple, and picks up where it left off on the next call to getNextWide().  The problem I'm seeing is the sometimes the current key advances between method calls, sometimes not.  ""Not"" being the correct behavior, since the code is saving the value into an instance variable, but when the key advances there is a key/value mismatch (the result being the values for two different keys are being glued together).  I think the problem might be related to keys that only have a single column???  I'm still trying to track that down to help assist in solving this case...

Maybe this will be clearer from me pasting a bunch of logging I added to the class.  The log messages are fairly self documenting (I hope):  

...lots of previous logging...
enter getNextWide
hasNext = true
set key = dVNhbXAxMzQ3ODM1OA%3D%3D
lastRow != null
added 1 items to bag from lastRow
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
key changed, new key = 669392df09572d0045b964bc65f86a2c
exit getNextWide
enter getNextWide
hasNext = true
//!!!THIS IS THE PROBLEM HERE I THINK!!!
//!!!Usually the key here == key before ""exit getNextWide""!!!
set key = 5f900ee4bb1850f8cf387cc3d5fc23ca
//!!! lastRow is data for 669392df09572d0045b964bc65f86a2c !!! 
//!!! but it's being added to key 5f900ee4bb1850f8cf387cc3d5fc23ca !!!
lastRow != null
added 1 items to bag from lastRow
//!!! Here are the real values for 5f900ee4bb1850f8cf387cc3d5fc23ca !!!
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
key changed, new key = 50438549-cdb6-8c44-f93a-d18d7daeffd8
exit getNextWide
enter getNextWide
hasNext = true
set key = 50438549-cdb6-8c44-f93a-d18d7daeffd8",oberman,oberman,Normal,Resolved,Fixed,10/Oct/12 20:48,16/Apr/19 09:32
Bug,CASSANDRA-4792,12611365,Digest mismatch doesn't wait for writes as intended,"As reported by Niklas Ekström on the dev list:

I’m looking in the file StorageProxy.java (Cassandra 1.1.5), and line 766 seems odd to me.

FBUtilities.waitOnFutures() is called with the repairResults from the RowRepairResolver resolver.

The problem though is that repairResults is only assigned when the object is created at line 737 in StorageProxy.java, and there it is assigned to Collections.emptyList(), and in the resolve() method in RowRepairResolver, which is indirectly called from line 771 in StorageProxy.java, that is, after the call to FBUtilities.waitOnFutures().

So the effect is that line 766 in StorageProxy.java is essentially a no-op.",jbellis,jbellis,Low,Resolved,Fixed,11/Oct/12 14:54,16/Apr/19 09:32
Bug,CASSANDRA-4793,12611368,Commitlog files replayed but not in the order of their ids,"I noticed that the commitlog files were not replayed in the order of their ids.
It seems that they are sorted by ""last modification date"" before being replayed, but this does not corresponds to their ids.

Moreover, ""last modification date"" is changed when a file is copied, so, this could also change the order of archived commitlogs.

Maybe it's safer to sort them using the id in the file name ?
",frousseau,frousseau,Low,Resolved,Fixed,11/Oct/12 15:10,16/Apr/19 09:32
Bug,CASSANDRA-4795,12611434,"replication, compaction, compression? options are not validated","When creating a keyspace and specifying strategy options, you can pass any k/v pair you like.",dbrosius,brandon.williams,Low,Resolved,Fixed,11/Oct/12 21:21,16/Apr/19 09:32
Bug,CASSANDRA-4796,12611438,composite indexes don't always return results they should,"composite_index_with_pk_test in the dtests is failing and it reproduces manually.

{noformat}
cqlsh:foo>            CREATE TABLE blogs (                 blog_id int,                 time1 int,                 time2 int,                 author text,                 content text,                 PRIMARY KEY (blog_id, time1, time2)             ) ;
cqlsh:foo> create index on blogs(author);
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (1, 0, 0, 'foo', 'bar1');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (1, 0, 1, 'foo', 'bar2');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (2, 1, 0, 'foo', 'baz');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (3, 0, 1, 'gux', 'qux');
cqlsh:foo> SELECT blog_id, content FROM blogs WHERE time1 = 1 AND author='foo';
cqlsh:foo>
{noformat}

The expected result is:
{noformat}

 blog_id | time1 | time2 | author | content
---------+-------+-------+--------+---------
       2 |     1 |     0 |    foo |     baz
{noformat}",slebresne,brandon.williams,Normal,Resolved,Fixed,11/Oct/12 21:37,16/Apr/19 09:32
Bug,CASSANDRA-4797,12611454,range queries return incorrect results,"I've only seen this fail once, but it's quite obviously returning incorrect results since the query is ""SELECT * FROM clicks WHERE userid >= 2 LIMIT 1"" and it's getting userid 0 in return.

{noformat}
======================================================================
FAIL: cql_tests.TestCQL.limit_ranges_test
Validate LIMIT option for 'range queries' in SELECT statements
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/var/lib/buildbot/cassandra-dtest/tools.py"", line 187, in wrapped
    f(obj)
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 302, in limit_ranges_test
    assert res == [[ 2, 'http://foo.com', 42 ]], res
AssertionError: [[0, u'http://foo.com', 42]]
{noformat}",,brandon.williams,Normal,Resolved,Fixed,11/Oct/12 22:55,16/Apr/19 09:32
Bug,CASSANDRA-4799,12611533,assertion failure in leveled compaction test,"It's somewhat rare, but I'm regularly seeing this failure on trunk:

{noformat}
    [junit] Testcase: testValidationMultipleSSTablePerLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testValidationMultipleSSTablePerLevel(LeveledCompactionStrategyTest.java:78)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED
{noformat}

I suspect there's a deeper problem, since this is a pretty fundamental assertion.",yukim,brandon.williams,Normal,Resolved,Fixed,12/Oct/12 14:47,16/Apr/19 09:32
Bug,CASSANDRA-4800,12611627,cqlsh help is obsolete for cql3,"For example, new syntax for CREATE KEYSPACE is

create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': 1}
",aleksey,jbellis,Low,Resolved,Fixed,12/Oct/12 20:24,16/Apr/19 09:32
Bug,CASSANDRA-4801,12611640,inet datatype does not work with cqlsh on windows,"{noformat}
create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
use foo;
create table one (id int primary key, c int);
TRACING ON;
insert into one (id, c) values (1, 2);

value '\x7f\x00\x00\x01' (in col 'source') can't be deserialized as inet: 'module' object has no attribute 'inet_ntop'
{noformat}",aleksey,jbellis,Low,Resolved,Fixed,12/Oct/12 21:37,16/Apr/19 09:32
Bug,CASSANDRA-4802,12611644,"Regular startup log has confusing ""Bootstrap/Replace/Move completed!"" without boostrap, replace, or move","A regular startup completes successfully, but it has a confusing message the end of the startup:

""  INFO 15:19:29,137 Bootstrap/Replace/Move completed! Now serving reads.""

This happens despite no bootstrap, replace, or move.

While purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?!  It should simply read something like ""Startup completed! Now serving reads"" unless it actually has done one of the actions in the error message.



Complete log at the end:


INFO 15:13:30,522 Log replay complete, 6274 replayed mutations
 INFO 15:13:30,527 Cassandra version: 1.0.12
 INFO 15:13:30,527 Thrift API version: 19.20.0
 INFO 15:13:30,527 Loading persisted ring state
 INFO 15:13:30,541 Starting up server gossip
 INFO 15:13:30,542 Enqueuing flush of Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,543 Writing Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,550 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-274-Data.db (80 bytes)
 INFO 15:13:30,563 Starting Messaging Service on port 7000
 INFO 15:13:30,571 Using saved token 31901471898837980949691369446728269823
 INFO 15:13:30,572 Enqueuing flush of Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,573 Writing Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,579 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-275-Data.db (163 bytes)
 INFO 15:13:30,581 Node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal
 INFO 15:13:30,598 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 15:13:30,600 Will not load MX4J, mx4j-tools.jar is not in the classpath
",vijay2win@yahoo.com,kmueller,Low,Resolved,Fixed,12/Oct/12 22:21,16/Apr/19 09:32
Bug,CASSANDRA-4803,12611668,CFRR wide row iterators improvements,"{code}
 public float getProgress()
    {
        // TODO this is totally broken for wide rows
        // the progress is likely to be reported slightly off the actual but close enough
        float progress = ((float) iter.rowsRead() / totalRowCount);
        return progress > 1.0F ? 1.0F : progress;
    }
{code}

The problem is iter.rowsRead() does not return the number of rows read from the wide row iterator, but returns number of *columns* (every row is counted multiple times). ",pkolaczk,pkolaczk,Normal,Resolved,Fixed,13/Oct/12 08:16,16/Apr/19 09:32
Bug,CASSANDRA-4804,12611757,Wrong assumption for KeyRange about range.end_token in get_range_slices().,"In get_range_slices() there is parameter KeyRange range.

There you can pass start_key - end_key, start_token - end_token, or start_key - end_token.

This is described in the documentation.

in thrift/ThriftValidation.java there is validation function validateKeyRange() (line:489) that validates correctly the KeyRange, including the case start_key - end_token.

However in thrift/CassandraServer.java in function get_range_slices() on line: 686 wrong assumption is made:

   if (range.start_key == null)

   {

      ... // populate tokens

   }

   else

   {

      bounds = new Bounds<RowPosition>(RowPosition.forKey(range.start_key, p), RowPosition.forKey(range.end_key, p));

   }

This means if there is start key, no end token is checked.
The opposite - null is ""inserted"" as end_key.

Solution:
same file - thrift/CassandraServer.java on next function - get_paged_slice(), on line:741 same code is written correctly

   if (range.start_key == null)

   {

      ... // populate tokens

   }

   else

   {

      RowPosition end = range.end_key == null ? p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)

                           : RowPosition.forKey(range.end_key, p);

      bounds = new Bounds<RowPosition>(RowPosition.forKey(range.start_key, p), end);

   }

",nmmm,nmmm,Low,Resolved,Fixed,14/Oct/12 15:45,16/Apr/19 09:32
Bug,CASSANDRA-4807,12611845,Compaction progress counts more than 100%,"'nodetool compactionstats' compaction progress counts more than 100%:

{code}
pending tasks: 74
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Validation        KSP        CF1           56192578305         84652768917    66.38%
               Compaction        KSP        CF2           162018591           119913592     135.11%
{code}

Hadn't experienced this before 1.1.3. Is it due to changes in 1.1.4-1.1.6 ?",yukim,omid,Low,Resolved,Fixed,15/Oct/12 15:10,16/Apr/19 09:32
Bug,CASSANDRA-4808,12611860,nodetool doesnt work well with negative tokens,"./apache-cassandra-1.2.0-beta1-SNAPSHOT/bin/nodetool move \-2253536297082652573
Unrecognized option: -2253536297082652573
usage: java org.apache.cassandra.tools.NodeCmd --host <arg> <command>
            
 -cf,--column-family <arg>   only take a snapshot of the specified column
                             family
",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,15/Oct/12 17:22,16/Apr/19 09:32
Bug,CASSANDRA-4810,12611905,unit test failing under long-test,"the following failure occurs when running ant long-test

junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionsTest
    [junit] Tests run: 5, Failures: 1, Errors: 0, Time elapsed: 31.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=2 rowsper=1 colsper=200000: 2173 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=2 rowsper=200000 colsper=1: 4531 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=100 rowsper=800 colsper=5: 1864 ms
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStandardColumnCompactions(org.apache.cassandra.db.compaction.LongCompactionsTest):	FAILED
    [junit] expected:<9> but was:<99>
    [junit] junit.framework.AssertionFailedError: expected:<9> but was:<99>
    [junit] 	at org.apache.cassandra.db.compaction.CompactionsTest.assertMaxTimestamp(CompactionsTest.java:207)
    [junit] 	at org.apache.cassandra.db.compaction.LongCompactionsTest.testStandardColumnCompactions(LongCompactionsTest.java:141)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongCompactionsTest FAILED
",yukim,bbucher,Low,Resolved,Fixed,15/Oct/12 20:59,16/Apr/19 09:32
Bug,CASSANDRA-4811,12611952,"Some cqlsh help topics don't work (select, create, insert and anything else that is a cql statement)","cqlsh> help select
Improper help command.

Same will happen if you look up a help topic for any other cql statement.
38748b43d8de17375c7cc16e7a4969ca4c1a2aa1 broke it (#4198) 5 months ago.
",aleksey,aleksey,Low,Resolved,Fixed,16/Oct/12 02:37,16/Apr/19 09:32
Bug,CASSANDRA-4813,12611994,Problem using BulkOutputFormat while streaming several SSTables simultaneously from a given node.,"The issue occurs when streaming simultaneously SSTables from the same node to a cassandra cluster using SSTableloader. It seems to me that Cassandra cannot handle receiving simultaneously SSTables from the same node. However, when it receives simultaneously SSTables from two different nodes, everything works fine. As a consequence, when using BulkOutputFormat to generate SSTables and stream them to a cassandra cluster, I cannot use more than one reducer per node otherwise I get a java.io.EOFException in the tasktracker's logs and a java.io.IOException: Broken pipe in the Cassandra logs.",yukim,ralph.romanos,Low,Resolved,Fixed,16/Oct/12 08:58,16/Apr/19 09:32
Bug,CASSANDRA-4816,12612041,Broken get_paged_slice,"get_paged_slice doesn't reset the start column filter for the second returned row sometimes. So instead of getting a slice:

row 0: <start_column>...<last_column_in_row>
row 1: <first column in a row>...<last_column_in_row>
row 2: <first column in a row>...

you sometimes get:

row 0: <start_column>...<last_column_in_row>
row 1: <start_column>...<last_column_in_row>
row 2: <first column in a row>...
",pkolaczk,pkolaczk,Normal,Resolved,Fixed,16/Oct/12 14:21,16/Apr/19 09:32
Bug,CASSANDRA-4817,12612088,Update sstable2json for 1.1.x,"This format is still needed in 1.1.5:

bin/json2sstable -K KS -c CF CF.json KS-CF-he-2-Data.db

to comply with this method:

https://github.com/apache/cassandra/blob/cassandra-1.1.5/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L160

even though 1.1.x now uses the directory structure KS/CF/CF-he-2-Data.db.",,j.casares,Low,Resolved,Fixed,16/Oct/12 19:17,16/Apr/19 09:32
Bug,CASSANDRA-4818,12612102,Update sstableloader for 1.1.x,"This was done on Cassandra 1.1.5:
{CODE}
$ ls -1 Keyspace1/Keyspace1/Standard1/
Standard1-51-Data.db
Standard1-51-Index.db
Standard1-hc-51-Data.db
Standard1-hc-51-Index.db
Standard1-tmp-hc-46-Data.db
Standard1-tmp-hc-46-Index.db
$ ~/repos/cassandra/bin/sstableloader -d localhost Keyspace1/Keyspace1/Standard1/
 WARN 15:41:56,023 Invalid file 'Standard1-51-Data.db' in data directory Keyspace1/Keyspace1/Standard1.
 WARN 15:41:56,024 Invalid file 'Standard1-51-Index.db' in data directory Keyspace1/Keyspace1/Standard1.
Skipping file Standard1-hc-51-Data.db: column family Keyspace1.hc doesn't exist
Skipping file Standard1-tmp-hc-46-Data.db: column family Keyspace1.tmp doesn't exist
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]
{CODE}",,j.casares,Normal,Resolved,Fixed,16/Oct/12 20:51,16/Apr/19 09:32
Bug,CASSANDRA-4820,12612139,Add a close() method to CRAR to prevent leaking file descriptors.,"The problem is that under heavy load Finalizer daemon is unable to keep up with number of ""source"" and ""channel"" fields from CRAR to finalize (as FileInputStream has custom finalize() which calls close inside) which creates memory/cpu pressure on the machine.",xedin,xedin,Normal,Resolved,Fixed,17/Oct/12 01:31,16/Apr/19 09:32
Bug,CASSANDRA-4822,12612166,CQL3: Allow renaming PK columns to ease upgrade from thrift,"Say you have a clicks CF in thrift storing for each user a timeline of which links it clicked on. It may have a definition like:
{noformat}
create column family clicks with key_validation_class = UUIDType and comparator = TimeUUIDType and default_validation_class = UTF8Type
{noformat}

In CQL3, you can access that thrift created CF as if it had been defined by:
{noformat}
CREATE TABLE clicks (
  key uuid,
  column timeuuid,
  value text,
  PRIMARY KEY (key, column)
) WITH COMPACT STORAGE
{noformat}
In other words, CQL3 will pick default names for the key_alias, column_aliases and value_alias metadata. It's ok but it would be more user friendly to use if the user could rename those to something better. Today, the only solution would be to remove the schema and re-create the table in CQL3. We can make that simpler by adding support for:
{noformat}
ALTER TABLE clicks RENAME key to user_id;
ALTER TABLE clicks RENAME column to insertion_time;
ALTER TABLE clicks RENAME value to url_clicked; 
{noformat}

Of course such rename statement won't be applicable to all columns. Namely, we can only allow renaming PK columns and in some compact storage cases the value. But that's probably still worth adding.",slebresne,slebresne,Low,Resolved,Fixed,17/Oct/12 07:56,16/Apr/19 09:32
Bug,CASSANDRA-4823,12612167,Fix cqlsh after move of CL to the protocol level,"CASSANDRA-4734 moved the consistency level at the protocol level (and in doing so, separated the cql3 thrift methods from the cql2 ones). We should adapt cqlsh to that change.",aleksey,slebresne,Normal,Resolved,Fixed,17/Oct/12 07:59,16/Apr/19 09:32
Bug,CASSANDRA-4826,12612301,Subcolumn slice ends not respected,"When performing {{get_slice()}} on a super column family with the {{supercolumn}} argument set as well as a slice range (meaning you're trying to fetch a slice of subcolumn from a particular supercolumn), the slice ends don't seem to be respected.",vijay2win@yahoo.com,thobbs,Normal,Resolved,Fixed,17/Oct/12 20:17,16/Apr/19 09:32
Bug,CASSANDRA-4827,12612325,cqlsh --cql3 unable to describe CF created with cli,"created CF with cli:

{noformat}
create column family playlists
with key_validation_class = UUIDType
 and comparator = 'CompositeType(UTF8Type, UTF8Type, UTF8Type)'
 and default_validation_class = UUIDType;
{noformat}

Then get this error with cqlsh:

{noformat}
cqlsh:music> describe table playlists;

/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:771: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected composite key CF to have column aliases, but found none
/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:794: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected [u'KEY'] length to be 3, but it's 1. comparator='org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)'
CREATE TABLE playlists (
  ""KEY"" uuid PRIMARY KEY
)
{noformat}",aleksey,jbellis,Low,Resolved,Fixed,17/Oct/12 21:55,16/Apr/19 09:32
Bug,CASSANDRA-4828,12612327,default cache provider does not match default yaml,"The yaml indicates SerializingCacheProvider is the default, however the default when not present in the yaml is actually ConcurrentLinkedHashCacheProvider.",brandon.williams,brandon.williams,Low,Resolved,Fixed,17/Oct/12 22:18,16/Apr/19 09:32
Bug,CASSANDRA-4830,12612363,JdbcDate.compose is not null safe,"I am using the cassandra-jdbc for CQL.  I have a table with timestamp column.  When timestamp column is null it throws, IndexOutOfBoundsException exception since JdbcDate.compose calls the new Date(ByteBufferUtil.toLong(value)).  The ByteBufferUtil.toLong(bytes) throws exception the exception since position and limit pointers are same (similar to null).  This has to be handled gracefully in the JdbcDate.compose method instead of throwing exception.  I would like to see implementation something like,

    public Date compose(ByteBuffer bytes)
    {
        if(bytes.limit() - bytes.position() > 0) 
        {
            return new Date(ByteBufferUtil.toLong(bytes));
        } 
      
        return  null;
    }

BTW, this matches exactly reverse with decompose method.  Logically it supposed to be implemented in the first place ;)

",skuppa,skuppa,Low,Resolved,Fixed,18/Oct/12 05:03,16/Apr/19 09:32
Bug,CASSANDRA-4832,12612366,AssertionError: keys must not be empty,"I'm getting errors like this logged:

 INFO 07:08:32,104 Compacting [SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-114-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-113-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-110-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-108-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-106-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-107-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-112-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-109-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-111-Data.db')]
ERROR 07:08:32,108 Exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.AssertionError: Keys must not be empty
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:154)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:154)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

I'm not really sure when this started happening; they tend to be logged during a repair but I can't reproduce the error 100% reliably.",mithrandi,mithrandi,Low,Resolved,Fixed,18/Oct/12 05:15,16/Apr/19 09:32
Bug,CASSANDRA-4833,12612449,get_count with 'count' param between 1024 and ~actual column count fails,"If you run get_count() with the 'count' param of the SliceRange set to a number between 1024 and (approximately) the actual number of columns in the row, something seems to silently fail internally, resulting in a client side timeout.  Using a 'count' param outside of this range (lower or much higher) works just fine.

This seems to affect all of 1.1 and 1.2.0-beta1, but not 1.0.",yukim,thobbs,Normal,Resolved,Fixed,18/Oct/12 16:56,16/Apr/19 09:32
Bug,CASSANDRA-4834,12612496,Old-style mapred interface only populates row key for first column when using wide rows,"When using the ColumnFamilyRecordReader with the old-style Hadoop interface to iterate over wide row columns, the row key is only populated on the first column.
See attached tests.

",bkempe,bkempe,Low,Resolved,Fixed,18/Oct/12 21:37,16/Apr/19 09:32
Bug,CASSANDRA-4835,12612680,Appending/Prepending items to list using BATCH,"As I know, there is no any guarantee that commands that are inside BATCH block will execute in same order, as they are stored in the BATCH block. But...

I have made two tests:
First appends some items to the empty list, and the second one, prepends items, also to the empty list. Both of them are using UPDATE commands stored in the BATCH block. 

Results of those tests are as follow:
First:
      When appending new items to list, USING commands are executed in the same order as they are stored i BATCH.

Second:
      When prepending new items to list, USING commands are executed in random order.  

So, in other words below code:
{code:xml}
BEGIN BATCH
 UPDATE... list_name = list_name + [ '1' ]  
 UPDATE... list_name = list_name + [ '2' ]
 UPDATE... list_name = list_name + [ '3' ] 
APPLY BATCH;{code}

 always results in [ '1', '2', '3' ],
 but this code:
{code:xml}
BEGIN BATCH
 UPDATE... list_name = [ '1' ] + list_name   
 UPDATE... list_name = [ '2' ] + list_name
 UPDATE... list_name = [ '3' ] + list_name
APPLY BATCH;{code}

results in randomly ordered list, like [ '2', '1', '3' ]    (expected result is [ '3', '2', '1' ])

So somehow, when appending items to list, commands from BATCH are executed in order as they are stored, but when prepending, the order is random.",slebresne,krzysztof cieslinski,Low,Resolved,Fixed,19/Oct/12 14:03,16/Apr/19 09:32
Bug,CASSANDRA-4840,12612800,remnants of removed nodes remain after removal,"After nodes are removed from the ring and no longer appear in any of the nodes' nodetool ring output, some of the dead nodes show up in the o.a.c.net.FailureDetector SimpleStates metadata.  Also, some of the JMX stats are updating for the removed nodes (ie RecentTimeoutsPerHost and ResponsePendingTasks).",brandon.williams,jeromatron,Low,Resolved,Fixed,19/Oct/12 22:28,16/Apr/19 09:32
Bug,CASSANDRA-4842,12612807,DateType in Column MetaData causes server crash,"when creating a column family with column metadata containing a date, there is a server crash that will prevent startup.

To recreate from the cli:
{code}
create keyspace test;
use test;
create column family foo
  with column_type = 'Standard'
  and comparator = 'CompositeType(LongType,DateType)'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and column_metadata = [ 
    { column_name : '1234:1350695443433', validation_class : BooleanType} 
  ];
{code}

Produces this error in the logs:

{code}
ERROR 21:11:18,795 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:194)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:141)
	at org.apache.cassandra.thrift.CassandraServer.system_add_column_family(CassandraServer.java:931)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3410)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3398)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 11 more
Caused by: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:117)
	at org.apache.cassandra.db.marshal.DateType.fromString(DateType.java:85)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.fromString(AbstractCompositeType.java:213)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:257)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1318)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1250)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.text.ParseException: Unable to parse the date: 2012-10-19 21
	at org.apache.commons.lang.time.DateUtils.parseDate(DateUtils.java:285)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:113)
	... 14 more
ERROR 21:11:18,795 Exception in thread Thread[MigrationStage:1,5,main]
org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:117)
	at org.apache.cassandra.db.marshal.DateType.fromString(DateType.java:85)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.fromString(AbstractCompositeType.java:213)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:257)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1318)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1250)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.text.ParseException: Unable to parse the date: 2012-10-19 21
	at org.apache.commons.lang.time.DateUtils.parseDate(DateUtils.java:285)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:113)
	... 14 more
{code}

This error is repeated when attempting to restart the server, and results in the server failing to start.",slebresne,devdazed,Low,Resolved,Fixed,20/Oct/12 01:20,16/Apr/19 09:32
Bug,CASSANDRA-4843,12612879,When upgrading from 1.1.6 to 1.20 change in partitioner causes nodes not to start,"ERROR 10:17:20,341 Cannot open /home/edward/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-hf-1 because partitioner does not match org.apache.cassandra.dht.RandomPartitioner != org.apache.cassandra.dht.Murmur3Partitioner

This is because 1.2 has a new default partitioner, why are we changing the default? Is this wise? The current partitioner has been rock solid for years. 

Should the previously known partition be stored in the schema like the previously know seed nodes, and schema?",jbellis,appodictic,Normal,Resolved,Fixed,21/Oct/12 14:15,16/Apr/19 09:32
Bug,CASSANDRA-4846,12612999,BulkLoader throws NPE at start up,"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml.",jbellis,yukim,Normal,Resolved,Fixed,22/Oct/12 17:10,16/Apr/19 09:32
Bug,CASSANDRA-4847,12613006,Bad disk causes death of node despite disk_failure_policy,"Steps:

# Create a bad disk via device mapper
# Specify good disk and bad disk is data directory
# Set {{disk_failure_policy}} to {{best_effort}} in cassandra.yaml
# Start node

Expected:

Attempts to create system directories to fail (as expected) on bad disk, and have it added to blacklisted directories.

Actual:

Node start up aborts due to uncaught error:

{noformat}
FSWriteError in /mnt/bad_disk/system_traces/sessions
        at org.apache.cassandra.io.util.FileUtils.createDirectory(FileUtils.java:258)
        at org.apache.cassandra.db.Directories.<init>(Directories.java:104)
        at org.apache.cassandra.db.Directories.create(Directories.java:90)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:404)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:227)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)
Caused by: java.io.IOException: Failed to mkdirs /mnt/bad_disk/system_traces/sessions
        ... 7 more
{noformat}
",kirktrue,kirktrue,Normal,Resolved,Fixed,22/Oct/12 18:11,16/Apr/19 09:32
Bug,CASSANDRA-4849,12613020,typo in tuning cassandra doc,"http://www.datastax.com/docs/1.1/operations/tuning#tuning-bloomfilters has a typo

ALTER TABLE addamsFamily WITH bloomfilter_fp_chance = 0.01; should be ALTER TABLE addamsFamily WITH bloom_filter_fp_chance = 0.01;",,mkjellman,Low,Resolved,Fixed,22/Oct/12 18:56,16/Apr/19 09:32
Bug,CASSANDRA-4850,12613134,RuntimeException when bootstrapping a node without an explicitely set token,"Trying to boostrap a node for which no initial token has been set result in:
{noformat}
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:603)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:490)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:386)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:305)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)
{noformat}

This has been broken by CASSANDRA-4416. More specifically, now that we storage the system metadata in the schema on startup, the check
{noformat}
                // if we see schema, we can proceed to the next check directly
                if (!Schema.instance.getVersion().equals(Schema.emptyVersion))
                {
                    logger.debug(""got schema: {}"", Schema.instance.getVersion());
                    break;
                }
{noformat}
in StorageService.joinTokenRing is broken. This result in the node trying to check the Load map to pick a token before any gossip state has been received.

Note sure what is the best fix (an easy would be to always wait RING_DELAY before attempting to pick the token, at least in the case where an initial token isn't set, but that's a big hammer).",xedin,slebresne,Normal,Resolved,Fixed,23/Oct/12 13:21,16/Apr/19 09:32
Bug,CASSANDRA-4855,12613352,Debian packaging doesn't do auto-reloading of log4j properties file,Cassandra isn't starting the log4j auto-reload thread because it requires -Dlog4j.defaultInitOverride=true on initialization. Is there a reason to not do this when installed from the Debian package?,brandon.williams,rbranson,Low,Resolved,Fixed,24/Oct/12 19:16,16/Apr/19 09:32
Bug,CASSANDRA-4860,12613379,Estimated Row Cache Entry size incorrect (always 24?),"After running for several hours the RowCacheSize was suspicious low (ie 70 something MB)  I used  CASSANDRA-4859 to measure the size and number of entries on a node:

In [3]: 1560504./65021
Out[3]: 24.0

In [4]: 2149464./89561
Out[4]: 24.0

In [6]: 7216096./300785
Out[6]: 23.990877204647838


That's RowCacheSize/RowCacheNumEntires  .  Just to prove I don't have crazy small rows the mean size of the row *keys* in the saved cache is 67 and Compacted row mean size: 355.  No jamm errors in the log

Config notes:
row_cache_provider: ConcurrentLinkedHashCacheProvider
row_cache_size_in_mb: 2048

Version info:
 * C*: 1.1.6
 * centos 2.6.32-220.13.1.el6.x86_64
 * java 6u31 Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)",vijay2win@yahoo.com,cburroughs,Normal,Resolved,Fixed,24/Oct/12 22:11,16/Apr/19 09:32
Bug,CASSANDRA-4863,12613503,cqlsh can't describe system tables,"{noformat}
cqlsh> describe table system_traces.sessions;

Unconfigured column family 'sessions'
{noformat}",aleksey,jbellis,Normal,Resolved,Fixed,25/Oct/12 15:12,16/Apr/19 09:32
Bug,CASSANDRA-4864,12613595,CQL2 CREATE COLUMNFAMILY checks wrong permission,"Permission is asked for resource [cassandra, keyspaces, <column family name>] instead of [cassandra, keyspaces, <keyspace name>]",aleksey,aleksey,Normal,Resolved,Fixed,26/Oct/12 00:39,16/Apr/19 09:32
Bug,CASSANDRA-4877,12614009,Range queries return fewer result after a lot of delete,"Hi, I'm testing on the trunk version
I'm using : [cqlsh 2.3.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]

My use case is :
I create a table
CREATE TABLE premier (
id int PRIMARY KEY,
value int
) WITH
comment='' AND
caching='KEYS_ONLY' AND
read_repair_chance=0.100000 AND
dclocal_read_repair_chance=0.000000 AND
gc_grace_seconds=864000 AND
replicate_on_write='true' AND
compression={'sstable_compression': 'SnappyCompressor'};

1) I insert 10 000 000 rows (they are like id = 1 and value =1)
2) I delete 2 000 000 rows (i use random method to choose the key value)
3) I do select * from premier ; and my result is 7944 instead of 10 000.
4) if if do select * from premier limit 20000 ; my result is 15839 .

So after a lot of delete, the range operator is not working.",slebresne,julien_campan,Normal,Resolved,Fixed,30/Oct/12 09:55,16/Apr/19 09:32
Bug,CASSANDRA-4878,12614024,No entry in TypesMap for InetAddressType,{{InetAddressType}} was added to {{o.a.c.db.marshal}} and {{JdbcInetAddress}} was added to {{o.a.c.cql.jdbc}} but no bridging entry was made for it in the {{o.a.c.cql.jdbc.TypesMap}} Class.,ardot,ardot,Low,Resolved,Fixed,30/Oct/12 14:03,16/Apr/19 09:32
Bug,CASSANDRA-4879,12614076,CQL help in trunk/doc/cql3/CQL.textile outdated,"https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile doesn't include the new create keyspace syntax or the collections. Last time, I updated the CQL.textile for Paul Cannon to review. Want me to do it again? 

BNR-like formatting needs to be replaced, right?, because the brackets now have literal meaning. I test-applied this custom formatting to commands and it seems ok: Uppercase means literal (lowercase nonliteral), italics mean optional, the | symbol means OR, ... means repeatable. The ... in italics doesn't strictly explain things like nested [...] does, but it's easier on the eyes and loosely understandable. Any doubt could be erased by examples, I think. ",urandom,khahn,Normal,Resolved,Fixed,30/Oct/12 20:07,16/Apr/19 09:32
Bug,CASSANDRA-4880,12614077,Endless loop flushing+compacting system/schema_keyspaces and system/schema_columnfamilies,"After upgrading a node from 1.1.2 to 1.1.6, the startup sequence entered a loop as seen here:

http://mina.naguib.ca/misc/cassandra_116_startup_loop.txt

Stopping and starting the node entered the same loop.

Reverting back to 1.1.2 started successfully.",xedin,minaguib,Normal,Resolved,Fixed,30/Oct/12 20:18,16/Apr/19 09:32
Bug,CASSANDRA-4881,12614094,Force provided columns in clustering key order in 'CLUSTERING ORDER BY',"Using this table:
CREATE TABLE video_event (
  videoid_username varchar,
  event varchar,
  event_timestamp timestamp,
  video_timestamp timestamp,
  PRIMARY KEY (videoid_username, event, event_timestamp)
)WITH CLUSTERING ORDER BY (event_timestamp DESC);

Inserting these records:

INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:05:00','2012-09-02 18:05:00');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:05:30','2012-09-02 18:05:30');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:35:00','2012-09-02 18:35:00');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:37:30','2012-09-02 18:37:30');

Running this select:

select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1;

I get this:

 videoid_username                           | event | event_timestamp          | video_timestamp
--------------------------------------------+-------+--------------------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start | 2012-09-02 18:05:00+0000 | 2012-09-02 18:05:00+0000

I would expect to see this:

 videoid_username                           | event | event_timestamp          | video_timestamp
--------------------------------------------+-------+--------------------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop | 2012-09-02 18:37:30+0000 | 2012-09-02 18:37:30+0000

where the first record pulled was the sorted record by event_timestamp in reverse order.
",slebresne,pmcfadin,Normal,Resolved,Fixed,30/Oct/12 22:18,16/Apr/19 09:32
Bug,CASSANDRA-4882,12614100,Short read protection don't count columns correctly for CQL3,"Using the same schema defined in https://issues.apache.org/jira/browse/CASSANDRA-4881

select * from video_event;
 videoid_username                           | event | event_timestamp | video_timestamp
--------------------------------------------+-------+-----------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346636100.0 | 2012-09-02 18:35:00+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346634300.0 | 2012-09-02 18:05:00+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop |    1346636250.0 | 2012-09-02 18:37:30+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop |    1346634330.0 | 2012-09-02 18:05:30+0000

And this:

select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1;
 videoid_username                           | event | event_timestamp | video_timestamp
--------------------------------------------+-------+-----------------+-----------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346636100.0 |            null

As you can see, we have some different output based on the select statement. The timestamp in both is formatted as a float or a double. ",slebresne,pmcfadin,Normal,Resolved,Fixed,30/Oct/12 23:08,16/Apr/19 09:32
Bug,CASSANDRA-4884,12614187,Composites index bug,Indexes on composite tables is currently broken (due to CASSANDRA-2897). Attaching patch to fix.,slebresne,slebresne,Low,Resolved,Fixed,31/Oct/12 14:50,16/Apr/19 09:32
Bug,CASSANDRA-4890,12614336,Don't allow prepared marker inside collections,"Currently the parser don't disallow preparing queries like (where l is a list<string>):
{noformat}
INSERT INTO test (k, l) VALUES (0, [1, ?, 2])
{noformat}

However, we don't handler it correctly. And in fact we can't really handle it properly currently since we return the name of the prepared column during prepare and here the marker don't correspond to a column (concretely, the code currently return l and list<string> for the name and type of the prepared value, which is incorrect). We also don't handle it during execute, though that last could in theory be fixed with some effort.

But overall I don't think allowing that kind of things is really useful (you can of course prepare the whole collection), so I suggest just refusing it for now.",slebresne,slebresne,Low,Resolved,Fixed,01/Nov/12 10:39,16/Apr/19 09:32
Bug,CASSANDRA-4892,12614400,no need to keep tombstones in HintsColumnFamily,"Once a hint is delivered, it is removed from the HintsColumnFamily.  Because it is local and would only be deleted after expiration or after a correct delivery, there is no need to keep any tombstones (i.e. gc_grace_seconds should be zero)",jbellis,mdennis,Low,Resolved,Fixed,01/Nov/12 17:13,16/Apr/19 09:32
Bug,CASSANDRA-4893,12614410,Don't throw internal exceptions over JMX,"Similarly to how we don't return internal objects over JMX we shouldn't throw internal exceptions over jmx as well.

The one example I encountered was throwing ConfigurationException for the move() command. We should check the rest of our jmx as well.",yukim,nickmbailey,Normal,Resolved,Fixed,01/Nov/12 18:33,16/Apr/19 09:32
Bug,CASSANDRA-4909,12614717,Bug when composite index is created in a table having collections,"CASSANDRA-4511 is open to add proper indexing of collection, but currently indexing doesn't work correctly if we index a value in a table having collection, even if that value is not a collection itself.

We also don't refuse creating index on collections, even though we don't support it. Attaching patch to fix both.",slebresne,slebresne,Normal,Resolved,Fixed,04/Nov/12 17:44,16/Apr/19 09:32
Bug,CASSANDRA-4910,12614758,CQL3 doesn't allow static CF definition with compact storage in C* 1.1,"In Cassandra 1.1, the following CQL3 definition:
{noformat}
CREATE TABLE user_profiles (
    user_id text PRIMARY KEY,
    first_name text,
    last_name text,
    year_of_birth int
) WITH COMPACT STORAGE;
{noformat}
yields:
{noformat}
Bad Request: COMPACT STORAGE requires at least one column part of the clustering key, none found
{noformat}

This works fine in 1.2 however.",slebresne,slebresne,Normal,Resolved,Fixed,05/Nov/12 08:31,16/Apr/19 09:32
Bug,CASSANDRA-4913,12614835,DESC KEYSPACE <ks> from cqlsh won't show cql3 cfs,I'm assuming because we made 'describe_keyspaces' from thrift not return cql3 cfs.,aleksey,nickmbailey,Normal,Resolved,Fixed,05/Nov/12 19:05,16/Apr/19 09:32
Bug,CASSANDRA-4916,12614862,Starting Cassandra throws EOF while reading saved cache,"Currently seeing nodes throw an EOF while reading a saved cache on the system schema when starting cassandra

 WARN 14:25:54,896 error reading saved cache /ssd/saved_caches/system-schema_columns-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:278)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:365)
	at org.apache.cassandra.db.Table.initCf(Table.java:334)
	at org.apache.cassandra.db.Table.<init>(Table.java:272)
	at org.apache.cassandra.db.Table.open(Table.java:102)
	at org.apache.cassandra.db.Table.open(Table.java:80)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:320)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:395)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:438)


to reproduce delete all data files, start a cluster, leave cluster up long enough to build a cache. nodetool drain, kill cassandra process. start cassandra process in foreground and note EOF thrown (see above for stack trace)",dbrosius,mkjellman,Low,Resolved,Fixed,05/Nov/12 22:28,16/Apr/19 09:32
Bug,CASSANDRA-4918,12614919,Remove CQL3 arbitrary select limit,"Let it be clear however that until CASSANDRA-4415 is resolved, it will put us in a situation where it will be easy to write queries that timeout (and potentially OOM the server). That being said, even with the auto-limit it's not too hard to write queries that timeout if you're not at least a bit careful and so far we've always answer that by saying 'you have to be mindful of how much data your query is asking for'. And while I'm all for adding protection against OOMing the server like suggested by Jonathan on CASSANDRA-4304, I think the arbitrary auto-limit is the worst possible solution to this problem.

Note that until CASSANDRA-4415 is resolved I wouldn't be totally opposed to force people to provide a LIMIT to select queries if we're really thing it will avoids lots of surprise, though tbh I do think it would be enough to just continue to be vocal about the fact that 'you have to be mindful of how much data your query is asking for' and its follow-up 'you should use an explicit LIMIT if in doubt about how much data will be returned'.

But I am *strongly opposed* in keeping the current arbitrary limit because it makes very little sense imo, and the little sense it makes will completely vanish once CASSANDRA-4415 is here, and I don't want to break the API and do a CQL4 to be able to remove that limit later.
",slebresne,slebresne,Normal,Resolved,Fixed,06/Nov/12 08:39,16/Apr/19 09:32
Bug,CASSANDRA-4919,12614979,StorageProxy.getRangeSlice sometimes returns incorrect number of columns,"When deployed on a single node, number of columns is correct.
When deployed on a cluster, total number of returned columns is slightly lower than desired. ",pkolaczk,pkolaczk,Normal,Resolved,Fixed,06/Nov/12 15:55,16/Apr/19 09:32
Bug,CASSANDRA-4925,12615031,Failure Detector should log or ignore sudden time change to the past,"If a machine goes back in time all of a sudden because of a problem, Gossip will insert a negative interArrivalTime. 
This will decrease the mean value and can cause this machine to mark other nodes as down and then mark them up as time passed. 
But we should log such occurrences.",kohlisankalp,kohlisankalp,Low,Resolved,Fixed,06/Nov/12 22:17,16/Apr/19 09:32
Bug,CASSANDRA-4928,12615193,CQL3 SelectStatement should not share slice query filters amongst ReadCommand,"In 1.2, SliceQueryFilter is stateful and should thus not be shared but SelectStatement is doing some sharing in the case of an IN query. This is the reason why cql_test:TestCQL.limit_multiget_test in the dtests is failing intermittently.

Let's be clear that the fact that SliceQueryFilter is stateful is ugly, but that is a concession made for performance until we can refactor all this more cleanly (which still being efficient). Such refactor being, as far as I can tell, far from trivial.",slebresne,slebresne,Normal,Resolved,Fixed,07/Nov/12 17:11,16/Apr/19 09:32
Bug,CASSANDRA-4929,12615198,Deal with broken ALTER DROP support in CQL3,"Currently, {{ALTER DROP}} only remove the metadata for the column, making it unavailable, but don't reclaim the data. This is unintuive and CASSANDRA-3919 is opened to fix it. However, that later issue won't make it for 1.2, and I think we should be very careful into shipping 1.2 with the current behavior because 1) it's unintuitive and 2) as unintuitive as it is, we don't want people to start relying on that behavior. So I thing we should do one of:
* remove support for {{ALTER DROP}} until CASSANDRA-3919 reintroduce it. After all, there is no real performance impact in keeping a colum that you don't use and if you really really want to get rid of the metadata, you still have the workaround of trashing the schema and recreating it without that column (obviously not user friendly, but at least it's vaguely possible).
* add a specific syntax for the current behavior of {{ALTER DROP}}, one that clearly imply that the data is not deleted, if we consider that this behavior can be sometimes useful (that is, even after CASSANDRA-3919 is resolved). One such syntax could one of (not sure which one I prefer):
{noformat}
ALTER TABLE foo DROP my_column SCHEMA ONLY
ALTER TABLE foo DROP my_column KEEP DATA
{noformat}

I have a slight preference for solution 2, but honestly because it will it easier to drop a column you've just added but maybe mispelled the name until CASSANDRA-3919. Once CASSANDRA-3919 is in, I'm not sure this will be so useful anymore.
",slebresne,slebresne,Low,Resolved,Fixed,07/Nov/12 17:29,16/Apr/19 09:32
Bug,CASSANDRA-4930,12615269,typo in ConfigHelper.java,"line 136 describing setOutputColumnFamily() is currently ""Set the column family for the input of this job.""

It should read ""Set the column family for the output of this job.""

similar typo on line 148.",,mkjellman,Low,Resolved,Fixed,08/Nov/12 02:00,16/Apr/19 09:32
Bug,CASSANDRA-4934,12615341,assertion error in offheap bloom filter,"Saw this while running the dtests:

{noformat}
 INFO [CompactionExecutor:2] 2012-11-08 09:35:18,206 CompactionTask.java (line 116) Compacting [SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]
ERROR [CompactionExecutor:2] 2012-11-08 09:35:18,257 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError: Memory was freed
    at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:134)
    at org.apache.cassandra.io.util.Memory.getByte(Memory.java:104)
    at org.apache.cassandra.utils.obs.OffHeapBitSet.get(OffHeapBitSet.java:60)
    at org.apache.cassandra.utils.BloomFilter.isPresent(BloomFilter.java:71)
    at org.apache.cassandra.db.compaction.CompactionController.shouldPurge(CompactionController.java:106)
    at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:64)
    at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
    at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:72)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at com.google.common.collect.Iterators$8.computeNext(Iterators.java:734)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:146)
    at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:69)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:179)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",jbellis,brandon.williams,Normal,Resolved,Fixed,08/Nov/12 15:57,16/Apr/19 09:32
Bug,CASSANDRA-4935,12615348,occasional TableTest failure,"The TableTest unit test fails somewhat rarely:

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.TableTest
    [junit] Tests run: 11, Failures: 1, Errors: 0, Time elapsed: 4.339 sec
    [junit] 
    [junit] Testcase: testGetSliceWithExpiration(org.apache.cassandra.db.TableTest):	FAILED
    [junit] Columns [636f6c31:false:4@1,636f6c32:true:4@1!1,636f6c33:false:4@1,])] is not expected [col1,col2]
    [junit] junit.framework.AssertionFailedError: Columns [636f6c31:false:4@1,636f6c32:true:4@1!1,636f6c33:false:4@1,])] is not expected [col1,col2]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:574)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:541)
    [junit] 	at org.apache.cassandra.db.TableTest$5.runMayThrow(TableTest.java:353)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.TableTest.reTest(TableTest.java:56)
    [junit] 	at org.apache.cassandra.db.TableTest.testGetSliceWithExpiration(TableTest.java:362)
{noformat}",yukim,brandon.williams,Low,Resolved,Fixed,08/Nov/12 16:33,16/Apr/19 09:32
Bug,CASSANDRA-4936,12615408,Less than operator when comparing timeuuids behaves as less than equal.,"If we define the following column family using CQL3:

CREATE TABLE useractivity (
  user_id int,
  activity_id 'TimeUUIDType',
  data text,
  PRIMARY KEY (user_id, activity_id)
);

Add some values to it.

And then query it like:

SELECT * FROM useractivity WHERE user_id = '3' AND activity_id < '2012-11-07 18:18:22-0800' ORDER BY activity_id DESC LIMIT 1;

the record with timeuuid '2012-11-07 18:18:22-0800' returns in the results.

According to the documentation, on CQL3 the '<' and '>' operators are strict, meaning not inclusive, so this seems to be a bug.",slebresne,cesar.nataren,Normal,Resolved,Fixed,09/Nov/12 01:13,16/Apr/19 09:32
Bug,CASSANDRA-4939,12615525,Add debug logging to list filenames processed by o.a.c.db.Directories.migrateFile method,"Customer is getting the following error when starting Cassandra:

ERROR 20:20:06,635 Exception encountered during startup
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(Unknown Source)
        at org.apache.cassandra.db.Directories.migrateFile(Directories.java:556)
        at org.apache.cassandra.db.Directories.migrateSSTables(Directories.java:493)

It looks like this is caused by an file with an unexpected name in one of his keyspace directories. However, because we don't log the name of the file as it is processed, it is hard to tell which file is causing it to choke.

It would be good to add a logger.debug statement at the beginning of the method to list the file currently being processed.",dbrosius,jblangston@datastax.com,Low,Resolved,Fixed,09/Nov/12 17:52,16/Apr/19 09:32
Bug,CASSANDRA-4940,12615547,Truncate doesn't clear row cache,"Truncate doesn't clear the row cache.  select * from <table> which skips the row cache returns no data, but selecting by key does.

cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

cqlsh:temp> truncate temp2;
cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

cqlsh:temp> select * from temp2;
cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

",jbellis,jjordan,Low,Resolved,Fixed,09/Nov/12 20:25,16/Apr/19 09:32
Bug,CASSANDRA-4945,12615647,"CQL3 does handle List append or prepend with a ""Prepared"" list","I can successfully update a List using the ""literal"" syntax:

{code}
UPDATE testcollection SET L = [98,99,100] + L WHERE k = 1;
{code}

And I can successfully ""upsert"" a List using the ""Prepared"" syntax:


{code}
UPDATE testcollection SET L = ? WHERE k = 1
{code}

by providing a decoded List<Integer> in the bind values.

But using the ""prepared"" syntax for an prepend like:
{code}
UPDATE testcollection SET L = ? + L WHERE k = 1
{code}
fails with the following message:
{code}
java.sql.SQLSyntaxErrorException: InvalidRequestException(why:line 1:33 mismatched input '+' expecting K_WHERE)
	at org.apache.cassandra.cql.jdbc.CassandraPreparedStatement.<init>(CassandraPreparedStatement.java:92)
...
...
{code}

and an append of a ""prepared"" syntax like:
{code}
UPDATE testcollection SET L = L + ? WHERE k = 1
{code}
fails as follows:
{code}
java.sql.SQLSyntaxErrorException: InvalidRequestException(why:invalid operation for non commutative columnfamily testcollection)
	at org.apache.cassandra.cql.jdbc.CassandraPreparedStatement.<init>(CassandraPreparedStatement.java:92)
...
...
{code}




",slebresne,ardot,Low,Resolved,Fixed,11/Nov/12 16:21,16/Apr/19 09:32
Bug,CASSANDRA-4955,12615924,some confusion around KEY pseudocolumn from Thrift tables,"Inserting into the schema created by cassandra-stress.  cqlsh {{DESCRIBE TABLE}} says

{code}
CREATE TABLE ""Standard1"" (
  ""KEY"" blob PRIMARY KEY,
  ""C0"" blob,
  ""C1"" blob,
  ""C2"" blob,
  ""C3"" blob,
  ""C4"" blob
) WITH COMPACT STORAGE AND
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true';
{code}

but that casing doesn't actually work:

{noformat}
cqlsh:Keyspace1> insert into ""Standard1"" (""KEY"", ""C0"", ""C1"", ""C2"", ""C3"", ""C4"") values ('FF', '00', '11', '22', '33', '44');
Bad Request: Unknown identifier KEY
{noformat}

lowercase does work:

{noformat}
cqlsh:Keyspace1> insert into ""Standard1"" (""key"", ""C0"", ""C1"", ""C2"", ""C3"", ""C4"") values ('FF', '00', '11', '22', '33', '44');
{noformat}
",aleksey,jbellis,Low,Resolved,Fixed,13/Nov/12 17:50,16/Apr/19 09:32
Bug,CASSANDRA-4956,12615929,exclude system_traces from repair,"When a repair is issued, the system ks is skipped but not system_traces.",yukim,brandon.williams,Low,Resolved,Fixed,13/Nov/12 18:04,16/Apr/19 09:32
Bug,CASSANDRA-4958,12616012,Snappy 1.0.4 doesn't work on OSX / Java 7,"Fixed in 1.0.5-M3 see :

https://github.com/xerial/snappy-java/issues/6

",yukim,coltnz,Low,Resolved,Fixed,14/Nov/12 03:10,16/Apr/19 09:32
Bug,CASSANDRA-4965,12616167,"in cqlsh, alter table with compaction_strategy_class does nothing","The following cqlsh code appears to do nothing.
{code}
alter table blah with compaction_strategy_class = ‘LeveledCompactionStrategy’;
{code}

It completes as though it worked but when you describe the table, it's still STCS.",aleksey,jeromatron,Low,Resolved,Fixed,15/Nov/12 00:18,16/Apr/19 09:32
Bug,CASSANDRA-4968,12616472,CQL3 Descrepancies,"I upgraded an environment from 1.1.6 to 1.2 beta

We found the following differences between cql3 versions:


The CLUSTERING ORDER BY requires all columns be specified in 1.2 whereas 1.1 defaulted to ASC

{code}
CREATE TABLE table1(
   col1 text,
   classtype ascii,
   kt bigint,
   id uuid,
   PRIMARY KEY (col1,classtype,kt)
) WITH COMPACT STORAGE
AND CLUSTERING ORDER BY(classtype ASC, kt DESC)
{code}

1.1 didn't require classtype in the ORDER BY, 1.2 does.


----------

In docs/cql3.textile there is mention of 'compaction_strategy' CREATE option but the actual option is 'compaction'

----------

cqlsh for some reason renders all bytes fields as strings in 1.2 whereas in 1.1 they were hex.






",,tjake,Low,Resolved,Fixed,16/Nov/12 21:19,16/Apr/19 09:32
Bug,CASSANDRA-4969,12616491,Set max frame size in CLI to avoid OOM when SSL is enabled,"If SSL is enabled on Cassandra but not on the cli, the cli will OOM when connecting to Cassandra because it thinks it's getting a message with a frame size of ~350mb.",thobbs,thobbs,Low,Resolved,Fixed,16/Nov/12 22:12,16/Apr/19 09:32
Bug,CASSANDRA-4970,12616535,cqlsh renders bytes fields as strings,Jake reports (CASSANDRA-4968) that this is a regression against 1.1.,aleksey,jbellis,Low,Resolved,Fixed,17/Nov/12 17:17,16/Apr/19 09:32
Bug,CASSANDRA-4975,12616796,AE in Bounds when running word count,"Just run the word count in examples:

{noformat}
ERROR 20:01:24,693 Exception in thread Thread[Thrift:16,5,main]
java.lang.AssertionError: [DecoratedKey(663380155395974698, 6b6579333137),max(-8100212023555812159)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:34)
        at org.apache.cassandra.thrift.CassandraServer.get_paged_slice(CassandraServer.java:1041)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice.getResult(Cassandra.java:3478)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice.getResult(Cassandra.java:3466)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,19/Nov/12 20:02,16/Apr/19 09:32
Bug,CASSANDRA-4976,12616849,"""AssertionError: Wrong class type"" at CounterColumn.diff()","Thousands of the following errors are getting logged to system.log:

ERROR [ReadRepairStage:150152] 2012-11-15 12:31:02,815 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadRepairStage:150152,5,main]
java.lang.AssertionError: Wrong class type.
        at org.apache.cassandra.db.CounterColumn.diff(CounterColumn.java:110)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:248)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:342)
        at org.apache.cassandra.service.RowRepairResolver.scheduleRepairs(RowRepairResolver.java:117)
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:94)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

There are also many of the following errors intermingled with the above:

ERROR [ReadRepairStage:150158] 2012-11-15 12:30:34,148 CounterContext.java (line 381) invalid counter shard detected; (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, 916) and (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, -1590) differ only in count; will pick highest to self-heal; this indicates a bug or corruption generated a bad counter shard

I am not 100% sure whether they are related.",slebresne,jblangston@datastax.com,Normal,Resolved,Fixed,19/Nov/12 22:30,16/Apr/19 09:32
Bug,CASSANDRA-4979,12617116,Stress for cql3 is broken on 1.2/trunk,,slebresne,slebresne,Low,Resolved,Fixed,21/Nov/12 15:23,16/Apr/19 09:32
Bug,CASSANDRA-4980,12617135,StorageServiceClientTest/RecoveryManager2Test fail on 1.2.0 and above branch,"Looks like change in c4cca2d8bba20a7651b956e1893727391bf5f10a (store schema_version to system.local) broke both StorageServiceClientTest and RecoveryManager2Test.
StorageServiceClientTest assert data directories are not created in client mode but this change actually creates data directories. RecoveryManager2Test fails with ""junit.framework.AssertionFailedError: Expecting only 1 replayed mutation, got 10"" error and I think extra commit log also comes from this insert to system.local.",slebresne,yukim,Normal,Resolved,Fixed,21/Nov/12 16:50,16/Apr/19 09:32
Bug,CASSANDRA-4982,12617225,cqlsh: alter table add column with table that has collection fails,"create keyspace collections with replication = {'class':'SimpleStrategy', 'replication_factor':1};
use collections;
create table collections (key int primary key, aset set<text>);
insert into collections (key, aset) values (1, {'fee', 'fi'});
alter table collections add aaa text;
 
 
ERROR 16:52:33,792 Error occurred during processing of message.
java.lang.UnsupportedOperationException: ColumnToCollectionType should only be used in composite types, never alone
        at org.apache.cassandra.db.marshal.ColumnToCollectionType.validate(ColumnToCollectionType.java:103)
        at org.apache.cassandra.config.CFMetaData.validate(CFMetaData.java:1094)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:202)
        at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:217)
        at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:73)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1706)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",slebresne,dbrosius,Normal,Resolved,Fixed,21/Nov/12 22:03,16/Apr/19 09:32
Bug,CASSANDRA-4984,12617298,error opening data file at startup,"I've found this in logfile, this happens at cassandra startup:

INFO 10:06:13,670 Opening /var/lib/cassandra/data/MYKSPC/MYCF/MYKSPC-MYCF-hf-5547 (1073761823 bytes)
ERROR 10:06:13,670 Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.AssertionError
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:166)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:242)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)


Every CF in this Keyspace has cashing set to 'NONE'",carlyeks,zenek_kraweznik0,Normal,Resolved,Fixed,22/Nov/12 09:17,16/Apr/19 09:32
Bug,CASSANDRA-4990,12617621,Do not allow use of collection with compact storage,"You can define ColumnFamily with collection type and compact storage as follows:

{code}
CREATE TABLE test (
  user ascii PRIMARY KEY,
  mails list<text>
) WITH COMPACT STORAGE;
{code}

This does not make sense and end up error when inserting data to collection.

{code}
INSERT INTO test (user, mails) VALUES ('foo', ['foo@foo.org']);
{code}

I think it is better not to allow defining such ColumnFamily.",slebresne,yukim,Low,Resolved,Fixed,26/Nov/12 14:57,16/Apr/19 09:32
Bug,CASSANDRA-4992,12617624,TTL/WRITETIME function against collection column returns invalid value,"Since we cannot query individual content of collection in 1.2, TTL/WRITETIME function on collection column does not make sense. But currently we can perform those function on collection and get deserialization error like:

{code}
value '\x00\x03\x00\x01c\x00\x01b\x00\x01a' (in col 'writetime(l)') can't be deserialized as bigint: unpack requires a string argument of length 8
{code}

Looks like it tries to deserialize whole list/set/map content as bigint for WRITETIME and int for TTL.",slebresne,yukim,Low,Resolved,Fixed,26/Nov/12 15:02,16/Apr/19 09:32
Bug,CASSANDRA-4995,12617734,CompactionSerializerTest fails to find jemalloc,"{noformat}
    [junit] Testcase: org.apache.cassandra.io.CompactSerializerTest:	Caused an ERROR
    [junit] Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
    [junit] java.lang.UnsatisfiedLinkError: Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
    [junit] 	at com.sun.jna.NativeLibrary.loadLibrary(NativeLibrary.java:163)
    [junit] 	at com.sun.jna.NativeLibrary.getInstance(NativeLibrary.java:236)
    [junit] 	at com.sun.jna.Library$Handler.<init>(Library.java:140)
    [junit] 	at com.sun.jna.Native.loadLibrary(Native.java:379)
    [junit] 	at com.sun.jna.Native.loadLibrary(Native.java:364)
    [junit] 	at org.apache.cassandra.io.util.JEMallocAllocator.<clinit>(JEMallocAllocator.java:32)
    [junit] 	at java.lang.Class.forName0(Native Method)
    [junit] 	at java.lang.Class.forName(Class.java:169)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:109)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest.scanClasspath(CompactSerializerTest.java:142)
{noformat}

If jemalloc is now the preferred allocator, we should add it to the debian packaging.  However, I did install the lib and it still didn't work. ",vijay2win@yahoo.com,brandon.williams,Low,Resolved,Fixed,27/Nov/12 04:10,16/Apr/19 09:32
Bug,CASSANDRA-4996,12617744,"After changing the compaction strategy, compression_strategy  always returning back to the ""SnappyCompressor"" through CQL 2.2.0","faced very strange behaviour when changing compression_parameters of exisiting CF. After changing the compaction strategy, compression_strategy returning back to the ""SnappyCompressor"".

Using cassandra version 1.1.5.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
I have one column family with following paramters:

cqlsh > describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
comment='' AND
comparator=text AND
read_repair_chance=0.100000 AND
gc_grace_seconds=864000 AND
default_validation=text AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
replicate_on_write='true' AND
compaction_strategy_class='SizeTieredCompactionStrategy' AND
compaction_strategy_options:sstable_size_in_mb='5' AND
compression_parameters:sstable_compression='SnappyCompressor';

Changing compression strategy to 'DeflateCompressor

cqlsh> ALTER TABLE auditlog_01 WITH compression_parameters:sstabl
e_compression = 'DeflateCompressor' AND compression_parameters:chunk_length_kb =
 64;
cqlsh> describe columnfamily auditlog_01;

CREATE TABLE auditlog_01 (
lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='DeflateCompressor';

it's sucessfuly changed the compression strategy to 'DeflateCompressor, after that when i am trying to change the compaction strategy, compression strategy returing back to ""SnappyCompressor"".
cqlsh> alter table auditlog_01 with compaction_strategy_class='Le
veledCompactionStrategy' AND compaction_strategy_options:sstable_size_in_mb=5;
cqlsh> describe columnfamily auditlog_01;

CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';",aleksey,shamim_ru,Low,Resolved,Fixed,27/Nov/12 08:06,16/Apr/19 09:32
Bug,CASSANDRA-5001,12618021,Generated time-based UUID don't conform to the spec,"When UUIDGen layout the clock sequence and and node part of version 1 UUID, it does so with
{noformat}
private long getClockSeqAndNode(InetAddress addr)
{
    long lsb = 0;
    lsb |= (clock & 0x3f00000000000000L) >>> 56; // was 58?
    lsb |= 0x0000000000000080;
    lsb |= (clock & 0x00ff000000000000L) >>> 48;
    lsb |= makeNode(addr);
    return lsb;
}
{noformat}
This is not correct however, as this layout the clock seq (and variant) on the right-most part of the lsb while it should be on the left-most one.

At a minimum, the generated UUID don't fully respect the spec since the variant is not set correctly. But it also means that the clock seq bits end up being all 0's (as can be trivially seen in the string representation of the generated UUIDs).

Note that none of those is a huge huge deal as there is still largely enough random bytes to ensure that two different nodes won't end up with the same lsb. And having the variant wrong has probably no practical implementation. There is no reason not to fix those though.

One other small details is that the getAdjustedTimestamp as a sign error so that it returns completely broken timestamps. That being said the method is currently unused so that's not a big deal. I'm attaching a fix for that part too because that method might be useful someday but I won't shed a tear if we prefer just removing it.

I'm marking this for 1.2 because I'm not sure it's worth bothering with 1.1.",slebresne,slebresne,Low,Resolved,Fixed,28/Nov/12 17:41,16/Apr/19 09:32
Bug,CASSANDRA-5002,12618145,UUIGen should never use another host IP for its node part,"UUIDGen allows to specify the inet address that we use to generate the node part of the created UUID. This is wrong however. More precisely, the node part is what make sure UUID generated on two different hosts are different, because we can't guarantee that the timestamp and clock parts will be different. In other words, generating on a host a UUID with the node part of another host is dangerous is clearly contrary to the spec.

And as it turns out, making sure we always use the local address means that the full lsb part of the UUID becomes constant (as it should) and we can remove the nodeCache from UUIDGen and simplify/speedup UUID generation, which is all the more reason to fix it.

I note that we were almost always using the local address to generate UUID anyway. The only place where we weren't is in Stream{In/Out}Session, and there is virtually no chance that this has ever broke anything (but we should still fix it).
",slebresne,slebresne,Low,Resolved,Fixed,29/Nov/12 12:32,16/Apr/19 09:32
Bug,CASSANDRA-5008,12618350,"""show schema"" command in cassandra-cli generates wrong ""index_options"" values.","Using cassandra-cli, launch the ""show schema"" command and save the output to a file.
Try to import it in order to recreate the schema, it fails with error message :
""Syntax error at position 626: no viable alternative at input '}'""

",yukim,thboileau,Low,Resolved,Fixed,30/Nov/12 17:42,16/Apr/19 09:32
Bug,CASSANDRA-5009,12618363,"RangeStreamer has no way to report failures, allowing bootstrap/move etc to complete without data","It looks like we fixed this for 1.2 by having RS.fetch() throw, but in 1.1 it does not and there doesn't appear to be a way to detect an RS failure, which among other things will cause bootstrap to succeed even though it failed to fetch any data.",brandon.williams,brandon.williams,Urgent,Resolved,Fixed,30/Nov/12 18:59,16/Apr/19 09:32
Bug,CASSANDRA-5012,12618499,alter table alter causes TSocket read 0 bytes,"Altering the type of a clustering key column causes TSocket error. 

To reproduce the problem:
1. CREATE SCHEMA ""Excalibur"" WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

2. CREATE TABLE ""Excalibur"".Test ( id int, species text, color text, PRIMARY KEY ((id, species), color)) WITH compaction = { 'class' : 'SizeTieredCompactionStrategy', 'min_compaction_threshold' : 6 };

3. Alter table ""Excalibur"".test ALTER color type int;

Expected result: Error message saying something like, ""Changing the type of a clustering key is not allowed.""

Actual result: TSocket read 0 bytes",slebresne,khahn,Normal,Resolved,Fixed,02/Dec/12 18:12,16/Apr/19 09:32
Bug,CASSANDRA-5013,12618512,disallow bloom filter false positive chance of 0,"{pre}
ERROR [CompactionExecutor:16] 2012-11-30 08:44:32,546 SSTableWriter.java (line 414) Bloom filter FP chance of zero isn't supposed to happen
{pre}

when attempting to set it to zero, C* should either disallow the change or should just interpret 0 as ""make it the default"" and not continually log the above error message
",jbellis,mdennis,Low,Resolved,Fixed,02/Dec/12 21:43,16/Apr/19 09:32
Bug,CASSANDRA-5016,12618585,Can't prepare an INSERT query,"Preparing an INSERT query fails with CQL3+binary protocol (maybe thrift as well, haven't checked). Preparing an equivalent UPDATE query works just fine.

demo (id int primary key, value text)
preparing ""INSERT INTO test.demo (id, value) VALUES (?, ?)"" -> 8704, Invalid definition for id, not a collection type
prparing ""UPDATE test.demo SET value = ? WHERE id = ?"" -> ok",slebresne,aleksey,Normal,Resolved,Fixed,03/Dec/12 16:29,16/Apr/19 09:32
Bug,CASSANDRA-5017,12618587,Preparing UPDATE queries with collections returns suboptimal metadata,"CQL3, binary protocol.

collections (id int primary key, amap map<int, varchar>);

preparing ""UPDATE test.collections SET amap[?] = ? WHERE id = ?"" returns the following metadata:
[{column,<<""test"">>,<<""collections"">>,<<""amap"">>,
                       {map,int,varchar}},
               {column,<<""test"">>,<<""collections"">>,<<""amap"">>,
                       {map,int,varchar}},
               {column,<<""test"">>,<<""collections"">>,<<""id"">>,int}]

Ideally it should return [int, varchar, int] types. Less ideally [{map, int, varchar}, int] and expect an encoded map with a single key-value pair. But certainly not what it currently returns.",slebresne,aleksey,Low,Resolved,Fixed,03/Dec/12 16:35,16/Apr/19 09:32
Bug,CASSANDRA-5022,12618606,Can't prepare an UPDATE query with a counter column (CQL3),"CQL3, binary protocol:

demo(id int primary key, counter counter)

Preparing ""UPDATE test.counters SET counter = counter + ? WHERE id = ?"" yields 8704, ""Invalid operation for commutative columnfamily counters"" error.",slebresne,aleksey,Low,Resolved,Fixed,03/Dec/12 18:24,16/Apr/19 09:32
Bug,CASSANDRA-5025,12618758,Schema push/pull race,"When a schema change is made, the coordinator pushes the delta to the other nodes in the cluster.  This is more efficient than sending the entire schema.  But the coordinator also announces the new schema version, so the other nodes' reception of the new version races with processing the delta, and usually seeing the new schema wins.  So the other nodes also issue a pull to the coordinator for the entire schema.

Thus, schema changes tend to become O(n) in the number of KS and CF present.",jbellis,jbellis,Low,Resolved,Fixed,04/Dec/12 17:52,16/Apr/19 09:32
Bug,CASSANDRA-5030,12618912,IndexHelper.IndexFor call throws AOB exception when passing multiple slices,"While testing multiple slices I'm seeing some exceptions when a slice hits the end of an index.

{code}
ERROR [ReadStage:138179] 2012-12-04 18:04:28,796 CassandraDaemon.java (line 132) Exception in thread Thread[ReadStage:138179,5,main]
java.lang.IndexOutOfBoundsException: toIndex = 6
        at java.util.SubList.<init>(AbstractList.java:602)
        at java.util.RandomAccessSubList.<init>(AbstractList.java:758)
        at java.util.AbstractList.subList(AbstractList.java:468)
        at org.apache.cassandra.io.sstable.IndexHelper.indexFor(IndexHelper.java:182)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.setNextSlice(IndexedSliceReader.java:253)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.<init>(IndexedSliceReader.java:246)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:68)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:44)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:267)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1387)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1247)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1159)
        at org.apache.cassandra.db.Table.getRow(Table.java:348)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:48)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}


I can reproduce this in a test, attached",tjake,tjake,Normal,Resolved,Fixed,05/Dec/12 17:26,16/Apr/19 09:32
Bug,CASSANDRA-5032,12618938,Downed node loses its host-id,"We took down one of our nodes for maintenance and during that time it seems the other nodes have lost the downed nodes node id

We also see lots of hint assertion exceptions ""Missing host ID for 10.6.27.98""

{code}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.6.27.96        129.37 GB  256     140.0%            59f3df94-e551-45ce-a3b0-51462f3ea868  27
UN  10.6.27.97        125.24 GB  256     133.7%            f5bb146c-db51-475c-a44f-9facf2f1ad6e  27
DN  10.6.27.98        ?          256     126.3%            null                                  27
{code}

We restarted c* on the two other nodes that are up, my guess is the host id was lost on restart of those.
",brandon.williams,tjake,Normal,Resolved,Fixed,05/Dec/12 20:22,16/Apr/19 09:32
Bug,CASSANDRA-5036,12619033,Wrong description of 'setstreamthroughput' option,"There is a typo in description of 'setstreamthroughput' option. It is measured in megabits per second. Page with wrong description:
http://www.datastax.com/docs/1.1/references/nodetool#nodetool-setstreamthroughput
Page with right description:
http://www.datastax.com/docs/1.1/configuration/node_configuration#stream-throughput-outbound-megabits-per-sec



Also I want to discuss possibility to reduce default value for this option. I think that 400 Mbs is too high for common cases. 

Preface:
This option is used only in case streams. There are two cases when streams are actual. They are rebuilding of a node and repair process. Let's skip first case and will talk only about the second. Let's imagine that we have replication factor 3.

Cross-datacenter connectivity case: 
When we start repair process it will borrow all network channel. Let's do some calculations. You start repair on an one node, e.g. 5 node (3 remote and 2 local) should send us some data. Note that 3 of them are from remote datacenter. So 400 * 3 = 1,2 Gbs we should receive through WAN. I'm sure that it's too high.

I suggest to make it 2 times less.
",azotcsit,azotcsit,Low,Resolved,Fixed,06/Dec/12 11:48,16/Apr/19 09:32
Bug,CASSANDRA-5040,12622913,Can't insert only a key in CQL3,"The following should work but well, doesnt:
{noformat}
cqlsh:k> CREATE TABLE t (k int PRIMARY KEY, v int);
cqlsh:k> INSERT INTO t (k) VALUES (0);
Bad Request: line 1:27 required (...)+ loop did not match anything at input ')'
{noformat}

The reason is just that the parser for INSERT has never been updated from the time where providing only a key was illegal.",slebresne,slebresne,Low,Resolved,Fixed,07/Dec/12 15:18,16/Apr/19 09:32
Bug,CASSANDRA-5046,12623040,cqlsh doesn't show correct timezone when SELECTing a column of type TIMESTAMP,"trying to figure out if i'm doing something wrong or a bug.  i am
creating a simple schema, inserting a timestamp using ISO8601 format,
but when retrieving the timestamp, the timezone is displayed
incorrectly.  i'm inserting using GMT, the result is shown with
""+0000"", but the time is for my local timezone (-0800)

tried with 1.1.6 (DSE 2.2.1), and 1.2.0-rc1-SNAPSHOT

here's the trace:

bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift
protocol 19.35.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE btoddb WITH replication =
{'class':'SimpleStrategy', 'replication_factor':1};
cqlsh>
cqlsh> USE btoddb;
cqlsh:btoddb> CREATE TABLE test (
          ...   id uuid PRIMARY KEY,
          ...   ts TIMESTAMP
          ... );
cqlsh:btoddb>
cqlsh:btoddb> INSERT INTO test
          ...   (id, ts)
          ...   values (
          ...     '89d09c88-40ac-11e2-a1e2-6067201fae78',
          ...     '2012-12-07T10:00:00-0000'
          ...   );
cqlsh:btoddb>
cqlsh:btoddb> SELECT * FROM test;

 id                                   | ts
--------------------------------------+--------------------------
 89d09c88-40ac-11e2-a1e2-6067201fae78 | 2012-12-07 02:00:00+0000

cqlsh:btoddb>",aleksey,btoddb,Low,Resolved,Fixed,09/Dec/12 03:24,16/Apr/19 09:32
Bug,CASSANDRA-5049,12623189,Auth.setup() is called too early,"Auth.setup() triggers a request against the system_auth keyspace, request that is not an internal one, so it at least require TokenMetadata to be set up. However, Auth.setup() is call much too early, even before the commit log is replayed. The only reason this doesn't trigger an assertionError everytime is because Auth.setup() actually only schedul it's request after RING_DELAY, but still, replaying the commit log can take much more than that, and even without that I suspect this would be racy with normal bootstrap.",slebresne,slebresne,Low,Resolved,Fixed,10/Dec/12 18:26,16/Apr/19 09:32
Bug,CASSANDRA-5050,12623218,Cql3 token queries broken,"Currently any select statement that uses a token() predicate breaks with ""Bad Input""

After tracing the logic this error is caused in getTokenBounds because it assumes the token term is an actual token string what will pass the tokenizer

",tjake,tjake,Normal,Resolved,Fixed,10/Dec/12 21:34,16/Apr/19 09:32
Bug,CASSANDRA-5052,12623347,cassandra-cli should escape keyspace name,"show schema yields ""use __someKeyspace"", expecting ""use '__someKeyspace'"".
",aleksey,shalupov,Low,Resolved,Fixed,11/Dec/12 08:39,16/Apr/19 09:32
Bug,CASSANDRA-5053,12623365,not possible to change crc_check_chance,"It is not possible to change crc_check_chance using a schema modification after CASSANDRA-4266

This patch fixes that and moves the setting out into a configuration parameter instead, you dont want to upgrade/scrub/.. all your sstables to change the crc_check_chance.

",marcuse,marcuse,Low,Resolved,Fixed,11/Dec/12 12:02,16/Apr/19 09:32
Bug,CASSANDRA-5058,12623485,debian packaging should include shuffle,"Our debian packaging doesn't currently include shuffle, but we should add it so people have a way of upgrading to vnodes.  This might also be a good time to consider a different name for shuffle, though I don't believe it currently conflicts with anything else.",slebresne,brandon.williams,Normal,Resolved,Fixed,12/Dec/12 04:49,16/Apr/19 09:32
Bug,CASSANDRA-5061,12623752,Upgraded cassandra loses all cfs on restart,"A bit dramatic summary, but hey;

If you upgrade cassandra and then restart it, you lose all your CFs, but they come back if you restart again.

This is due to fixSchemaNanoTimestamp not flushing the new data after truncating the CF and re-doing the mutations.",marcuse,marcuse,Normal,Resolved,Fixed,13/Dec/12 14:02,16/Apr/19 09:32
Bug,CASSANDRA-5064,12623780,'Alter table' when it includes collections makes cqlsh hang,"Having just installed 1.2.0-beta3 issue the following CQL into cqlsh:
{code}
drop keyspace test;

create keyspace test with replication = {
          'class': 'SimpleStrategy',
          'replication_factor': '1'
        };

use test;

create table users (
            user_id text PRIMARY KEY,
            first_name text,
            last_name text,
            email_addresses set<text>
        );

alter table users add mailing_address_lines list<text>;
{code}

As soon as you issue the alter table statement cqlsh hangs, and the java process hosting Cassandra consumes 100% of a single core's CPU.

If the alter table doesn't include a collection, it runs fine.",slebresne,enigmacurry,Urgent,Resolved,Fixed,13/Dec/12 16:48,16/Apr/19 09:32
Bug,CASSANDRA-5065,12623814,nodetool ownership is incorrect with vnodes,"Example:

{noformat}
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Tokens  Owns   Host ID                               Rack
UN  10.179.65.102     197.96 MB  256     0.1%   6ac56251-08ff-46be-be06-5b8dd929b937  rack1
UN  10.179.111.137    209.3 MB   256     0.0%   aade8ef6-c907-427c-87be-a5fe05a27fa4  rack1
UN  10.179.64.227     205.86 MB  256     0.1%   4634cc80-0832-4ea1-b4a6-39ae54985206  rack1
{noformat}
",jbellis,brandon.williams,Normal,Resolved,Fixed,13/Dec/12 20:16,16/Apr/19 09:32
Bug,CASSANDRA-5066,12623832,Compression params validation assumes SnappyCompressor,"This hasn't caused any issues yet since DeflateCompressor and SnappyCompressor have the same empty set for supportedOptions, but is a potential issue.

Combined with CASSANDRA-4996 this also brings back CASSANDRA-4266.",slebresne,aleksey,Low,Resolved,Fixed,13/Dec/12 21:24,16/Apr/19 09:32
Bug,CASSANDRA-5068,12623863,"CLONE - Once a host has been hinted to, log messages for it repeat every 10 mins even if no hints are delivered","We have ""0 row"" hinted handoffs every 10 minutes like clockwork. This impacts our ability to monitor the cluster by adding persistent noise in the handoff metric.

Previous mentions of this issue are here:
http://www.mail-archive.com/user@cassandra.apache.org/msg25982.html

The hinted handoffs can be scrubbed away with
nodetool -h 127.0.0.1 scrub system HintsColumnFamily
but they return after anywhere from a few minutes to multiple hours later.

These started to appear after an upgrade to 1.1.6 and haven't gone away despite rolling cleanups, rolling restarts, multiple rounds of scrubbing, etc.

A few things we've noticed about the handoffs:
1. The phantom handoff endpoint changes after a non-zero handoff comes through

2. Sometimes a non-zero handoff will be immediately followed by an ""off schedule"" phantom handoff to the endpoint the phantom had been using before

3. The sstable2json output seems to include multiple sub-sections for each handoff with the same ""deletedAt"" information.



The phantom handoff endpoint changes after a non-zero handoff comes through:
 INFO [HintedHandoff:1] 2012-12-11 06:57:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.1
 INFO [HintedHandoff:1] 2012-12-11 07:07:35,092 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.1
 INFO [HintedHandoff:1] 2012-12-11 07:07:37,915 HintedHandOffManager.java (line 392) Finished hinted handoff of 1058 rows to endpoint /10.10.10.2
 INFO [HintedHandoff:1] 2012-12-11 07:17:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.2
 INFO [HintedHandoff:1] 2012-12-11 07:27:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.2



Sometimes a non-zero handoff will be immediately followed by an ""off schedule"" phantom handoff to the endpoint the phantom had been using before:
 INFO [HintedHandoff:1] 2012-12-12 21:47:39,335 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 21:57:39,335 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 22:07:43,319 HintedHandOffManager.java (line 392) Finished hinted handoff of 1416 rows to endpoint /10.10.10.4
 INFO [HintedHandoff:1] 2012-12-12 22:07:43,320 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 22:17:39,357 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.4
 INFO [HintedHandoff:1] 2012-12-12 22:27:39,337 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.4



The first few entries from one of the json files:
{
    ""0aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"": {
        ""ccf5dc203a2211e20000e154da71a9bb"": {
            ""deletedAt"": -9223372036854775808, 
            ""subColumns"": []
        }, 
        ""ccf603303a2211e20000e154da71a9bb"": {
            ""deletedAt"": -9223372036854775808, 
            ""subColumns"": []
        }, 
",brandon.williams,peter-librato,Low,Resolved,Fixed,13/Dec/12 23:50,16/Apr/19 09:32
Bug,CASSANDRA-5070,12623989,LOCAL_QUORUM consistency causes Tracing to fail,"{code}
cqlsh:prod4> CONSISTENCY LOCAL_QUORUM;
Consistency level set to LOCAL_QUORUM.

cqlsh:prod4> TRACING ON;
Now tracing requests.
cqlsh:prod4> select * from table1 limit 10 

Bad Request: consistency level LOCAL_QUORUM not compatible with replication strategy (org.apache.cassandra.locator.SimpleStrategy)
{code}


Looks to be the issue with LocalStrategy",aleksey,tjake,Low,Resolved,Fixed,14/Dec/12 15:21,16/Apr/19 09:32
Bug,CASSANDRA-5072,12624114,Bug in creating EnumSet in SimpleAuthorizer example,"In SimpleAuthorizer around line 47 we have,

EnumSet<Permission> authorized = EnumSet.copyOf(Permission.NONE);

This results in an IllegalArgumentException since Permission.NONE is an empty set. I think it should be changed to,

EnumSet<Permission> authorized = EnumSet.noneOf(Permission.class);
",aleksey,jsanda,Low,Resolved,Fixed,15/Dec/12 13:24,16/Apr/19 09:32
Bug,CASSANDRA-5074,12624486,Add an official way to disable compaction,"We've traditionally used ""min or max compaction threshold = 0"" to disable compaction, but this isn't exactly intuitive and it's inconsistently implemented -- allowed from jmx, not allowed from cli.",marcuse,jbellis,Low,Resolved,Fixed,18/Dec/12 16:13,16/Apr/19 09:32
Bug,CASSANDRA-5076,12624529,Murmur3Partitioner#describeOwnership calculates ownership% wrong,"When I issued 'nodetool status' on Murmur3-partitioned cluster I got the following output:

{code}
$ bin/nodetool -p 7100 status                                                                                                                                                                                                                                                                                       (git)-[5065]-[~/Developments/workspace/cassandra]
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Owns   Host ID                               Token                                    Rack
UN  127.0.0.1         24.78 KB   66.7%  ace7e54c-9fe1-4b23-83b0-949772b24c30  -9223372036854775808                     rack1
UN  127.0.0.2         29.22 KB   66.7%  67146442-dbfd-449c-82e1-26729b8ac89c  -3074457345618258603                     rack1
UN  127.0.0.3         6.19 KB    66.7%  3fab9f18-daf3-4452-8b9c-204ea0ee2e15  3074457345618258602                      rack1
{code}

Notice that 'Owns' percentages add up to 200%.

I think the problem is that Murmur3Partitioner#describeOwnership currently calculate ownership% based on [0, Long.MAX_VALUE] range, but we have to consider about negative tokens.",yukim,yukim,Low,Resolved,Fixed,18/Dec/12 20:37,16/Apr/19 09:32
Bug,CASSANDRA-5079,12624763,Compaction deletes ExpiringColumns in Secondary Indexes,"From this discussion http://www.mail-archive.com/user@cassandra.apache.org/msg26599.html

CompactionManager.getDefaultGcBefore() set's the gc_before to be Integer.MAX_VALUE. 

In the example all entries in the secondary index have a TTL. In PreCompactedRow.removeDeletedAndOldShards() the CF is determined to have irrelevant data, the call to CFS.removeDeleted() results in the ExpiringColumns being removed and the row is treated as empty. CompactionTask.execute() exits at the {{if (!nni.hasNext())}} test, so the sstables are marked as compacted and soon deleted. 

In the example the localDeletionTime was Thu, 21 Mar 2013 08:25:22 GMT and should not have been purged. 

In the example when the first compaction on the secondary index runs the on disk data for the index is deleted. The logs show a compaction starting and no associated ""Compacted to"" message from that compaction thread. 

The impact is incorrect results from secondary indexes queries.",amorton,amorton,Normal,Resolved,Fixed,20/Dec/12 04:05,16/Apr/19 09:32
Bug,CASSANDRA-5080,12624803,cassandra-cli doesn't support JMX authentication.,"It seems that cassandra-cli doesn't support JMX user authentication.

Specifically I went about securing our Cassandra cluster slightly -- I've added cassandra-level authentication (which cassandra-cli does support), but then I discovered that nodetool is still completely unprotected. So I went ahead and secured JMX (via -Dcom.sun.management.jmxremote.password.file and -Dcom.sun.management.jmxremote.access.file). Nodetool supports JMX authentication via -u and -pw options.

However it seems that cassandra-cli doesn't support JMX authentication, e.g.:
{quote}
apache-cassandra-1.1.6\bin>cassandra-cli -h hostname -u experiment -pw password
Starting Cassandra Client
Connected to: ""db"" on hostname/9160
Welcome to Cassandra CLI version 1.1.6

[experiment@unknown] show keyspaces;
WARNING: Could not connect to the JMX on hostname:7199, information won't be shown.

Keyspace: system:
  Replication Strategy: org.apache.cassandra.locator.LocalStrategy
  Durable Writes: true
    Options: [replication_factor:1]
... (rest of keyspace output snipped)
{quote}

help connect; and cassandra-cli --help do not seem to indicate that there's any way to specify JMX login information.",michalm,solf,Normal,Resolved,Fixed,20/Dec/12 11:17,16/Apr/19 09:32
Bug,CASSANDRA-5082,12624811,Disallow counters in collection (CQL3),We don't support counters in collections but we don't throw an error when someone tries to create such a thing. Attaching patch to return a validation error.,slebresne,slebresne,Low,Resolved,Fixed,20/Dec/12 13:10,16/Apr/19 09:32
Bug,CASSANDRA-5087,12625016,Changing from higher to lower compaction throughput causes long (multi hour) pause in large compactions,"We're running a major compaction against a column family that is 2.1TB (yes, I know it's crazy huge, that's an entirely different discussion). During the evenings, we run a setcompactionthroughput 0 to unthrottle completely, and throttle again down to 20mb at the end of the maintenance window. 

Every morning we've come in to check progress, we find that the progress completely halts as soon as the compaction throttling command is issued. Eventually, compaction continues. I was looking at the throttling code, and I think I see the issue, but would like confirmation:

throttleDelta (org.apache.cassandra.utils.Throttle.throttleDelta) sets a sleep time based on the amount of data transferred since the last throttle time. Since we've gone from 20 MB to wide open, and back to 20MB, the wait that is calculated is based on an attempt to average the new throttling rate over the last 6.5 hours of running wide open.

I think this could be fixed by adding a reset of bytesAtLastDelay and timeAtLastDelay to the current values after the check at line 64:

Current:

        // if the target changed, log
        if (newTargetBytesPerMS != targetBytesPerMS) 
            logger.debug(""{} target throughput now {} bytes/ms."", this, newTargetBytesPerMS);
        targetBytesPerMS = newTargetBytesPerMS;

New:

 
        // if the target changed, log
        if (newTargetBytesPerMS != targetBytesPerMS) {
            logger.debug(""{} target throughput now {} bytes/ms."", this, newTargetBytesPerMS);
            if(newTargetBytesPerMS < targetBytesPerMS || targetBytesPerMS < 1) {
            	bytesAtLastDelay += bytesDelta;
            	timeAtLastDelay = System.currentTimeMillis();
                targetBytesPerMS = newTargetBytesPerMS;
            	return;
            }
            targetBytesPerMS = newTargetBytesPerMS;
        }

Some redundancies that can be removed there, but I wanted to keep the approach local to where I thought the problem was. ",jblangston@datastax.com,jblangston@datastax.com,Low,Resolved,Fixed,21/Dec/12 16:17,16/Apr/19 09:32
Bug,CASSANDRA-5088,12625118,Major compaction IOException in 1.1.8,"Upgraded 1.1.6 to 1.1.8.

Now I'm trying to do a major compaction, and seeing this:

ERROR [CompactionExecutor:129] 2012-12-22 10:33:44,217 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:129,1,RMI Runtime]
java.io.IOError: java.io.IOException: Bad file descriptor
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:195)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:298)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Bad file descriptor
        at sun.nio.ch.FileDispatcher.preClose0(Native Method)
        at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.FileInputStream.close(FileInputStream.java:258)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:121)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
        at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:130)
        at org.apache.cassandra.io.sstable.SSTableScanner.close(SSTableScanner.java:89)
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:61)
        ... 9 more
",jbellis,kmueller,Normal,Resolved,Fixed,22/Dec/12 18:38,16/Apr/19 09:32
Bug,CASSANDRA-5089,12625154,get_range_slices does not validate end_token,"get_range_slices times out, java log has the following exception:
ERROR [Thrift:1] 2012-12-22 08:14:30,120 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: [DecoratedKey(28555413689034504124051437792156504, 6434313866653035643631663962323635323937343337653666636265616162),max(0)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:45)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:38)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:698)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)

We see it every time on the SECOND get_range_slices call when we clear start_token and set start_key in the key range.
We noticed this in 1.1.7 first, 1.1.8 still affected. 1.1.6 is fine.
Please contact me if you need more information.
 ",jbellis,apesternikov,Low,Resolved,Fixed,23/Dec/12 21:04,16/Apr/19 09:32
Bug,CASSANDRA-5093,12625373,Wrong bloom_filter_fp_chance for newly created CFs with LeveledCompactionStrategy,"0.1 is supposed to be the default bloom_filter_fp_chance for LeveledCompactionStrategy (and 0.01 for all other strategies).
However, CFPropDefs#applyToCFMetadata() sets bloom_filter_fp_chance before setting compaction strategy class, so the default bloom_filter_fp_chance is always 0.01 no matter what the compaction strategy is.

The fix is to move cfm#bloomFilterFpChance() call below cfm#compressionParameters().

The attached patch also kills dead default consistency level-related code.",aleksey,aleksey,Low,Resolved,Fixed,27/Dec/12 23:58,16/Apr/19 09:32
Bug,CAMEL-4850,12536787,Ftp consumer - NPE if using 2 slashes as starting directory,"See nabble
http://camel.465427.n5.nabble.com/Continuation-of-NPE-for-FTP-endpoint-tp5107992p5107992.html

The workaround is to not use double slashes.

This may also affect using a root as absolute path for FTP consumer, as the starting directory will be interpreted as empty.",davsclaus,davsclaus,Minor,Resolved,Fixed,02/Jan/12 10:08,02/Jan/12 10:51
Bug,CAMEL-4854,12537045,BAM - database constraint violation when restaring application,"Take BAM example from http://camel.apache.org/bam-example.html  (or any other one, it doesn't matter).

Here is the one I'm using:

        ActivityBuilder request = activity(""eaiBroker:topic:SOME_TOPIC?concurrentConsumers=1"").name(""Request"")
                .correlate(xpath(""/MessageRequest/@Id"").stringResult());

        ActivityBuilder response = activity(""eaiBroker:topic:SOME_OTHER_TOPIC?concurrentConsumers=1"").name(""Response"")
                .correlate(xpath(""MessageResponse/@Id"").stringResult());

        response
                .starts().after(request.completes())
                .expectWithin(seconds(15))
                .errorIfOver(seconds(30))
                .to(""log:com.eai?level=error"");



 First run of the application and everything works. Restart application and try to activate one of BAM rules, there will be an exception that database constraint has been violated. Every time application is started, Camel tries to do following sql insert

insert into CAMEL_ACTIVITYDEFINITION (name, processDefinition_id, id) values (?, ?, ?)

but name columne must be unique in CAMEL_ACTIVITYDEFINITION .


Workaround for this is to purge BAM tables every time you want to restart the application, but it's not a solution.",davsclaus,marcin84,Major,Resolved,Fixed,03/Jan/12 13:04,03/Jan/12 17:03
Bug,CAMEL-4857,12537080,Endpoint URI normalization: information in path is lost,"A project with a test case is here: https://github.com/alaz/camel290_uri

The test case with the problem description (the problem occured during migration of camel-beanstalk https://github.com/osinka/camel-beanstalk component onto 2.9.0):

{code:title=UriTest.java}
package camel290.uri;

import java.util.Map;
import org.apache.camel.Consumer;
import org.apache.camel.Endpoint;
import org.apache.camel.Processor;
import org.apache.camel.Producer;
import org.apache.camel.impl.DefaultComponent;
import org.apache.camel.impl.DefaultEndpoint;
import org.apache.camel.test.CamelTestSupport;
import org.junit.Before;
import org.junit.Test;
import static org.junit.Assert.*;

public class UriTest extends CamelTestSupport {

  /**
   * An URI of Camel Beanstalk component consists of a hostname, port and a list
   * of tube names. Tube names are separated by ""+"" character (which is more or less
   * usualy used on the Web to make lists), but every tube name may contain URI special
   * characters like ? or +
   */

  class MyEndpoint extends DefaultEndpoint {
    String uri = null;
    String remaining = null;

    public MyEndpoint(final String uri, final String remaining) {
      this.uri = uri;
      this.remaining = remaining;
    }

    public Producer createProducer() throws Exception {
      throw new UnsupportedOperationException(""Not supported yet."");
    }

    public Consumer createConsumer(Processor prcsr) throws Exception {
      throw new UnsupportedOperationException(""Not supported yet."");
    }

    public boolean isSingleton() {
      return true;
    }
  }

  class MyComponent extends DefaultComponent {
    @Override
    protected Endpoint createEndpoint(final String uri, final String remaining, final Map<String, Object> parameters) throws Exception {
      return new MyEndpoint(uri, remaining);
    }
  }

  @Before
  @Override
  public void setUp() throws Exception {
    super.setUp();
    context.addComponent(""my"", new MyComponent());
  }

  @Test
  public void testExclamationInUri() {
    /**
     * %3F is not an ?, it's part of tube name.
     */
    MyEndpoint endpoint = context.getEndpoint(""my:host:11303/tube1+tube%2B+tube%3F"", MyEndpoint.class);
    assertNotNull(""endpoint"", endpoint);
  }

  @Test
  public void testPath() {
    /**
     * Here a tube name is ""tube+"" and written in URI as ""tube%2B"", but it gets
     * normalized, so that an endpoint sees ""tube1+tube+""
     */
    MyEndpoint endpoint = context.getEndpoint(""my:host:11303/tube1+tube%2B"", MyEndpoint.class);
    assertEquals(""Path contains several tube names, every tube name may have + or ? characters"", ""host:11303/tube1+tube%2B"", endpoint.remaining);
  }
}
{code} ",davsclaus,azarov,Major,Resolved,Fixed,03/Jan/12 17:08,02/May/13 02:29
Bug,CAMEL-4858,12537143,xpath expression with $header with saxon may throw exception if header did not exist,"See CAMEL-747

We should return an empty String instead of a Void.class, as this works with both JDK + Saxon XPathEngine.
Saxon does not work with the Void.class.

",davsclaus,davsclaus,Major,Resolved,Fixed,04/Jan/12 07:15,04/Jan/12 08:26
Bug,CAMEL-4870,12537547,Attachments is not propagated in the template components such as camel-stringtemplate,"In the StringTemplateEndpoint we should propagate attachments as well, currently its only the headers.
",muellerc,davsclaus,Minor,Closed,Fixed,06/Jan/12 17:17,07/Jan/12 14:49
Bug,CAMEL-4871,12537602,"Request with Content-Type= application/x-www-form-urlencoded  throws ""Cannot read request parameters due Invalid parameter, expected to be a pair but was "" when body is empty.","ISSUE
-----------
When CAMEL Servlet component receiving request with Content-Type= application/x-www-form-urlencoded. as below it throws Cannot read request parameters due Invalid parameter, expected to be a pair but was "" when body is empty.

Request
------------
<html>
    </head>

    <body>
        <form method=""post"" action=""http://test/assertionConsumer/1.0"">
          <div>
            <input type=""hidden"" name=""ID"" value=""fim page"" />
            <input type=""hidden"" name=""RelayState"" value=""http://dev.ecosys.com?count=id-afc417c792b413bd0208e2f7454fd030"" />
            <input type=""hidden"" name=""SAMLResponse"" value=""PHNhbWxwOlJlc3="" />
          </div>
        </form>
        <span id=""user_msg""></span>
        <script type=""text/javascript"">sso();</script>
    </body>
</html>

RESOLUTION
-------------
Added defensive check, body is not null and not blank, to avoid Invalid parameter exception. I've updated org.apache.camel.component.http.DefaultHttpBinding class for that.


// Push POST form params into the headers to retain compatibility
			// with DefaultHttpBinding
			String body = message.getBody(String.class);
			//Added defensive check, body is not null and not blank, to avoid Invalid parameter exception. ( My changes)
			if (body != null && !body.equals("""")) {
				for (String param : body.split(""&"")) {
					String[] pair = param.split(""="", 2);
					if (pair.length == 2) {
						String name = URLDecoder.decode(pair[0], charset);
						String value = URLDecoder.decode(pair[1], charset);
						if (headerFilterStrategy != null
								&& !headerFilterStrategy
										.applyFilterToExternalHeaders(name,
												value, message.getExchange())) {
							HttpHelper.appendHeader(headers, name, value);
						}
					} else {
						throw new IllegalArgumentException(
								""Invalid parameter, expected to be a pair but was ""
										+ param);
					}
				}
			}

",davsclaus,amit1000,Minor,Resolved,Fixed,06/Jan/12 21:56,12/Mar/12 20:26
Bug,CAMEL-4872,12537624,Route Info Command Exchange Date NPE,"Executing the camel:info-route/camel:route-info against a route that hasn't processed an exchange will throw an NPE due to First Exchange Date and Last Exchange Completed Date are null.

Patch is forthcoming.

Thanks,
Scott ES",davsclaus,sully6768,Major,Resolved,Fixed,07/Jan/12 04:45,17/Jan/12 15:14
Bug,CAMEL-4877,12537768,API-breaking changes in Validator-component,"Hi, we tried to upgrade to 2.9.0 last week but ran into trouble when using the newly moved validator component. 

Before upgrading (running 2.8.0), having a root xsd that imported or included other xsd-files on the class-path was no problem for us. But upgrading to 2.9.0 breaks this functionality. Resulting in the following error 

{code}
...
Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: validator://com/yyy/infra/applications/yyy-application/consumer/ws/consumer-report-request.xsd due to: src-resolve: Cannot resolve the name 'ns6:YyyReport' to a(n) 'element declaration' component.
...
{code}

I can see in the new documentation for the validator-component that there is an new property that you can set called *resourceResolver*. Given the name and the description of the interface LSResourceResolver my guess is that an implementation of that interface is required to get the old functionality back. 

I might be wrong, but to me this is API-breaking. It would be great if you could provide a ""defalut"" resourceResolver to mimic the old behavior since we rely a lot on the ability to compose schemas via includes or imports. 

",davsclaus,hutchkintoot,Major,Resolved,Fixed,09/Jan/12 14:55,19/Jan/12 07:13
Bug,CAMEL-4882,12538003,Timed out Exchanges should be removed from seda queues,When the SedaProducer times out and stops waiting for an Exchange to be processed it should remove it from the queue as well. Same applies to the vm: component.,hadrian,hadrian,Major,Resolved,Fixed,11/Jan/12 01:10,16/Jan/12 08:02
Bug,CAMEL-4892,12538331,CamelContext autoStartup=false and starting context programmatically doesnt work,If a <camelContext> is set to autoStartup=false and then later tried to start it via code or JMX doesn't start routes associated with it. ,davsclaus,davsclaus,Minor,Resolved,Fixed,13/Jan/12 15:41,13/Jan/12 15:46
Bug,CAMEL-4894,12538387,Parameter binding not working with BeanExpression,The parameter [binding options|http://camel.apache.org/bean-binding.html] available in camel-2.9.0 do not work for BeanExpression. The reason is the fake ognl parser that gets confused about the syntax. We need to align the two scenarios and probably replace the ognl like parsing.,hadrian,hadrian,Major,Resolved,Fixed,14/Jan/12 04:27,01/Mar/12 17:33
Bug,CAMEL-4896,12538402,validation of camel-websocket feature failed,"{noformat}
Christian-Muellers-MacBook-Pro:camel cmueller$ cd platforms/karaf/
Christian-Muellers-MacBook-Pro:karaf cmueller$ mvn clean install -Pvalidate
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Camel :: Platforms :: Apache Karaf
[INFO] Camel :: Platforms :: Apache Karaf :: Commands
[INFO] Camel :: Platforms :: Apache Karaf :: Features
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Camel :: Platforms :: Apache Karaf 2.10-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ karaf ---
[INFO] Deleting /Users/cmueller/workspaceCamel/camel/platforms/karaf/target
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (create-prop) @ karaf ---
[INFO] Executing tasks

main:
     [echo] Maven version: 2.10-SNAPSHOT
     [echo] OSGi version: 2.10.0.SNAPSHOT
[INFO] Executed tasks
[INFO] 
[INFO] --- properties-maven-plugin:1.0-alpha-2:read-project-properties (default) @ karaf ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.1:process (default) @ karaf ---
[INFO] Setting property: classpath.resource.loader.class => 'org.codehaus.plexus.velocity.ContextClassLoaderResourceLoader'.
[INFO] Setting property: velocimacro.messages.on => 'false'.
[INFO] Setting property: resource.loader => 'classpath'.
[INFO] Setting property: resource.manager.logwhenfound => 'false'.
[INFO] 
[INFO] --- ianal-maven-plugin:1.0-alpha-1:verify-legal-files (default) @ karaf ---
[INFO] 
[INFO] --- maven-install-plugin:2.3.1:install (default-install) @ karaf ---
[INFO] Installing /Users/cmueller/workspaceCamel/camel/platforms/karaf/pom.xml to /Users/cmueller/.m2/repository/org/apache/camel/karaf/2.10-SNAPSHOT/karaf-2.10-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Camel :: Platforms :: Apache Karaf :: Commands 2.10-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ camel-karaf-commands ---
[INFO] Deleting /Users/cmueller/workspaceCamel/camel/platforms/karaf/commands/target
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (create-prop) @ camel-karaf-commands ---
[INFO] Executing tasks

main:
     [echo] Maven version: 2.10-SNAPSHOT
     [echo] OSGi version: 2.10.0.SNAPSHOT
[INFO] Executed tasks
[INFO] 
[INFO] --- properties-maven-plugin:1.0-alpha-2:read-project-properties (default) @ camel-karaf-commands ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.1:process (default) @ camel-karaf-commands ---
[INFO] 
[INFO] --- maven-resources-plugin:2.4.3:resources (default-resources) @ camel-karaf-commands ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ camel-karaf-commands ---
[INFO] Compiling 15 source files to /Users/cmueller/workspaceCamel/camel/platforms/karaf/commands/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.4.3:testResources (default-testResources) @ camel-karaf-commands ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /Users/cmueller/workspaceCamel/camel/platforms/karaf/commands/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ camel-karaf-commands ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.8:test (default-test) @ camel-karaf-commands ---
[INFO] Surefire report directory: /Users/cmueller/workspaceCamel/camel/platforms/karaf/commands/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
There are no tests to run.

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-bundle-plugin:2.3.4:bundle (default-bundle) @ camel-karaf-commands ---
[WARNING] Warning building bundle org.apache.camel.karaf:camel-karaf-commands:bundle:2.10-SNAPSHOT : Split package org/apache/karaf/util
Use directive -split-package:=(merge-first|merge-last|error|first) on Export/Private Package instruction to get rid of this warning
Package found in   [Jar:org.apache.karaf.jaas.modules, Jar:org.apache.karaf.util]
Reference from     /Users/cmueller/.m2/repository/org/apache/karaf/org.apache.karaf.util/2.2.5/org.apache.karaf.util-2.2.5.jar
Classpath          [Jar:., Jar:camel-core, Jar:slf4j-api, Jar:org.apache.karaf.shell.console, Jar:jline, Jar:jansi, Jar:org.osgi.core, Jar:org.osgi.compendium, Jar:org.apache.karaf.jaas.modules, Jar:org.apache.karaf.jaas.config, Jar:org.apache.karaf.jaas.boot, Jar:org.apache.aries.blueprint, Jar:org.apache.aries.util, Jar:org.apache.aries.testsupport.unit, Jar:org.apache.aries.proxy.api, Jar:org.apache.felix.gogo.runtime, Jar:org.apache.karaf.util]
[INFO] 
[INFO] --- ianal-maven-plugin:1.0-alpha-1:verify-legal-files (default) @ camel-karaf-commands ---
[INFO] Checking legal files in: camel-karaf-commands-2.10-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.3.1:install (default-install) @ camel-karaf-commands ---
[INFO] Installing /Users/cmueller/workspaceCamel/camel/platforms/karaf/commands/target/camel-karaf-commands-2.10-SNAPSHOT.jar to /Users/cmueller/.m2/repository/org/apache/camel/karaf/camel-karaf-commands/2.10-SNAPSHOT/camel-karaf-commands-2.10-SNAPSHOT.jar
[INFO] Installing /Users/cmueller/workspaceCamel/camel/platforms/karaf/commands/pom.xml to /Users/cmueller/.m2/repository/org/apache/camel/karaf/camel-karaf-commands/2.10-SNAPSHOT/camel-karaf-commands-2.10-SNAPSHOT.pom
[INFO] 
[INFO] --- maven-bundle-plugin:2.3.4:install (default-install) @ camel-karaf-commands ---
[INFO] Installing org/apache/camel/karaf/camel-karaf-commands/2.10-SNAPSHOT/camel-karaf-commands-2.10-SNAPSHOT.jar
[INFO] Writing OBR metadata
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Camel :: Platforms :: Apache Karaf :: Features 2.10-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ apache-camel ---
[INFO] Deleting /Users/cmueller/workspaceCamel/camel/platforms/karaf/features/target
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (create-prop) @ apache-camel ---
[INFO] Executing tasks

main:
     [echo] Maven version: 2.10-SNAPSHOT
     [echo] OSGi version: 2.10.0.SNAPSHOT
[INFO] Executed tasks
[INFO] 
[INFO] --- properties-maven-plugin:1.0-alpha-2:read-project-properties (default) @ apache-camel ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.1:process (default) @ apache-camel ---
[INFO] 
[INFO] --- maven-resources-plugin:2.4.3:resources (filter) @ apache-camel ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] >>> features-maven-plugin:2.2.5:validate (validate) @ apache-camel >>>
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (create-prop) @ apache-camel ---
[INFO] Executing tasks

main:
     [echo] Maven version: 2.10-SNAPSHOT
     [echo] OSGi version: 2.10.0.SNAPSHOT
[INFO] Executed tasks
[INFO] 
[INFO] --- properties-maven-plugin:1.0-alpha-2:read-project-properties (default) @ apache-camel ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.1:process (default) @ apache-camel ---
[INFO] 
[INFO] --- maven-resources-plugin:2.4.3:resources (filter) @ apache-camel ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 3 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] <<< features-maven-plugin:2.2.5:validate (validate) @ apache-camel <<<
[INFO] 
[INFO] --- features-maven-plugin:2.2.5:validate (validate) @ apache-camel ---
[INFO] == Preparing for validation ==
[INFO]  - getting list of system bundle exports
[INFO]  - getting list of provided bundle exports
[INFO]     scanning org.apache.camel.karaf:apache-camel:pom:2.10-SNAPSHOT for exports
[INFO]     scanning org.osgi:org.osgi.core:jar:4.2.0:provided for exports
[INFO]     scanning org.ops4j.pax.logging:pax-logging-api:jar:1.5.3:provided for exports
[INFO]     scanning org.ops4j.pax.logging:pax-logging-service:jar:1.5.3:provided for exports
[INFO]     scanning log4j:log4j:jar:1.2.16:provided for exports
[INFO]     scanning org.apache.felix:org.apache.felix.framework:jar:2.0.5:provided for exports
[INFO]     scanning org.osgi:org.osgi.core:jar:4.2.0:provided for exports
[INFO]     scanning org.osgi:org.osgi.compendium:jar:4.2.0:provided for exports
[INFO]     scanning org.apache.felix:org.apache.felix.configadmin:jar:1.2.4:provided for exports
[INFO]     scanning org.apache.felix:org.osgi.core:jar:1.0.0:provided for exports
[INFO]     scanning org.apache.felix:org.osgi.compendium:jar:1.0.0:provided for exports
[INFO]     scanning org.apache.felix:org.osgi.core:jar:1.0.0:provided for exports
[INFO]     scanning org.apache.felix:javax.servlet:jar:1.0.0:provided for exports
[INFO]     scanning org.apache.felix:org.osgi.foundation:jar:1.0.0:provided for exports
[INFO]     scanning junit:junit:jar:4.8.1:provided for exports
[INFO]     scanning org.apache.aries.blueprint:org.apache.aries.blueprint:jar:0.3:provided for exports
[INFO]     scanning org.apache.aries:org.apache.aries.util:jar:0.3:provided for exports
[INFO]     scanning org.slf4j:slf4j-api:jar:1.6.1:provided for exports
[INFO]     scanning org.apache.aries.testsupport:org.apache.aries.testsupport.unit:jar:0.3:provided for exports
[INFO]     scanning org.apache.aries.proxy:org.apache.aries.proxy.api:jar:0.3:provided for exports
[INFO]     scanning org.apache.aries:org.apache.aries.util:jar:0.3:provided for exports
[INFO]     scanning org.apache.karaf.shell:org.apache.karaf.shell.console:jar:2.2.5:provided for exports
[INFO]     scanning org.sonatype.jline:jline:jar:2.5:provided for exports
[INFO]     scanning org.fusesource.jansi:jansi:jar:1.7:provided for exports
[INFO]     scanning org.osgi:org.osgi.core:jar:4.2.0:provided for exports
[INFO]     scanning org.osgi:org.osgi.compendium:jar:4.2.0:provided for exports
[INFO]     scanning org.apache.karaf.jaas:org.apache.karaf.jaas.modules:jar:2.2.5:provided for exports
[INFO]     scanning org.apache.karaf.jaas:org.apache.karaf.jaas.config:jar:2.2.5:provided for exports
[INFO]     scanning org.apache.karaf.jaas:org.apache.karaf.jaas.boot:jar:2.2.5:provided for exports
[INFO]     scanning org.apache.karaf:org.apache.karaf.util:jar:2.2.5:provided for exports
[INFO]     scanning org.apache.aries.blueprint:org.apache.aries.blueprint:jar:0.3:provided for exports
[INFO]     scanning org.apache.felix:org.apache.felix.gogo.runtime:jar:0.10.0:provided for exports
[INFO]  - populating repositories with Karaf core features descriptors
[INFO]  - validation of file:/Users/cmueller/workspaceCamel/camel/platforms/karaf/features/target/classes/features.xml
[INFO] == Analyzing feature descriptor ==
[INFO]  - read /Users/cmueller/workspaceCamel/camel/platforms/karaf/features/target/classes/features.xml
[INFO]  - adding repository from mvn:org.apache.karaf.assemblies.features/standard/2.2.5/xml/features
[INFO]  - validation of mvn:org.apache.karaf.assemblies.features/standard/2.2.5/xml/features
[INFO]     scanning feature karaf-framework for exports
[INFO]     scanning feature spring for exports
[INFO]     scanning feature spring-web for exports
[INFO]     scanning feature spring for exports
[INFO]     scanning feature spring-aspects for exports
[INFO]     scanning feature spring-dm for exports
[INFO]     scanning feature spring-dm-web for exports
[INFO]     scanning feature spring-instrument for exports
[INFO]     scanning feature spring-jdbc for exports
[INFO]     scanning feature spring-jms for exports
[INFO]     scanning feature spring-struts for exports
[INFO]     scanning feature spring-test for exports
[INFO]     scanning feature spring-orm for exports
[INFO]     scanning feature spring-oxm for exports
[INFO]     scanning feature spring-tx for exports
[INFO]     scanning feature spring-web for exports
[INFO]     scanning feature spring-web-portlet for exports
[INFO]     scanning feature wrapper for exports
[INFO]     scanning feature obr for exports
[INFO]     scanning feature config for exports
[INFO]     scanning feature jetty for exports
[INFO]     scanning feature http for exports
[INFO]     scanning feature war for exports
[INFO]     scanning feature kar for exports
[INFO]     scanning feature webconsole-base for exports
[INFO]     scanning feature webconsole for exports
[INFO]     scanning feature ssh for exports
[INFO]     scanning feature management for exports
[INFO]     scanning feature eventadmin for exports
[INFO]     scanning feature jasypt-encryption for exports
[INFO]  - adding repository from mvn:org.apache.karaf.assemblies.features/enterprise/2.2.5/xml/features
[INFO]  - validation of mvn:org.apache.karaf.assemblies.features/enterprise/2.2.5/xml/features
[INFO]     scanning feature transaction for exports
[INFO]     scanning feature jpa for exports
[INFO]     scanning feature jndi for exports
[INFO]     scanning feature application-without-isolation for exports
[INFO]  - validation of mvn:org.apache.cxf.karaf/apache-cxf/2.5.1/xml/features
[INFO]  - adding 12 known features from mvn:org.apache.cxf.karaf/apache-cxf/2.5.1/xml/features
[INFO]     scanning feature activemq for exports
[INFO]     scanning feature cxf-specs for exports
[INFO]     scanning feature cxf-jaxb for exports
[INFO]     scanning feature cxf-abdera for exports
[INFO]     scanning feature opensaml for exports
[INFO]     scanning feature wss4j for exports
[INFO]     scanning feature cxf-saaj-impl for exports
[INFO]     scanning feature cxf-war-java5 for exports
[INFO]     scanning feature cxf for exports
[INFO]     scanning feature cxf-sts for exports
[INFO]     scanning feature cxf-wsn-api for exports
[INFO]     scanning feature cxf-wsn for exports
[INFO]  - validation of mvn:org.jclouds.karaf/jclouds-karaf/1.2.2/xml/features
[INFO]  - adding 46 known features from mvn:org.jclouds.karaf/jclouds-karaf/1.2.2/xml/features
[INFO]     scanning feature guice for exports
[INFO]     scanning feature jclouds for exports
[INFO]     scanning feature jclouds-compute for exports
[INFO]     scanning feature jclouds-api-filesystem for exports
[INFO]     scanning feature jclouds-api-eucalyptus for exports
[INFO]     scanning feature jclouds-api-elasticstack for exports
[INFO]     scanning feature jclouds-api-vcloud for exports
[INFO]     scanning feature jclouds-driver-jsch for exports
[INFO]     scanning feature jclouds-driver-sshj for exports
[INFO]     scanning feature jclouds-driver-slf4j for exports
[INFO]     scanning feature jclouds-driver-log4j for exports
[INFO]     scanning feature jclouds-driver-bouncycastle for exports
[INFO]     scanning feature jclouds-aws-cloudwatch for exports
[INFO]     scanning feature jclouds-aws-ec2 for exports
[INFO]     scanning feature jclouds-aws-s3 for exports
[INFO]     scanning feature jclouds-azureblob for exports
[INFO]     scanning feature jclouds-bluelock-vcloud-zone01 for exports
[INFO]     scanning feature jclouds-cloudfiles-uk for exports
[INFO]     scanning feature jclouds-cloudfiles-us for exports
[INFO]     scanning feature jclouds-cloudloadbalancers-us for exports
[INFO]     scanning feature jclouds-cloudonestorage for exports
[INFO]     scanning feature jclouds-cloudserver-uk for exports
[INFO]     scanning feature jclouds-cloudserver-us for exports
[INFO]     scanning feature jclouds-cloudsigma-zrh for exports
[INFO]     scanning feature jclouds-elastichosts-lon-b for exports
[INFO]     scanning feature jclouds-elastichosts-lon-p for exports
[INFO]     scanning feature jclouds-elastichosts-sat-p for exports
[INFO]     scanning feature jclouds-eucalyptus-s3 for exports
[INFO]     scanning feature jclouds-eucalyptus-ec2 for exports
[INFO]     scanning feature jclouds-gogrid for exports
[INFO]     scanning feature jclouds-go2cloud-jhb1 for exports
[INFO]     scanning feature jclouds-greenhousedata-element-vcloud for exports
[INFO]     scanning feature jclouds-ninefold-storage for exports
[INFO]     scanning feature jclouds-openhosting-east1 for exports
[INFO]     scanning feature jclouds-rimuhosting for exports
[INFO]     scanning feature jclouds-savvis-symphonyvpdc for exports
[INFO]     scanning feature jclouds-serverlove-z1-man for exports
[INFO]     scanning feature jclouds-skalicloud-sdg-my for exports
[INFO]     scanning feature jclouds-softlayer for exports
[INFO]     scanning feature jclouds-slicehost for exports
[INFO]     scanning feature jclouds-synaptic-storage for exports
[INFO]     scanning feature jclouds-stratogen-vcloud-mycloud for exports
[INFO]     scanning feature jclouds-trmk-ecloud for exports
[INFO]     scanning feature jclouds-trmk-vcloudexpress for exports
[INFO]     scanning feature jclouds-services for exports
[INFO]     scanning feature jclouds-commands for exports
[INFO] == Validating feature descriptor ==
[INFO]  - validating 105 features
[INFO]  - step 1: Checking if all artifacts exist
[INFO]     OK: all 500 OSGi bundles have been found
[INFO]  - step 2: Checking if all imports for bundles can be resolved
[INFO]     OK: imports resolved for xml-specs-api
[INFO]     OK: imports resolved for camel-core
[INFO]     OK: imports resolved for camel-spring
[INFO]     OK: imports resolved for camel
[INFO]     OK: imports resolved for camel-blueprint
[INFO]     OK: imports resolved for camel-test
[INFO]     OK: imports resolved for camel-context
[INFO]     OK: imports resolved for camel-cxf
[INFO]     OK: imports resolved for camel-cache
[INFO]     OK: imports resolved for camel-castor
[INFO]     OK: imports resolved for camel-crypto
[INFO]     OK: imports resolved for camel-http
[INFO]     OK: imports resolved for camel-http4
[INFO]     OK: imports resolved for camel-mina
[INFO]     OK: imports resolved for camel-mina2
[INFO]     OK: imports resolved for camel-jetty
[INFO]     OK: imports resolved for camel-servlet
[INFO]     OK: imports resolved for camel-jms
[INFO]     OK: imports resolved for camel-jmx
[INFO]     OK: imports resolved for camel-ahc
[INFO]     OK: imports resolved for camel-amqp
[INFO]     OK: imports resolved for camel-atom
[INFO]     OK: imports resolved for camel-aws
[INFO]     OK: imports resolved for camel-bam
[INFO]     OK: imports resolved for camel-bean-validator
[INFO]     OK: imports resolved for camel-bindy
[INFO]     OK: imports resolved for camel-cometd
[INFO]     OK: imports resolved for camel-csv
[INFO]     OK: imports resolved for camel-dozer
[INFO]     OK: imports resolved for camel-eventadmin
[INFO]     OK: imports resolved for camel-exec
[INFO]     OK: imports resolved for camel-flatpack
[INFO]     OK: imports resolved for camel-freemarker
[INFO]     OK: imports resolved for camel-ftp
[INFO]     OK: imports resolved for camel-guice
[INFO]     OK: imports resolved for camel-groovy
[INFO]     OK: imports resolved for camel-hazelcast
[INFO]     OK: imports resolved for camel-hawtdb
[INFO]     OK: imports resolved for camel-hdfs
[INFO]     OK: imports resolved for camel-hl7
[INFO]     OK: imports resolved for camel-ibatis
[INFO]     OK: imports resolved for camel-irc
[INFO]     OK: imports resolved for camel-jackson
[INFO]     OK: imports resolved for camel-jasypt
[INFO]     OK: imports resolved for camel-jaxb
[INFO]     OK: imports resolved for camel-jclouds
[INFO]     OK: imports resolved for camel-jcr
[INFO]     OK: imports resolved for camel-jing
[INFO]     OK: imports resolved for camel-jibx
[INFO]     OK: imports resolved for camel-jdbc
[INFO]     OK: imports resolved for camel-josql
[INFO]     OK: imports resolved for camel-jpa
[INFO]     OK: imports resolved for camel-jxpath
[INFO]     OK: imports resolved for camel-juel
[INFO]     OK: imports resolved for camel-kestrel
[INFO]     OK: imports resolved for camel-krati
[INFO]     OK: imports resolved for camel-ldap
[INFO]     OK: imports resolved for camel-lucene
[INFO]     OK: imports resolved for camel-mail
[INFO]     OK: imports resolved for camel-msv
[INFO]     OK: imports resolved for camel-mvel
[INFO]     OK: imports resolved for camel-mybatis
[INFO]     OK: imports resolved for camel-nagios
[INFO]     OK: imports resolved for camel-netty
[INFO]     OK: imports resolved for camel-ognl
[INFO]     OK: imports resolved for camel-paxlogging
[INFO]     OK: imports resolved for camel-printer
[INFO]     OK: imports resolved for camel-protobuf
[INFO]     OK: imports resolved for camel-quartz
[INFO]     OK: imports resolved for camel-quickfix
[INFO]     OK: imports resolved for camel-restlet
[INFO]     OK: imports resolved for camel-rmi
[INFO]     OK: imports resolved for camel-routebox
[INFO]     OK: imports resolved for camel-script
[INFO]     OK: imports resolved for camel-ruby
[INFO]     OK: imports resolved for camel-rss
[INFO]     OK: imports resolved for camel-saxon
[INFO]     OK: imports resolved for camel-scala
[INFO]     OK: imports resolved for camel-script-jruby
[INFO]     OK: imports resolved for camel-script-javascript
[INFO]     OK: imports resolved for camel-script-groovy
[INFO]     OK: imports resolved for camel-sip
[INFO]     OK: imports resolved for camel-shiro
[INFO]     OK: imports resolved for camel-smpp
[INFO]     OK: imports resolved for camel-snmp
[INFO]     OK: imports resolved for camel-soap
[INFO]     OK: imports resolved for camel-solr
[INFO]     OK: imports resolved for camel-spring-integration
[INFO]     OK: imports resolved for camel-spring-javaconfig
[INFO]     OK: imports resolved for camel-spring-security
[INFO]     OK: imports resolved for camel-spring-ws
[INFO]     OK: imports resolved for camel-sql
[INFO]     OK: imports resolved for camel-stax
[INFO]     OK: imports resolved for camel-stream
[INFO]     OK: imports resolved for camel-string-template
[INFO]     OK: imports resolved for camel-syslog
[INFO]     OK: imports resolved for camel-tagsoup
[INFO]     OK: imports resolved for camel-twitter
[INFO]     OK: imports resolved for camel-velocity
[WARNING] Failed to validate feature camel-websocket
[WARNING] No export found to match org.eclipse.jetty.websocket;version=""[7.0,8)"" (imported by mvn:org.apache.camel/camel-websocket/2.10-SNAPSHOT)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Camel :: Platforms :: Apache Karaf ................ SUCCESS [2.190s]
[INFO] Camel :: Platforms :: Apache Karaf :: Commands .... SUCCESS [8.200s]
[INFO] Camel :: Platforms :: Apache Karaf :: Features .... FAILURE [3.677s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 15.020s
[INFO] Finished at: Sat Jan 14 17:09:24 CET 2012
[INFO] Final Memory: 16M/81M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.karaf.tooling:features-maven-plugin:2.2.5:validate (validate) on project apache-camel: Unable to validate /Users/cmueller/workspaceCamel/camel/platforms/karaf/features/target/classes/features.xml: 1 unresolved imports in feature camel-websocket -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :apache-camel
{noformat}",muellerc,muellerc,Major,Closed,Fixed,14/Jan/12 16:11,14/Jan/12 17:48
Bug,CAMEL-4900,12538454,Quartz component doesn't start with a persistent scheduler,"I have scheduler persistence on but clustering is not turned on. When starting my app the first time, it starts OK. After shutting down and staring a second time, an exception is thrown and the app doesn't start up. This did not occur in 2.8.3. The following exception occurs:

Caused by: org.quartz.SchedulerException: Trigger's related Job's name cannot be null
	at org.quartz.Trigger.validate(Trigger.java:955) ~[quartz-1.8.5.jar:na]
	at org.quartz.core.QuartzScheduler.rescheduleJob(QuartzScheduler.java:982) ~[quartz-1.8.5.jar:na]
	at org.quartz.impl.StdScheduler.rescheduleJob(StdScheduler.java:302) ~[quartz-1.8.5.jar:na]
	at org.apache.camel.component.quartz.QuartzComponent.doAddJob(QuartzComponent.java:233) ~[camel-quartz-2.9.0.jar:2.9.0]

In 2.9.0, the code was changed here and this is where it craashes:
{code}
if (!isClustered()) {
    trigger.setStartTime(Calendar.getInstance().getTime());
    scheduler.rescheduleJob(trigger.getName(), trigger.getGroup(), trigger);
}
{code}

My component definition looks like this:

{code}
<endpoint id=""myschedule"" uri=""quartz://fmc/mycron=${myschedule.cron}&amp;stateful=true"" />
{code}

Again, this affects 2.9.0 but not 2.8.3. I have not found a workaround yet.
",davsclaus,bryanck,Major,Resolved,Fixed,15/Jan/12 20:04,16/Mar/12 15:06
Bug,CAMEL-4902,12538491,path to xsd in spring.schemas not equivalent to path on http site,"in spring.schemas we have line
http\://camel.apache.org/schema/cxf/camel-cxf-2.9.0.xsd=schema/cxfEndpoint.xsd
but there isn't such file on http://camel.apache.org/schema/cxf/ only camel-cxf.xsd and camel-cxf-2.9.0-spring.xsd
please make it correct.",njiang,akudrevatych,Major,Closed,Fixed,16/Jan/12 08:35,17/Jan/12 09:23
Bug,CAMEL-4904,12538560,"org.apache.camel.util.concurrent.ExecutorServiceHelper.getThreadName(String, String) method throws IllegalArgumentException when name parameter contains '$', ""${"" or '}'","When the ""name"" parameter contains a '$', ""${"" or '}' substring, the getThreadName method will throw an erroneous IllegalArgumentException.",davsclaus,saucier,Minor,Resolved,Fixed,16/Jan/12 19:29,05/Apr/12 17:10
Bug,CAMEL-4908,12538625,Twitter component fixes,"Fix ConcurrentModificationException for StreamingConsumer.
Fix NPE for FilterConsumer
Use additional query parameters for FilterConsumer like userIds, keywords.
I didn't added tests for these fixes, because currently it seems like it connects to the internet and does real twitter queries. I will check this in a separate issue. ",bibryam,bibryam,Minor,Resolved,Fixed,17/Jan/12 13:21,24/Jan/12 08:45
Bug,CAMEL-4913,12538775,camel-blueprint - Using namespaces with xpath does not work as the namesapces is not parsed and injected into the camel model,"Using xpath predicates in a blueprint xml file does not work if you use custom namespaces, as the camel-blueprint component does not detect the custom namespaces, as camel-spring does.",davsclaus,davsclaus,Major,Resolved,Fixed,18/Jan/12 11:00,18/Jan/12 11:25
Bug,CAMEL-4915,12538806,Serialisation Data Format is unable to unmarshal in OSGi environment,"This happens because camel-core doesn't have access to custom classes. Maybe this data format should be isolated into it's own package and have DynamicImport-Package: * in its manifest headers. That way it will be able to access any class from the environment.

Another possibility is to dig deeper into what classloader is being used to resolve the class. It seems like the camel-core classloader is used rather than the TCCL. Possible culprit could be http://docs.oracle.com/javase/6/docs/api/java/io/ObjectInputStream.html#resolveClass(java.io.ObjectStreamClass).",davsclaus,raulvk,Minor,Resolved,Fixed,18/Jan/12 14:13,19/Jan/12 10:51
Bug,CAMEL-4920,12538967,PollEnrich in default mode should block,"Currently it uses receiveNoWait, it should use receive (eg block). You should use timeout = -1 for receiveNoWait. This is also what we write in the documentation and javadoc.",davsclaus,davsclaus,Minor,Resolved,Fixed,19/Jan/12 13:49,19/Jan/12 19:39
Bug,CAMEL-4925,12539115,ThreadsProcessor configured with ExecutorService with DiscardPolicy or DiscardOldestPolicy leaves inflight exchanges for discarded tasks unprocessed.,"ThreadsProcessor configured with ExecutorService with DiscardPolicy or DiscardOldestPolicy leaves inflight exchanges for discarded tasks unprocessed.

Here is the code from ThreadsProcessor. In case of DiscardPolicy or DiscardOldestPolicy executorService will no throw RejectedExecutionException, so exchange remains unprocessed and count of inflight exchanges will not be decremented for such discarded exchanges.

{code:java|title=ThreadsProcessor#process(Exchange, AsyncCallback)}
public boolean process(Exchange exchange, AsyncCallback callback) {
    if (shutdown.get()) {
        throw new IllegalStateException(""ThreadsProcessor is not running."");
    }

    ProcessCall call = new ProcessCall(exchange, callback);
    try {
        executorService.submit(call);
        // tell Camel routing engine we continue routing asynchronous
        return false;
    } catch (RejectedExecutionException e) {
        if (isCallerRunsWhenRejected()) {
            if (shutdown.get()) {
                exchange.setException(new RejectedExecutionException());
            } else {
                callback.done(true);
            }
        } else {
            exchange.setException(e);
        }
        return true;
    }
}
{code}

Unit test is attached.",davsclaus,szhemzhitsky,Minor,Resolved,Fixed,20/Jan/12 10:02,23/Jan/12 17:00
Bug,CAMEL-4926,12539187,HTTP TRACE should be disabled by default,"Jetty, Apache HTTP, have HTTP TRACE disabled by default. We should also do this in Camel. ",janstey,janstey,Major,Resolved,Fixed,20/Jan/12 18:04,20/Jan/12 18:47
Bug,CAMEL-4938,12539678,ManagedBrowsableEndpoint are broken,"ManagedeBrowsableEndpoint are broken which means any component that inherit ManagedBrowsableEndpoint have errors as well. Please see the link below which will have error screen shot of queueSize JMX operations.


http://camel.465427.n5.nabble.com/JMX-queue-size-operation-fails-Camel-2-9-0-td5233089.html",bvahdat,v_cheruvu@hotmail.com,Major,Resolved,Fixed,24/Jan/12 22:06,30/Jan/12 17:22
Bug,CAMEL-4942,12539865,exchange.getIn().getBody(<SomeClass>.class) throws an exception for Jaxb model objects,"{code}
exchange.getIn().getBody(Foo.class);
{code}

should return a Foo instance or null, if no suitable type converter is available. The java doc says: ""return the body of the message as the specified type, or <tt>null</tt> if not possible to convert""

If Foo is an Jaxb object model, this method call:
- throws an org.xml.sax.SAXParseException if the body content is a string
- throws a javax.xml.bind.UnmarshalException if the body content is another Jaxb model object

Please have a look on the attached sample eclipse project.",bvahdat,muellerc,Major,Resolved,Fixed,26/Jan/12 06:15,27/Jan/12 20:13
Bug,CAMEL-4945,12539895,CXF Producer calls done method of Camel callback several times,"If the CXF Producer of Camel is called asynchronously for a WS-RM endpoint, Camel receives multiple final events, like ExchangeCompletedEvent and ExchangeFailedEvent. This leads also to negative JMX counters for inflight exchanges. The root cause is a bug in the handleResponse and handleException methods of org.apache.camel.component.cxf.CxfClientCallback. In those methods, which are called for WS-RM communication, although it is oneway, the done method of camelAsyncCallback is called also for oneway exchanges. However this callback method is already called in the process method of org.apache.camel.component.cxf.CxfProducer for oneway exchanges, which causes the mentioned effects.
The following changes in org.apache.camel.component.cxf.CxfClientCallback can fix this:
    public void handleResponse(Map<String, Object> ctx, Object[] res) {
        try {
            super.handleResponse(ctx, res);            
        } finally {
            // bind the CXF response to Camel exchange
            if (!boi.getOperationInfo().isOneWay()) {
                // copy the InMessage header to OutMessage header
                camelExchange.getOut().getHeaders().putAll(camelExchange.getIn().getHeaders());
                binding.populateExchangeFromCxfResponse(camelExchange, cxfExchange, ctx);
				camelAsyncCallback.done(false);
            }
            if (LOG.isDebugEnabled()) {
                LOG.debug(""{} calling handleResponse"", Thread.currentThread().getName());
            }
        }
    }
    
    public void handleException(Map<String, Object> ctx, Throwable ex) {
        try {
            super.handleException(ctx, ex);
            camelExchange.setException(ex);
        } finally {
            // copy the context information
            if (!boi.getOperationInfo().isOneWay()) {
                // copy the InMessage header to OutMessage header
                camelExchange.getOut().getHeaders().putAll(camelExchange.getIn().getHeaders());
                binding.populateExchangeFromCxfResponse(camelExchange, cxfExchange, ctx);
				camelAsyncCallback.done(false);
            }
            if (LOG.isDebugEnabled()) {
                LOG.debug(""{} calling handleException"", Thread.currentThread().getName());
            }
        }
    }        
",njiang,stefanbo72,Major,Resolved,Fixed,26/Jan/12 13:38,01/Feb/12 09:00
Bug,CAMEL-4947,12540028,Delay using bean does not work in XML DSL,"Reported on the user forum
",davsclaus,davsclaus,Minor,Resolved,Fixed,27/Jan/12 11:48,27/Jan/12 13:19
Bug,CAMEL-4949,12540200,On shutdown JmsConsumer doesn't shut down direct subroutes properly and loops forever,"Our route looks like this:

{code}
from(""activemq:start"")
   .to(""direct:dir"")
{code}

On shutdown, if there is a message in the direct: route, Camel will wait indefinitely for the message to finish. Regardless of the shutdown strategy.

See attached unit test.",davsclaus,dragisak,Major,Resolved,Fixed,27/Jan/12 21:45,02/May/13 02:29
Bug,CAMEL-4954,12540342,Camel 2.9.0 incapable of working with % in endpoint URIs,"In the class org.apache.camel.util.URISupport which will be used to resolve endpoints (DefaultCamelContext#normalizeEndpointUri) the method parseParameters will be called.
At first the java.net.Uri#getQuery will be called with according to the javadoc ""Returns the decoded query component of this URI"" returns a decoded URI. If that fails the java.net.Uri#getSchemeSpecificPart method will be called which according to the javadoc ""Returns the decoded scheme-specific part of this URI."" returns a decoded URI.
So to summarize we get in any case a decoded URI.
This URI will then be than in the method org.apache.camel.util.URISupport#parseQuery(String) again decoded with java.net.URLDecoder#decode(String,String).
This code leads to the following behaviour:
 If a % is properly encoded with %25test the %25test will be substituted by the first call to %test and the decoded again which leads to an Exception.

In the http://svn.apache.org/viewvc?view=revision&revision=1166508 commit you can see that the % was uncommented from org.apache.camel.util#UnsafeUriCharactersEncoder. Maybe this is related.

However... Double encoding of URIs seems quite odd. With any URI char there is no issue with that. But with % the % will be decoded again, which makes a % unusable in Camel.",hadrian,sruehl,Critical,Resolved,Fixed,30/Jan/12 10:26,18/Mar/13 18:15
Bug,CAMEL-4959,12540504,Incorrect caching type converter misses for NaN,"When converting Double or Float with value NaN, org.apache.camel.converter.ObjectConverter returns ""null"". But org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(Class, Exchange, Object) interpret this ""null"" as ""suitable conversion not found"" and cache misses. This lead to completely forgetting of conversion for given types.
For example, when conversing Double to Long, all works until Double is NaN. After that, conversion for ""Double-to-Long"" marked as misses. And  camel stop do any conversion for Double-to-Long until restart.

Possible solution is to modify ObjectConverter`s methods to return ""Void.TYPE"" instead of ""null"" for NaN.",davsclaus,ciand7,Minor,Resolved,Fixed,31/Jan/12 07:06,28/Feb/12 11:43
Bug,CAMEL-4960,12540519,NettyProducer with user defined client pipeline factory ,"If user defined client pipeline factory is configured, Netty Producer shares the same instance of ClientPipelineFactory for all created connections. See code below:
  configuration.getClientPipelineFactory().setProducer(this);
  configuration.getClientPipelineFactory().setExchange(exchange);
  configuration.getClientPipelineFactory().setCallback(callback);
  clientPipeline = configuration.getClientPipelineFactory().getPipeline();

Which can cause an issue in multithreaded environment. Maybe it is better to create user defined client pipeline factory for each connection as it is done with DefaultClientPipelineFactory?",davsclaus,dmitrijz,Major,Resolved,Fixed,31/Jan/12 09:16,08/Apr/12 13:49
Bug,CAMEL-4970,12540919,Cannot use xquery predicate in filter after an xpath splitter.  ,"After upgrading from 2.5.0 to 2.9.0 using xqury predicate after an xpath filter throws  java.lang.ClassCastException: net.sf.saxon.tinytree.TinyElementImpl cannot be cast to net.sf.saxon.om.DocumentInfo . Attached unit test doesn't fail in camel 2.5.0.

from(""direct:xpath"") 
.split(xpathsplitter) 
.filter().xquery(""//record[type=2]"") 
.to(""mock:result"");    	

See nabble discussion http://camel.465427.n5.nabble.com/Unit-test-fails-after-upgrading-to-2-7-1-Cannot-use-xquery-expression-after-xpath-td4422582.html",njiang,ltsall,Major,Resolved,Fixed,02/Feb/12 13:18,06/Feb/12 15:12
Bug,CAMEL-4973,12541043,Camel CXF Transport should update the content-type as other CXF transport does,"If you want to leverage the camel transport for CXF to route the message with sub camel context, you may face an issue that camel transport doesn't setup the content type with the encoding information like this ""text/xml; charset=UTF-8""
It just sets the Content-Type like this ""text/xml"" which fails WS-I validation. 
",njiang,njiang,Major,Resolved,Fixed,03/Feb/12 08:08,03/Feb/12 14:46
Bug,CAMEL-4976,12541151,Problem Removing Endpoints ,"(see context on the [users@|http://camel.465427.n5.nabble.com/Problem-Removing-Endpoints-td5455029.html] list)

When using removeEndpoints(String) with a pattern, some endpoints (like jms topics) are reported as removed but they are not.
",hadrian,hadrian,Major,Resolved,Fixed,03/Feb/12 23:17,07/Feb/12 02:40
Bug,CAMEL-4984,12541384,camel-http does not send header values that are empty strings,"HttpProducer does not send HTTP header if header value from camel was an empty string.

This is because ObjectHelper.createIterator(headerValue, null) will produce an empty iteration if headerValue is an empty string!",davsclaus,falkmarcus,Minor,Resolved,Fixed,06/Feb/12 09:32,27/Feb/12 09:00
Bug,CAMEL-4985,12541390,Spring security example does not work,"When you try to run the spring security example and access it from a browser you get 404. I debugged into it and the reason is that the camel context does not attach to the camel http servlet.
",cschneider,cschneider,Major,Resolved,Fixed,06/Feb/12 10:36,10/Feb/12 05:25
Bug,CAMEL-4986,12541405,ManagedPerformanceCounterMBean should extend ManagedCounterMBean,"ManagedPerformanceCounterMBean does not extend ManagedCounterMBean, while ManagedPerformanceCounter does extend ManagedCounter.",bvahdat,eamelink,Minor,Resolved,Fixed,06/Feb/12 13:02,07/Feb/12 16:07
Bug,CAMEL-4988,12541628,Problems with Simple language,"If you don't use a simple expresion that falls back to the SimpleBackwardsCompatibleParser, the simple language doesn't seem to work. If I override the start and end token with an explicit definition of the simplelanguage everything works well.

  <bean id=""simple"" class=""org.apache.camel.language.simple.SimpleLanguage"">
      <constructor-arg name=""functionStartToken"" value=""${""/>
      <constructor-arg name=""functionEndToken"" value=""}""/>
  </bean> 

It seems that org.apache.camel.language.simple.SimpleExpressionParser.SimpleExpressionParser(String) fails to create an expression because startToken and endToken are ""automagically"" changed for strange Strings. It seems that when exchange.getContext().resolveLanguage(""simple"") tries to resolve the languaje it doesn't use the default construtor and injects a strange start and end token in the constructor SimpleLanguage(String functionStartToken, String functionEndToken).",davsclaus,dcondevigo,Major,Resolved,Fixed,07/Feb/12 14:08,23/Feb/12 12:14
Bug,CAMEL-4989,12541633,BindyDataFormat should not use the default encoding,"Here is the mail thread[1] which discuss about it.
[1]http://camel.465427.n5.nabble.com/Bindy-CSV-parser-uses-default-encoding-tp5456136p5456136.html",njiang,njiang,Major,Resolved,Fixed,07/Feb/12 15:07,08/Feb/12 01:26
Bug,CAMEL-4996,12541919,DigitalSignatureEndpoint not clearing headers after verify,"The DigitalSignatureEndpoint has a clearHeaders option that is supposed to clear headers after a verify operation. However that is not working because the headers cleared are not the actual header values, but the name of the java String fields, which is irrelevant at runtime.

The default value of the clearHeaders should also be true, so that Camel does not leave debris in the Exchange after a verify operation.",hadrian,hadrian,Major,Resolved,Fixed,09/Feb/12 03:31,09/Feb/12 03:51
Bug,CAMEL-4999,12542145,Upgrade from 2.8.3 to 2.9.0 breaks the schema validation capability in the Camel Validation component ,"A Camel user reported the following in the Camel User Forum issue
http://camel.465427.n5.nabble.com/Validation-error-after-upgrade-from-Camel-2-8-3-to-2-9-0-td5472816.html

Details 
--------
I have a route that use 
http://camel.apache.org/validation.html


after upgrade Camel from 2.8.3 to 2.9.0 during validation I receive 
the following error 
by: org.xml.sax.SAXParseException: src-resolve: Cannot resolve the 
name 'ecc:PositionType' to a(n) 'type definition' component. 
        at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown 
Source)[:] 
        at org.apache.xerces.util.ErrorHandlerWrapper.error(Unknown Source)[:] 
        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDHandler.getGlobalDecl(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseNamedElement(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseLocal(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDHandler.traverseLocalElements(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.traversers.XSDHandler.parseSchema(Unknown 
Source)[:] 
        at org.apache.xerces.impl.xs.XMLSchemaLoader.loadSchema(Unknown Source)[:] 
        at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:] 
        at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:] 
        at org.apache.xerces.jaxp.validation.XMLSchemaFactory.newSchema(Unknown 
Source)[:] 
        at javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:524)[:] 
        at org.apache.camel.processor.validation.ValidatingProcessor.createSchema(ValidatingProcessor.java:231)[90:org.apache.camel.camel-core:2.9.0] 
        at org.apache.camel.processor.validation.ValidatingProcessor.loadSchema(ValidatingProcessor.java:105)[90:org.apache.camel.camel-core:2.9.0] 
        at org.apache.camel.component.validator.ValidatorComponent.createEndpoint(ValidatorComponent.java:55)[90:org.apache.camel.camel-core:2.9.0] 
        at org.apache.camel.impl.DefaultComponent.createEndpoint(DefaultComponent.java:91)[90:org.apache.camel.camel-core:2.9.0] 
        at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:432)[90:org.apache.camel.camel-core:2.9.0] 

",davsclaus,akarpe,Major,Resolved,Fixed,10/Feb/12 15:51,11/Feb/12 13:38
Bug,CAMEL-5000,12542227,Error handler not called when recipient list has non existing endpoint,"Given this route
{code}

                onException(Exception.class)
                        .handled(true)
                        .to(""mock:dead"");

                from(""direct:start"")
                    .recipientList(constant(""fail:endpoint""))
                    .to(""mock:result"");
{code}

You would assume the on exception will handle the exception from the recipient list with the non existing endpoint.
This is not the case. We should let recipient list detect and let the error handler handle it.",davsclaus,davsclaus,Major,Resolved,Fixed,11/Feb/12 12:16,11/Feb/12 13:19
Bug,CAMEL-5004,12542537,Camel simple language wrong working with real number,"I compute real number in route:
   <setBody>0.02</setBody>
After that I want to filter:
   <filter><simple>${body} > 0 </simple> ... </filter>
This expression return false for value in body = 0.02",davsclaus,akudinov,Major,Resolved,Fixed,14/Feb/12 14:35,16/Feb/12 07:03
Bug,CAMEL-5005,12542549,simple without embedded text results in an NPE,"After switching to 2.9 

the following results in a NPE

{code}
<camel:setBody>
	<camel:simple></camel:simple>
</camel:setBody>
{code}

I'm not sure if it is now required to have something in the content of the simple element. 
I used it for setting the body element to an empty string before (worked with 2.8.1)",davsclaus,achim_nierbeck,Minor,Resolved,Fixed,14/Feb/12 15:08,15/Feb/12 15:00
Bug,CAMEL-5006,12542591,"camel-jms component,  request/reply results in NullPointer when JMSCorrelationID is set to empty string on inbound JMS message. ","A request/reply invocation results in the following exception when JMSCorrelationID is set to empty String ("""") on the inbound JMS message. 

{code}
[JmsConsumer[myqueue]] DefaultErrorHandler ERROR Failed delivery for exchangeId: ID-myexchange-id. Exhausted after delivery attempt: 1 caught: java.lang.NullPointerException
java.lang.NullPointerException
at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:882)[:1.6.0_30]
at org.apache.camel.util.DefaultTimeoutMap.put(DefaultTimeoutMap.java:99)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.reply.CorrelationMap.put(CorrelationMap.java:68)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.reply.TemporaryQueueReplyManager.registerReply(TemporaryQueueReplyManager.java:42)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsProducer$1.createMessage(JmsProducer.java:157)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate.doSendToDestination(JmsConfiguration.java:199)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate.access$100(JmsConfiguration.java:142)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate$1.doInJms(JmsConfiguration.java:156)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.springframework.jms.core.JmsTemplate.execute(JmsTemplate.java:466)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate.send(JmsConfiguration.java:153)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsProducer.doSend(JmsProducer.java:343)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsProducer.processInOut(JmsProducer.java:163)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.JmsProducer.process(JmsProducer.java:98)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:114)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:284)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:109)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.fabric.FabricTraceProcessor.process(FabricTraceProcessor.java:59)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:318)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:209)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:306)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.Pipeline.process(Pipeline.java:116)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.Pipeline.process(Pipeline.java:79)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:139)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:106)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:104)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)[camel-core-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.apache.camel.component.jms.EndpointMessageListener.onMessage(EndpointMessageListener.java:91)[camel-jms-2.8.0-fuse-01-20.jar:2.8.0-fuse-01-20]
at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:560)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:498)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:467)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:325)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:263)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1058)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1050)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:947)[spring-jms-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)[:1.6.0_30]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)[:1.6.0_30]
at java.lang.Thread.run(Thread.java:662)[:1.6.0_30]

{code}

I have also tested this against Trunk version 1239497 
",hadrian,pgfox,Minor,Resolved,Fixed,14/Feb/12 20:17,15/Feb/12 01:36
Bug,CAMEL-5009,12542681,Can't use the resourceClass Parameter of CXF-RS Component,"When I use the resourceClass parameter as described in the docs, I get an unexpected error:

import javax.ws.rs.Path;
import javax.ws.rs.PathParam;

@Path(""/sample/"")
public final class SampleResource {

  @Path(""/{param1}/{param2}"")
  public Object sampleOperation(@PathParam(""param1"") final String param1, @PathParam(""param2"") final String param2) {
    return null;
  }

}

import org.apache.camel.builder.RouteBuilder;

public final class SampleRouteBuilder extends RouteBuilder {

  @Override
  public void configure() {
    from(""cxfrs:http://0.0.0.0:8080?resourceClass=SampleResource"")...
  }

}

The workaround has been using the resourceClasses parameter instead.",njiang,fribeiro,Major,Resolved,Fixed,15/Feb/12 13:54,16/Feb/12 16:43
Bug,CAMEL-5021,12543404,camel-file: file is renamed but error handling continues.,"Symptoms: 
# Exception is thrown from the child route with NoErrorHandler configured.
# Parent and child routes are linked with async. endpoints (seda, vm, nmr, etc.), which are configured to behave *synchronously* (attribute *waitForTaskToComplete=Always* in seda, vm-endpoints and attribute *synchronous=true* in nmr-endpoint).

The behavior with nmr endpoint is almost the same except for the next file is picked up before the lock on the previous one is released.

Here is a unit test to reproduce the issue:

{code:java|title=org.foo.bar.FileRedeliveryWithoutErrorHandlerTest.java}
package org.foo.bar;

import org.apache.camel.LoggingLevel;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.component.mock.MockEndpoint;
import org.apache.camel.impl.JndiRegistry;
import org.apache.camel.processor.RedeliveryPolicy;
import org.apache.camel.test.junit4.CamelTestSupport;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.io.File;

import static org.hamcrest.CoreMatchers.equalTo;

public class FileRedeliveryWithoutErrorHandlerTest extends CamelTestSupport {

    private File newFile;
    private File errorFile;
    
    @Before
    @SuppressWarnings(""ResultOfMethodCallIgnored"")
    public void createFile() throws Exception {
        newFile = new File(""target/files/in/newFile.txt"");
        newFile.createNewFile();
        errorFile = new File(""target/files/in/.error/newFile.txt"");
    }
    
    @After
    @SuppressWarnings(""ResultOfMethodCallIgnored"")
    public void deleteFile() throws Exception {
        newFile.delete();
        errorFile.delete();
    }
    
    @Test
    public void testFileRedeliveryWithoutErrorHandler() throws Exception {
        MockEndpoint result = getMockEndpoint(""mock:result"");
        result.setExpectedMessageCount(1);

        result.assertIsNotSatisfied();
        
        // created file have to exist because redelivery attempts are not completed
        assertThat(newFile.exists(), equalTo(true));
        assertThat(errorFile.exists(), equalTo(false));
    }

    @Override
    protected JndiRegistry createRegistry() throws Exception {
        JndiRegistry registry = super.createRegistry();

        RedeliveryPolicy policy = new RedeliveryPolicy();
        policy.setAsyncDelayedRedelivery(false);
        policy.setLogRetryStackTrace(true);
        policy.setMaximumRedeliveries(100);
        policy.setMaximumRedeliveryDelay(30000);
        policy.setRedeliveryDelay(1000);
        policy.setUseExponentialBackOff(false);
        policy.setRetryAttemptedLogLevel(LoggingLevel.WARN);
        registry.bind(""redeliveryPolicy"", policy);

        return registry;
    }

    @Override
    protected RouteBuilder createRouteBuilder() throws Exception {
        return new RouteBuilder() {
            @Override
            public void configure() throws Exception {
                errorHandler(defaultErrorHandler())
                    .onException(Exception.class)
                    .redeliveryPolicyRef(""redeliveryPolicy"")
                    .log(LoggingLevel.ERROR, ""Error"");

                from(""file:target/files/in?initialDelay=100&delay=100&move=.backup&moveFailed=.error"")
                    .to(""seda:async?waitForTaskToComplete=Always&size=1"")
                    .to(""mock:result"");
                from(""seda:async"")
                    .errorHandler(noErrorHandler())
                    .delay(1000)
                    .throwException(new RuntimeException(""Hello World!""));
            }
        };
    }
}
{code}",davsclaus,szhemzhitsky,Major,Resolved,Fixed,20/Feb/12 21:28,24/Feb/12 06:56
Bug,CAMEL-5022,12543410,camel-twitter feature is invalid,"{noformat}
christian-muellers-macbook-pro:camel cmueller$ cd platforms/karaf/
christian-muellers-macbook-pro:karaf cmueller$ mvn clean install -Pvalidate
...
[WARNING] Failed to validate feature camel-twitter
[WARNING] No export found to match com.google.appengine.api.urlfetch (imported by mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.twitter4j/2.2.5_1)
[WARNING] No export found to match twitter4j.internal.http.alternative (imported by mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.twitter4j/2.2.5_1)
{noformat}",muellerc,muellerc,Major,Closed,Fixed,20/Feb/12 22:18,21/Feb/12 17:43
Bug,CAMEL-5023,12543475,Printer endpoint can only print a single job,"The JavaDoc for javax.print.DocPrintJob.print(Doc, PrintRequestAttributeSet) reads:

""This method should only be called once for a given print job.""

However org.apache.camel.component.printer.PrinterOperations reuses the same DocPrintJob to print over and over again. This results in:

javax.print.PrintException: already printing
	at sun.print.UnixPrintJob.print(UnixPrintJob.java:286) ~[na:1.6.0_29]
	at org.apache.camel.component.printer.PrinterOperations.print(PrinterOperations.java:107) ~[camel-printer-2.9.0.jar:2.9.0]
	at org.apache.camel.component.printer.PrinterOperations.print(PrinterOperations.java:101) ~[camel-printer-2.9.0.jar:2.9.0]
	at org.apache.camel.component.printer.PrinterProducer.print(PrinterProducer.java:55) ~[camel-printer-2.9.0.jar:2.9.0]
	at org.apache.camel.component.printer.PrinterProducer.process(PrinterProducer.java:49) ~[camel-printer-2.9.0.jar:2.9.0]
",njiang,agattik,Major,Resolved,Fixed,21/Feb/12 12:46,22/Feb/12 06:31
Bug,CAMEL-5024,12543482,Streaming splitter ignores exception handling,"If an exception occurs on the next() call of an Iterator in a streaming splitter, the exception is never propagated to the exception or the error handler. This will lead to improper route termination, and inifite rollback/retry cycles.

I attached the minimal test case demonstrating the issue.",njiang,rludvig,Major,Resolved,Fixed,21/Feb/12 13:22,23/Nov/12 13:59
Bug,CAMEL-5027,12543647,JMX statistics problem with some components,"Here is the test class:
{noformat}
public class TestStatistics {
 
    public static void main(String[] args) throws Exception {
       DefaultCamelContext context = new DefaultCamelContext();
       context.addRoutes(new RouteBuilder() {
 
           @Override
           public void configure() throws Exception {
              from(""timer:foo?period=5000"").routeId(""timer"").loadBalance()
                     .roundRobin().id(""load_balance_1"").to(""log:one"")
                     .id(""log_one"").to(""log:two"").id(""log_two"").end();
 
           }
       });
       context.start();
       Thread.sleep(500000);
       context.stop();
    }
 
}
{noformat}

The MBean [org.apache.camel/processors/XXX/load_balancer_1/Attributes/ExchangesTotal] property has no value, therefore no statistics information can be accessed. The same problem exists in Intercept,Log.",davsclaus,salever,Minor,Resolved,Fixed,22/Feb/12 09:17,22/Feb/12 16:49
Bug,CAMEL-5033,12543820,Seda producer should not handover completions if waiting for task to complete,"See CAMEL-5021 which causes the file consumer to rollback the file to early

So if you link the seda producer to be synchronous by using the wait for task to complete, then the task should not have on completions handed over, as we are waiting for that task to complete, and copy its result back.",davsclaus,davsclaus,Minor,Resolved,Fixed,23/Feb/12 08:57,23/Feb/12 09:42
Bug,CAMEL-5035,12543950,CxfProducer should release the CXF when the Producer is stopped.,"CXF ClientImpl has the requestContext need to be cleaned when the CxfProducer is stopped.
If we don't call the Client.destory() method, the requestContext will cause the memory leak.",njiang,njiang,Major,Resolved,Fixed,24/Feb/12 04:30,24/Feb/12 06:18
Bug,CAMEL-5036,12543970,Camel route with bean element sends on startup invalid message to target route that is represented by a proxy who exposes route,"I expose a Camel route with a direct endpoint defined in an OSGi bundle with the <camel:proxy> and <osgi:service> tags. The reason for this is to be able to call this direct-route in other routes (defined in other bundles) as a ""sub-route"" directly as bean [.bean(routeproxy, ""method"")]. This works as expected.

But on route startup the toString method is called on the route (for a description) which is propagated to the elements of the route. Therefore my bean-element (which is the direct-route-proxy) receives the toString call and the CamelInvocationHandler converts this toString call to a message and sends it to the direct-route. This more or less empty message is of course not valid for the target route and produces an error on bundle startup.  

Shouldn't the toString call be suppressed by the proxy, since toString is not part of the Interface the proxy represents? Or is my approach a not recommended or problematic approach?


Parts of stacktrace to show the way from route startup to the produced message

=> route receives ""empty"" message
[omitted]
	at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:45)
	at org.apache.camel.component.bean.CamelInvocationHandler.invoke(CamelInvocationHandler.java:65)
	at $Proxy568.toString(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor406.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
=> toString call is converted to message and sent to route

[omitted]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
	at $Proxy583.toString(Unknown Source)
	at org.apache.camel.component.bean.ConstantBeanHolder.toString(ConstantBeanHolder.java:48)
=> toString called on bean-element in route (bean is a proxy for a route) 

[omitted]
	at java.lang.String.valueOf(String.java:2826)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.camel.impl.EventDrivenConsumerRoute.toString(EventDrivenConsumerRoute.java:46)
	at org.apache.camel.management.mbean.ManagedRoute.<init>(ManagedRoute.java:41)
	at org.apache.camel.management.DefaultManagementLifecycleStrategy.onRoutesAdd(DefaultManagementLifecycleStrategy.java:368)
	at org.apache.camel.impl.RouteService.doStart(RouteService.java:109)
=> Route startup",davsclaus,sburkard,Major,Resolved,Fixed,24/Feb/12 09:44,24/Feb/12 15:43
Bug,CAMEL-5038,12543990,Timer endpoint should support multiple consumers ,"See nabble
http://camel.465427.n5.nabble.com/Multiple-timers-with-same-name-in-same-context-td5505128.html

Basically you may have 2+ routes with the same timer endpoint, as you want to kick-off 2+ tasks at about the same time. The endpoint should just support multiple consumers by default, to let this happen.",davsclaus,davsclaus,Minor,Resolved,Fixed,24/Feb/12 14:03,24/Feb/12 14:37
Bug,CAMEL-5040,12544116,"Thread pools should not be enlisted in JMX if part of adding a new route, and JMX is disabled for new routes","If you have configured registerNewRoutes=false for jmxAgent, then thread pools created during a route, such as a seda consumer, should not be enlisted in JMX.",davsclaus,davsclaus,Minor,Resolved,Fixed,25/Feb/12 12:27,25/Feb/12 12:37
Bug,CAMEL-5041,12544161,"Removing a route with route scoped error handler, should remove the error handler from JMX","A route scoped error handler should be removed if the route is removed from JMX.
Context scoped error handlers, should be kept.",davsclaus,davsclaus,Minor,Resolved,Fixed,26/Feb/12 09:03,02/Mar/12 13:26
Bug,CAMEL-5045,12544171,Memory leak when adding/removing a lot of routes with JMX enabled,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the DefaultManagementLifecycleStrategy will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed.

Memory will accumulate as the map has reference to old objects which cannot be GC.",davsclaus,davsclaus,Major,Resolved,Fixed,26/Feb/12 14:42,27/Feb/12 07:12
Bug,CAMEL-5046,12544226,Memory leak when adding/removing a lot of routes with JMX enabled related to Tracer,"Related to CAMEL-5045 but this ticket is about CAMEL-4500 which introduced a leak on the 2.9 branch on, in relation to tracer.",davsclaus,davsclaus,Minor,Resolved,Fixed,27/Feb/12 07:11,27/Feb/12 12:33
Bug,CAMEL-5048,12544376,Memory leak in SedaComponent if adding and removing a lot of seda endpoints with different queue names,"If you add and remove a lot of seda endpoints with different queue names, then the seda component may keep references to non used queues.
",davsclaus,davsclaus,Minor,Resolved,Fixed,28/Feb/12 05:40,28/Feb/12 09:20
Bug,CAMEL-5049,12544421,Memory leak in TimerListener if JMX enabled,"If adding and removing a lot of routes, and you have JMX enabled with load statistics, then it may leak memory, as the route listener is not properly removed when the route is removed, due invalid equals/hashCode.

",davsclaus,davsclaus,Minor,Resolved,Fixed,28/Feb/12 13:29,28/Feb/12 13:34
Bug,CAMEL-5051,12544453,NullPointerException when assigning a simple Object as final bean,"Considering this route :

from(""direct://myendpoint"").routeId(""myendpoint.route"").bean(new Object());

I get a NPE. I know, this is a very rare case that noone will reach again...
Anyway the fix is very simple, and is attached with it's test case.
",bvahdat,fabdouglas,Trivial,Resolved,Fixed,28/Feb/12 17:51,29/Feb/12 00:29
Bug,CAMEL-5053,12544578,Using a custom thread pool profile does not work with EIP,"See nabble
http://camel.465427.n5.nabble.com/ThreadPoolProfile-and-multicast-tp5524245p5524245.html
",davsclaus,davsclaus,Major,Resolved,Fixed,29/Feb/12 08:54,29/Feb/12 12:26
Bug,CAMEL-5058,12544960,Bug: Unique Endpoints Leaking in DefaultInflightRepository,"If you have an endpoint protocol which uses unique URIs you will leak Strings in the HashMap stored in the DefaultInflightRepository (org.apache.camel.impl.DefaultInflightRepository)

It seems there is a reference counting scheme in place, but it doesn't do a remove until the ""stop"" method is called to shut the system down.  We are running XMPP endpoints, which use a protocol like xmpp://someaccount@domain/password?to=someOtherAccount
When there are 10 million accounts, not all of which are active, but all of which may message at some time or another, no references are removed to the endpointCount.

When the count becomes 0, the reference should be removed and the size method will still return the appropriate result.

Please be careful in the implementation to synchronize on some object (perhaps the AtomicInteger) reflecting a read/write lock on the endpoint count modification.",davsclaus,zilatica,Major,Resolved,Fixed,02/Mar/12 16:35,08/Mar/12 16:08
Bug,CAMEL-5060,12545093,camel-bam is not thread safe and throws ConcurrentModificationException,"camel-bam is not thread safe and throws ConcurrentModificationException:

{noformat}
2012-03-04 14:36:39,528 [d #0 - seda://a] ERROR BamProcessorSupport            - Caught: java.util.ConcurrentModificationException
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$KeyIterator.next(HashMap.java:828)
	at org.apache.camel.bam.model.ProcessInstance.getActivityState(ProcessInstance.java:123)
	at org.apache.camel.bam.TimeExpression.evaluate(TimeExpression.java:55)
	at org.apache.camel.bam.rules.TemporalRule.processExchange(TemporalRule.java:100)
	at org.apache.camel.bam.rules.ActivityRules.processExchange(ActivityRules.java:63)
	at org.apache.camel.bam.rules.ProcessRules.processExchange(ProcessRules.java:44)
	at org.apache.camel.bam.processor.JpaBamProcessor.processEntity(JpaBamProcessor.java:58)
	at org.apache.camel.bam.processor.JpaBamProcessor.processEntity(JpaBamProcessor.java:37)
	at org.apache.camel.bam.processor.BamProcessorSupport$1.doInTransaction(BamProcessorSupport.java:97)
	at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:130)
	at org.apache.camel.bam.processor.BamProcessorSupport.process(BamProcessorSupport.java:89)
	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:333)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:223)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:71)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:261)
	at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:173)
	at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:132)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}",muellerc,muellerc,Major,Closed,Fixed,04/Mar/12 14:15,02/Jun/12 21:00
Bug,CAMEL-5063,12545354,npe in camel 2.9.1 validator,"validator component seems broken in 2.9.1. Gives you a npe when loading xsd from classpath during unittesting.
See http://camel.465427.n5.nabble.com/npe-in-camel-2-9-1-validator-tc5540894.html
prefixing with classpath eg. <to uri=""validator:classpath:BroadcastMonitor.xsd""  /> doesn't help.",davsclaus,preben,Minor,Resolved,Fixed,06/Mar/12 15:27,19/Mar/12 19:04
Bug,CAMEL-5066,12545618,JMX statistics problem with intercept(),"Here is the test class:

{noformat}
public class TestStatistics {
 
    public static void main(String[] args) throws Exception {
       DefaultCamelContext context = new DefaultCamelContext();
       context.addRoutes(new RouteBuilder() {
 
           @Override
           public void configure() throws Exception {
              	intercept().to(""log:intercept"");
		from(""timer://myTimer?period=2000"").setBody()
				.simple(""Current time is ${header.firedTime}"").to(""log:out"");
 
           }
       });
       context.start();
       Thread.sleep(500000);
       context.stop();
    }
}

{noformat}
The MBean [org.apache.camel/processors/XXX/to2/Attributes/ExchangesTotal] property has no value, therefore no statistics information can be accessed.

I used 2.9.2-SNAPSHOT.",davsclaus,salever,Minor,Resolved,Fixed,08/Mar/12 06:35,09/Nov/12 08:59
Bug,CAMEL-5070,12545675,Message Loss when using Weighted Round Robin LoadBalancer,"chooseProcessor method accesses resources in a non synchronized fashion. This leads in errors during loadbalancing and as a result messages are lost. I have created a project that provides an integration test (using karaf 2.2.5 and a custom command to check messages of the activemq broker) with a custom weighted round robin loadbalancer that ""seems"" to solve the issue of lost messages.
 
The problem with the provided solution is that when messages are dequeued from the second stage of queues (queues1, 2 and 3) in custom-loadbalancer-route subproject the jmsconsumer threads also block (checked this using profiler). I would expect only the jmsconsumer threads of the first queue (initial.queue) to block waiting for the synchronized chooseProcessor method. Any clues on why this happens?",akarpe,nikosd23,Major,Resolved,Fixed,08/Mar/12 16:48,10/Mar/12 09:25
Bug,CAMEL-5080,12546272,camel-jt400 - Parameter length when doing Program Call issue,"When the length of each parameter of a program.PGM is greater than the length of his values, the parameters are setted incorrectly.

Example:

Input of program.pgm:
param1 -> A5 (alphanumeric with length 5) 
param2 -> A5 (alphanumeric with length 5) 

When the String array is {""123"", ""456""} the values are setted on each parameter as:
param1 = 12345
param2 = 6

",davsclaus,janacleto,Minor,Resolved,Fixed,13/Mar/12 18:14,15/Mar/12 12:44
Bug,CAMEL-5084,12546398,Error when mocking all endpoints and using cxf,"When upgrading from Apache Camel 2.9.0 to 2.9.1 I suddently get an error when using camel-cxf endpoints. When running my JUnit tests (extending CamelSpringTestSupport) I get this exception:

Caused by: java.lang.NullPointerException
	at org.apache.camel.component.cxf.CxfProducer.getBindingOperationInfo(CxfProducer.java:331)
	at org.apache.camel.component.cxf.CxfProducer.prepareBindingOperation(CxfProducer.java:202)
	at org.apache.camel.component.cxf.CxfProducer.process(CxfProducer.java:141)
	at org.apache.camel.impl.SynchronousDelegateProducer.process(SynchronousDelegateProducer.java:61)
	at org.apache.camel.impl.InterceptSendToEndpoint$1.process(InterceptSendToEndpoint.java:144)
	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:99)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:56)
When I debug my test I see that the method doStart() in CxfProducer is never called, resulting in client being null (hence the NullPointerException in CxfProducer.java:331 ). 

I suspect the mocking all endpoints with the InterceptSendToEndpoint, 
short-circut the startup procedure to not delegate the start call to 
the intercepted endpoint. Or there abouts. 

See discussion in camel user forum: http://camel.465427.n5.nabble.com/Error-using-apache-cxf-w-Camel-2-9-1-tc5560906.html

",njiang,hefiso,Major,Resolved,Fixed,14/Mar/12 14:31,15/Mar/12 08:57
Bug,CAMEL-5085,12546403,Failover EIP - Should use defensive copy of exchange before failover to avoid side effects,"When using the failover load balancer, it works directly on the given exchange, and in case of a failover, it clears the exception state etc.

However this does not work too well as if you use a processor directly then you can mutate the message before failover, which mean when the exchange is failed over, then its a 100% copy of the input message, but the previously mutated message.

We should do like the other EIPs by doing a defensive copy of the exchange.

",davsclaus,davsclaus,Major,Resolved,Fixed,14/Mar/12 14:52,15/Mar/12 10:44
Bug,CAMEL-5086,12546425,"Archetype: camel-archetype-web retains loaded classes after redeployment, causing permgen space to fill up.","Steps to reproduce:
1. Create a new camel web project (based on instructions here: http://camel.apache.org/camel-maven-archetypes.html ):

mvn archetype:generate -DarchetypeGroupId=org.apache.camel.archetypes -DarchetypeArtifactId=camel-archetype-web -DarchetypeVersion=2.9.1 -DarchetypeRepository=https://repository.apache.org/content/groups/snapshots-group

2. Build the new project:
mvn clean install

3. Copy .war file in target/ to tomcat's webapps directory.
4. Reload webapp (easiest way is to touch the .war file)
5. If you are using a profiler, you will see that the permgen space increases on every reload (the previous webapp's classes don't get unloaded)
6. Eventually the webapp container will crash with a permgen out of memory error.

Workaround: restarting the webapp container will reset the permgen space.
Removing the Spring nature of the project makes this go away, so it could be a problem with Spring itself.

This also happens when using Jetty7, so I don't think it is a problem with the container.

",davsclaus,drichelson,Minor,Resolved,Fixed,14/Mar/12 16:53,20/Mar/12 06:39
Bug,CAMEL-5087,12546524,cometD component doesn't support multiple endpoints with different ports,"Current camel-cometD component always use on server and it doesn't support to create multiples endpoints with different ports.

Here is the thread which discuss about it.
[1]http://camel.465427.n5.nabble.com/Error-with-multiple-CometD-components-td5562936.html",njiang,njiang,Major,Resolved,Fixed,15/Mar/12 02:58,15/Mar/12 11:24
Bug,CAMEL-5089,12546534,InterceptSendToMockEndpoint doesn't start or stop the producer which is intercepted,The producer which is intercepted by the InterceptSendToMockEndpoint should manage the lifecycle of  the delegate producer otherwise the delegate producer will complain about it.,njiang,njiang,Major,Resolved,Fixed,15/Mar/12 07:25,15/Mar/12 08:55
Bug,CAMEL-5097,12546711,Incorrect feature definition for camel-fop,,hadrian,hadrian,Minor,Resolved,Fixed,16/Mar/12 02:47,16/Mar/12 02:49
Bug,CAMEL-5098,12546713,Exchange.FAILURE_ENDPOINT header points to incorrect endpoint with multicast,"With multicast when an Exchange is sent to multiple endpoints, the FAILURE_ENDPOINT header points to the last endpoint hit, not the (last) one that actually caused the exception.

I have a fix for it, but I need to see if this happens for other EIPs as well.",hadrian,hadrian,Major,Resolved,Fixed,16/Mar/12 02:54,21/Mar/12 14:55
Bug,CAMEL-5102,12546812,Timer endpoint getState does not work in JMX,The getState attribute does not work.,davsclaus,davsclaus,Trivial,Resolved,Fixed,16/Mar/12 16:19,16/Mar/12 16:26
Bug,CAMEL-5107,12547021,cxfrs producer should retain the message headers when it gets the response,"Here is the discussion[1] about it.
[1]http://camel.465427.n5.nabble.com/CXFRS-producer-removing-JMS-headers-tp5568925p5574037.html",njiang,njiang,Major,Resolved,Fixed,19/Mar/12 14:21,20/Mar/12 08:51
Bug,CAMEL-5109,12547333,axiom 1.2.12 can't work with abdera 1.1.2,"CAMEL-4192 upgrade to axiom 1.2.12, and we have camel-atom feature which use latest axiom 1.2.12 and abdera 1.1.2 together, however, actually abdera 1.1.2  need work with axiom 1.2.10.

In org.apache.abdera.parser.stax.util.FOMExtensionIterator(abdera code), it need use a field currentChild which inherit from org.apache.axiom.om.impl.traverse.OMAbstractIterator(axiom code),  but field currentChild was removed from axiom 1.2.12, so that we can see exception like
java.lang.NoSuchFieldError: currentChild
	at org.apache.abdera.parser.stax.util.FOMExtensionIterator.hasNext(FOMExtensionIterator.java:74)
	at org.apache.abdera.parser.stax.util.FOMList.buffer(FOMList.java:60)
	at org.apache.abdera.parser.stax.util.FOMList.size(FOMList.java:77)


As abdera 1.1.2 already the latest version so that I think we need rollback to axiom 1.2.10 utill next abdera version which can work with axiom 1.2.12

Btw I believe this is the reason that why CXF still use axiom 1.2.10.",ffang,ffang,Major,Resolved,Fixed,21/Mar/12 05:19,10/Jan/13 08:48
Bug,CAMEL-5110,12547443,Using shareUnitOfWork with DeadLetterChannel sending message to a route doesnt work as expected,"See nabble
http://camel.465427.n5.nabble.com/Error-handling-issue-for-splitter-with-sharedUnitOfWork-and-stopOnException-td5550093.html

The issue is that the when the message is handled by the dead letter channel, then some state from the sub unit of work, should be cleared, to ensure the exchange can be processed by the dead letter channel. This becomes only an issue when the dead letter endpoint is another route.",davsclaus,davsclaus,Minor,Resolved,Fixed,21/Mar/12 19:15,21/Mar/12 20:30
Bug,CAMEL-5114,12547931,Download page does not include any hashes,Download pages should include either MD5 or SHA1 hashes (or both),muellerc,sebb,Minor,Closed,Fixed,24/Mar/12 01:46,25/Mar/12 13:30
Bug,CAMEL-5115,12547997,CxfBeanDestination should copy the in message header to the out message,"User complains about can not access any other headers after the cxfbean in this mail thread[1].
We should let the CxfBeanDestination copy the in message header before it sends the message back.

[1]http://camel.465427.n5.nabble.com/Camel-CXF-Drops-Headers-tt5590792.html ",njiang,njiang,Major,Resolved,Fixed,25/Mar/12 09:29,31/Mar/12 05:42
Bug,CAMEL-5116,12547998,CxfDestination should copy the in message header to the out message when it sends the message back,,njiang,njiang,Major,Resolved,Fixed,25/Mar/12 09:33,31/Mar/12 05:43
Bug,CAMEL-5122,12548308,RMI endpoint doesn't handle nested exception and raise CamelRuntimeException instead of RemoteException,"When we have a route looking like:

<from uri=""rmi:...""/>
<to uri=""myBean""/>

if an exception is raised on myBean, the RMI endpoint ignores the exception described in the RemoteInterface and always raises a RuntimeCamelException nesting the original exceptions.

More over, RuntimeCamalException should not be raised by the RMI endpoint. It would make more sense to raise a RemoteException, as we are on a RMI endpoint.",davsclaus,jbonofre,Major,Resolved,Fixed,27/Mar/12 13:38,11/Jun/12 07:42
Bug,CAMEL-5129,12548495,interceptFrom() doesn't work with property placeholders,"When using property placeholders in consumer endpoints, interceptFrom is not able to match the pattern against the Endpoint URI because the latter is passed unresolved. {{EndpointHelper.matchEndpoint(String uri, String pattern)}} receives the placeholder in the first parameter rather than the resolved value of the property.",davsclaus,raulvk,Major,Resolved,Fixed,28/Mar/12 13:48,05/Apr/12 10:31
Bug,CAMEL-5131,12548662,camel-hdfs feature does not work as it refers to jetty-bundle-version which does not resolve,"In the feature file the camel-hdfs feature loads the bundle:
<bundle>mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.jetty/${jetty6-bundle-version}</bundle>

As jetty-bundle-version is not defined that does not work.

I see two possible solutions:
1. use jetty-version instead
2. Remove the bundle as the feature also loads the war feature which should load jetty

I will try to fix with variant 1. Does anyone know why we need the bundle if we have the war feature?
",cschneider,cschneider,Major,Resolved,Fixed,29/Mar/12 13:17,29/Mar/12 14:12
Bug,CAMEL-5133,12548943,BindyCsvDataFormat breaks for non-required fields when separator is a whitespacecharacter like tab.,"When the separator is a whitespacecharacter and the last two (or more) fields are not required and empty, the parsing fails.

The cause is this:
BindyCsvDataFormat.java

 138               // Read the line
 139              String line = scanner.nextLine().trim();

String.trim() trims whitespace characters as in Character.isWhitespace(separatorChar)

See also:
http://camel.465427.n5.nabble.com/Bindy-Having-trouble-with-the-required-field-tp5597196p5597196.html

I can provide testcases and patches if necessary.",njiang,magnuspalmer,Minor,Closed,Fixed,31/Mar/12 11:17,16/Apr/12 18:00
Bug,CAMEL-5137,12549189,Timer component does not suspend,A route which begins with a Timer consumer does not suspend the consumer when the route is suspended.,davsclaus,gordonkl,Minor,Resolved,Fixed,02/Apr/12 17:04,05/Apr/12 17:06
Bug,CAMEL-5140,12549663,bean component - @Handler should take precedence in a bean that implements Predicate,"If you use a bean in a Camel route, and have not specified the method name to invoke. Then Camel has to scan for suitable methods to use. And for that we have the @Handler annotation which should take precedence in this process. However if the bean implements Predicate, or Processor, then Camel will use that. However the @Handler should be used instead, as this is what the end-user expects. And also what we tell in the docs.",davsclaus,davsclaus,Minor,Resolved,Fixed,05/Apr/12 10:55,05/Apr/12 11:37
Bug,CAMEL-5142,12549742,StAXConverter is not threadsafe when not using Woodstox,"The StAXConverter uses singletons for the XMLInputFactory and XMLOutputFactory.   That works fine with woodstox where those implementations are threadsafe.   However, the sun parser built into the JDK is not thread safe.
",dkulp,dkulp,Major,Resolved,Fixed,05/Apr/12 15:38,06/Apr/12 14:34
Bug,CAMEL-5151,12550235,camel-netty psuedo proxy,"I have a route:

   from(""netty:tcp://somewhere:1234?..."")
     .to(""bean:myBean?method=callOut"")

Where the callOut method does several things, followed by a producerTemplate call to another netty endpoint:

   producerTemplate.sendBody(""netty:tcp://somewhereElse:6789..."", ExchangePattern.InOut, body);

I'm unable to proxy ""directly"" from/to the netty endpoints, and need to pass through this bean.  However, I receive the following exception:

   Caused by: java.lang.IllegalStateException: await*() in I/O thread causes a dead lock or sudden performance drop. Use addListener()    instead or call await*() from a different thread.",davsclaus,matt.narrell,Major,Resolved,Fixed,09/Apr/12 15:17,13/Jun/12 11:40
Bug,CAMEL-5153,12550314,Simple language - OGNL - Invoking explicit method with no parameters should not cause ambiguous exception for overloaded methods,"If you want to invoke a method on a bean which is overloaded, such as a String with toUpperCase having
- toUpperCase()
- toUpperCase(Locale)

Then if you specify this in a simple ognl expression as follows
{code}
${body.toUpperCase()}
{code}

Then Camel bean component should pick the no-parameter method as specified.
",davsclaus,davsclaus,Minor,Resolved,Fixed,10/Apr/12 05:08,11/Apr/12 04:56
Bug,CAMEL-5154,12550315,Simple language - OGNL - Invoking explicit method with no parameters should not cause ambiguous exception for overloaded methods,"If you want to invoke a method on a bean which is overloaded, such as a String with toUpperCase having
- toUpperCase()
- toUpperCase(Locale)

Then if you specify this in a simple ognl expression as follows
{code}
${body.toUpperCase()}
{code}

Then Camel bean component should pick the no-parameter method as specified.
",davsclaus,davsclaus,Minor,Resolved,Fixed,10/Apr/12 05:08,10/Apr/12 06:07
Bug,CAMEL-5157,12550364,Message hasAttachments need to populate initial attachments,"This is needed to ensure we answer the correct.

For example it may report false, before the attachments has been initial populated.
hasAttachments -> false
getAttachments.size -> 2

",davsclaus,davsclaus,Minor,Resolved,Fixed,10/Apr/12 12:34,11/Apr/12 09:47
Bug,CAMEL-5158,12550375, camel-cxfrs producer should keep the response detail when the exception is thrown,"CxfRsProducer doesn't put the response detail message into the exception instead of just calling response.toString().
We should fix it by looking up the response from input stream of the response entity.
",njiang,njiang,Major,Resolved,Fixed,10/Apr/12 14:00,11/Apr/12 05:03
Bug,CAMEL-5161,12550449,If MyBatis experiences an error on commit (e.g. unique PK violation) it appears to hold the session open and cause DB writelock errors,"When MyBatis experiences an error committing a statement the session.close() is not called and the DB is caught in a writelock state until camel is exited.
I have not tested a fix yet, but I believe that if, in: camel/components/camel-mybatis/src/main/java/org/apache/camel/component/mybatis/MyBatisProducer.java
session.commit() is placed in a try/catch block with session.close() in its finally it will ensure the session is closed even when MyBatis throws an exception during a commit.

One of the exceptions I've experienced this with is 'java.sql.BatchUpdateException' where a row was attempting to be inserted with a field that violated a table's unique index constraint.

I should be able to submit a patch later this week if necessary.",muellerc,daubman,Major,Closed,Fixed,10/Apr/12 22:01,11/Apr/12 22:03
Bug,CAMEL-5162,12550474,OnException - Handler or Continued predicate is invoked twice per exception,"See CAMEL-5139

When using handled(predicate) or continued(predicate) those is evaluated twice per exception. We should only do this once, which is what the end user would normally except.",davsclaus,davsclaus,Minor,Resolved,Fixed,11/Apr/12 06:23,11/Apr/12 07:16
Bug,CAMEL-5163,12550480,If throttler or delayer expression evaluation throws exception then error handler is not used,"If you use the throttler or delayer EIP and the expression evaluated at runtime throws an exception, then that is not caught and handled on the exchange, causing the error handler not to react.

See CAMEL-5126",davsclaus,davsclaus,Minor,Resolved,Fixed,11/Apr/12 08:10,11/Apr/12 08:47
Bug,CAMEL-5164,12550582,Camel error handler stop routing on original route when an exception occur in the splitter using jaxb marshalled objects ,"I've been digging into this problem for some time now and I haven't find why this is happening. I'm using camel 2.9.1. 
The whole problems is quite complicated to explain, I've created a test that reproduce the error that might be more useful.

I have a route that get some XML and transform it with a processor in an object generated from XSD with JAXB. After that the object (transported as XML) is then split based a certain logic. The route uses the deadLetterCHannel error handler to a 'reject' endpoint. 

Now, sometimes when I get some invalid character in the input file (0x10 eg.) (in my processor I don't polish them), the splitter will crash but, instead of detouring only the messages that contains errors to the deadLetterChanne, it will detour every message coming after the error occurs.

I know that I can fix the problem in the processor by removing the invalid characters, but in my opinion, camel should be more robust and doesn't block the entire processing for a ""non fatal"" error. Or I should use a different way for doing what I'm doing... 

Please have a look at the code and let me know, I think is a bug but I'm waiting for your opinion.

Thanks in advance ",davsclaus,lfoppiano,Major,Resolved,Fixed,11/Apr/12 21:12,17/Jul/12 22:12
Bug,CAMEL-5180,12551105,Backmerge CAMEL-5018 to the Bindy component for non-required fields,"The issue, originally reported via CAMEL-5133, is to backmerge the CAMEL-5018 fix into the 2.9.x branch.

I merged revision 1292164 into my local 2.9.x branch that I had updated with two new test cases.
Before merge one test failed, after merge all tests were successfull.
Attaching the updated model and tabtest.
--- 

Revision: 1292164
Author: ningjiang
Date: den 22 februari 2012 08:49:40
Message:
CAMEL-5018 support the set the default value on the field of bindy CVS
----
Modified : /camel/trunk/components/camel-bindy/src/main/java/org/apache/camel/dataformat/bindy/BindyCsvFactory.java
Modified : /camel/trunk/components/camel-bindy/src/main/java/org/apache/camel/dataformat/bindy/annotation/DataField.java
Modified : /camel/trunk/components/camel-bindy/src/test/java/org/apache/camel/dataformat/bindy/csv/BindySimpleCsvUnmarshallTest.java
Modified : /camel/trunk/components/camel-bindy/src/test/java/org/apache/camel/dataformat/bindy/model/simple/oneclass/Order.java
",davsclaus,magnuspalmer,Minor,Resolved,Fixed,16/Apr/12 17:51,17/Apr/12 07:57
Bug,CAMEL-5187,12551223,JMX issues on WebSphere,"While setting up a Camel web application for WebSphere (7) I encountered two issues

1. Documentation: the Camel JMX docs proposes the following settings for WebSphere:
{code}
<camel:jmxAgent id=""agent"" createConnector=""true"" mbeanObjectDomainName=""org.yourname"" mbeanServerDefaultDomain=""WebSphere""/>
{code}

This registers the beans with the PlatformMbeanServer instead of the WebSphere MBean server. The following setup works better:
{code}
<camel:jmxAgent id=""agent"" createConnector=""false"" mbeanObjectDomainName=""org.yourname"" usePlatformMBeanServer=""false"" mbeanServerDefaultDomain=""WebSphere""/>
{code}

2. For each Camel route, the same Tracer and DefaultErrorHandler MBeans are tried to be registered over and over again. Because WebSphere changes the ObjectNames on registration, 

{{server.isRegistered(name);}} in {{DefaultManagementAgent#registerMBeanWithServer}} always returns false, which causes the MBean to be re-registered, which again cause Exceptions, e.g.

{code}
14:35:48,198 [WebContainer : 4] [] WARN  - DefaultManagementLifecycleStrategy.onErrorHandlerAdd(485) | Could not register error handler builder: ErrorHandlerBuilderRef[CamelDefaultErrorHandlerBuilder] as ErrorHandler MBean.
javax.management.InstanceAlreadyExistsException: org.apache.camel:cell=wdf-lap-0319Node01Cell,name=""DefaultErrorHandlerBuilder(ref:CamelDefaultErrorHandlerBuilder)"",context=wdf-lap-0319/camelContext,type=errorhandlers,node=wdf-lap-0319Node01,process=server1
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:465)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1496)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:975)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:929)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:494)
	at com.ibm.ws.management.PlatformMBeanServer.registerMBean(PlatformMBeanServer.java:484)
	at org.apache.camel.management.DefaultManagementAgent.registerMBeanWithServer(DefaultManagementAgent.java:320)
	at org.apache.camel.management.DefaultManagementAgent.register(DefaultManagementAgent.java:236)
...
{code}

The web application starts up, but with a lot of exceptions in the log.

Proposal:
Instead of using a Set<ObjectName> for mbeansRegistered, use a Map<ObjectName, ObjectName> where the key is the ""Camel"" ObjectName and the value is the actually deployed ObjectName.

I will provide a patch that illustrates the idea.
",davsclaus,christian.ohr,Minor,Resolved,Fixed,17/Apr/12 13:18,18/Apr/12 08:00
Bug,CAMEL-5192,12551585,spring.schema is not up to date,"The URI of the Spring schema has been updated in Camel 2.9.1 and 2.8.4:

http://camel.apache.org/schema/cxf/

We can see that starting from 2.9.1 and 2.8.4, we suffix the schema with -spring or -blueprint.

However, the spring.schema resource file has not been updated with this suffix.",davsclaus,jbonofre,Major,Resolved,Fixed,19/Apr/12 08:15,03/Jun/12 14:53
Bug,CAMEL-5193,12551589,Strange warning appears: WARN  org.apache.camel.processor.DeadLetterChannel - Cannot determine current route from Exchange with id ,"The warning gets printed but other than that, everything seems to work. (I hope)

{code}
12.04.19 10:34:26:463 [drisCamelContext JmsConsumer[dispatchHttpQueue] #4] WARN  org.apache.camel.processor.DeadLetterChannel - Cannot determine current route from Exchange with id: ID-dris-ixor-be-42350-1334569164986-0-1052464, will 
fallback and use first error handler.
12.04.19 10:34:26:463 [drisCamelContext JmsConsumer[dispatchHttpQueue] #4] WARN  distribution.kv78.dispatch - Failed delivery (attempt = 199) : org.apache.http.conn.HttpHostConnectException: Connection to http://81.82.232.183:9292 ref
used - 0299c1d9 - KV8passtimes - 81.82.232.183:9292 - queue_dispatchHttpQueue_ID_dris.ixor.be-36583-1334569163882-4_1_24_1_1007
{code}

When looking up the log statement in RedeliveryErrorHandler, I find this:

{code}
else if (!exceptionPolicy.getErrorHandlers().isEmpty()) {
                // note this should really not happen, but we have this code as a fail safe
                // to be backwards compatible with the old behavior
                log.warn(""Cannot determine current route from Exchange with id: {}, will fallback and use first error handler."", exchange.getExchangeId());
                processor = exceptionPolicy.getErrorHandlers().iterator().next();
            }
{code}

So it shouldn't happen but it happens after all...


Here is the route causing the warning. I think it started when I made the route transacted. 
As shown in the log snippet above, the warning is printed when the HTTP endpoint throws an IOException (route ENDPOINT_DISPATCH)
{code}        
        from(ENDPOINT_DISPATCH_QUEUE +
            ""?cacheLevelName=CACHE_CONSUMER&"" + // necessary for message groups to work
            ""concurrentConsumers={{hermes.dris.distribution.kv78.concurrentOutgoingHttpDispatchers}}&"" +
            ""maxConcurrentConsumers={{hermes.dris.distribution.kv78.maxConcurrentOutgoingHttpDispatchers}}"")
            .routeId(ROUTE_ID_DISPATCH_QUEUE)
            .onException(Throwable.class)
                // transacted routes override the default error handler
                // to avoid the message going to the the ActiveMQ DLQ, forward to our DLQ here
                .to(ENDPOINT_DLQ)
                .handled(true)
            .end()
            .transacted()
             // determine action
             // the timingPointSequencer will check what needs to happen with the message
            .beanRef(""timingPointSequencer"", ""handleDispatch"")
            .choice()
                 // normal situation, the message can be sent
                .when(header(TimingPointSequencer.HEADER_ACTION).isEqualTo(constant(TimingPointSequencer.ACTION_PROCEED)))
                    .to(ENDPOINT_DISPATCH)
                 // delay message, usually because of previous errors for this destination
                .when(header(TimingPointSequencer.HEADER_ACTION).isEqualTo(constant(TimingPointSequencer.ACTION_REENQUEUE)))
                    .log(LoggingLevel.INFO, ""Re-enqueuing"" + MESSAGE_LOG_FORMAT)
                    .to(ENDPOINT_DISPATCH_QUEUE)
                .otherwise()
                    .log(LoggingLevel.ERROR, ""No action header set ???"" + MESSAGE_LOG_FORMAT)
                    .to(ENDPOINT_DLQ)
                .end()
            .end();

        from(ENDPOINT_DISPATCH)
            .routeId(ROUTE_ID_DISPATCH)
            .onException(IOException.class)
                // in case of IO exceptions, the message is always re-enqueued
                // reschedule parameters are set by the timingPointSequencer
                .beanRef(""timingPointSequencer"", ""handleFailure"")
                .log(LoggingLevel.WARN, ""Failed delivery (attempt = ${in.header.tpSecAttempt}) : ${in.header.CamelExceptionCaught}"" + MESSAGE_LOG_FORMAT)
                .to(ENDPOINT_DISPATCH_QUEUE)
                .handled(true)
            .end()
             // clear http headers to avoid interference from other http endpoints
            .removeHeaders(""CamelHttp*"")
            .setHeader(Exchange.HTTP_URI, simple(""http://${header."" + HEADER_DESTINATION_ADDRESS + ""}""))
            .setHeader(Exchange.HTTP_PATH, simple(""${properties:hermes.dris.distribution.kv78.controller.urlpath}""))
            .setHeader(Exchange.CONTENT_TYPE, constant(""application/xml""))
            .setHeader(Exchange.CONTENT_ENCODING, constant(""gzip"")) // use gzip compression
            .log(verboseLoggingLevel, ""Sending request to ${in.header.CamelHttpUri}/${in.header.CamelHttpPath}"" + MESSAGE_LOG_FORMAT)
             // use multicast instead of a pipeline, or the audit log will contain the http response!
            .multicast()
                .stopOnException()
                .to(ENDPOINT_TIMINGPOINT_HTTP + ""?httpClientConfigurerRef=timingPointHttpClientConfigurer&headerFilterStrategy=#distributionHttpHeaderFilterStrategy"", ENDPOINT_AUDIT_OUTGOING)
            .end()
            .beanRef(""timingPointSequencer"", ""handleSuccess"");
{code}",davsclaus,syberyan,Major,Resolved,Fixed,19/Apr/12 08:56,29/Apr/12 16:04
Bug,CAMEL-5195,12551629,XSLTUriResolver fails to include from classpath under windows,org.apache.camel.builder.xml.XsltUriResolver uses File.separator to separate classpath parts instead of simply '/'. This prevents <xsl:include /> to work under windows when the including xslt is located in a subfolder of the classpath.,davsclaus,juliengb,Minor,Resolved,Fixed,19/Apr/12 14:50,20/Apr/12 16:01
Bug,CAMEL-5196,12551742,SftpChangedExclusiveReadLockStrategy does not check the correct field to determine if a file is changing,"I have recently upgraded from Camel 2.7 to a more recent version. I can confirm that the bug is still in trunk. 

In Camel 2.7, there was a generic FileChangedExclusiveReadLockStrategy. This checked the modification time of the file using GenericFile.lastModified. This field was set by SftpComponent#asRemoteFile as file.getAttrs().getMTime() * 1000. 

In Camel 2.8 onwards, this behaviour is implemented by SftpChangedExclusiveReadLockStrategy. This incorrectly (to my mind) retrieves the timestamp on line 67 using: 

newLastModified = f.getAttrs().getATime();

Taking the atime of the file doesn't make a great deal of sense to me; on a POSIX compliant filesystem, the atime will be incremented every time the file is polled, meaning that this strategy will never be able to mark a file as unchanged. While some filesystems are mounted nowadays as noatime or relatime, the mtime is still absolutely the safest mechanism to determine if something is writing to a target file. 

The impact of this bug is that we cannot reliably poll files from a remote SFTP server. 
",davsclaus,steve_barham,Major,Resolved,Fixed,20/Apr/12 08:22,20/Apr/12 09:41
Bug,CAMEL-5199,12551933,initialDelay and idempotentRepository Endpoint URI parameters fail to be read from Camel context when Turkish language is set in host operating system.,"org.apache.camel.util.ObjectHelper.capitalize() method uses native JAVA String.toUpperCase() method (without parameters), which performs capitalization according to operating system current language and regional settings configuration. When these are set to Turkish, a call to capitalize() method, with a String starting with ""i"" as parameter, returns Turkish dotted ""I"" as capitalized ""i"", which is wrong according to programmatic language and expected behavior.

This behaviour impacts, at least, in Endpoint URI parameters which names start with char ""i"", such as ""idempotentRepository"" and ""initialDelay"" URI parameters. When setter/getter methods lookup is performed for these parameters, wrong method names are returned due to ObjectHelper.capitalize() method. This method uses standard Java String.toUpperCase() function which returns a dotted ""I"" instead of dotless ""I"" for idempotentRepository and initialDelay parameters.

To solve this, the attached patch consists of refactoring ObjectHelper.capitalize() method. Instead of calling toUpperCase() method, it is called toUpperCase(Locale.ENGLISH) to enforce 'I' character (dotless) return.",davsclaus,morenoisidro,Major,Resolved,Fixed,20/Apr/12 21:08,22/Apr/12 09:18
Bug,CAMEL-5200,12551977,Potential dead-lock when shutting down Camel with NotifyBuilder doing notication,"See nabble
http://camel.465427.n5.nabble.com/Gracefull-shutdown-timeouts-due-to-NotifyBuilder-blocks-on-context-tp5654048p5654048.html

We should avoid the synchronized getRoutes method which is really not needed.",davsclaus,davsclaus,Minor,Resolved,Fixed,21/Apr/12 08:07,25/Apr/12 11:24
Bug,CAMEL-5201,12551989,redelivery exhausted marker should be cleared by try catch EIP and when errorHandler handles the message,"The logic which clear redelivery state should also clear the redelivery exhausted flag in try .. catch, and when the error handler handles the message.
",davsclaus,davsclaus,Major,Resolved,Fixed,21/Apr/12 14:42,21/Apr/12 14:55
Bug,CAMEL-5206,12552091,Different servlets interfere with each other,"See http://java.dzone.com/articles/gotcha-when-using-camel

If using two wars with one servlet in each with the same name then servlet endpoints may end up on the wrong servlet. I think this should not happen with pure war deploys. Trying to check if camel is in a shared lib folder.

In any case this behaviour is not what people expect naively so we should try to fix that so that each war has its own combinaation of servlets and servlet endpoints that do not interfere with others.
",davsclaus,cschneider,Major,Resolved,Fixed,23/Apr/12 07:18,07/Feb/13 14:03
Bug,CAMEL-5210,12552248,Loop causes NullPointerException on error,"The following context will cause NullPointerException.

    <routeContext id=""myCamelContext"" xmlns=""http://camel.apache.org/schema/spring"">    
        <route>
            <from uri=""jms:myQueue""/>
            <onException>
                <exception>org.xml.sax.SAXParseException</exception>
                <redeliveryPolicy maximumRedeliveries=""2""/>
                <handled><constant>true</constant></handled>
                <to uri=""log:camelfatal?showAll=true&amp;multiline=true&amp;level=ERROR""/> 
            </onException>
            <loop copy=""true"">
              <constant>3</constant>
              <inOnly uri=""smtp://you_can_leave_this_unchanged""/>
            </loop>
        </route>
    </routeContext>


The callstack on the NullPointerException:
Daemon Thread [Camel (appServerCamelContext) thread #1 - JmsConsumer[emailOutbox]] (Suspended (exception NullPointerException))	
	DefaultErrorHandler(RedeliveryErrorHandler).processErrorHandler(Exchange, AsyncCallback, RedeliveryErrorHandler$RedeliveryData) line: 262	
	DefaultErrorHandler(RedeliveryErrorHandler).process(Exchange, AsyncCallback) line: 223	
	RouteContextProcessor.processNext(Exchange, AsyncCallback) line: 45	
	RouteContextProcessor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	DefaultChannel.process(Exchange, AsyncCallback) line: 304	
	AsyncProcessorHelper.process(AsyncProcessor, Exchange, AsyncCallback) line: 73	
	LoopProcessor(DelegateAsyncProcessor).processNext(Exchange, AsyncCallback) line: 99	
	LoopProcessor.process(Exchange, AsyncCallback, AtomicInteger, AtomicInteger) line: 103	
	LoopProcessor.process(Exchange, AsyncCallback) line: 74	
	AsyncProcessorHelper.process(AsyncProcessor, Exchange, AsyncCallback) line: 73	
	InstrumentationProcessor(DelegateAsyncProcessor).processNext(Exchange, AsyncCallback) line: 99	
	InstrumentationProcessor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	InstrumentationProcessor.process(Exchange, AsyncCallback) line: 71	
	AsyncProcessorHelper.process(AsyncProcessor, Exchange, AsyncCallback) line: 73	
	TraceInterceptor(DelegateAsyncProcessor).processNext(Exchange, AsyncCallback) line: 99	
	TraceInterceptor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	TraceInterceptor.process(Exchange, AsyncCallback) line: 91	
	AsyncProcessorHelper.process(AsyncProcessor, Exchange, AsyncCallback) line: 73	
	DefaultErrorHandler(RedeliveryErrorHandler).processErrorHandler(Exchange, AsyncCallback, RedeliveryErrorHandler$RedeliveryData) line: 333	
	DefaultErrorHandler(RedeliveryErrorHandler).process(Exchange, AsyncCallback) line: 223	
	RouteContextProcessor.processNext(Exchange, AsyncCallback) line: 45	
	RouteContextProcessor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	DefaultChannel.process(Exchange, AsyncCallback) line: 304	
	RouteContextProcessor.processNext(Exchange, AsyncCallback) line: 45	
	RouteContextProcessor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	UnitOfWorkProcessor.processAsync(Exchange, AsyncCallback, UnitOfWork) line: 150	
	UnitOfWorkProcessor.process(Exchange, AsyncCallback) line: 117	
	RouteInflightRepositoryProcessor.processNext(Exchange, AsyncCallback) line: 50	
	RouteInflightRepositoryProcessor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	AsyncProcessorHelper.process(AsyncProcessor, Exchange, AsyncCallback) line: 73	
	InstrumentationProcessor(DelegateAsyncProcessor).processNext(Exchange, AsyncCallback) line: 99	
	InstrumentationProcessor(DelegateAsyncProcessor).process(Exchange, AsyncCallback) line: 90	
	InstrumentationProcessor.process(Exchange, AsyncCallback) line: 71	
	AsyncProcessorHelper.process(AsyncProcessor, Exchange) line: 99	
	InstrumentationProcessor(DelegateAsyncProcessor).process(Exchange) line: 86	
	EndpointMessageListener.onMessage(Message) line: 104	
	DefaultJmsMessageListenerContainer(AbstractMessageListenerContainer).doInvokeListener(MessageListener, Message) line: 560	
	DefaultJmsMessageListenerContainer(AbstractMessageListenerContainer).invokeListener(Session, Message) line: 498	
	DefaultJmsMessageListenerContainer(AbstractMessageListenerContainer).doExecuteListener(Session, Message) line: 467	
	DefaultJmsMessageListenerContainer(AbstractPollingMessageListenerContainer).doReceiveAndExecute(Object, Session, MessageConsumer, TransactionStatus) line: 325	
	DefaultJmsMessageListenerContainer(AbstractPollingMessageListenerContainer).receiveAndExecute(Object, Session, MessageConsumer) line: 263	
	DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener() line: 1058	
	DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop() line: 1050	
	DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run() line: 947	
	ThreadPoolExecutor$Worker.runTask(Runnable) line: 886	
	ThreadPoolExecutor$Worker.run() line: 908	
	Thread.run() line: 662	
",davsclaus,hynek,Major,Resolved,Fixed,24/Apr/12 06:07,27/Apr/12 07:59
Bug,CAMEL-5212,12552317,"split().tokenizeXML(""child"", ""parent"").streaming() does not work correctly when parent's namespace declarations are separated with new line","when I try to split a big XML file using split().tokenizeXML(""child"", ""parent"").streaming() and if the parent's namespace declaration are separated with new line in the XML file, child element does not inherit the namespace declarations. When all declarations are on the same line it works ok.

<?xml version=""1.0"" encoding=""UTF-8""?>
<parent xmlns:ns1=""ns1.url""
        xmlns:ns2=""ns2.url"">
  <child></child>
  <child></child>
</parent>

doesn't work but


<?xml version=""1.0"" encoding=""UTF-8""?>
<parent xmlns:ns1=""ns1.url"" xmlns:ns2=""ns2.url"">
  <child></child>
  <child></child>
</parent>

is OK",davsclaus,belis,Major,Resolved,Fixed,24/Apr/12 14:00,29/Apr/12 13:36
Bug,CAMEL-5215,12552541,The file producer should use the charset encoding when writing the file if configured,"When writing to a file, we offer the charset option on the endpoint, as well the charset property set on the exchange.
However in a route  that is optimized as
{code}
from file
 to file
{code}

Then we optimize to do a file move operation instead. We should detect the charset configured and then we would need to stream and write using the configured charset.",davsclaus,davsclaus,Minor,Resolved,Fixed,25/Apr/12 09:49,25/Apr/12 11:31
Bug,CAMEL-5218,12552693,HazelcastComponent.doStop bombing out,"I am setting up an own hazelcast instance on HazelcastComponent, it is created by Spring,
the problem is when HazelcastComponent.doStop is called, a NullPointerException is throw, because
createOwnInstance is null.
 ",muellerc,henriqueo,Major,Closed,Fixed,25/Apr/12 17:40,25/Apr/12 22:26
Bug,CAMEL-5222,12552800,The file consumer should use the charset encoding when reading the file if configured,"See CAMEL-5215.

This applies to the consumer as well. We should tighten this up, to ensure the charset is always used if configured. Currently the charset option could be shadowed if you did a convertBodyTo and specified another charset etc.

Also we should tighten up to use the charset configuration on the file endpoints. And added DEBUG logging which charset is being used for reading/write the files.",davsclaus,davsclaus,Major,Resolved,Fixed,26/Apr/12 06:49,26/Apr/12 07:29
Bug,CAMEL-5223,12552807,Cannot find resource in classpath using OSGi blueprint,"I switched from Apache Camel version 2.8.4 (in ServiceMix 4.4.1) to the most recent version 2.9.2. Resource loading in 2.8.4 worked well, but not in the named release.

Find attached a small test case that uses XSLT and String-Template Endpoints. Neither of them can load their resource from the separate bundle.

# Install Apache ServiceMix 4.4.1
# Configure ServiceMix to use Apache Camel 2.9.2 (see {{etc/org.apache.karaf.features.cfg}})
# Start up ServiceMix
# Install feature 'camel-string-template'
# Build the attached test project using maven
# Copy/Link the built artifact into the {{deploy}} folder
# Copy/Link the route {{src/test/resources/camel-route}} into the {{deploy}} folder

The XSLT Component tries to load the resource eagerly. Therefore the route will not start and throws the following exception:
{noformat}
Caused by: java.io.FileNotFoundException: Cannot find resource in classpath for URI: com/basis06/apache/camel/HelloWorld.xslt
	at org.apache.camel.util.ResourceHelper.resolveMandatoryResourceAsUrl(ResourceHelper.java:122)
	at org.apache.camel.component.xslt.XsltEndpoint.loadResource(XsltEndpoint.java:67)
{noformat}

The same problem exists with the String-Template Component. But this component will fail not until it is called in the route.
",davsclaus,tsh,Major,Resolved,Fixed,26/Apr/12 07:46,15/Jun/12 15:46
Bug,CAMEL-5224,12552825,"The done file got deleted, when using the file component even if noop property set to true","We are consuming a feed from a mounted windows network drive, where we have rw access.
During the download we shouldn't touch anything so other users see the directory intact.

However even if we turn noop=true the done file got deleted after successfull conumptions
",davsclaus,edvicif,Major,Resolved,Fixed,26/Apr/12 10:10,29/Apr/12 12:51
Bug,CAMEL-5225,12552870,camel-netty can't distinguish between Sharable and Unsharable codecs,"Camel-netty uses general configuration model for referenced encoders/decoders for channel pipelines, see DefaultServerPipelineFactory. That is, create encoder/decoder objects at configuration parsing time and store them in a list, then use them when a pipeline is established. However, this will make encoder/decoder objects shared among different pipelines, that may cause data conflicts, when the encoder/decoder is not Sharable(has object status/not annotated as @Sharable), e.g. a LengthFieldBasedFrameDecoder.

Although we can avoid the problem by totally writing a new serverpipelinefactory for our apps, several problem still remains, please see detailed description and testcase for this bug at:

http://camel.465427.n5.nabble.com/camel-netty-bug-and-the-need-of-best-practice-for-creating-referenced-parameter-object-on-looking-up-td5627926.html",davsclaus,edge,Major,Resolved,Fixed,26/Apr/12 16:03,13/Jun/12 09:33
Bug,CAMEL-5227,12553070,Camel 2.9.2 jetty component no longer works with Jetty 7,"When attempting to create a camel-jetty endpoint in Camel 2.9.2, the following exception is thrown:
{noformat}
java.lang.NoSuchMethodError: org.eclipse.jetty.servlet.ServletContextHandler.addFilter(Lorg/eclipse/jetty/servlet/FilterHolder;Ljava/lang/String;Ljava/util/EnumSet;)V
at org.apache.camel.component.jetty.JettyHttpComponent.enableMultipartFilter(JettyHttpComponent.java:407)
    at org.apache.camel.component.jetty.JettyHttpComponent.connect(JettyHttpComponent.java:325)
    at org.apache.camel.component.http.HttpEndpoint.connect(HttpEndpoint.java:148)
    at org.apache.camel.component.http.HttpConsumer.doStart(HttpConsumer.java:56)
    at org.apache.camel.support.ServiceSupport.start(ServiceSupport.java:60)
    at org.apache.camel.impl.DefaultCamelContext.startService(DefaultCamelContext.java:1707)
    at org.apache.camel.impl.DefaultCamelContext.doStartOrResumeRouteConsumers(DefaultCamelContext.java:1979)
    at org.apache.camel.impl.DefaultCamelContext.doStartRouteConsumers(DefaultCamelContext.java:1934)
    at org.apache.camel.impl.DefaultCamelContext.safelyStartRouteServices(DefaultCamelContext.java:1862)
    at org.apache.camel.impl.DefaultCamelContext.doStartOrResumeRoutes(DefaultCamelContext.java:1646)
    at org.apache.camel.impl.DefaultCamelContext.doStartCamel(DefaultCamelContext.java:1533)
    at org.apache.camel.impl.DefaultCamelContext.doStart(DefaultCamelContext.java:1420)
    at org.apache.camel.spring.SpringCamelContext.doStart(SpringCamelContext.java:179)
    at org.apache.camel.support.ServiceSupport.start(ServiceSupport.java:60)
    at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1388)
.
.
.
{noformat}

This appears to be a regression of a caused by the [this merge from trunk|https://fisheye6.atlassian.com/changelog/camel?cs=1307901], which I suspect requires Jetty 8 to be used.

Creating the endpoint under 2.9.1 works fine.

",davsclaus,snortasprocket,Critical,Closed,Fixed,27/Apr/12 00:19,30/Apr/12 13:57
Bug,CAMEL-5229,12553102,camel-blueprint - Should not set empty location for properties component,"Property placeholders using blueprint, should avoid setting empty locations by default",davsclaus,davsclaus,Minor,Resolved,Fixed,27/Apr/12 07:47,27/Apr/12 07:55
Bug,CAMEL-5234,12553389,Spring-WS does neither set message body nor message headers if exchange is not outCapable,"Spring-WS component does not behave as expected from pipes and filters pattern if exchange is not _outCapable_.
If _ExchangeHelper_._isOutCapable_ returns false for the given _exchange_ the IN-message is returned instead of the WS-Response.
Example:
{code:title=ExampleRoute}
        from(""timer://foo?fixedRate=true&period=1000"")//
                .setBody().simple(""<ex:getExampleResponse xmlns:ex=\""http://example.com/\"">"" //
                        + ""       <id>1</id>"" //
                        + ""     </ex:getExampleResponse>"")//
                .to(""spring-ws:http://localhost:9000/Example"")//
                .to(""file://responses"");
{code}

In the example above I would expect the WS-response written to the files in the responses directory. Currently (since 2.7.3) the IN message is written to the files.
This is caused by _SpringWebserviceProducer#process_ because it only sets headers and body for the OUT-message if _isOutCapable_ is _true_.

Workaround (maybe this has side effects!):
{code:title=ExampleRoute}
                from(""timer://foo?fixedRate=true&period=1000"")//
                .setExchangePattern(ExchangePattern.InOut) // <-- Override with InOut Pattern
                .setBody().simple(""<ex:getExampleResponse xmlns:ex=\""http://example.com/\"">"" //
                        + ""       <id>1</id>"" //
                        + ""     </ex:getExampleResponse>"")//
                .to(""spring-ws:http://localhost:9000/Example"")//
                .to(""file://responses"");
{code}

This behavior has been implemented to fix CAMEL-3974. From my point of view its counter intuitive since other processing-steps don't check the exchange's _outCapability_.
It took me several hours to find out why I always got the IN message back, although the webservice was called correctly and returned correct results.
Maybe an option should be provided to control this behavior. At least a log-message should be written to explain, that the webservice-reponse is thrown away.

",bvahdat,benjamin.gniza,Major,Resolved,Fixed,30/Apr/12 17:15,11/Jun/12 21:10
Bug,CAMEL-5235,12553453,Make sure to close file input stream when converting file to string to avoid locking file on windows,"See nabble
http://camel.465427.n5.nabble.com/File-Processor-Not-deleting-the-files-tp5670301.html

Need to explicit close the file input/output streams to avoid the files to be locked on windows.
",davsclaus,davsclaus,Major,Resolved,Fixed,01/May/12 06:51,02/May/12 08:48
Bug,CAMEL-5248,12554036,Running camel:run with blueprint gets a java.lang.NoClassDefFoundError: org.osgi.vendor.framework property not set,"When running mvn camel:run with a blueprint app, you may get this exception.",davsclaus,davsclaus,Major,Resolved,Fixed,05/May/12 09:34,05/May/12 09:43
Bug,CAMEL-5257,12554446,camel-jpa feature can not be installed out of box,"When I try to run the camel-itest-karaf, I found the camel-jpa feature can not be install out of box, as the servlet api cannot be resolved. ",njiang,njiang,Major,Resolved,Fixed,09/May/12 08:31,02/Jun/12 18:36
Bug,CAMEL-5260,12554452,LDAP Component lose headers,"When we call to a LDAP endpoint, the result is returned in the Out body and we lose the IN headers (see http://camel.465427.n5.nabble.com/LDAP-Component-lose-headers-td5696602.html). 

A fix seems relatively easy. The solution could be add this line inside the process() method in LdapProducer.java
 exchange.getOut().setHeaders(exchange.getIn().getHeaders());",muellerc,anacortes,Major,Closed,Fixed,09/May/12 09:12,10/May/12 18:37
Bug,CAMEL-5261,12554453,SEDA/VM requires completely same URI on producer and consumer side when consumer route is adviced,"The producer side and consumer side of the SEDA (and VM) component seems to require the completely same URI to be able to communicate. Completely same meaning that all URI options must be the same on both sides. The strange thing is that this only is required when I have adviced the consumer route. 2.9.0 does not have this problem.

Attached a unit test - the producerWithDifferentUri will fail on 2.9.1 and 2.9.2. If the advice is removed it will not.",davsclaus,thxmasj,Major,Resolved,Fixed,09/May/12 09:15,20/May/12 19:57
Bug,CAMEL-5263,12554465,Cometd component does not pass EnableSessionHeaders parameter to CometdBinding properly.,,njiang,jwatkins,Minor,Resolved,Fixed,09/May/12 10:55,10/May/12 01:34
Bug,CAMEL-5265,12554493,Incorrect Content-Length returned for converted result objects with Unicode characters,"The camel-servlet component responds with an incorrect http Content-Length header, causing the HTTP client to retreive a incomplete response, when the body of the out message is of a POJO type and is converted into a string that contains unicode characters.

For example, the following route returns ""Büe Wör"", but should return ""Büe Wörld"".
{quote}     
from(""servlet:///testUnicodeWithObjectResponse?matchOnUriPrefix=true"") 
   .process(new Processor() {
       public void process(Exchange exchange) throws Exception {                    
           String contentType = exchange.getIn().getHeader(Exchange.CONTENT_TYPE, String.class);
           exchange.getOut().setHeader(Exchange.CONTENT_TYPE, contentType + ""; charset=UTF-8"");  
    }})
    .transform(constant(
        new Object(){
            @Override
            public String toString() {
                return ""Büe Wörld"";
            }
        }
    ));
{quote}

The attached patch contains a fix for the problem and a test to reproduce the problem.",davsclaus,manuelh9r,Major,Resolved,Fixed,09/May/12 14:03,10/May/12 07:38
Bug,CAMEL-5268,12554584,camel-restlet producer not passing along the Accept header ,"Refer to the thread on user forum.
http://camel.465427.n5.nabble.com/camel-restlet-Accept-header-td5692917.html

DefaultRestletBinding.populateRestletRequestFromExchange() may be missing a Restlet API call to set the accepted media types the Request object.       

Request.getClientInfo().setAcceptedMediaTypes(acceptedMediaTypes)

",wtam,wtam,Major,Resolved,Fixed,09/May/12 23:15,22/May/12 03:42
Bug,CAMEL-5274,12555006,Jetty client can corrupt streams if used in a bridge mode (proxy),"The jetty http producer should favor streams over Strings, which otherwise could cause the payload to be corrupted if doing a bridge mode, eg from jetty to jetty.",davsclaus,davsclaus,Major,Resolved,Fixed,12/May/12 07:34,15/Jun/12 09:47
Bug,CAMEL-5277,12555977,Forgot a space between hours and minutes in org/apache/camel/util/TimeUtils.java,"org/apache/camel/util/TimeUtils.java

in the printDuration method is something wrong on line 63 and 72.

63: s += "" "" + fmtI.format(minutes) + (minutes > 1 ? "" minutes"" : ""minute"");
has to be:
63: s += "" "" + fmtI.format(minutes) + (minutes > 1 ? "" minutes"" : "" minute"");

72: s += "" "" + fmtI.format(hours) + (hours > 1 ? "" hours"" : ""hour"");
has to be:
72: s += "" "" + fmtI.format(hours) + (hours > 1 ? "" hours"" : "" hour"");",muellerc,olifant1990,Minor,Closed,Fixed,16/May/12 07:54,24/May/12 06:58
Bug,CAMEL-5279,12556091,camel-stream overwrites files instead of appending,"When a file stream is used with camel-stream, the file is not opened in append mode, therefore the file is overwritten with every message, ending up with just the last message, all others are lost.

This contrasts with the documented behavior.",hadrian,hadrian,Major,Closed,Fixed,16/May/12 17:48,05/Jul/12 19:59
Bug,CAMEL-5284,12556328,camel-stream should not close stream after each write,"The StreamProducer in camel-stream closes the stream after every write, which significantly affects performance.",hadrian,hadrian,Major,Resolved,Fixed,18/May/12 00:18,02/Jun/12 12:49
Bug,CAMEL-5285,12556464,trim() in SimpleLanguage.createExpression strips new-lines from expression,"A newly added (since 2.8.3) expression.trim() in SimpleLanguage.createExpression(String expression) strips new-lines from the expression before evaluation.

I was using this feature to log exceptions to a status file in my camel route. I built the status message using a simple expression and terminated with &#10; in the spring-dsl route. With 2.9.2, the individual messages are no longer delimited by new-line because of the added expression.trim().",davsclaus,apejavar,Major,Resolved,Fixed,18/May/12 21:47,27/May/12 12:23
Bug,CAMEL-5299,12556894,EventNotifiers don't work under Blueprint,"EventNotifiers don't work with Blueprint Camel Contexts. Testing on latest 2.10-SNAPSHOT, updated on 22 May 16:45 GMT.

After some debugging, it looks like the bean lookup performed on line 202 of {{AbstractCamelContextFactoryBean.afterPropertiesSet()}} triggers the initialization of the CamelContext as part of the lookup. 

As part of the init, all services are started, including the ManagementContext and its EventNotifiers (which we were just looking up).

EventNotifiers are then added but never started because they came in late.",davsclaus,raulvk,Major,Resolved,Fixed,22/May/12 18:11,23/Nov/12 21:22
Bug,CAMEL-5300,12556930,The flatpack component is swallowing it's parsing errors,"When flatpack parses the exchange any errors that are generated are stored in an error list in the parser. Currently, this list isn't being checked so the exchange is moving on even when there are errors generated.",davsclaus,geerzo,Major,Resolved,Fixed,22/May/12 21:59,23/May/12 14:46
Bug,CAMEL-5303,12557048,InOut over seda/vm with multiple consumers can cause a timeout exception,"See nabble
http://camel.465427.n5.nabble.com/ExchangeTimedOutException-when-using-vm-seda-with-multipleConsumers-true-td5710951.html",davsclaus,davsclaus,Major,Resolved,Fixed,23/May/12 18:46,23/May/12 19:07
Bug,CAMEL-5309,12558157,Replies with Incorrect CorrelationIDs Received After Reinstantiating a Route with ActiveMQ Endpoint and Exclusive Reply Queue,"When I first instantiate the following route, it works as expected. The Replies that come have the right correlation ids, just as Camel has assigned them.

{code}
from(""direct:fetchStateStart"")
.setExchangePattern(ExchangePattern.InOut)
.to(""activemq:queue:tasksQueue?replyTo=completionsQueue"" +
        ""&replyToType=Exclusive"" +
        ""&requestTimeout="" + FETCH_INDEX_TIMEOUT)
.threads(10)
.routeId(""route-001"");
{code}

When this route completes, it is stopped and removed from the camel context. 

When a similar route is instantiated:

{code}
from(""direct:processStateStart"")
.setExchangePattern(ExchangePattern.InOut)
.to(""activemq:queue:tasksQueue?replyTo=completionsQueue"" +
        ""&replyToType=Exclusive"" +
        ""&requestTimeout="" + PROCESS_INDEX_TIMEOUT)
.threads(10)
.routeId(""route-002"");
{code}

Half of the replies come as expected, while the other half results in the following warning: *Reply received for unknown correlationID*.

A workaround for this issues is to use a different *ReplyTo*-queue for each new instantiation of a similar route.

E.g. for the second route, it'll work if:
{code}
from(""direct:processStateStart"")
.setExchangePattern(ExchangePattern.InOut)
.to(""activemq:queue:tasksQueue?replyTo=processedIndecesQueue"" +
        ""&replyToType=Exclusive"" +
        ""&requestTimeout="" + PROCESS_INDEX_TIMEOUT)
.threads(10)
.routeId(""route-002"");
{code}",davsclaus,npacemo,Major,Resolved,Fixed,25/May/12 18:29,09/Aug/12 15:59
Bug,CAMEL-5315,12558404,Camel-blueprint fails to install on Karaf 3 as it does not accept the version 1.0.0,"karaf@root()> feature:install camel-blueprint
Error executing command: Could not start bundle mvn:org.apache.camel/camel-blueprint/2.10-SNAPSHOT in feature(s) camel-blueprint-2.10-SNAPSHOT: Unresolved constraint in bundle org.apache.camel.camel-blueprint [171]: Unab
le to resolve 171.0: missing requirement [171.0] osgi.wiring.package; (&(osgi.wiring.package=org.apache.aries.blueprint)(version>=0.3.0)(!(version>=1.0.0)))

",cschneider,cschneider,Major,Resolved,Fixed,29/May/12 09:51,29/May/12 10:02
Bug,CAMEL-5317,12558432,NettyConverter throws NullPointerException when Exchange is not set,In Camel TypeConverter API we have two ways to convert payload - with exchange or without. NettyConverter doesn't support ChannelBuffer -> toString conversion without exchange instance and throws NullPointerException.,ldywicki,ldywicki,Major,Resolved,Fixed,29/May/12 14:08,29/May/12 14:50
Bug,CAMEL-5321,12558702,Validator Component Fails on XSD with Classpath Relative Imports,"When using the Validator Component with an XSD file from a Jar that contains a relative import, the component will throw a FileNotFoundException.

This is due to the ObjectHelper.loadResourceAsStream(String) operation attempting to load the classpath resource using file path semantics that will not resolve canonically.

Test case and patch to follow.

[Thread Reference|http://camel.465427.n5.nabble.com/org-apache-camel-RuntimeCamelException-java-io-FileNotFoundException-Cannot-find-resource-in-classpaI-td5713557.html]

Best Regards,
Scott England-Sullivan
http://fusesource.com",njiang,sully6768,Major,Resolved,Fixed,30/May/12 23:55,01/Jun/13 19:18
Bug,CAMEL-5331,12559267,NPE when using bare SOAP parameter binding with payload mode,"When combining a webservice that uses bare SOAP parameter binding with payload mode in camel-cxf, you get this NullPointerException:

{noformat}
java.lang.NullPointerException
        at org.apache.camel.component.cxf.CxfEndpoint$CamelCxfClientImpl.setParameters(CxfEndpoint.java:877)
        at org.apache.cxf.endpoint.ClientImpl.doInvoke(ClientImpl.java:512)
        at org.apache.cxf.endpoint.ClientImpl.invoke(ClientImpl.java:457)
        at org.apache.camel.component.cxf.CxfProducer.process(CxfProducer.java:112)
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
...
{noformat}",njiang,gertvanthienen,Major,Resolved,Fixed,04/Jun/12 20:35,15/Jun/12 21:47
Bug,CAMEL-5333,12559335,The camel aggregation example generates an error whenever the completion predicate is entered.,"The camel aggregate example will generate an error every time that someone uses the completion predicate 'STOP'.

The issue is the type converter dont swallow conversion errors anymore.",davsclaus,davsclaus,Minor,Resolved,Fixed,05/Jun/12 07:59,05/Jun/12 08:02
Bug,CAMEL-5342,12559629,Shaded concurrentlinkedhashmap-lru conflicts with existing jar,"Package com.googlecode.concurrentlinkedhashmap:concurrentlinkedhashmap-lru is included/shaded inside the camel-core jar.

It is not very nice if concurrentlinkedhashmap-lru.jar is already on the path. It is a deal breaker, if their versions are different. For example cassandra-1.1.1 requires ConcurrentLinkedHashMap$Builder.maximumWeightedCapacity(), which is missing from the version included in camel.

It would be nice if concurrentlinkedhashmap-lru was included as a normal dependency. Comment in the pom.xml says ""Shade the googlecode stuff for OSGi"". Well, if that is strictly required, maybe it could be better included in camel-core-osgi package.

In any case, if it must be shaded at all, it would be safer to use relocation property of the maven-shade-plugin. In this case, camel could stay with the version it wants, without conflicting with explicit dependencies.",njiang,belamiru,Major,Resolved,Fixed,07/Jun/12 01:33,14/Jun/12 13:25
Bug,CAMEL-5346,12559797,camel-solr feature missing the bundle for http4 client,Just found the solr bundle is the dependencies of http4 client by running the validation on the apache-camel feature.,njiang,njiang,Major,Resolved,Fixed,08/Jun/12 06:34,09/Jun/12 13:18
Bug,CAMEL-5348,12559852,Uptime reported by Camel can be incorrect for values > 1 day.,Similar to issue KARAF-760 the context.getUptime() reports invalid durations - such as '19 days 54 hours'. Problem is in org/apache/camel/util/TimeUtils.java,njiang,markwaller,Minor,Resolved,Fixed,08/Jun/12 14:56,11/Jun/12 07:08
Bug,CAMEL-5355,12560150,camel-sql - Should support % sign in query,"See nabble


We should look into supporting the % sign in the sql component.
It should possible be less strict in uri validation as people can enter SQL queries. And to make it easy, they should just be able to dump in the SQL as is.

We should also consider adding an option to refer to a SQL that is enlisted in the registry.

Also people should be able to externalize their SQL in a .properties file and have Camel lookup that using the {{ }} notation",davsclaus,davsclaus,Minor,Resolved,Fixed,11/Jun/12 10:58,02/May/13 02:29
Bug,CAMEL-5356,12560153,CXF endpoint doesn't play nice with doTry/doCatch,"When using a CXF client endpoint to call a web service via SOAP/HTTP there are two possible error scenarios:

1) The call fails immediately with an exception (e.g. because the service is down/the address is wrong)
2) The call ""succeeds"" but returns a SOAP fault. This could also signal an error condition to the application.

Currently, using doTry/doCatch doesn't work properly in either scenario because, apprently, the CXF endpoint nulls the message when receiving an exception or fault.",onders,jensgr,Major,Resolved,Fixed,11/Jun/12 11:37,21/Dec/17 17:41
Bug,CAMEL-5357,12560154,URI normalization - Should detect already percent encoded values,"If an uri has a percent encoded value, eg using %20, %25 etc, then the normalization logic in Camel should detect this and keep the value as is.

Currently it would end up double encoding %25, that becomes %2525, and so forth.

Its the code in UnsafeUriCharactersEncoder that has the bug",davsclaus,davsclaus,Major,Resolved,Fixed,11/Jun/12 11:39,02/May/13 02:29
Bug,CAMEL-5358,12560223,camel-castor does not work in apache service mix modules,"Sorry for putting this in the wrong component but I did not see a camel-castor component.

By default, the castor org.exolab.castor.mapping.Mapping will use the class's class loader to resolve java classes declared in the castor mapping XML file.  However when the CastorDataFormat is used an a service mix OSGi module, the class loader the contains the Java classes is not the same as the class loader that loads the Mapping object.  Therefore it cannot load the mapping file because the Java classes cannot be resolved.  Instead the constructor that takes Classloader as an argument should be called when initializing the mapping object.  The classloader it should use is the same classloader used to resolve the castor mapping XML file.

The following change in AbstractCastorDataFormat seemed to resolve the problem.

public XMLContext getXmlContext(ClassResolver resolver) throws Exception {
        if (xmlContext == null) {
            xmlContext = new XMLContext();

            if (ObjectHelper.isNotEmpty(getMappingFile())) {
                Mapping xmlMap = new Mapping(
                		Thread.currentThread().getContextClassLoader());
                xmlMap.loadMapping(resolver.loadResourceAsURL(getMappingFile()));
                xmlContext.addMapping(xmlMap);
            }




",njiang,jtv,Minor,Resolved,Fixed,11/Jun/12 18:44,21/Jan/13 16:10
Bug,CAMEL-5360,12560250,Camel features file doesn't validate,"If you execute
$camel/platforms/karaf>mvn clean install -Pvalidate

it fails in the feature definition validation for the following components:
camel-quickfix
camel-cxf
camel-cdi
",hadrian,muellerc,Major,Resolved,Fixed,11/Jun/12 21:55,20/Jun/12 20:01
Bug,CAMEL-5363,12560343,camel-jms - Exception thrown from destination resolver should trigger Camel exception handling,"See nabble
http://camel.465427.n5.nabble.com/Camel-Error-handling-throwing-exception-not-moving-to-error-queue-tp5714076.html

The jms producer should catch unhandled exceptions from the Spring JMS, such as its destination resolver. And ensure the callback is invoked, to let Camel error handler be triggered.",davsclaus,davsclaus,Major,Resolved,Fixed,12/Jun/12 13:09,12/Jun/12 13:35
Bug,CAMEL-5367,12560613,DefaultJettyHttpBinding should not convert binary HTTP body,"In class DefaultJettyHttpBinding, method ""extractResponseBody"", any non-Java HTTP body is considered as string.
This corrupts binary files such as images when camel-jetty is used as a proxy http://camel.apache.org/how-to-use-camel-as-a-http-proxy-between-a-client-and-server.html
Only bodies with MIME types of non-binary data should be converted to strings.
Method ""doWriteDirectResponse"" of class ""DefaultHttpBinding"" (in camel-http) converts it again, which corrupts it even more.
",davsclaus,metatech,Major,Resolved,Fixed,14/Jun/12 09:22,18/Jun/12 13:55
Bug,CAMEL-5372,12560843,Shutting down one of the contexts breaks async redeliveries in others: RedeliveryErrorHandler has static reference to a thread pool,"ErrorRedeliveryHandler has static reference to a thread pool used for scheduling retries, thus when it's stopped it stops a pool used by other handler instances(e.g. from other contexts), thus they now can't redeliver.",davsclaus,soboleiv,Minor,Resolved,Fixed,15/Jun/12 22:47,16/Jun/12 14:00
Bug,CAMEL-5376,12594930,Mail component does not work as expected (Email Deletion is partially broken et Disconnect does not work well),"The way disconnect is implemented causes issues with some other options of the consumer. For instance ""disconnect"" option is not compatible with ""delete"" option.

The delete action is done in completion action (processCommit: line 185). On line 305, processCommit method checks if folder is open, but ""disconnect"" option force folder at null value at the end of poll method (Line 149).

I guess disconnect method should be called on completion after any other completion actions occured: It is not possible to make completion actions if connection to mail server is closed.

The result of the usage of disconnect option and delete option is a NullPointerException on test: ""if (!folder.isOpen())"" statement on line 308.

Issue should be always reproductible.

I let you fix the priority of the issue, but it is an annoying issue even if there is a workaround by disabling disconnect option ...",davsclaus,alexiskinsella,Minor,Resolved,Fixed,18/Jun/12 12:38,04/Sep/13 09:05
Bug,CAMEL-5381,12595207,Exchange's headers lost after pass through dynamicRouter if the consume from activemq,"Please see the unit test below, if consume from other endpoint such as direct, things work properly but does not when consume from activemq",davsclaus,somchaij,Major,Resolved,Fixed,20/Jun/12 05:05,25/Jun/12 10:16
Bug,CAMEL-5391,12595734,specify Export-Package version for wrap:mvn:org.fusesource.leveldbjni,,ffang,ffang,Major,Resolved,Fixed,25/Jun/12 01:03,25/Jun/12 01:07
Bug,CAMEL-5401,12596063,Not all RoutePolicy callback methods are invoked,"I am looking for a way to get notified when route is being stopped/started (resumed/suspended). I have implemented trivial logging _RoutePolicy_ (extending from _RoutePolicySupport_). However, during my tests I found that only _onInit_, _onExchangeBegin_ and _onExchangeDone_ methods are invoked. I tried both approaches - stopping/starting and suspending/resuming, but neither one worked.

Route state is being handled from another thread using ""CamelContext.*Route"" methods group. 

I am using SpringCamelApplicationContext. Route is configured using: _routePolicyRef_ attribute of _route_ tag.

I am fairly new to Camel. Let me know if I am missing something. Thanks.",davsclaus,arubtsov,Major,Resolved,Fixed,27/Jun/12 09:48,29/Jun/12 11:46
Bug,CAMEL-5404,12596086,MinaProducer does not disconnect on response timeouts,"When MinaProducer is configured with {noformat}sync=true&disconnect=true&disconnectOnNoReply=true&timeout=X{noformat} and a timeout occurs, the connection is not disconnected.

First of all disconnectOnNoReply only applies to consumers, which can be (and was) very confusing. Allthough this is according to documentation.

Secondly this might (and did!) lead to wrong response messages set on an Exchange. The following scenario:
1. Client sends a request and waits for a response (InOut) on exchange A.
2. Client times out waiting for a response after X milliseconds.
3. Client sends another request on exchange B.
4. Server sends the response for exchange A.
5. Client gets response for exchange A on exchange B (disaster!).

This happens because the producer reuses exchange A's connection for exchange B, because it was not disconnected when exchange A timed out.",davsclaus,thxmasj,Major,Resolved,Fixed,27/Jun/12 13:33,29/Jun/12 08:29
Bug,CAMEL-5405,12596121,CXF Transport loses HTTP Matrix parameters,"DefaultCxfMesssageMapper (note the typo - to be fixed in a follow-up JIRA) wrongly initializes CXF Message Message.REQUEST_URI property with the value of Camel Exchange.HTTP_PATH, even though Camel Exchange.HTTP_URI is available.

The net effect is that the HTTP matrix parameters if any attached to the last HTTP path segment are lost. Another side-effect is that Exchange.HTTP_PATH will contain the decoded value (done at HTTPServletRequest level) which can lead to the matching issues at the JAX-RS level.

A simple patch is attached",njiang,sergey_beryozkin,Major,Resolved,Fixed,27/Jun/12 17:24,27/Sep/12 12:17
Bug,CAMEL-5406,12596144,Threads EIP hangs Junit4 tests using adviceWith,"If the {code} threds() {code} DSL keyword is used in a route and a Junit4 test is performed, the test hangs indefenitely.

The issue is descibed here:

http://camel.465427.n5.nabble.com/Threads-EIP-hangs-Unit-Tests-JUnit4-td5715031.html

please find attached a self-consistent sample project to reproduce it.",davsclaus,egherardini,Minor,Resolved,Fixed,27/Jun/12 19:12,30/Jun/12 17:31
Bug,CAMEL-5411,12596395,MQTT Endpoint does not correctly return the MQTT-Payload,"MQTT Endpoint does not correctly return the MQTT-Payload. It returns the whole MQTT Message (header + payload).

The bug is in line 89 of MQTTEndpoint.java (version 2.11, but also in 2.10)

 exchange.getIn().setBody(body.getData());

must become

 exchange.getIn().setBody(body.toByteArray()); 

because getData() ignores the offset (to skip the MQTT header) set in the body byte array.
",rajdavies,ralfkornberger,Major,Resolved,Fixed,29/Jun/12 13:02,15/Aug/12 17:27
Bug,CAMEL-5412,12596403,"ObjectAlreadyExistsException when ""redeploying"" routes that uses CronScheduledRoutePolicy","When a route that has a CronScheduledRoutePolicy is removed it does not properly clean up the quartz jobs. 

The result (apart from possible memory leak) is that if the route is removed from the context and then readded with the same route id a ObjectAlreadyExistsException.


This is done using the following steps.
1. Stop the route
2. Remove the route (and it's endpoints) from the context.
3. (Re)add the route to the context using the same name.

Attached is a testcase that shows the problem.


I've solved this by adding my own implementation of onRemove (that calls doStop()) to CronScheduledRoutePolicy.
",davsclaus,pontus.ullgren,Major,Resolved,Fixed,29/Jun/12 13:44,29/Jun/12 15:01
Bug,CAMEL-5414,12596544,SqsEndpoint can't retrieve existing queue url with visibility timeout different than default,"This would happen in 2 scenarios:
1. Queue already exists with vsibility timeout different than 30 seconds (say use AWS Console to create a queue and set a different visibility timeout). Using this queue as an endpoint and problem will occur
2. Queue DOES NOT already exist and is created by SqsEndpoint (createQueue). If ""defaultVisibilityTimeout"" configured, it will still work...the first time. But restarting the endpoint (or the whole camel app) and the problem will occur.",muellerc,lpezet,Major,Closed,Fixed,01/Jul/12 20:12,19/Sep/12 20:23
Bug,CAMEL-5415,12597332,Simple does not handle empty values in predicates properly,"Example code:
{code:xml}
...
<choice>
    <when>
        <simple>${property.someProperty} == ''</simple>
        ...
    </when>
...
{code}

Error message: right must be specified",davsclaus,m0zgster,Major,Resolved,Fixed,03/Jul/12 19:56,04/Jul/12 07:24
Bug,CAMEL-5417,12597356,Scala Archetype with incorrect Maven Eclipse configuration,"The Scala archetype (camel-archetype-scala) has incorrect configuration for the Maven Eclipse Plugin to generate Eclipse files integrated with the Scala IDE.

classpath, nature and build commands 

Source: http://www.assembla.com/wiki/show/scala-ide/With_Maven_CLI",njiang,bruno.borges,Major,Resolved,Fixed,03/Jul/12 22:17,04/Jul/12 01:52
Bug,CAMEL-5419,12597445,camel-soap - Sets wrong namespace for faults,"If there is an exception on the Exchange which gets marshalled then the namespace of the fault is not the same namespace as the envelope.

The namespace on soap 1.1 is: ""http://schemas.xmlsoap.org/soap/envelope/""
But the fault is using soap 1.2: ""http://www.w3.org/2003/05/soap-envelope""

We should use the same namespace so its consistent.",davsclaus,davsclaus,Minor,Resolved,Fixed,04/Jul/12 12:03,04/Jul/12 13:36
Bug,CAMEL-5420,12597473,Camel transforms relative uri in a bad way,"When defining an endpoint with a relative uri such as 
{code}
   protocol:mypath1/mypath2
{code}
camel transforms the given uri into the following:
{code}
   protocol://mypath1/mypath2
{code}

Note that this transformation is performed before the component is given the uri as it is done in DefaultCamelContext#getEndpoint() in the call to normalizeEnpointUri().

This has the big problem that mypath1 is not considered the path anymore, but rather the authority (host:port).

So if a component wants to support both relative and absolute uris, it has no real way to know if the original uri contained an authority or not.
It is possible to support absolute uris with no authority though, as
{code}
   protocol:/mypath1/mypath2
{code}
is converted to
{code}
   protocol:///mypath1/mypath2
{code}

I'm not sure why relative uris are transformed into absolute uris, which does not really seem like a good idea to me.
",davsclaus,gnodet,Major,Resolved,Fixed,04/Jul/12 15:23,02/May/13 02:29
Bug,CAMEL-5424,12597663,@MockEndpointsAndSkip does not work,"Due to copy & paste error in CamelSpringTestContextLoader.handleMockEndpointsAndSkip @MockEndpointsAndSkip annotation is ignored and it looks for @MockEndpoints annotation. Test for this annotation is wrong as well, as it tries to mock {{mock:*}} endpoint that cannot be mocked again.

I've attached a patch that fixes both CamelSpringTestContextLoader and the test. In case I missed anything, I'm happy to improve the patch.",davsclaus,msvab,Minor,Resolved,Fixed,05/Jul/12 22:25,06/Jul/12 12:47
Bug,CAMEL-5426,12597762,Camel stream makes Karaf console hang,"features:chooseurl camel 2.10.0
features:install camel-blueprint camel-stream

Deploy the following file into the deploy folder
https://github.com/cschneider/Karaf-Tutorial/blob/master/camel/simple-blueprint/simple-camel-blueprint.xml
Undeploy the file again

Now the karaf shell hangs

@Hadrian
I think this may be caused by as the issue does not happen with camel 2.9.1
https://fisheye6.atlassian.com/changelog/camel?cs=1339933

Could you take a look at it?
",hadrian,cschneider,Major,Resolved,Fixed,06/Jul/12 12:36,23/Nov/12 08:49
Bug,CAMEL-5429,12597867,Sending null in body when forcing JMS message type to Object throws an Exception,"I set ""jmsMessageType=Object"", and when I try to send a null in the body, I get an exception. See program and log.",davsclaus,bjpeter,Minor,Resolved,Fixed,07/Jul/12 00:36,08/Jul/12 08:49
Bug,CAMEL-5432,12597991,Dynamically added SEDA-route is not working,"Dynamically removing and adding a SEDA-route creates a not working route in Camel 2.10.0.
It is working in 2.9.2.

Test-Code:
{code}
public class DynamicRouteTest extends CamelTestSupport {

    @Override
    protected RouteBuilder createRouteBuilder() throws Exception {
        return new RouteBuilder() {

            @Override
            public void configure() throws Exception {
                from(""seda:in"").id(""sedaToMock"").to(""mock:out"");
            }
        };
    }
    
    @Test
    public void testDynamicRoute() throws Exception {
        MockEndpoint out = getMockEndpoint(""mock:out"");
        out.expectedMessageCount(1);
        
        template.sendBody(""seda:in"", ""Test Message"");
        
        out.assertIsSatisfied();
        
        CamelContext camelContext = out.getCamelContext();
        camelContext.stopRoute(""sedaToMock"");
        camelContext.removeRoute(""sedaToMock"");
        
        camelContext.addRoutes(createRouteBuilder());
        out.reset();
        out.expectedMessageCount(1);
        
        template.sendBody(""seda:in"", ""Test Message"");
        
        out.assertIsSatisfied();
        
    }
} 

{code}",bvahdat,hlang,Major,Resolved,Fixed,09/Jul/12 08:05,09/Jul/12 16:12
Bug,CAMEL-5437,12598323,Add support for batch consumer's empty messages to aggregator,"Aggregator supports completion based on the batch consumer data (option completionFromBatchConsumer)

Some batch consumers (eg. File) can send an empty message if there is no input (option sendEmptyMessageWhenIdle for File consumer).

Aggregator is unable to handle such messages properly - the messages are aggregated, but Aggregator never completes.


Here is the relevant fragment from AggregateProcessor.isCompleted(String,
Exchange)

int size = exchange.getProperty(Exchange.BATCH_SIZE, 0, Integer.class);
if (size > 0 && batchConsumerCounter.intValue() >= size) {
    ....
}


Please add support for this combination of options.",davsclaus,fuzebest,Minor,Resolved,Fixed,11/Jul/12 07:01,12/Jul/12 06:59
Bug,CAMEL-5440,12598509,mock endpoint should not add the expect header check task every time we call the expectedHeaderReceived(),"If the message header value can just be consumed once, the mockEndpoint will complain the header value is null in second check, even the header value is checked rightly once.",njiang,njiang,Major,Resolved,Fixed,12/Jul/12 08:06,24/Jul/12 01:14
Bug,CAMEL-5449,12598987,"CxfPayloadConverter converts to NodeList by default, affecting performance","
The first thing CxfPayloadConverter tries to do is convert the payload to a NodeList.  When used in combination of something like XSLT which would produce a ""String"", this results in a full DOM being created for the String.   With the streaming support in the CXF component, it should FIRST attempt to convert to ""Source"" and use that.   This can then be a StreamSource or similar that can be streamed directly.

",dkulp,dkulp,Major,Resolved,Fixed,16/Jul/12 15:54,16/Jul/12 17:40
Bug,CAMEL-5451,12598996,Camel-Blueprint should wait for camel-core to be started since there is a strong dependency,"When a camel/blueprint bundle is started without camel-core being started previously, there is a problem where the camel context factory bean tries to access the {{properties}} component, which is only available when {{camel-core}} is started.

We need to ensure that the blueprint definitions created by the namespace handler also contains the needed reference to the {{properties}} component in order to make sure that blueprint will always wait for {{camel-core}} to be started (and the properties component actually available) before initializing the camel context.
",gnodet,gnodet,Major,Resolved,Fixed,16/Jul/12 16:58,16/Jul/12 17:19
Bug,CAMEL-5453,12599134,"An AWS key with ""+"" gets stripped out by DefaultCamelContext.normalizeEndointUri  and URISupport.parseQuery for at least the SQS Component","When defining a camel route that includes an endpoint for the SQS component that includes the accessKey and secretKey as parameters, such as the following:

{code}
aws-sqs://queue-name?accessKey=ABCDEFGHIJKLMNOP&secretKey=ABCDEFGHI+JK
{code}

if the secretKey contains a + then that + is stripped out in URISupport.parseQuery and the parameters received by SqsComponent.createEndpoint has a space where the + used to be in the secretKey, thus making calls to SQS by the SqsComponent fail.  

Even if the endpoint URI string is URL encoded such that secretKey=ABCDEFGHI%2BJK, then DefaultCamelContext.normalizeEndointUri will decode the %2B to a ""+"" earlier in the call stack before URISupport.parseQuery is called (which then removes it).

A ""+"" is a valid and common character in generated AWS secret keys and thus needs to be left alone when it is placed in the parameter Map<String,Object> received in SqsComponent.createEndpoint, or replaced with custom parsing in SqsComponent.createEndpoint.",davsclaus,whaley,Major,Resolved,Fixed,17/Jul/12 16:38,25/Mar/13 16:20
Bug,CAMEL-5454,12599325,camel-example-gae silently fails (needs updates to use latest CamelHttpTransportServlet),"Example silently fails.
It depends on CamelHttpTransportServlet which appears to have changed at 2.7 -  Spring context initializing RouteBuilder is never read.
Following steps were taken in 2.10 to get example to function again:

1) web.xml: needs CamelHttpTransportServlet configuration as it is on http://camel.apache.org/servlet-tomcat-example.html including adding Spring ContextLoaderListener.

2) pom.xml: Needs the following dependency added to access Spring ContextLoaderListener:

        <!-- we need spring web to read context -->
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-web</artifactId>
            <version>3.0.7.RELEASE</version>
        </dependency>

",njiang,jaysonraymond,Major,Resolved,Fixed,18/Jul/12 19:17,02/Aug/12 08:16
Bug,CAMEL-5455,12599351,Allow configuring dataCoding independend of alphabet,"See
- http://camel.465427.n5.nabble.com/Datacoding-issue-td5715971.html
- http://camel.465427.n5.nabble.com/Datacoding-Alphabet-issue-in-SMPP-td5280997.html",muellerc,muellerc,Major,Closed,Fixed,18/Jul/12 21:41,07/Nov/14 10:54
Bug,CAMEL-5456,12599590,Context scoped exception clauses incorrectly applying across RouteBuilders,"The attached patch shows a bug in the DefaultExceptionPolicyStrategy or the way ExceptionPolicys are added to a RouteBuilder's errorhandler.

In ContextScopedOnExceptionMultipleRouteBuildersReverseTest, the ordering of the routes causes the onException(IllegalArgumentException.class) to added to the ExceptionProcessors for the direct:foo-RouteBuilder. When the route is run, the OnExceptionDefinition matches exactly on the exception-type, despite this OnExceptionDefiniton being registered on a different RouteBuilder. As far as we can tell, the processor is later ignored because it's identified as being from a different route-context.

We have attached corresponding tests for camel-spring that show that the issue is tied to alphabetical ordering of RouteBuilders when using contextscan.
",davsclaus,andreaja,Major,Resolved,Fixed,20/Jul/12 07:42,30/Sep/12 12:12
Bug,CAMEL-5459,12599844,Folder closed too early when working asynchronously,"When using a MailConsumer connected with IMAP to a mailserver and routing the messages to SEDA or any other asynchronous endpoint, 
it can cause a com.sun.mail.util.FolderClosedIOException or javax.mail.FolderClosedException to happen if you are trying to process the mails/messages attachments.

This is because the IMAP folder gets closed at the end of each MailConsumer.poll()
I propose to add a new MailConfiguration option ""closeFolder"", which can override this behaviour.


Here's an example on how to reproduce the problem. (Scala code, sorry! But it should be self-explanatory)
You need a couple of mails with attachments in your mailbox to test this. 
~10 mails should be enough to cause the exceptions.



import javax.mail.internet.MimeMultipart
import org.apache.camel.impl.DefaultCamelContext
import org.apache.camel.scala.dsl.builder.RouteBuilder
import scala.io.Source

object Main { 
  def main(args: Array[String]): Unit = {
    val camelContext = new DefaultCamelContext
  
    val host = ""imap.gmail.com""
    val username = ""*****@gmail.com""
    val password = ""*****""
    
    // this will cause com.sun.mail.util.FolderClosedIOException or javax.mail.FolderClosedException
    val endpoint = ""imaps://""+host+""?password=""+password+""&username=""+username+""&folderName=INBOX&unseen=false&disconnect=false&mapMailMessage=true&connectionTimeout=60000""
    
    // with fix / new feature 'closeFolder' set to false
    //val endpoint = ""imaps://""+host+""?password=""+password+""&username=""+username+""&folderName=INBOX&unseen=false&disconnect=false&mapMailMessage=true&connectionTimeout=60000&closeFolder=false""
    
    camelContext.addRoutes(
      new RouteBuilder {
        endpoint
        .to(""seda:process"")
         
        from(""seda:process?concurrentConsumers=5"") ==> {
          process { ex => 
            val att = ex.getIn.getBody.asInstanceOf[MimeMultipart].getBodyPart(1)
            println(""Attachment Name:"" + att.getFileName)
            println(""Attachment Content:"" + att.getDataHandler.getContent)
            println(""Attachment Content:"" + Source.fromInputStream(att.getDataHandler.getInputStream).mkString)
            Thread.sleep(2500) // simulate some work being done here with the attachment, takes time..
          }
        }
      }
    )

    camelContext.start()
    Thread.sleep(45 * 1000)
    camelContext.stop()
  }
}
",davsclaus,jamesclonk,Minor,Resolved,Fixed,23/Jul/12 12:57,03/Feb/13 09:56
Bug,CAMEL-5461,12599983,IOConverter.toInputStream opens a BufferReader which will never be closed,If called with a non-null charset IOConvert.toInputStream returns a InputStream which will never close the opened BufferedReader.,njiang,funkmutterdekka,Major,Resolved,Fixed,24/Jul/12 12:16,26/Jul/12 07:18
Bug,CAMEL-5464,12599998,camel-jms consumer doesn't send back a reply in all cases,"In a very simple route consuming from a Camel JMS endpoint receiving InOut exchanges (i.e. JMSReplyTo header present), the endpoint will not send back replies.

This happens because Camel JMS only returns a reply if the OUT message is set. But if the route looks like: consumer => processor, and Camel doesn't find the need to ""weave in"" an implicit Pipeline processor, no one will implicitly take care of mapping the IN message to an OUT message (unless the user knows about these internal aspects - but we shouldn't expect them too).

As a result, these routes DON'T WORK...

{code}
<route>
   <from uri=""timer:foo?fixedRate=true&amp;period=10000"" />
   <setBody><constant>Hello Raul</constant></setBody>
   <to uri=""log:SendingRequest?showAll=true"" />
   <inOut uri=""activemq:queue:test1?requestTimeout=1000"" />
   <to uri=""log:ReceivedReply?showAll=true"" />
</route>
        
<route>
   <from uri=""activemq:queue:test1"" />
   <to uri=""log:ReceivedRequest?showAll=true"" />
</route>
{code}

... but just by adding an additional log endpoint to the second route (or any other thing, for that matter), it starts to work because Camel weaves in the Pipeline processor.

Other workarounds that work:
* -explicitly wrapping the log endpoint in a <pipeline> DSL-
* <setBody><simple>${in.body}</simple></setBody>

Or simply introducing anything that will force Camel to insert a Pipeline processor.

IMHO, there are two solutions to avoid this issue:
# Always weave in a Pipeline processor (adds overhead in simple routes and may cause regressions)
# Adapt EndpointMessageListener to pick the IN message when the exchange is out capable and expectation of a reply exists

I'm happy to work on a patch for Camel 2.10.1.

*EDIT:* Just wrapping the single endpoint in <pipeline /> doesn't function as a workaround.",njiang,raulvk,Major,Resolved,Fixed,24/Jul/12 13:01,07/Nov/12 10:30
Bug,CAMEL-5468,12600165,RegistryBean.getBean()  is not thread safe,"Here is the mail thread[1] of it.

[1]http://camel.465427.n5.nabble.com/ClassCastException-using-cxf-bean-td5599810.html#a5716454",njiang,njiang,Major,Resolved,Fixed,25/Jul/12 13:57,10/Sep/13 11:59
Bug,CAMEL-5470,12600285,camel-example-gae gtask doesn't work,"I just checked the log and found the gtask doesn't work due to 
{code}
Caused by: java.lang.ClassNotFoundException: javax.xml.transform.stax.StAXSource
	at com.google.apphosting.runtime.security.UserClassLoader.loadClass(UserClassLoader.java:451)
{code}
",njiang,njiang,Major,Resolved,Fixed,26/Jul/12 09:19,02/Aug/12 08:15
Bug,CAMEL-5472,12600365,missing dependency to spring-web in camel-example-cxf-tomcat ,,muellerc,muellerc,Major,Closed,Fixed,26/Jul/12 19:16,26/Jul/12 19:35
Bug,CAMEL-5475,12600548,Can't unmarshal pgp messages encrypted with ElGamal.,"Current strategy for finding the private key is to iterate the secret keychain and matching on the configured email address.  This is incorrect; the secret key chain should be searched by the key id found in the public key encrypted message. What happens now is the DSA signing key is matched instead of the ElGamal decryption key, and this throws java.security.InvalidKeyException.",njiang,scrybz,Major,Resolved,Fixed,28/Jul/12 03:52,13/Aug/12 12:26
Bug,CAMEL-5485,12601467,camel-mybatis should do a proper transaction demarcation while reading/writing from/to database.,"See http://camel.465427.n5.nabble.com/MyBatis-Component-Bug-consumer-onConsume-hits-error-transactions-are-committed-instead-of-rollback-td5716774.html

Other than DefaultMyBatisProcessingStrategy the same issue exists by MyBatisProducer as well.

Another thing which is currently missing completley is that there's no transaction demarcation by the Producer at all while *reading* from the database, e.g. by SelectOne or SelectList statement types.",bvahdat,bvahdat,Minor,Resolved,Fixed,04/Aug/12 21:12,04/Aug/12 21:57
Bug,CAMEL-5487,12601642,HttpProducer should close temporary file in CachedOutputStream when the IOException is thrown,"Here is the mail thread about it.
 http://camel.465427.n5.nabble.com/HTTP4-HttpProducer-close-temporary-file-in-CachedOutputStream-tp5716885.html",njiang,njiang,Major,Resolved,Fixed,07/Aug/12 00:20,08/Aug/12 06:30
Bug,CAMEL-5490,12602129,Queue is Full in DefaultServicePool,"I use a Netty endPoint to forward messages to a client. These messages come from an another Netty endPoint.

After several forwarded messages, an error occurs on every forwarded message:
DEBUG 2012-08-06 15:30:06,482 [New I/O  worker #27] org.apache.camel.component.netty.handlers.ClientChannelHandler - Closing channel as an exception was thrown from Netty
java.lang.IllegalStateException: Queue full
        at java.util.AbstractQueue.add(AbstractQueue.java:71)
        at java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:209)
        at org.apache.camel.impl.DefaultServicePool.release(DefaultServicePool.java:96)
        at org.apache.camel.impl.ProducerCache$1.done(ProducerCache.java:304)
        at org.apache.camel.processor.SendProcessor$2$1.done(SendProcessor.java:125)
        at org.apache.camel.component.netty.handlers.ClientChannelHandler.messageReceived(ClientChannelHandler.java:162)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:423)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:538)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:437)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:91)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:373)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:247)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",njiang,philippe.suray,Major,Resolved,Fixed,08/Aug/12 07:14,24/Oct/12 15:40
Bug,CAMEL-5495,12602556,file endpoints on windows without volume name in the path url not working in some cases,"I am using file endpoints on Windows without the volume name just like the normal unix based paths look.
That means, I have something like  file:///tmp/file-in and file:///tmp/file-out

The funny thing that I noticed is that each endpoint itself works fine, but when a route is set up from one to the other, it fails because there seems to be an inconsistency in the way the windows path is handled in the camel-core's file component.

As mentioned, each file endpoint itself is working fine. For example, the consumer file endpoint configured in a route
from(""file:///tmp/file-in"").to(""mock:test"")
works fine.

Similarly, the produce file endpoint configured in a route
from(""direct:test"").to(""file:///tmp/file-out"")
works fine.

But when a route is setup to connect these two file endpoints, the producer endpoint fails to create the output file.
Concretely, there is the following code in GenericFileEndpoint.configureMessage that determines the file name.

             String name = file.isAbsolute() ? file.getAbsoluteFilePath() : file.getRelativeFilePath();

            // skip leading endpoint configured directory
            String endpointPath = getConfiguration().getDirectory() + getFileSeparator();

In this particular case, the name variable is set to ""C:\tmp\file-in\sample.xml"" while the endpointPath variable is set to ""\tmp\file-in"".
So, the subsequent code to extract the file name part, shown below, fails to match the path.

            if (ObjectHelper.isNotEmpty(endpointPath) && name.startsWith(endpointPath)) {
                name = ObjectHelper.after(name, endpointPath);
            }

As a result, the file name is not extracted as ""sample.xml"" but remain unchanged as ""C:\tmp\file-in\sample.xml"".
Consequently, when the file producer endpoint tries to write this file in the file system, it tries to write a file as ""/tmp/file-out/C:\tmp\file-in\sample.xml"", resulting in an error.

I modified FileComponent and FileEndpoint so that the endpoint path is stored correctly in this case to make the above extraction code find the file name part correctly.

Attached is this proposed patch and a test case which runs on windows and on non-windows but this issue can only be demonstrated under windows.

testRouteToFileOnly and testRouteFromFileOnly work without this patch while testRouteFileToFile fails without this patch on windows.

Thanks for looking into this issue.

Regards, Aki


",njiang,ay,Minor,Resolved,Fixed,09/Aug/12 16:48,13/Aug/12 09:31
Bug,CAMEL-5499,12603162,Cxf fallback convert should return the Void.value instead of null,"As Camel will unregister the fallback converter if it convert the value to be null, we should avoid unregistering the fallback converter by returning the Void.value.",njiang,njiang,Major,Resolved,Fixed,13/Aug/12 02:08,13/Aug/12 03:47
Bug,CAMEL-5500,12603205,camel-testng - The spring base test class should use @AfterMethod on teardown as its parent does,"See nabble

http://camel.465427.n5.nabble.com/Problem-with-testng-CamelSpringTestSupport-several-contexts-runs-simultaneously-tp5716447.html",davsclaus,davsclaus,Minor,Resolved,Fixed,13/Aug/12 09:03,13/Aug/12 09:06
Bug,CAMEL-5501,12603221,@XPath annotation should ensure cleanup after evaluation,"See CAMEL-3813

Look into the @XPath bean parameter binding and ensure its resources is cleaned up after usage.",davsclaus,davsclaus,Minor,Resolved,Fixed,13/Aug/12 11:07,13/Aug/12 16:15
Bug,CAMEL-5504,12603429,Bridging http endpoints with spaces in content path causes problem when forwarding request,"Having spaces in the content path in the <from> jetty endpoint that is being bridged causes problem when request is forwarded.

{code}
Caused by: org.apache.camel.RuntimeExchangeException: Cannot analyze the Exchange.HTTP_PATH header, due to: cannot find the right HTTP_BASE_URI on the exchange: Exchange[Message: [Body is instance of org.apache.camel.StreamCache]]
	at org.apache.camel.component.http.helper.HttpHelper.createURL(HttpHelper.java:195)
{code}",davsclaus,davsclaus,Major,Resolved,Fixed,14/Aug/12 10:20,14/Aug/12 10:46
Bug,CAMEL-5505,12603450,Unable to customize URIResolver in xslt component,"In xslt component we can't customize uriResolver For instance in route with uri ""xslt:org/apache/camel/component/xslt/include_not_existing_resource.xsl?uriResolver=#customURIResolver"" camel still uses default XsltUriResolver.",hadrian,asbestos,Minor,Resolved,Fixed,14/Aug/12 13:33,22/Aug/12 16:03
Bug,CAMEL-5506,12603493,HBase component does not extract that table name in all cases and HBase runtime version issues.,"The current hbase component uses the URI getHost method to retrieve the table name but this method returns null if the host name does not conform to the requirements of this method.  For instance, it should include a '.' in the name, or is a ipv4 or ipv6 valid address string.  If the table name is a simple name then the getHost method will return null instead of the name of the table.  This will in turn cause a NPE exception.

The other issue is that if you are using 0.92.x or greater with the current implementation of the camel-hbase component.  An IllegalArgumentException is generated because of an incompatible host:port pair.  This is caused by the fact that the current hbase integration uses 0.90.5 and this will conflict with those versions.

I have a patch that solves both issues but in the second case.  It will require that the user define the specific hbase client version that they need to use.

btw, the camel-hbase component is not available as an option for the Component/s drop down list.
",davsclaus,ccorsi,Major,Resolved,Fixed,14/Aug/12 19:06,15/Aug/12 15:20
Bug,CAMEL-5507,12603496,CamelBlueprintTestSupport Cannot correctly delete target\bundles\routetest.jar,"When creating a new fuse project using the camel-blueprint archetype and  having multiple test methods it has problems deleting 'target\bundles\routetest.jar' in the setUp method of CamelBlueprintTestSupport

Reproduction steps:
- Create new fuse project from camel-blueprint archetype
- Open the RouteTest.java in (src/test/java/com/mycompany/camel/blueprint/RouteTest.java)
- Copy the 'testRoute()' method a couple of times and give it a different method name to best see this problem.
- Run the maven tests

The console will say this after the first test runs:
{code}
                    Thread-7] BlueprintCamelContext          INFO  Apache Camel 2.9.0.fuse-7-061 (CamelContext: blueprintContext) is shutdown in 0.005 seconds. Uptime 0.085 seconds.
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles\routetest.jar failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles\routetest.jar failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles\routetest.jar failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles\routetest.jar failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles\routetest.jar failed
[                          main] TestSupport                    WARN  Deletion of file: C:\FuseIDEEnterprise-2.1.462\workspace\camel-blueprint\target\bundles failed
[                          main] CamelBlueprintHelper           INFO  Using Blueprint XML file: /C:/FuseIDEEnterprise-2.1.462/workspace/camel-blueprint/target/classes/OSGI-INF/blueprint/blueprint.xml
{code}

You can also also get PojoSR class loader issues when running the exact same project as JUnit tests, rather than maven tests. But these errors may be related so I won't raise a ticket for that.

I will however give the stack trace for this, in case it _isn't_ related
{code}
org.osgi.framework.BundleException: Unable to start bundle
	at de.kalpatec.pojosr.framework.PojoSRBundle.start(PojoSRBundle.java:144)
	at de.kalpatec.pojosr.framework.PojoSR.startBundles(PojoSR.java:405)
	at de.kalpatec.pojosr.framework.PojoSR.<init>(PojoSR.java:322)
	at de.kalpatec.pojosr.framework.PojoServiceRegistryFactoryImpl.newPojoServiceRegistry(PojoServiceRegistryFactoryImpl.java:51)
	at org.apache.camel.test.blueprint.CamelBlueprintHelper.createBundleContext(CamelBlueprintHelper.java:111)
	at org.apache.camel.test.blueprint.CamelBlueprintTestSupport.setUp(CamelBlueprintTestSupport.java:35)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.lang.NoClassDefFoundError: org/eclipse/core/runtime/Plugin
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(Unknown Source)
	at java.security.SecureClassLoader.defineClass(Unknown Source)
	at java.net.URLClassLoader.defineClass(Unknown Source)
	at java.net.URLClassLoader.access$100(Unknown Source)
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at de.kalpatec.pojosr.framework.PojoSRBundle.start(PojoSRBundle.java:130)
	... 30 more
Caused by: java.lang.ClassNotFoundException: org.eclipse.core.runtime.Plugin
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	... 43 more
{code}",davsclaus,alanfoster,Major,Resolved,Fixed,14/Aug/12 19:37,20/Aug/12 14:03
Bug,CAMEL-5509,12603591,"Jetty http consumer doesn't suppor the uri ""jetty:http://localhost:9000""","When you using ""jetty:http://localhost:9000"" as the jetty http consumer uri, you will get a complain of 404 when you using ""http://localhost:9000"" to access the service.
",njiang,njiang,Minor,Resolved,Fixed,15/Aug/12 14:39,20/Aug/12 02:30
Bug,CAMEL-5510,12603638,Competing Consumers fails for jms queue using AdviceWith,"I was trying to test two routes that start from the same endpoint.  The endpoint is a jms queue, so it should be able to have multiple consumers attach to it.  When I use the standard CamelTestSupport methodology and let it create and start my context, mock my out points, etc. everything passes (see CompetingConsumersWorks.java).  However when I try and use the strategy of not letting the junit support create everything, ie. 
        public boolean isUseRouteBuilder() { return false; } 
        public boolean isUseAdviceWith() { return true; } 
and try and use AdviceWithRouteBuilder to mock out routes the test doesn't pass (see CompetingConsumersDoesNotWork.java).  I can actually get it to work, but only if I start the context before adding the second set of routes to the context (see comments in code).   ",davsclaus,wjmcdonald,Minor,Resolved,Fixed,15/Aug/12 20:36,19/Sep/13 05:44
Bug,CAMEL-5517,12603859,Multiple calls to CXF factory beans.getFeatures causing multiple logging features to be added,"
Each call to getFeatures() is adding another LoggingFeature into the array.  Would be better to add/remove the feature when it's configured.",dkulp,dkulp,Major,Resolved,Fixed,17/Aug/12 13:54,17/Aug/12 14:11
Bug,CAMEL-5518,12603885,Camel Proxy ignores camelContextId property in Spring configuration,"Camel Proxy, if configured outside camelContext element in Spring configuration, ignores camelContextId property. (Actual code only cares about this property, if camelContext isn't set - which it always is by then.)

Steps to reproduce: Configure two contexts in spring configuration. Then in the same config file configure two proxies, each pointing to the same URI, but with different camelContextId specified. Both proxies will be created pointing to the same (first) camel context, despite being configured to different contexts.",njiang,koscejev,Critical,Resolved,Fixed,17/Aug/12 16:37,20/Aug/12 02:35
Bug,CAMEL-5519,12603950,CamelContextId should be effected when there are more than two camel context in the spring configuration file ,,njiang,njiang,Major,Resolved,Fixed,18/Aug/12 09:52,20/Aug/12 02:32
Bug,CAMEL-5525,12604189,Load balancing with “Random” policy is not halfway evenly distributed to endPoint with vast exchange ,"Look the code about choosing processor in camel: http://camel.apache.org/maven/current/camel-core/apidocs/org/apache/camel/processor/loadbalancer/RandomLoadBalancer.html
{noformat} 
 protected synchronized Processor chooseProcessor(List<Processor> processors, Exchange exchange) {
        int size = processors.size();
        while (true) {
            int index = (int) Math.round(Math.random() * size);
            if (index < size) {
                return processors.get(index);
            }
        }
}
{noformat} 
For example, processors.size() = 3,
so, Math.random() * size = 0.0 .. 3.0,
So, index is either 0, or 1, or 2, or 3,
so there is four variants, while we are looking for three variants.
and, variant ""3"" is then thrown away in ""if"" comparison and re-selected.that is a source of uneveningness.when there are vast message exchange, the load radio for three endpoints are approach to 20%,40%,40%. it is unreasonable radio,should approach 33% allowing the amount of difference for each endpoint(smaller than 10%). 

For the above code, can replace ""int index = (int) Math.round(Math.random() * size)"" with ""int index = (int)Math.ceil(Math.random() * 3 - 1)""
",davsclaus,xlding,Minor,Resolved,Fixed,21/Aug/12 08:34,26/Aug/12 10:15
Bug,CAMEL-5526,12604190,"""ThreadID is already used"" exception under heavy load","In situations with heavy load or under load tests we rarely got exception:
{noformat} 
2012-08-21 12:28:02,620 [ool-26-thread-7] ERROR DefaultErrorHandler            - Failed delivery for (MessageId: ID-atitov-pc-60956-1345537671897-2-143205 on ExchangeId: ID-atitov-pc-60956-1345537671897-2-143206). Exhausted after delivery attempt: 1 caught: java.lang.IllegalArgumentException: ThreadID is already used
java.lang.IllegalArgumentException: ThreadID is already used
	at org.jivesoftware.smack.ChatManager.createChat(ChatManager.java:163)
	at org.apache.camel.component.xmpp.XmppPrivateChatProducer.getOrCreateChat(XmppPrivateChatProducer.java:97)
	at org.apache.camel.component.xmpp.XmppPrivateChatProducer.process(XmppPrivateChatProducer.java:65)
	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:120)
	at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:292)
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:115)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:330)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:220)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:117)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122)
	at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:61)
	at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:86)
	at org.apache.camel.processor.UnitOfWorkProducer.process(UnitOfWorkProducer.java:63)
	at org.apache.camel.impl.ProducerCache$2.doInProducer(ProducerCache.java:360)
	at org.apache.camel.impl.ProducerCache$2.doInProducer(ProducerCache.java:1)
	at org.apache.camel.impl.ProducerCache.doInProducer(ProducerCache.java:227)
	at org.apache.camel.impl.ProducerCache.sendExchange(ProducerCache.java:331)
	at org.apache.camel.impl.ProducerCache.send(ProducerCache.java:169)
	at org.apache.camel.impl.DefaultProducerTemplate.send(DefaultProducerTemplate.java:111)
	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:124)
	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:131)
	at org.apache.camel.component.xmpp.XmppProducerConcurrentTest$1.call(XmppProducerConcurrentTest.java:79)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2012-08-21 12:28:02,621 [ool-26-thread-5] ERROR DefaultErrorHandler            - Failed delivery for (MessageId: ID-atitov-pc-60956-1345537671897-2-143207 on ExchangeId: ID-atitov-pc-60956-1345537671897-2-143208). Exhausted after delivery attempt: 1 caught: java.lang.IllegalArgumentException: ThreadID is already used
java.lang.IllegalArgumentException: ThreadID is already used
	at org.jivesoftware.smack.ChatManager.createChat(ChatManager.java:163)
	at org.apache.camel.component.xmpp.XmppPrivateChatProducer.getOrCreateChat(XmppPrivateChatProducer.java:97)
	at org.apache.camel.component.xmpp.XmppPrivateChatProducer.process(XmppPrivateChatProducer.java:65)
	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:120)
	at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:292)
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:115)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:330)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:220)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:117)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122)
	at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:61)
	at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:86)
	at org.apache.camel.processor.UnitOfWorkProducer.process(UnitOfWorkProducer.java:63)
	at org.apache.camel.impl.ProducerCache$2.doInProducer(ProducerCache.java:360)
	at org.apache.camel.impl.ProducerCache$2.doInProducer(ProducerCache.java:1)
	at org.apache.camel.impl.ProducerCache.doInProducer(ProducerCache.java:227)
	at org.apache.camel.impl.ProducerCache.sendExchange(ProducerCache.java:331)
	at org.apache.camel.impl.ProducerCache.send(ProducerCache.java:169)
	at org.apache.camel.impl.DefaultProducerTemplate.send(DefaultProducerTemplate.java:111)
	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:124)
	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:131)
	at org.apache.camel.component.xmpp.XmppProducerConcurrentTest$1.call(XmppProducerConcurrentTest.java:79)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

{noformat} 

After debugging and looking at source code we figured out, that smack use some sort of map with week references for chat cache. When memory is low java GC discards chat instances, but in Process() method they instantiated over and over again without synchronization:
{code:title=XmppPrivateChatProducer.java|borderStyle=solid}
 public void process(Exchange exchange) {
 .........................
	Chat chat = chatManager.getThreadChat(endpoint.getChatId());
	if (chat == null) {
		LOG.trace(""Creating new chat instance with thread ID {}"", endpoint.getChatId());
		chat = chatManager.createChat(getParticipant(), endpoint.getChatId(), new MessageListener() {
			public void processMessage(Chat chat, Message message) {
				// not here to do conversation
				if (LOG.isDebugEnabled()) {
					LOG.debug(""Received and discarding message from {} : {}"", getParticipant(), message.getBody());
				}
			}
		});
	}
 .........................
}
{code} 
But smack implementation prohibits chat instances with same chatId.",davsclaus,asbestos,Minor,Resolved,Fixed,21/Aug/12 08:58,26/Aug/12 09:47
Bug,CAMEL-5527,12604198,Maven archetype - Generates wrong plugin for surefire plugin,The blueprint archetype creates wrong pom.xml. The groupId for the surefire plugin is wrong.,davsclaus,davsclaus,Minor,Resolved,Fixed,21/Aug/12 10:13,21/Aug/12 10:34
Bug,CAMEL-5535,12604449,"Multiple restlet routes of the same restlet URI but different VERBs does not work in Apache Camel 2.10.0 , both routes work in Apache Camel 2.9.1.  ","Multiple restlet routes of the same restlet URI but different VERBs does not work in Apache Camel 2.10.0 , both routes work in Apache Camel 2.9.1.  

Reproduce Steps:
-----------------
1) Add a restlet route with a from uri of restlet:///nimbus/hello.restlet/1.0?restletMethods=get

<route xmlns=""http://camel.apache.org/schema/spring"" trace=""true"" >
      <from uri=""restlet:///nimbus/hello.restlet/1.0?restletMethods=get""/>
    <transform>
      <simple>Hello Restlet Method: ${header.CamelHttpMethod}  ID:${header.id}</simple>
    </transform>

  </route>


2) Add a restlet route with a from uri of restlet:///nimbus/hello.restlet/1.0?restletMethods=post

   <route xmlns=""http://camel.apache.org/schema/spring"" trace=""true"" >
      <from uri=""restlet:///nimbus/hello.restlet/1.0?restletMethods=post""/>
    <transform>
      <simple>Hello Restlet Method: ${header.CamelHttpMethod}  ID:${header.id}</simple>
    </transform>

  </route>

3)  route 1work fine, but #2 results in the following exception. both route work fine in Camel 2.9.1.

Exception:

012-07-27 10:29:20,646-0400 ERROR grails.app.service.nimbus.ComputeService localhost addRoute for hello.restletpost failed - Failed to start route nimbus.hello.restletpost.1.0 because of Multiple consumers for the same endpoint is not allowed: Endpoint[/nimbus/hello.restlet/1.0]
 Caused by: org.apache.camel.FailedToStartRouteException: Failed to start route nimbus.hello.restletpost.1.0 because of Multiple consumers for the same endpoint is not allowed: Endpoint[/nimbus/hello.restlet/1.0]
 at org.apache.camel.impl.DefaultCamelContext(doStartOrResumeRouteConsumers:1993) 


I've already posted on 
http://camel.465427.n5.nabble.com/Apache-Camel-2-10-0-multiple-restlet-routes-of-the-same-restlet-URI-but-different-VERBs-does-not-work-td5716556.html
 
",njiang,amit1000,Major,Resolved,Fixed,22/Aug/12 15:35,10/Sep/12 14:18
Bug,CAMEL-5536,12604478,Possible issue in camel-apns - Reported on user forum,"See nabble
http://camel.465427.n5.nabble.com/Issue-with-ApnsServiceFactory-tp5717573.html",davsclaus,davsclaus,Minor,Resolved,Fixed,22/Aug/12 19:46,27/Sep/12 10:49
Bug,CAMEL-5537,12604581,Mina2 Consumer doesn't reliably work with SSL,"I've been trying to write a service using Camel 2.10.0 which uses the mina2 component to expose a service, and which uses SSL, using an endpoint of:

mina2:tcp://localhost:6500?sync=true&filters=#hl7SslFilterFactory,#hl7CodecFilter&allowDefaultCodec=false

however, what I found was that the SSL handshake was failing quite often for no apparent reason.  Investigating further, it appeared that messages sent during the handshake were being processed out of order - specifically, when the client (which uses a standard java SSLSocket) sent a ""TLSv1 Change Cipher Spec"" followed by a ""TLSv1 Handshake"", the ""TLSv1 Handshake"" was being processed before the ""TLSv1 Change Cipher Spec"", breaking the process.

This appears to have been caused because when the the Mina2Consumer configures MINA for tcp (in Mina2Consumer.setupSocketProtocol), it is using a UnorderedThreadPoolExecutor - which allows messages to be processed in any order.  Switching this to use a OrderedThreadPoolExecutor instead has fixed the issue.

May I request that the use of UnorderedThreadPoolExecutor vs OrderedThreadPoolExecutor be configurable via endpoint parameters? (or OrderedThreadPoolExecutor is used when SSL if configured).  I'm assuming the reason for the use of UnorderedThreadPoolExecutor is simply for performance. ",davsclaus,andrewlawrenson,Major,Resolved,Fixed,23/Aug/12 09:28,26/Aug/12 09:29
Bug,CAMEL-5540,12604965,File consumer - NPE if configured to consume from root directory,"See nabble
http://camel.465427.n5.nabble.com/Setting-root-folder-as-endpoint-Could-not-poll-endpoint-NPE-tp5717845.html",davsclaus,davsclaus,Minor,Resolved,Fixed,25/Aug/12 09:08,25/Aug/12 10:19
Bug,CAMEL-5542,12605050,Timer component - Should defer scheduling tasks during startup of CamelContext,We should defer scheduling timer tasks during startup of CamelContext. As they may fire very fast. We should defer this using a StartupListener just as we do with quartz component etc.,davsclaus,davsclaus,Minor,Resolved,Fixed,27/Aug/12 08:51,18/Mar/14 11:46
Bug,CAMEL-5543,12605201,Karaf Command 'camel:route-list' throws NullpointerException,"Sometimes I get an NPE when listing routes in Karaf:
2012-08-28 12:20:01,734 | INFO  | l Console Thread | Console                          | 36 - org.apache.karaf.shell.console - 2.2.8 | Exception caught while executing command
java.lang.NullPointerException
        at org.apache.camel.karaf.commands.RouteList.doExecute(RouteList.java:65)[129:org.apache.camel.karaf.camel-karaf-commands:2.10.0]
        at org.apache.karaf.shell.console.OsgiCommandSupport.execute(OsgiCommandSupport.java:38)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.commands.basic.AbstractCommand.execute(AbstractCommand.java:35)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.CommandProxy.execute(CommandProxy.java:78)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.Closure.executeCmd(Closure.java:474)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.Closure.executeStatement(Closure.java:400)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.Pipe.run(Pipe.java:108)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:183)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:120)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.felix.gogo.runtime.CommandSessionImpl.execute(CommandSessionImpl.java:89)[36:org.apache.karaf.shell.console:2.2.8]
        at org.apache.karaf.shell.console.jline.Console.run(Console.java:166)[36:org.apache.karaf.shell.console:2.2.8]
        at java.lang.Thread.run(Thread.java:662)[:1.6.0_24]
",davsclaus,laeubi,Minor,Resolved,Fixed,28/Aug/12 11:14,19/Nov/13 13:10
Bug,CAMEL-5555,12605770,In ScheduledRoutePolicy a suspended route cannot be started,"If a route is suspended using the ScheduledRoutePolicy and its subclasses (CronScheduledRoutePolicy), it cannot be resumed or started.

Looking at the code in ScheduledRoutePolicy.onJobExecute(), on start/resume actions, the route status is first checked and must be stopped or suspended before any actions is taken.

Suspension of a route, only suspends the consumer and not the route, and hence the start/resume after a suspend does nothing.

One solution could be to also check the status of the consumer on start/resume. This would require adding a isSuspended() to serviceHelper and should be relatively straightforward.",njiang,apejavar,Major,Resolved,Fixed,31/Aug/12 18:47,10/Sep/12 14:57
Bug,CAMEL-5556,12605869,HazelcastIdempotentRepository is not thread safe,"The Hazelcast idempotent repository does not add and confirm messages in a thread safe manner.

Patch attached to handle that.",njiang,krishy,Major,Resolved,Fixed,01/Sep/12 23:14,10/Sep/12 14:27
Bug,CAMEL-5562,12606061,camel-beanio using beanio 2.0 should be using the 2012/03 beanio namespace,"camel-beanio which uses beanio-2.0.x library should use the beanio 2.0's namepsace (http://www.beanio.org/2012/03) and not the 1.2 namespace ( http://www.beanio.org/2011/01) in its test cases.

The background to this is described in this mail thread.
http://camel.465427.n5.nabble.com/issue-with-camel-beanio-s-version-used-in-trunk-td5718596.html
",davsclaus,ay,Minor,Resolved,Fixed,04/Sep/12 11:46,07/Sep/12 06:17
Bug,CAMEL-5564,12606086,Should shutdown management load task thread pool explicit,"If using JMX with Camel there is a background thread, that gathers load stat periodically. We should shutdown this task/thread pool explicit.

This ensures cleaner shutdown.",davsclaus,davsclaus,Minor,Resolved,Fixed,04/Sep/12 14:07,05/Sep/12 12:16
Bug,CAMEL-5570,12606219,maximumRedeliveries is inherited for other exceptions thrown while redelivering with maximumRedeliveries(-1),"Given a route:

{code}
from(""direct:source"")
   .onException(FirstException.class)
         .handled(true)
         .maximumRedeliveries(-1)
    .end()
    .onException(SecondException.class)
        .handled(true)
        .to(""direct:error"")
    .end()
    .to(""direct:destination"");
{code}

If the consumer of direct:destination throws a FirstException, the message will be redelivered. Now if a SecondException is thrown while redelivering the message to direct:destination, it does NOT go to direct:error, as you would expect, but is redelivered again; using the same RedeliveryPolicy as for FirstException.

I have attached a test that illustrates this.

In OnExceptionDefinition.createRedeliveryPolicy, maximumRedeliveries is set to 0 if the OnExceptionDefinition has outputs and the parent RedeliveryPolicy has explicitly set maximumRedeliveries > 0. The latter check fails when maximumRedeliveries is -1 (infinite retries), and the parent RedeliveryPolicy is returned.

I have attached a patch that ensures that we don't inherit the parent maximumRedeliveries even if it is set to -1.",davsclaus,oyvindio,Minor,Resolved,Fixed,05/Sep/12 09:06,05/Sep/12 13:52
Bug,CAMEL-5571,12606232,Camel proxies should not forward hashCode() method invocations,"Given a Camel proxy for an @InOnly service interface, and a route from the proxy to a JMS endpoint, calling hashCode() on the proxy throws an exception, either immediately or after a number of retries, depending on the route configuration.

See the attached test case for different scenarios.

The reason is that hashCode() is forwarded by the CamelInvocationHandler to the remote endpoint, which does not make sense in this case.
",davsclaus,hwellmann,Major,Resolved,Fixed,05/Sep/12 12:28,28/Sep/12 14:02
Bug,CAMEL-5574,12606484,camel-sftp's stepwise behavior to walk up and down relatively to avoid potential traversal issues,"I am having a directory traversal problem using the stepwise mode of camel-sftp at the producer side. Basically, it can walk down the path from the starting directory to the walking directory, but it cannot go back correctly to the original starting directly. The server is SSH-2.0-Cleo VLProxy/3.0.1 SSH FTP server.
I saw a related JIRA issue CAMEL-3309 that describes some issues in the stepwise traversal and some background to this related problem.

My question is on the part that changes the working directory back to the original starting folder. I am wondering why we are using stepwise traversal from the ancestor directory towards the starting child directory, instead of moving upwards relatively from the working directory back to the starting directory. This reverse traversal does not require accessing the ancestry path above the staring directory (hence, not affected by the accessing problem). And in fact, I think this reverse stepwise traversal seems more natural order than doing stepwise traversal each time from top down. How do you think?

I have made a change that implements this reverse stepwise traversal in SftpOperations and also made a few minor improvement changes.
This changed version passes all the existing tests and works also against the above server.

I am attaching a patch file for this change. I would appreciate if you can comment on it.

Thanks.
regards, aki
",njiang,ay,Major,Resolved,Fixed,06/Sep/12 20:57,24/Jan/13 16:54
Bug,CAMEL-5580,12606673,The RowModel Data does not include the correct cell value type and some minor changes.,"The hbase component create a row model from the passed endpoint configuration using the family/qualifier/etc information from the uri.  

This process currently does not extra the value type for value type n greater than 1.  It currently uses the value type from the one defined for value 1.  This is currently not a problem since this information is not being used within the hbase component but if we intend to use it the future this fix insures that it will extract the correct information for the row model.

The extracted CellMappingStrategy within the HBaseConsumer is always going to return the same instance throughout the for loop.  This has been moved outside of the for loop so that it is extract only once instead of multiple times.

Added a check to the setValueType method of the HBaseCell class to sure that the passed value is non-null else raise an exception.
",davsclaus,ccorsi,Minor,Resolved,Fixed,07/Sep/12 20:54,09/Sep/12 08:42
Bug,CAMEL-5582,12606682,PGP data format doesn't close file input streams,"The following route doesn't work on Windows:

{code}from(""file:foo"").marshal().pgp(...).to(""ftp:blah""){code}

The reason is that PGPDataFormat.marshal() and PGPDataFormat.unmarshal() both use IOUtils.toByteArray() to read the incoming stream into a byte array, but neither of these two methods closes the streams after they have been read from. My understanding is that not closing these streams prevents Camel from subsequently renaming the file once the route has completed. I've attached a sample stack trace at the bottom.

The following seems to fix the issue for me:

Replacing the following code in {{org.apache.camel.converter.crypto.PGPDataFormat.marshal(...)}}:

{code}InputStream plaintextStream = ExchangeHelper.convertToMandatoryType(exchange, InputStream.class, graph);

byte[] compressedData = PGPDataFormatUtil.compress(IOUtils.toByteArray(plaintextStream),
        PGPLiteralData.CONSOLE, CompressionAlgorithmTags.ZIP);{code}

With this code:

{code}byte[] plaintextData;
InputStream plaintextStream = null;
try {
    plaintextStream = ExchangeHelper.convertToMandatoryType(exchange, InputStream.class, graph);
    plaintextData = IOUtils.toByteArray(plaintextStream);
} finally {
    IOUtils.closeQuietly(plaintextStream);
}

byte[] compressedData = PGPDataFormatUtil.compress(plaintextData, PGPLiteralData.CONSOLE, CompressionAlgorithmTags.ZIP);{code}

And replacing the following code in {{org.apache.camel.converter.crypto.PGPDataFormat.unmarshal(...)}}:

{code}InputStream in = new ByteArrayInputStream(IOUtils.toByteArray(encryptedStream));
in = PGPUtil.getDecoderStream(in);{code}

With this code:

{code}InputStream in;
try {
    byte[] encryptedData = IOUtils.toByteArray(encryptedStream);
    InputStream byteStream = new ByteArrayInputStream(encryptedData);
    in = PGPUtil.getDecoderStream(byteStream);
} finally {
    IOUtils.closeQuietly(encryptedStream);
}{code}

And here's the stack trace mentioned above:

{code}org.apache.camel.component.file.GenericFileOperationFailedException: Error renaming file from C:\opt\connect\just\a\test\lax\blah.txt to C:\opt\connect\just\a\test\lax\.sent\blah.txt
    at org.apache.camel.component.file.FileOperations.renameFile(FileOperations.java:72)
    at org.apache.camel.component.file.strategy.GenericFileProcessStrategySupport.renameFile(GenericFileProcessStrategySupport.java:107)
    at org.apache.camel.component.file.strategy.GenericFileRenameProcessStrategy.commit(GenericFileRenameProcessStrategy.java:86)
    at org.apache.camel.component.file.GenericFileOnCompletion.processStrategyCommit(GenericFileOnCompletion.java:132)
    at org.apache.camel.component.file.GenericFileOnCompletion.onCompletion(GenericFileOnCompletion.java:82)
    at org.apache.camel.component.file.GenericFileOnCompletion.onComplete(GenericFileOnCompletion.java:53)
    at org.apache.camel.util.UnitOfWorkHelper.doneSynchronizations(UnitOfWorkHelper.java:55)
    at org.apache.camel.impl.DefaultUnitOfWork.done(DefaultUnitOfWork.java:226)
    at org.apache.camel.processor.UnitOfWorkProcessor.doneUow(UnitOfWorkProcessor.java:199)
    at org.apache.camel.processor.UnitOfWorkProcessor.access$000(UnitOfWorkProcessor.java:37)
    at org.apache.camel.processor.UnitOfWorkProcessor$1.done(UnitOfWorkProcessor.java:157)
    at org.apache.camel.processor.RouteContextProcessor$1.done(RouteContextProcessor.java:56)
    at org.apache.camel.processor.Pipeline.process(Pipeline.java:106)
    at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
    at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
    at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)
    at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)
    at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48)
    at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
    at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
    at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
    at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
    at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)
    at org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:336)
    at org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:189)
    at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:155)
    at org.apache.camel.impl.ScheduledPollConsumer.doRun(ScheduledPollConsumer.java:139)
    at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:91)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Renaming file from: C:\opt\connect\just\a\test\lax\blah.txt to: C:\opt\connect\just\a\test\lax\.sent\blah.txt failed due cannot delete from file: C:\opt\connect\just\a\test\lax\blah.txt after copy succeeded
    at org.apache.camel.util.FileUtil.renameFile(FileUtil.java:362)
    at org.apache.camel.component.file.FileOperations.renameFile(FileOperations.java:70)
    ... 36 more{code}",davsclaus,sdanig,Minor,Resolved,Fixed,07/Sep/12 21:16,11/Sep/12 06:51
Bug,CAMEL-5592,12607166,"Logger Level is always at default level(""ERROR"") when error handler is LoggingErrorHandler","when logger level is set to other level (WARN, DEBUG or INFO) except for ERROR, but actually the log is always at ERROR level (it's the default level).",davsclaus,mfshen,Minor,Resolved,Fixed,12/Sep/12 03:22,28/Sep/12 13:05
Bug,CAMEL-5595,12607180,http4 component should not assume the scheme by looking up the request address,,njiang,njiang,Major,Resolved,Fixed,12/Sep/12 06:05,17/Sep/12 07:03
Bug,CAMEL-5605,12607450,sftp component: Cannot acquire read lock for files in sub directories if property recursive=true and readLock=changed,"I use endpoint to consummer files from sftp:
<endpoint id=""sftp_server"" uri=""sftp://${server_path}?privateKeyFile=${sftp_key_path}&amp;localWorkDirectory=${tmp}&amp;idempotent=true&amp;idempotentRepository=#fileStore&amp;readLock=changed&amp;recursive=true&amp;filter=#myAntFilter&amp;fastExistsCheck=true"" />

And if file exist in the sub directory of ${server_path}, I get warning:
16:31:16,288 WARN  [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Cannot acquire read lock within 20000 millis. Will skip the file: GenericFile[qwe/rty/test.csv]


16:31:01,081 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Using fast exists to update file information for GenericFile[qwe/rty/test.csv]
16:31:01,082 TRACE [org.apache.camel.component.file.remote.SftpOperations] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) listFiles(inbox2/qwe/rty/test.csv)
16:31:01,148 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) List files inbox2/qwe/rty/test.csv found 1 files
16:31:01,149 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Previous last modified: 0, new last modified: 0
16:31:01,150 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Previous length: 0, new length: 0
16:31:01,151 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Exclusive read lock not granted. Sleeping for 5000 millis.


16:30:56,005 TRACE [org.apache.camel.component.file.remote.SftpConsumer] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Processing file: GenericFile[qwe/rty/test.csv]
16:30:56,007 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Waiting for exclusive read lock to file: GenericFile[qwe/rty/test.csv]
16:30:56,008 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Using fast exists to update file information for GenericFile[qwe/rty/test.csv]
16:30:56,010 TRACE [org.apache.camel.component.file.remote.SftpOperations] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) listFiles(inbox2/qwe/rty/test.csv)
16:30:56,078 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) List files inbox2/qwe/rty/test.csv found 1 files
16:30:56,079 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Previous last modified: -9223372036854775808, new last modified: 0
16:30:56,079 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Previous length: -9223372036854775808, new length: 0
16:30:56,080 TRACE [org.apache.camel.component.file.remote.strategy.SftpChangedExclusiveReadLockStrategy] (Camel (camel-1) thread #0 - sftp://test@testserver/inbox2) Exclusive read lock not granted. Sleeping for 5000 millis.",davsclaus,basssjm,Major,Closed,Fixed,13/Sep/12 12:54,18/Sep/12 10:43
Bug,CAMEL-5608,12607457,Validator component - Lock file on windows if input is file based and an validation failed occurred,"See nabble
http://camel.465427.n5.nabble.com/Validator-component-onException-tp5717756.html

We should ensure the input stream gets closed in all accounts with the validator component.",davsclaus,davsclaus,Major,Resolved,Fixed,13/Sep/12 13:23,17/Oct/12 17:14
Bug,CAMEL-5610,12607546,camel-hl7 bundle should specify the mina version ,"As there could be different version of mina or mina2 exist in the OSGi platform, we need to specify the right version rang on camel-hl7 which has the dependency on the mina.",njiang,njiang,Major,Resolved,Fixed,13/Sep/12 23:57,14/Sep/12 08:50
Bug,CAMEL-5615,12607660,camel-soap - Generates empty tag name if marshalling JAXB generated sources with no name in @XmlType,"This can cause camel soap to marshal to XML which contains an empty tag name, causing the XML to be invalid.
",davsclaus,davsclaus,Minor,Resolved,Fixed,14/Sep/12 15:18,14/Sep/12 15:27
Bug,CAMEL-5616,12607664,Share properties configuration between CDI and CamelContext,"Apache DeltaSpike provides convenient way to inject configuration using @ConfigProperty value.
In ideal world @ConfigProperty injected by deltaspike should be same as configuration property visible from Camel Context.",hadrian,ldywicki,Major,Resolved,Fixed,14/Sep/12 15:39,21/Sep/12 07:05
Bug,CAMEL-5620,12607831,JUnit ConverterTest testFileToString() fails,The test fails on windows when there is a space in the folder structure,njiang,alanfoster,Minor,Resolved,Fixed,16/Sep/12 20:11,24/Sep/12 10:53
Bug,CAMEL-5621,12607886,JettyHttpProducer should close the input stream when the exception is throw,"User complains about there are lots of file is open when sending the input stream and getting the exception.
Here is the mail thread[1] is talking about it.
[1]http://camel.465427.n5.nabble.com/To-many-opened-files-td5719451.html ",njiang,njiang,Major,Resolved,Fixed,17/Sep/12 10:24,27/Sep/12 11:18
Bug,CAMEL-5626,12608169,JmsToJmsTransactedSecurityTest in camel-jms fails every now and then,"Test {{JmsToJmsTransactedSecurityTest}} fails every now and then because of JMS message redelivery. When the entire test class is executed, it sometimes happens that the message that failed at {{testJmsSecurityFailure}} test method pops up in the assertions of {{testJmsSecurityOK}}. ",raulvk,raulvk,Minor,Resolved,Fixed,19/Sep/12 00:12,19/Sep/12 23:09
Bug,CAMEL-5627,12608190,camel-zookeeper endpoints should not share the configuration ,"User complains that  ""The URI parameters on my consumer endpoint seem to be affecting the behaviour of my producer endpoint."" If he has two zookeeper endpoint with different uri configuration in the different routes. 
You can find more information in this thread[1]
[1]http://camel.465427.n5.nabble.com/URI-parameters-for-one-Zookeeper-endpoint-are-contaminating-another-endpoint-td5719559.html",njiang,njiang,Major,Resolved,Fixed,19/Sep/12 03:40,19/Sep/12 13:50
Bug,CAMEL-5631,12608431,EventHelper.notifyRouteStarted skips all remaining notifiers if one if the notifiers ignores route events,"Having two or more notifiers registered at the management strategy of a camel context, where one of the notifiers (not the last one) is set up to ignore route events, results in all remaining notifiers to be skipped when notifying a RouteStartedEvent.

Look at http://svn.apache.org/viewvc/camel/branches/camel-2.10.x/camel-core/src/main/java/org/apache/camel/util/EventHelper.java?revision=1340446&view=markup, line 237: There's a ""return"" which actually should be a ""continue"".

Note: CamelContextStoppingEvents are also affected.",muellerc,frank.schoenheit,Minor,Closed,Fixed,20/Sep/12 11:18,21/Oct/12 18:16
Bug,CAMEL-5632,12608620,"ObjectHelper.resolveUriPath is broken, can not handle:  '../../' correctly","In my xsl-files I have imports and includes that resides in folders a few levels up.

Ex:  

<xsl:include href=""../../common/myfunctions.xsl""/>

When camel starts up and tries to resolve the paths it crashes because of the current implementation in ObjectHelper.resolveUriPath(..)


Made a simple testcase that demonstrates my problem.




{code}
public void testResolveUriPath() throws Exception {
        assertEquals(""xslt/"", ObjectHelper.resolveUriPath(""xslt/one/../""));         //  OK
        assertEquals(""xslt/"", ObjectHelper.resolveUriPath(""xslt/one/../two/../""));  // OK
        assertEquals(""xslt/"", ObjectHelper.resolveUriPath(""xslt/""));                // OK
        assertEquals("""", ObjectHelper.resolveUriPath(""./""));                        // OK
        assertEquals(""xslt/"", ObjectHelper.resolveUriPath(""xslt/one/two/../../""));  // Fails, gives 'xslt/one/.'
        assertEquals(""xslt/"", ObjectHelper.resolveUriPath(""xslt/one/two/../.././""));// Fails, gives 'xslt/one/.'
    }
{code}",davsclaus,poker_jocke,Major,Resolved,Fixed,21/Sep/12 13:00,22/Sep/12 09:44
Bug,CAMEL-5636,12608688,Enricher with async routing not handling exceptions thrown from AggregationStrategy,"The Enricher does not handle exceptions thrown from the AggregationStrategy when the async routing engine has kicked in. It leads to the following thread dump, callback thread remains blocked forever.

{code}
""Camel (context) thread #10 - JmsConsumer[queue]"" daemon prio=5 tid=103666000 nid=0x113c25000 waiting on condition [113c24000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <7fd4a8de0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:120)
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)
        at org.apache.camel.component.jms.EndpointMessageListener.onMessage(EndpointMessageListener.java:91)
        at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:560)
        at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:498)
        at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:467)
        at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:325)
        at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:263)
        at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1058)
        at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1050)
        at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

",raulvk,raulvk,Major,Resolved,Fixed,21/Sep/12 20:07,20/Oct/12 16:47
Bug,CAMEL-5641,12608743,JmsBinding Does Not Handle BigInteger and BigDecimal Properly,"According to the documentation:

""The values must be primitives or their counter objects (such as
Integer, Long, Character). The types, String, CharSequence, Date,
BigDecimal and BigInteger are all converted to their toString()
representation. All other types are dropped.""

So, it would seem that BigInteger should be toString()ed.  However, in
the JmsBinding class, we see the following code:

{code}
protected Object getValidJMSHeaderValue(String headerName, Object headerValue) {
        if (headerValue instanceof String) {
            return headerValue;
        } else if (headerValue instanceof Number) {
            return headerValue;
        } else if (headerValue instanceof Character) {
            return headerValue;
        } else if (headerValue instanceof CharSequence) {
            return headerValue.toString();
        } else if (headerValue instanceof Boolean) {
            return headerValue;
        } else if (headerValue instanceof Date) {
            return headerValue.toString();
        }
        return null;
    }
{code}

Since BigInteger extends Number, it will merely return the instance
itself.",muellerc,jwcarman,Minor,Closed,Fixed,22/Sep/12 11:41,22/Sep/12 18:05
Bug,CAMEL-5644,12608835,Bean component - Should use try conversion when choosing method based on parameter type matching,"When the bean component has to pick among overloaded methods, then it does best matching on parameter types etc.

We should relax the type conversion to try attempt.
",davsclaus,davsclaus,Minor,Resolved,Fixed,24/Sep/12 08:41,24/Sep/12 09:05
Bug,CAMEL-5647,12608979,EIPs using custom aggregation strategy should fail if not possible to find in registry,"Some EIPs support using a custom aggregations strategy. And if you use XML DSL to refer to a custom strategy, then you may have typo, or the strategy do not implement the interface.

What we should do is to ensure to fail if the custom strategy could not be looked up. 

Some EIPs such as the content enricher will fallback and use a default strategy. This is wrong.",davsclaus,davsclaus,Major,Resolved,Fixed,25/Sep/12 07:37,25/Sep/12 11:36
Bug,CAMEL-5653,12609327,camel-hbase producer should be able to perform get with just the id as a header,"In many cases the user would specify the families/columns of interest as part of the uri and then just specify the row id as part of the headers.

The way it currently works, it requires at least one family/column along with the row id as part of the headers.",iocanel,iocanel,Major,Resolved,Fixed,27/Sep/12 00:36,27/Sep/12 18:43
Bug,CAMEL-5655,12609372,Camel Endpoint ignores camelContextId property in Spring configuration,"Camel Endpoint, if configured outside camelContext element in Spring configuration, ignores camelContextId property. 

Steps to reproduce: Configure two contexts in spring configuration. Then in the same config file configure two  endpoints outside camelContext, but with camelContextId specified. Both endpoints will be created pointing to the same (first) camel context, despite being configured to different contexts. Attempting to use them by reference results in failure, as endpoint camelContext doesn't match the expected one.",davsclaus,koscejev,Major,Resolved,Fixed,27/Sep/12 10:02,27/Sep/12 11:40
Bug,CAMEL-5657,12609395,Recipient list EIP - Should shutdown thread pool if parallel mode enabled,"We should shutdown the thread pool for recipient list if a pool was in use, and it can be shutdown.

The logic in doShutdown is missing that.

Camel would though have a fail-safe when it shutdown itself to shutdown that pool otherwise. But we should shutdown when the EIP is shutdown itself.",davsclaus,davsclaus,Minor,Resolved,Fixed,27/Sep/12 14:15,27/Sep/12 14:20
Bug,CAMEL-5663,12609517,camel-blueprint - Should add check for bean scope in bean injector,"This check is missing.

Which mean that prototype scoped bean would have been regarded as singleton by Camel.",davsclaus,davsclaus,Minor,Resolved,Fixed,28/Sep/12 09:49,28/Sep/12 09:54
Bug,CAMEL-5668,12609676,camel-jetty - A soap fault should trigger http response code 500,"See nabble
http://camel.465427.n5.nabble.com/Help-with-nmr-cxf-endpoints-and-fault-handling-tp5719720.html

If a response is fault=true, then we should force the http response code to be 500. Which is what the SOAP 1.1 spec mandates.

See section 6.2 at: http://www.w3.org/TR/soap11/#_Ref477795996",njiang,davsclaus,Minor,Resolved,Fixed,30/Sep/12 09:55,08/Oct/12 02:00
Bug,CAMEL-5670,12609726,camel-mqtt - Nullpointer Exception when disconnected from broker,"Hi, 

from time to time, we get the following exception when producing messages via camel-mqtt:

bq.
{code}
Caused by: java.lang.NullPointerException
	at org.fusesource.mqtt.client.CallbackConnection.publish(CallbackConnection.java:551)
	at org.fusesource.mqtt.client.CallbackConnection.publish(CallbackConnection.java:545)
	at org.apache.camel.component.mqtt.MQTTEndpoint.publish(MQTTEndpoint.java:166)
	at org.apache.camel.component.mqtt.MQTTProducer.doProcess(MQTTProducer.java:66)
	at org.apache.camel.component.mqtt.MQTTProducer.process(MQTTProducer.java:38)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
[...]
{code}

From looking at the source of both camel-mqtt and the fusesource mqtt client, it looks like that this is caused by the mqtt-client being disconnected and trying to invoke the onFailure method on the callback:

{code:title=CallbackConnection.java}
public void publish(UTF8Buffer topic, Buffer payload, QoS qos, boolean retain, Callback<Void> cb) {
        queue.assertExecuting();
        if( disconnected ) {
            cb.onFailure(createDisconnectedError());
            return;
        }
        PUBLISH command = new PUBLISH().qos(qos).retain(retain);
        command.topicName(topic).payload(payload);
        send(command, cb);
    }
{code}

camel-mqtt passes in null for the callback though. Wouldn't it be better if a more fitting exception were thrown, or if camel passed in a Callback in order to receive the onFailure event when disconnected from the broker?
",davsclaus,stefan.hudelmaier,Minor,Resolved,Fixed,01/Oct/12 08:44,06/Oct/12 15:12
Bug,CAMEL-5677,12609974,Stopping and starting a seda consumer leaks a thread each time,"Just did something like this:

        for (int i = 0; i < 1000; i++) {
            context.stopRoute(""sedaToMock"");
            context.startRoute(""sedaToMock"");	        
        }      

and yeah, I had over 1000 threads in my JVM... not good :)",janstey,janstey,Major,Resolved,Fixed,02/Oct/12 20:15,03/Oct/12 11:26
Bug,CAMEL-5681,12610094,Using recipient list in a doTry ... doCatch situation dont work properly,"See nabble
http://camel.465427.n5.nabble.com/Issue-with-doTry-doCatch-not-routing-correctly-tp5720325.html

The end user would expect that doTry .. doCatch will overrule. However it gets a bit further more complicated if the try block routes to other routes and using EIPs such as recipient list.
",davsclaus,davsclaus,Major,Resolved,Fixed,03/Oct/12 16:21,06/Oct/12 06:19
Bug,CAMEL-5682,12610096,"possible NullPointerException in org.apache.camel.util.ObjectHelper.getCamelPropertiesWithPrefix(prefix, camelContext)","If the camel context's properties contain an entry set with a null key, and ObjectHelper.getCamelPropertiesWithPrefix(prefix, camelContext) is called, a NullPointerException will be thrown at line 659.",davsclaus,ski309,Minor,Resolved,Fixed,03/Oct/12 16:30,05/Oct/12 12:41
Bug,CAMEL-5683,12610112,JMS connection leak with request/reply producer on temporary queues,"Over time I see the number of temporary queues in ActiveMQ slowly climb. Using JMX information and memory dumps in MAT, I believe the cause is a connection leak in Apache Camel.

My environment contains 2 ActiveMQ brokers in a network of brokers configuration. There are about 15 separate applications which use Apache Camel to connect to the broker using the ActiveMQ/JMS component. The various applications have different load profiles and route configurations.

In the more active client applications, I found that ActiveMQ was listing 300+ consumers when, based on my configuration, I would expect no more than 75. The vast majority of the consumers are sitting on a temporary queue. Over time, the 300 number increments by one or two over about a 4 hour period.

I did a memory dump on one of the more active client applications and found about 275 DefaultMessageListenerContainers. Using MAT, I can see that some of the containers are referenced by JmsProducers in the ProducerCache; however I can also see a large number of listener containers that are no longer being referenced at all. I was also able to match up a soft-references producer/listener endpoint with an unreferenced listener which means a second producer was created at some point.

Looking through the ProducerCache code, it looks like the LRU cache uses soft-references to producers, in my case a JmsProducer. This seems problematic for two reasons:
- If memory gets constrained and the GC cleans up a producer, it is never properly stopped.
- If the cache gets full and the map removes the LRU producer, it is never properly stopped.

What I believe is happening, is that my application is sending a few request/reply messages to a JmsProducer. The producer creates a TemporaryReplyManager which creates a DefaultMessageListenerContainer. At some point, the JmsProducer is claimed by the GC (either via the soft-reference or because the cache is full) and the reply manager is never stopped. This causes the listener container to continue to listen on the temporary queue, consuming local resources and more importantly, consuming resources on the JMS broker.

I haven't had a chance to write an application to reproduce this behavior, but I will attach one of my route configurations and a screenshot of the MAT analysis looking at DefaultMessageListenerContainers. If needed, I could provide the entire memory dump for analysis (although I rather not post it publicly). The leak depends on memory usage or producer count in the client application because the ProducerCache must have some churn. Like I said, in our production system we see about 12 temporary queues abandoned per client per day.

Unless I'm missing something, it looks like the producer cache would need to be much smarter to support stopping a producer when the soft-reference is reclaimed or a member of the cache is ejected from the LRU list.

",davsclaus,mpilone,Major,Resolved,Fixed,03/Oct/12 17:37,02/May/13 02:29
Bug,CAMEL-5686,12610553,"While stopping, the QuickfixjEngine should unregister the MBean being registered for the Initiator at startup.",,bvahdat,bvahdat,Minor,Resolved,Fixed,05/Oct/12 12:21,05/Oct/12 12:36
Bug,CAMEL-5693,12610838,camel-smpp - Consumer error handling is not correct,"See nabble
http://camel.465427.n5.nabble.com/Problem-with-SMPP-Consumer-error-handling-tp5720516.html

",davsclaus,davsclaus,Minor,Resolved,Fixed,08/Oct/12 16:04,08/Oct/12 16:56
Bug,CAMEL-5694,12610940,camel-jms - Should also allow multiple consumers from multiple routes from the same queue,"If having 2+ routes that consumes from the same queue, then that should be allowed.

Its a bit unusual though for queues in Camel as you most likely want to use the same route for the same queue. But there is nothing in the JMS spec that forbids this.

",davsclaus,davsclaus,Minor,Resolved,Fixed,09/Oct/12 07:18,09/Oct/12 07:41
Bug,CAMEL-5699,12611207,LogFormatter throws a NPE when all elements are disabled,"There are perfectly valid cases where you may want to output a log message with no elements displayed, i.e. with showExchangeId=false, showBody=false, etc.

For example, when you want to print a ""signal"" log line for a particular transaction and you're already using MDC logging with breadcrumbs enabled. You may already have all the info you need: logging category, severity, breadcrumbId. You are not interested in anything else.

Currently, disabling all elements leads to a NPE.",raulvk,raulvk,Minor,Resolved,Fixed,10/Oct/12 17:21,29/Nov/12 11:33
Bug,CAMEL-5704,12611766,Split inside Split - Parallel processing issue - Thread is getting wrong Exchange when leaving inner split ,"A small JUnit recreation case is attached.
When using embedded split inside a split with parallel processing, threads are getting a wrong exchange (or wrong exchange copy) just after leaving the inner split and returning to the parent split.

In the test case, we split a file by comma in a parent split (Block split), then by line separator in inner split (Line Split). 
We expect 2 files in output, each of them containing the respective Blocks.

However, once inner split is complete, each thread is supposed to add a 11th line in the result(i).txt file saying split(i) is complete.  
Bug is that one of the thread ends up with parent split Exchange (copy?) from the other thread, and appends wrong information into the wrong file.

Expected:
---------
(result0.txt)
Block1 Line 1:Status=OK
Block1 Line 2:Status=OK
Block1 Line 0:Status=OK
Block1 Line 4:Status=OK
Block1 Line 3:Status=OK
Block1 Line 8:Status=OK
Block1 Line 5:Status=OK
Block1 Line 6:Status=OK
Block1 Line 7:Status=OK
Block1 Line 9:Status=OK
0 complete

(result1.txt)
Block2 Line 0:Status=OK
Block2 Line 3:Status=OK
Block2 Line 1:Status=OK
Block2 Line 2:Status=OK
Block2 Line 6:Status=OK
Block2 Line 4:Status=OK
Block2 Line 7:Status=OK
Block2 Line 9:Status=OK
Block2 Line 5:Status=OK
Block2 Line 8:Status=OK
1 complete

Actual:
-------
(result0.txt)
Block1 Line 1:Status=OK
Block1 Line 2:Status=OK
Block1 Line 0:Status=OK
Block1 Line 4:Status=OK
Block1 Line 3:Status=OK
Block1 Line 8:Status=OK
Block1 Line 5:Status=OK
Block1 Line 6:Status=OK
Block1 Line 7:Status=OK
Block1 Line 9:Status=OK
0 complete0 complete

(result1.txt)
Block2 Line 0:Status=OK
Block2 Line 3:Status=OK
Block2 Line 1:Status=OK
Block2 Line 2:Status=OK
Block2 Line 6:Status=OK
Block2 Line 4:Status=OK
Block2 Line 7:Status=OK
Block2 Line 9:Status=OK
Block2 Line 5:Status=OK
Block2 Line 8:Status=OK


This issue exist in 2.8.x, and probably in 2.10.x as well.
This is a Splitter/MulticastProcessor or Pipeline issue but not quite familiar with the code, I am having hard time tracking it. ",njiang,gill3s,Major,Resolved,Fixed,14/Oct/12 19:09,23/Sep/13 20:14
Bug,CAMEL-5707,12612032,NotifyBuilder should be thread safe,In high concurrent tests the NotifyBuilder may miss a counter.,davsclaus,davsclaus,Minor,Resolved,Fixed,16/Oct/12 13:42,16/Oct/12 13:47
Bug,CAMEL-5710,12612349,Rest route returns response with http Status code 500 Internal Server Error when response has Last-Modified or Header-expires in header as string. ,"Rest route returns Response with http Status code 500 Internal Server Error and expected response headers/body when response has Last-Modified or Header-expires in header as string. 

I found the issue in below code it try to convert string Wed, 31 Dec 1969 18:00:00 CS to Date and it fails. Is it posible to convert String Date without knowing Date format?


DefaultRestletBinding
=======================



 if (header.equalsIgnoreCase(HeaderConstants.HEADER_LAST_MODIFIED)) {
                if (value instanceof Calendar) {
                    message.getEntity().setModificationDate(((Calendar) value).getTime());
                } else if (value instanceof Date) {
                    message.getEntity().setModificationDate((Date) value);
                } else {
                    Date date = exchange.getContext().getTypeConverter().mandatoryConvertTo(Date.class, value); //  Here it try to convert String to Date it throws No type converter available to convert from type: java.lang.String to the required type: java.util.Date with value Wed, 31 Dec 1969 18:00:00 CS
                    message.getEntity().setModificationDate(date);
                }
            }

Stack trace
===========

7 Oct 2012 08:28:06,010 WARN org.restlet.Component.StatusFilter - Exception or error caught in status service
org.apache.camel.RuntimeCamelException: Cannot process request
        at org.apache.camel.component.restlet.RestletConsumer$1.handle(RestletConsumer.java:62)
        at org.apache.camel.component.restlet.MethodBasedRouter.handle(MethodBasedRouter.java:54)
        at org.restlet.routing.Filter.doHandle(Filter.java:159)
        at org.restlet.routing.Filter.handle(Filter.java:206)
        at org.restlet.routing.Router.doHandle(Router.java:500)
        at org.restlet.routing.Router.handle(Router.java:740)
        at org.restlet.routing.Filter.doHandle(Filter.java:159)
        at org.restlet.routing.Filter.handle(Filter.java:206)
        at org.restlet.routing.Router.doHandle(Router.java:500)
        at org.restlet.routing.Router.handle(Router.java:740)
        at org.restlet.routing.Filter.doHandle(Filter.java:159)
        at org.restlet.engine.application.StatusFilter.doHandle(StatusFilter.java:154)
        at org.restlet.routing.Filter.handle(Filter.java:206)
        at org.restlet.routing.Filter.doHandle(Filter.java:159)
        at org.restlet.routing.Filter.handle(Filter.java:206)
        at org.restlet.engine.ChainHelper.handle(ChainHelper.java:114)
        at org.restlet.Component.handle(Component.java:391)
        at org.restlet.Server.handle(Server.java:491)
        at org.restlet.engine.ServerHelper.handle(ServerHelper.java:75)
        at org.restlet.engine.http.HttpServerHelper.handle(HttpServerHelper.java:153)
        at org.restlet.ext.servlet.ServerServlet.service(ServerServlet.java:1031)
        at org.apache.shiro.grails.SavedRequestFilter.doFilter(SavedRequestFilter.java:56)
        at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:360)
        at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:276)
        at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
        at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
        at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:344)
        at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:272)
        at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:81)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:909)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.camel.NoTypeConversionAvailableException: No type converter available to convert from type: java.lang.String to the required type: java.util.Date with value Wed, 31 Dec 1969 18:00:00 CST
        at org.apache.camel.impl.converter.BaseTypeConverterRegistry.mandatoryConvertTo(BaseTypeConverterRegistry.java:169)
",davsclaus,amit1000,Major,Resolved,Fixed,18/Oct/12 01:02,18/Oct/12 16:39
Bug,CAMEL-5712,12612418,BlueprintCamelContext should not get started in the init() method but later when the blueprint container is fully initialized,"The init() method in BlueprintCamelContext should not call maybeStart(). but instead should be started when the blueprint container is done doing all its dance and initialization.

For example Spring sends on ContextRefreshedEvent when its done.

We need something similar for blueprint to tell us when its done doing its initialization et all.

The problem is that the CamelContext should not be started until all that other stuff is initialized as well.",davsclaus,davsclaus,Critical,Resolved,Fixed,18/Oct/12 12:41,24/Oct/12 08:09
Bug,CAMEL-5718,12612703,Bodies of SMs with 8-bit data_coding are mangled,"Bytes in the body of 8-bit SUBMIT_SMs which do not fall within the chosen charset's range are set to '?', which is obviously wrong because 8-bit/binary data should not be modified in any way.

EDIT: Turns out the RX SMs (DELIVER_SM, etc.) were also affected.",muellerc,frankzinger,Major,Closed,Fixed,19/Oct/12 15:27,15/Nov/12 21:15
Bug,CAMEL-5720,12612826,Aggregate EIP - Dynamic completion size should override fixed values if in exchange,"See nabble
http://camel.465427.n5.nabble.com/Bug-with-completionSize-on-AggregatorProcessor-tp5721307.html",davsclaus,davsclaus,Minor,Resolved,Fixed,20/Oct/12 10:01,20/Oct/12 10:12
Bug,CAMEL-5722,12612837,Classloader mixup when consumers across bundles share the same camel-jetty port,"Happens in an OSGi environment; but it is also applicable in modular servers that build dynamic classloaders (e.g. JBoss AS) per deployed artifacts (WAR, module, EAR, etc.).

If bundles A and B are both creating camel-jetty consumers on the same TCP port, the consumers will share the underlying Jetty connector. Moreover, the Jetty connector is indeed created by the first consumer that initialises on that TCP port.

Since the Camel route executes on the Jetty Connector thread, all class resolutions will happen against the classloader of the bundle that initialises first.

This makes class resolution absolutely undeterministic and erratic. Quite a nasty bug IMHO.

Perhaps this can be fixed by adding a call to {{Thread.setContextClassLoader()}} passing the classloader of the JettyHttpEndpoint, as soon as a request comes in?",raulvk,raulvk,Critical,Closed,Fixed,20/Oct/12 16:02,27/Oct/12 18:33
Bug,CAMEL-5729,12612935,camel-crypto - Signer producer should close input stream after usage,"See nabble
http://camel.465427.n5.nabble.com/The-file-is-not-deleted-if-it-is-submitted-to-digital-signature-end-point-crypto-sign-td5721377.html",davsclaus,davsclaus,Major,Resolved,Fixed,22/Oct/12 10:18,22/Oct/12 10:28
Bug,CAMEL-5730,12612962,SqsConsumer receive thread is tightly coupled to extendMessageVisiblity's scheduled task,"When I made the patch file for CAMEL-5306 I made a mistake and coupled the cancelling of the visibility extending scheduled task with the thread that receives the message inside SqsConsumer.  This causes issues when try to write Processors that delegate the completion of an exchange to another thread.

Actually the cancelling of the scheduled task should have been handled via exchange.addOnCompletion(), the same way the the overall SQS transaction is completed.

The attached patch fixes this issue.",davsclaus,alexhutter,Major,Resolved,Fixed,22/Oct/12 13:13,22/Oct/12 13:37
Bug,CAMEL-5731,12612971,Setting alphabet header to 8-bit doesn't update data coding,"For derivatives of SmppSmCommand, setting the SmppConstants.ALPHABET header to e.g. Alphabet.ALPHA_8_BIT doesn't change the data coding of the resulting command.

I am not 100% clear on how the alphabet header is meant to interact with the data coding, but this seems like a bug to me.

Patch with demo unit test to follow.",muellerc,frankzinger,Minor,Closed,Fixed,22/Oct/12 14:02,03/Nov/12 15:37
Bug,CAMEL-5732,12612977,Data coding of 0x02 not considered 8-bit,"SmppUtils.parseAlphabetFromDataCoding()) doesn't identify an input of 0x02 as 8-bit, but the SMPP spec (3.4 and 5.0) clearly states that 0x02 is octet-unspecified (8-bit).

The bug is actually in jsmpp, but the attached patch contains a workaround.

This seems like a pretty blatant mistake to make, so perhaps there is good explanation that I am not aware of?",muellerc,frankzinger,Major,Closed,Fixed,22/Oct/12 14:22,23/Oct/12 21:01
Bug,CAMEL-5733,12613000,NullPointerException with SpringBatchProducer and null header value,"A NullPointerException occurs in SpringBatchProducer line 60 when a header with a null value exists.

Null headers should just be ignored or perhaps be added in jobParameters with a null value. I don't known what is the best strategy.",njiang,vanackej,Major,Resolved,Fixed,22/Oct/12 17:15,23/Oct/12 07:42
Bug,CAMEL-5735,12613053,SnsConfiguration's toString() uses amazonSQSClient as key string rather than amazonSNSClient,"The toString() method in SnsConfiguration uses the incorrect string ""amazonSQSClient"" as the key for the AmazonSNSClient reference in the registry.",njiang,rhowlett,Trivial,Resolved,Fixed,22/Oct/12 21:46,23/Oct/12 07:45
Bug,CAMEL-5737,12613124,camel-ftp - LocalWorkDir option did not work as expected,"There is a flaw when using from ftp -> to file. And using the localWorkDirectory option. 

See nabble
http://camel.465427.n5.nabble.com/camel-ftp-option-localWorkDirectory-doesn-t-work-properly-on-windows-tp5721438.html",davsclaus,davsclaus,Major,Resolved,Fixed,23/Oct/12 11:38,23/Oct/12 12:24
Bug,CAMEL-5738,12613129,HttpConnectionManager configuration is not injected correctly when using blueprint ,"The creation of HttpEndpoint is done with the default HttpConnectionManager. As a consequence, the http configuration concerning max connections, soTimeouts, etc is overridden by the default params. Attached you can find a unit test using camel-test-blueprint that identifies the issue. The problem is not replicated using spring-dm. ",njiang,nikosd23,Major,Resolved,Fixed,23/Oct/12 12:53,24/Oct/12 08:10
Bug,CAMEL-5741,12613280,camel-cdi - Adding duplicate route builder beans to internal list,"The camel-itest-cdi fails. So I took a look, and notice that duplicate beans is added to CamelContextConfig from camel-cdi.

So I made a patch to fix that, and the test got a bit further.

I am attaching the patch here then the ppl working on the new camel-cdi module can take a look.",davsclaus,davsclaus,Major,Resolved,Fixed,24/Oct/12 13:32,30/Oct/12 14:55
Bug,CAMEL-5746,12613625,WireTap will always copy the origin Message,"I found the wiretap will always copy the original exchange message after deprecated those methods like -wireTap(String uri, boolean copy, Expression body)- and -wireTap(String uri, boolean copy, Processor processor)- .

Checked the source code of *org.apache.camel.model.WireTapDefinition<Type>*, the *isCopy()* and *copy()* method are:
{code}
    public WireTapDefinition<Type> copy() {
        setCopy(true);
        return this;
    }

    public boolean isCopy() {
        // should default to true if not configured
        return copy != null ? copy : true;
    }
{code}
So, no matter, we use '.wireTap(""XX"")' or '.wireTap(""XX"").copy()' , it will always copy the original exchange message.
",njiang,gangliu,Minor,Resolved,Fixed,26/Oct/12 09:14,26/Oct/12 14:01
Bug,CAMEL-5747,12613734,Cold restart doesn't clear properties component cache,"The properties component optionally (and by default) caches the property values that it resolves. However, the component does not clear the cache on stop, so the properties values are never refreshed after a cold restart of Camel (stop() + start()).",njiang,sdanig,Minor,Resolved,Fixed,26/Oct/12 23:28,30/Oct/12 14:59
Bug,CAMEL-5748,12613768,Classloader mixup in Direct VM component,"The Direct VM component allows routes in different Camel Contexts to communicate with each other with all the benefits of the Direct endpoint (namely thread-sharing, thus enabling transaction to propagate across Camel Contexts).

However, the TCCL of the consumer route still remains the TCCL of the producer route. See CAMEL-5722 for a similar case.

I suggest fixing this by introducing a proxy processor at the consumer that sets the appropriate classloader, and sets it back when done. Note: remember to take into account exception scenarios, i.e. we should catch exceptions from the forward call to .process(), reset the classloader and rethrow the exception.
",raulvk,raulvk,Major,Resolved,Fixed,27/Oct/12 18:03,21/Jan/13 12:44
Bug,CAMEL-5756,12613878,"OSGi resolvers for component, dataformat and language should check if service object is assignable before casting","Currently, the OSGi resolvers, get all services for the target component, dataformat and language and then cast the first service object to the type of choice.

This can be problematic if for any reason multiple version of a component is installed inside the container.",iocanel,iocanel,Major,Resolved,Fixed,29/Oct/12 12:13,29/Oct/12 18:51
Bug,CAMEL-5757,12613916,HTTP 1.1 Host header not set correctly with camel-http4,"The HTTP 1.1 Host header is not set correctly when using the camel-http4 component as a proxy with bridgeEnpoint=true.

The original request Host header is set in the proxied request instead of the host of the service requested by the proxy (ref: http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html section 14.23).

This simple route shows the problem:

 <route autoStartup=""true"" id=""TestHost"" xmlns:ns2=""http://camel.apache.org/schema/web"" xmlns=""http://camel.apache.org/schema/spring"">
        <from uri=""jetty:http://0.0.0.0:8090/TestHost""/>
        <to uri=""https4://www.google.com?bridgeEndpoint=true&amp;throwExceptionOnFailure=false""/>
 </route> 

This is what I get in the log:

INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] DefaultClientConnection        DEBUG Sending request: GET / HTTP/1.1
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""GET / HTTP/1.1[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""breadcrumbId: ID-localhost-1329-1351277552803-0-2[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:15.0) Gecko/20100101 Firefox/15.0[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""Accept-Encoding: gzip, deflate[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""Host: localhost:8090[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""Accept-Language: en-us,en;q=0.5[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""Connection: Keep-Alive[\r][\n]""
INFO   | jvm 1    | 2012/10/26 14:52:48 | [    qtp31541880-35 - /TestHost] wire                           DEBUG >> ""[\r][\n]""
",njiang,trembmax,Major,Resolved,Fixed,29/Oct/12 16:13,30/Dec/15 15:35
Bug,CAMEL-5761,12614061,Update SqsConsumer to extend Visibility Timeout correctly,"Currently SqsConsumer will scheduled a future to happen start occruing 1/2 the visibility timeout, and reoccur again at the visibility timeout period. Because of the way SQS works, when you request a ChangeMessageVisibility timeout it does not add that to the time remaining on the message visibility. Instead it will set the message visibility to the time out given. This means that what actually happens in the camel code is that:
* we request the ChangeMessageVisibility timeout to be set to X seconds from now after X/2 seconds
* then every X seconds after that we we request the ChangeMessageVisibility timeout to be set to X from now

This means that our message will become visible every now and again as the ChangeMessageVisibility timeout is not updated in time as period between successive executions is the same as the timeout period we request.

I've attached two patches which would fix this issue.

* ""aws_sqs_visibility_timeout_simple.patch"" - This patch changes the way we set up the future. The delay and period are now the same so that we call ChangeMessageVisibility every X/2 after the message has been pulled. 

* ""aws_sqs_visibility_timeout_phased_timeout.patch"" - This one keeps the delay to 1/2 the period and requests ChangeMessageVisibility every period. This time though it requests the visibility to be 1.5 * the period. This means that it behaves as originally planned, as if it was adding duration to the visibility duration left. This means we call Amazon less times and still keep the message hidden as expected.



This was looked at with Alex Hutter (both working on same project).",njiang,carlosroman,Major,Resolved,Fixed,30/Oct/12 18:09,09/Jan/13 00:44
Bug,CAMEL-5766,12614499,Configuring jetty component with a different name dont work with jetty consumer,"For example if you do
{code}
		<route>
			<from uri=""jetty2:http://localhost:9090/myapp""/>
			<transform>
				<simple>Hello ${body}</simple>
			</transform>
		</route>
{code}

Then it fails with
{code}
java.net.SocketException: Unresolved address
	at sun.nio.ch.Net.translateToSocketException(Net.java:58)
	at sun.nio.ch.Net.translateException(Net.java:84)
	at sun.nio.ch.Net.translateException(Net.java:90)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:61)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:286)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.camel.component.jetty.JettyHttpComponent.connect(JettyHttpComponent.java:315)
	at org.apache.camel.component.http.HttpEndpoint.connect(HttpEndpoint.java:149)
	at org.apache.camel.component.http.HttpConsumer.doStart(HttpConsumer.java:56)
	at org.apache.camel.support.ServiceSupport.start(ServiceSupport.java:60)
...
{code}",davsclaus,davsclaus,Major,Resolved,Fixed,02/Nov/12 11:40,02/Nov/12 12:10
Bug,CAMEL-5769,12614644,Camel JMS producer can block a thread under specific circumstances,"This is an edge case, but when the following circumstances concur, the asynchronous routing engine in Camel become blocked:

- Sending an InOut exchange over JMS (request/reply), where:
- Persistent messages are enabled (deliveryPersistent=true)
- useMessageIDasCorrelationID is enabled
- replyToType is Temporary queues (default) (using TemporaryQueueReplyManager)
- a timeout is set

You have to be really unlucky, but if the following condition is satisfied, a thread will become blocked:

{{time the broker takes to ACK the produced message > message timeout}}

Hence, if we have a timeout of 3000ms, and the broker takes 4000ms to ACK the message (e.g. slow Disk I/O while the journal is rotating - it happened to us), at some point we'll see this.

{code}
2012-10-30 10:46:57,680 | WARN  | CorrelationMap | 89 - org.apache.camel.camel-core - 2.8.0.fuse-06-11 | Exception occurred during purge task. This exception will be ignored.
java.lang.NullPointerException
{code}

The CorrelationMap is rendered useless, so if subsequent exchanges time out, the ReplyHandler may never get an onTimeout callback, thus leading to the waiting thread getting blocked forever if the async routing engine was in place.",raulvk,raulvk,Critical,Closed,Fixed,03/Nov/12 12:57,04/Nov/12 23:34
Bug,CAMEL-5773,12614905,camel-twitter streaming consumer broken,CAMEL-5529 was an incomplete fix and broke the streaming consumer.  Patch attached.,davsclaus,3riverdev,Major,Resolved,Fixed,06/Nov/12 06:19,06/Nov/12 08:00
Bug,CAMEL-5774,12614928,ActiveMQ target endpoint with transferExchange=true does not work together with source File endpoint with preMove option set,"Got failure on a simple route reading from file and sending to ActiveMQ queue.
On source File endpoint set preMove option to some folder, and on target activemq endpoint set transferExchange=true.
It fails with FileNotFound exception, trying to read file from original location, not from temporary preMoved one.
Without preMove all works perfect.

Simplified Spring application context with Camel routes looks like this:

<?xml version=""1.0"" encoding=""UTF-8""?>
<beans xmlns=""http://www.springframework.org/schema/beans""
       xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
       xsi:schemaLocation=""
         http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
         http://camel.apache.org/schema/spring http://camel.apache.org/schema/spring/camel-spring.xsd"">

    <bean id=""brokerService"" class=""org.apache.activemq.broker.BrokerService"" init-method=""start"" destroy-method=""stop"">
        <property name=""brokerName"" value=""localhost"" />
        <property name=""useJmx"" value=""false"" />
        <property name=""transportConnectorURIs"">
            <list>
                <value>vm://localhost</value>
            </list>
        </property>
    </bean>
    
    <bean id=""jmsConnectionFactory"" class=""org.apache.activemq.ActiveMQConnectionFactory"" depends-on=""brokerService"">
        <property name=""brokerURL"" value=""vm://localhost""/>
        <property name=""redeliveryPolicy"" ref=""redeliveryPolicyConfigActiveMQ""/>
    </bean>
    
    <bean id=""pooledConnectionFactory"" class=""org.apache.activemq.pool.PooledConnectionFactory"" init-method=""start"" destroy-method=""stop"">
        <property name=""maxConnections"" value=""8"" />
        <property name=""maximumActive"" value=""500"" />
        <property name=""connectionFactory"" ref=""jmsConnectionFactory"" />
    </bean>
    
     <bean id=""redeliveryPolicyConfigActiveMQ"" class=""org.apache.activemq.RedeliveryPolicy"">
        <property name=""maximumRedeliveries"" value=""1""/>
        <property name=""redeliveryDelay"" value=""5000""/>
    </bean>
    
			
    <camelContext id=""testCustomer"" xmlns=""http://camel.apache.org/schema/spring"">
        
		<endpoint id=""sourceEndpoint"" uri=""file:e:/work/ids/testCustomer/input/?preMove=.tmp&amp;delay=1000&amp;readLock=rename&amp;copyAndDeleteOnRenameFail=false&amp;move=.archive&amp;moveFailed=.failed"" />
		<endpoint id=""destJms"" uri=""activemq:queue:source?transferExchange=true&amp;connectionFactory=#pooledConnectionFactory"" />

        <route id=""testRoute"">
            <from ref=""sourceEndpoint"" />
            <to ref=""destJms"" />
        </route>
    </camelContext>
</beans>


Exception trace:
org.apache.camel.TypeConversionException: Error during type conversion from type: java.io.File to the required type: byte[] with value e:\work\ids\testCustomer\input\payment.xml due java.io.FileNotFoundException: e:\work\ids\testCustomer\input\payment.xml (The system cannot find the file specified)
	at com.it.ids.exceptions.HandleException.process(HandleException.java:20)
	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:163)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:117)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.FatalFallbackErrorHandler.processNext(FatalFallbackErrorHandler.java:42)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.RedeliveryErrorHandler.deliverToFailureProcessor(RedeliveryErrorHandler.java:766)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:273)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:220)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)
	at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)
	at org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:336)
	at org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:189)
	at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:155)
	at org.apache.camel.impl.ScheduledPollConsumer.doRun(ScheduledPollConsumer.java:142)
	at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:92)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.camel.RuntimeCamelException: java.io.FileNotFoundException: e:\work\ids\testCustomer\input\payment.xml (The system cannot find the file specified)
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1270)
	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:926)
	at org.apache.camel.impl.converter.StaticMethodTypeConverter.convertTo(StaticMethodTypeConverter.java:47)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(BaseTypeConverterRegistry.java:253)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.mandatoryConvertTo(BaseTypeConverterRegistry.java:155)
	at org.apache.camel.component.file.FileBinding.loadContent(FileBinding.java:57)
	at org.apache.camel.component.file.GenericFileConverter.genericFileToInputStream(GenericFileConverter.java:123)
	at org.apache.camel.component.file.GenericFileConverter.genericFileToSerializable(GenericFileConverter.java:152)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:922)
	at org.apache.camel.impl.converter.StaticMethodTypeConverter.convertTo(StaticMethodTypeConverter.java:47)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(BaseTypeConverterRegistry.java:253)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:111)
	at org.apache.camel.impl.DefaultExchangeHolder.checkMapSerializableObjects(DefaultExchangeHolder.java:209)
	at org.apache.camel.impl.DefaultExchangeHolder.safeSetProperties(DefaultExchangeHolder.java:177)
	at org.apache.camel.impl.DefaultExchangeHolder.marshal(DefaultExchangeHolder.java:93)
	at org.apache.camel.impl.DefaultExchangeHolder.marshal(DefaultExchangeHolder.java:71)
	at org.apache.camel.component.jms.JmsBinding.createJmsMessage(JmsBinding.java:439)
	at org.apache.camel.component.jms.JmsBinding.makeJmsMessage(JmsBinding.java:287)
	at org.apache.camel.component.jms.JmsProducer$2.createMessage(JmsProducer.java:266)
	at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate.doSendToDestination(JmsConfiguration.java:215)
	at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate.access$100(JmsConfiguration.java:158)
	at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate$3.doInJms(JmsConfiguration.java:192)
	at org.springframework.jms.core.JmsTemplate.execute(JmsTemplate.java:466)
	at org.apache.camel.component.jms.JmsConfiguration$CamelJmsTemplate.send(JmsConfiguration.java:189)
	at org.apache.camel.component.jms.JmsProducer.doSend(JmsProducer.java:398)
	at org.apache.camel.component.jms.JmsProducer.processInOnly(JmsProducer.java:352)
	at org.apache.camel.component.jms.JmsProducer.process(JmsProducer.java:132)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:122)
	at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:298)
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:117)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:163)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:334)
	... 28 more
Caused by: java.io.FileNotFoundException: e:\work\ids\testCustomer\input\payment.xml (The system cannot find the file specified)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:106)
	at org.apache.camel.converter.IOConverter.toInputStream(IOConverter.java:73)
	at org.apache.camel.converter.IOConverter.toByteArray(IOConverter.java:243)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:922)
	... 71 more
",davsclaus,vvnick,Minor,Resolved,Fixed,06/Nov/12 10:04,03/Feb/13 08:55
Bug,CAMEL-5776,12615168,.camelLock is deleted by another camel instance,"When camel file component tries to aquire a readLock, e.g. readLock=changed, and a camelLock already exists (aquired by another camel instance), the method MarkerFileExclusiveReadLockStrategy.acquireExclusiveReadLock fails (correct).

Then in GenericFileConsumer.processExchange, row 291 processStrategy.abort is called. That will delete the camelLock which another camel instance owns.

I don't know if abort should call exclusiveReadLockStrategy.releaseExclusiveReadLock() or if we have to remember if we own the lock or not.

",njiang,dave.larsson,Major,Resolved,Fixed,07/Nov/12 15:25,16/Nov/12 15:19
Bug,CAMEL-5782,12615484,"regression : invalid SetQueueAttributesRequest created, works on 2.10.1","In org.apache.camel.component.aws.sqs.SqsEndpoint.updateQueueAttributes,
if I don't have any configuration, the created SetQueueAttributesRequest contains a null atttribute collection and AWS emit an error.

In 2.10.1, no problem.

Workaround in 2.10.2 : force the create SetQueueAttributesRequest to contain a valid attribute collection by defining a configuration in camel. 

For exemple: 
from(""aws-sqs://""+queue+""?amazonSQSClient=#amazonSQSClient&delay=""+pollCycle.getMillis()+""&maxMessagesPerPoll=10&deleteAfterRead=false"")   

   -> works on 2.10.1, fail on 2.10.2

if I add an argument to my URI""&defaultVisibilityTimeout=30"" 
    -> works on 2.10.1, works on 2.10.2


Exception : 

{{
Caused by: org.apache.camel.FailedToCreateRouteException: Failed to create route SQS-to-MongoDB-EVENTS: Route[[From[aws-sqs://EVENTS?amazonSQSClient=#amazonSQSClien... because of Failed to resolve endpoint: aws-sqs://EVENTS?amazonSQSClient=%23amazonSQSClient&delay=60000&deleteAfterRead=false&maxMessagesPerPoll=10 due to: The request must contain the parameter Attribute.Name.
        at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:176) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.startRoute(DefaultCamelContext.java:722) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.startRouteDefinitions(DefaultCamelContext.java:1789) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.doStartCamel(DefaultCamelContext.java:1575) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.doStart(DefaultCamelContext.java:1444) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.support.ServiceSupport.start(ServiceSupport.java:60) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1412) ~[camel-core-2.10.2.jar:2.10.2]
        at com.netceler.iv5measure.contract.impl.sqs.CamelRoutesLauncher.start(CamelRoutesLauncher.java:55) ~[webapp-0.1.2-SNAPSHOT.jar:0.1.2-SNAPSHOT]
        at com.netceler.iv5measure.contract.impl.config.ConfiguredStreamInitializer.startStreams(ConfiguredStreamInitializer.java:39) ~[webapp-0.1.2-SNAPSHOT.jar:0.1.2-SNAPSHOT]
        ... 58 common frames omitted
Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: aws-sqs://EVENTS?amazonSQSClient=%23amazonSQSClient&delay=60000&deleteAfterRead=false&maxMessagesPerPoll=10 due to: The request must contain the parameter Attribute.Name.
        at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:479) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.util.CamelContextHelper.getMandatoryEndpoint(CamelContextHelper.java:50) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.model.RouteDefinition.resolveEndpoint(RouteDefinition.java:186) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:108) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:114) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.model.FromDefinition.resolveEndpoint(FromDefinition.java:72) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultRouteContext.getEndpoint(DefaultRouteContext.java:90) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:851) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:171) ~[camel-core-2.10.2.jar:2.10.2]
        ... 66 common frames omitted
Caused by: com.amazonaws.AmazonServiceException: The request must contain the parameter Attribute.Name.
        at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:614) ~[aws-java-sdk-1.3.23.jar:na]
        at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:312) ~[aws-java-sdk-1.3.23.jar:na]
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:165) ~[aws-java-sdk-1.3.23.jar:na]
        at com.amazonaws.services.sqs.AmazonSQSClient.invoke(AmazonSQSClient.java:812) ~[aws-java-sdk-1.3.23.jar:na]
        at com.amazonaws.services.sqs.AmazonSQSClient.setQueueAttributes(AmazonSQSClient.java:246) ~[aws-java-sdk-1.3.23.jar:na]
        at com.netceler.infra.aws.sqs.JMXSQS.setQueueAttributes(JMXSQS.java:108) ~[infra-sqs-0.0.3-SNAPSHOT.jar:0.0.3-SNAPSHOT]
        at com.netceler.infra.aws.sqs.AmazonSQSClientProxy.setQueueAttributes(AmazonSQSClientProxy.java:63) ~[infra-sqs-0.0.3-SNAPSHOT.jar:0.0.3-SNAPSHOT]
        at org.apache.camel.component.aws.sqs.SqsEndpoint.updateQueueAttributes(SqsEndpoint.java:139) ~[camel-aws-2.10.2.jar:2.10.2]
        at org.apache.camel.component.aws.sqs.SqsEndpoint.doStart(SqsEndpoint.java:93) ~[camel-aws-2.10.2.jar:2.10.2]
        at org.apache.camel.support.ServiceSupport.start(ServiceSupport.java:60) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.startService(DefaultCamelContext.java:1763) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.doAddService(DefaultCamelContext.java:931) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.addService(DefaultCamelContext.java:892) ~[camel-core-2.10.2.jar:2.10.2]
        at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:475) ~[camel-core-2.10.2.jar:2.10.2]
        ... 74 common frames omitted
}}",muellerc,jgraglia,Minor,Closed,Fixed,09/Nov/12 14:02,12/Nov/12 07:31
Bug,CAMEL-5794,12616363,xmljson dataformat should be added into marshall and unmarshal definition module,,njiang,njiang,Minor,Resolved,Fixed,16/Nov/12 08:02,18/Nov/12 16:25
Bug,CAMEL-5796,12616415,The combination of the transacted DSL together with the <setHeader> or <setBody> prohibits to resolve the properties properly.,"Given the property {{myKey}} defined as:
{code}
myKey=myValue
{code}

Then consider the following trivial route:
{code:xml}
<route>
  <from uri=""activemq:queue:okay"" />
    <transacted />
    <setHeader headerName=""myHeader"">
      <constant>{{myKey}}</constant>
    </setHeader>
  <to uri=""mock:test"" />
</route>
{code}

Because of the usage of the {{transacted}} DSL the property placeholder {{{{myKey}}}} will not be resolved to {{myValue}} properly. This behaviour would disappear if you would remove the {{transacted}} DSL. And I'm observing the same behaviour using the {{setBody}} DSL as well.
",bvahdat,bvahdat,Major,Resolved,Fixed,16/Nov/12 14:52,18/Nov/12 19:11
Bug,CAMEL-5810,12616943,CXF + Code first + No recipient list + doCatch = route returns null,"See this topic for the description: 
http://camel.465427.n5.nabble.com/doCatch-returns-null-td5722851.html",njiang,maxence.dewil,Major,Resolved,Fixed,20/Nov/12 14:28,22/Nov/12 09:53
Bug,CAMEL-5816,12617410,NPE is generated when using camel karaf command when JMX is disabled,NPE is generated when using camel karaf command and JMX is disabled,cmoulliard,cmoulliard,Minor,Resolved,Fixed,23/Nov/12 11:25,11/Dec/12 11:49
Bug,CAMEL-5818,12617435,Splitter does not allow errorhandler to handle exception in the splitter-expression,"This might relate to CAMEL-5024


The following route works ok in camel 2.9.3 
{code}

from(""direct:sampleUri"")
  .errorHandler(deadLetterChannel(""direct:errorhandler"").disableRedelivery())
  .multicast()
    .stopOnException()
    .split(simple(""${body.list}""))
      .streaming()
      .stopOnException()
      .process(new Processor() {
        @Override
	public void process(Exchange exchange) throws Exception {

	
	}
	
     }).end()
    .end()
    .to(""direct:somewhere_else_1"")
    .to(""direct:somewhere_else_2"")
.end();
{code}

-> if an exception is thrown by body.list the errorhandler kicks in and the
message is handled by ""direct:errorhandler"". Since 2.9.4 (and also with the latest version, 2.10.2) the errorhandler is no longer
active.",davsclaus,chris889,Major,Resolved,Fixed,23/Nov/12 13:56,25/Nov/12 10:58
Bug,CAMEL-5821,12617505,Camel Jetty chunked parameter appears broken,"I have several Jetty endpoints that use ?chunked=false

They all stopped working in 2.9.3. The parameter is ignored and chunked encoding is used.

I'm suspicious of CAMEL-5367 and CAMEL-5274 due to the timing, but nothing else.
",njiang,cott@internetstaff.com,Major,Resolved,Fixed,24/Nov/12 22:57,12/Nov/15 03:50
Bug,CAMEL-5823,12617599,CxfConsumer should not populate the cxf response with the original input message if the cxfExchange is oneway,"When you have a camel route from a CXF oneway endpoint to some endpoint, cxfConsumer currently uses the input message to fill the cxf's response message at the cxf endpoint. This can lead to a problem in CXF as there can be an exception raised during this step.

To avoid this issue, I think we should not fill the cxf response if the cxf's exchange is set to oneway.

I have attached a test case and a possible fix for this issue.

There is also one question regarding this part of the processing in CxfConsumer. I noticed that there are no exception handling in that part in camel and therefore the exception is not visible in camel. I was not sure if this was intentional or if we should be setting the exception to camel's exchange in that case so that this exception is also picked up by camel.

",njiang,ay,Major,Resolved,Fixed,26/Nov/12 12:34,29/Nov/12 11:07
Bug,CAMEL-5826,12617792,Apache Camel 2.9 Splitter with tokenize dont work with namespaces,"when trying to tokenize a stream having namespaces, no tokens are produced with inheritNamespaceTagName property.

-------------------------------------------------------------------

<route id=""hrp.connectorsCtxt.sddRcvFile2"">
<from
                           uri=""file:C:\Temp\esb\sdd\in?recursive=true&amp;preMove=.processing&amp;move=../.processed"" />
                    <camel:split streaming=""true"">
                           <tokenize token=""suiviDemande"" inheritNamespaceTagName=""suivisDemandes"" xml=""true""/>
                           <log message=""${header.CamelSplitIndex} : ${in.body}"" />
                    </camel:split>
             </route>

-------------------------------------------------------------------
",rnewcomb,shrishs,Major,Resolved,Fixed,27/Nov/12 14:05,29/Nov/12 11:06
Bug,CAMEL-5830,12617943,NullPointerException in DefaultCamelContext::stopRouteService,"When an exception occures during the startup of a route Route::getConsumer() might return null.
During the shutdown of this route the logging in DefaultCamelContext::stopRouteService yields to a NullPointerException:

log.info(""Route: "" + route.getId() + "" stopped, was consuming from: "" + route.getConsumer().getEndpoint())",njiang,kramer,Major,Resolved,Fixed,28/Nov/12 08:17,28/Nov/12 17:17
Bug,CAMEL-5835,12618233,"XPathBuilder is documented as being thread-safe, but it has thread-safety bugs.","The javadoc for org.apache.camel.builder.xml.XPathBuilder states that it is thread-safe, but an inspection of the source code in the trunk reveals that there are thread-safety bugs.",davsclaus,petesramka,Minor,Resolved,Fixed,29/Nov/12 21:25,07/Jan/13 12:21
Bug,CAMEL-5837,12618308,Problem with nested schema imports when upgrading from 2.8.6 to 2.10.2,"Hello

I'm experiencing trouble with the validiator component when upgrading from 2.8.6 to 2.10.2. 

The problem seems to be related to imported schemas that does additional imports (all using relative paths). XSD A importing XSD B works fine. But if B in turn imports XSD C. That import seems to be resolved with the base path of A. 

All works fine in 2.8.6 but breaks when upgrading to 2.10.2.

I will attach an example project where you can just switch version to verify the scenario.  ",njiang,hutchkintoot,Major,Resolved,Fixed,30/Nov/12 11:58,21/Mar/13 10:46
Bug,CAMEL-5844,12618554,Camel Tracer not showing some EIP names,"In order to debug Camel routes, I have enabled the Tracer as follows:         getContext().setTracing(true);

However, I have observed that some EIP names and routes are not being printed on console, making it a bit confusing to follow. As far as I know, this happens with:
* process(): the processor is not printed in the tracer; it's just empty (see below)
* marshall(): the marshaller name is not printed in the tracer; it's just empty (see below)
* setBody(): this step is also printed empty
* from(""activiti:...""): this route step is not printed altogether

For simplicity, I only provide the examples for process() and marshall(), bit I can provide more information if needed.

{panel:title=Route2 Config}
from(""vm:processIncomingOrders"")
  .process(new IncomingOrdersProcessor())
  .split(body())	// iterate list of Orders
  .to(""log:incomingOrder1?showExchangeId=true"")
  .process(new ActivitiStarterProcessor())
  .to(""log:incomingOrder2?showExchangeId=true"")			
  .to(""activiti:activiti-camel-example"");
{panel}

{panel:title=Route2 Tracer}
INFO  03-12 12:09:31,899 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-3 >>> (route2) from(vm://processIncomingOrders) -->  <<< Pattern:InOnly, [...]
INFO  03-12 12:09:34,899 (IncomingOrdersProcessor.java:process:39)  -Processing incoming orders (from Web Services)
[ORDER id:120 partName: wheel amount: 2 customerName: Honda Mechanics]
[ORDER id:121 partName: engine amount: 4 customerName: Volvo]
[ORDER id:122 partName: steering wheel amount: 3 customerName: Renault]
INFO  03-12 12:09:34,900 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-3 >>> (route2)  --> split[body] <<< Pattern:InOnly, [...]
{panel}



{panel:title=Route6 config}
from(""direct:ordercsv"")
  .marshal().bindy(BindyType.Csv, ""net.atos.camel.entities"")
  .to(""file:d://cameldata/orders?fileName=orders-$\{date:now:yyyyMMdd-hhmmss}.csv"");
{panel}

{panel:title=Route6 Tracer}
INFO  03-12 12:09:37,313 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-8 >>> (route6) direct://ordercsv -->  <<< Pattern:InOnly, [...]
INFO  03-12 12:09:37,320 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-8 >>> (route6)  --> file://d://cameldata/orders?fileName=orders-%24%7Bdate%3Anow%3AyyyyMMdd-hhmmss%7D.csv <<< Pattern:InOnly,  [...]
{panel}

",muellerc,reckless82,Minor,Closed,Fixed,03/Dec/12 11:31,14/Dec/12 21:43
Bug,CAMEL-5846,12618733,camel-bindy - Honor precision when formatting a decimal number,"Let's take the class org.apache.camel.dataformat.bindy.model.complex.generateheader.Order in which the field amount is defined as follow:

@DataField(pos = 9, precision = 2)
private BigDecimal amount;

This will properly un-marshall 10.00 and 10 but if I format it back I can only have 10 regardless of the precison. 

Patch available as part of the fix for CAMEL-5827",davsclaus,lb,Minor,Resolved,Fixed,04/Dec/12 13:48,17/Dec/12 10:25
Bug,CAMEL-5848,12618839,File consumer - When using doneFileName then avoid picking up files in middle of group if done file is written during scanning,"See nabble
http://camel.465427.n5.nabble.com/Files-not-Picked-up-with-doneFile-tp5723623.html

We need to use an internal cache we store whether a done file exists or not, when scanning a directory. Then we won't ""see"" a new done file being written during scanning a group of files. The cache is only needed per directory as doneFileName mandates the file to be in same dir.",davsclaus,davsclaus,Minor,Resolved,Fixed,05/Dec/12 07:58,17/Dec/12 08:39
Bug,CAMEL-5854,12622875,adviceWith() just needs to start the route when the camel context is started,"An user complains the adviceWith doesn't work well with the camel-http4 component[1].
After digging the code a while, I found the NPE is thrown because the camel-http4 endpoint is created before camel-http4 component is started. It is caused by the adviceWith() try to start the route after it finished the work. 

We need to make sure the adviceWith doesn't start the route when the camel context is not started yet to avoid the NPE.

[1]http://camel.465427.n5.nabble.com/Test-Intercept-with-adviceWith-and-http-td5723473.html",njiang,njiang,Major,Resolved,Fixed,07/Dec/12 08:01,13/Dec/12 15:04
Bug,CAMEL-5860,12623149,Regression in validator component in 2.10.3,"I get:

{code}
CaughtExceptionType:java.lang.NullPointerException, CaughtExceptionMessage:null, StackTrace:java.lang.NullPointerException  at 
org.apache.camel.converter.jaxp.XmlConverter.toStreamSource(XmlConverter.java:516)   at 
org.apache.camel.converter.jaxp.XmlConverter.toSAXSource(XmlConverter.java:399)      at 
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at 
java.lang.reflect.Method.invoke(Method.java:601)     at 
org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:923)       at 
org.apache.camel.impl.converter.InstanceMethodTypeConverter.convertTo(InstanceMethodTypeConverter.java:66)   
at org.apache.camel.support.TypeConverterSupport.convertTo(TypeConverterSupport.java:34)        at 
org.apache.camel.processor.validation.ValidatingProcessor.getSource(ValidatingProcessor.java:343)    at 
org.apache.camel.processor.validation.ValidatingProcessor.process(ValidatingProcessor.java:100)      at 
org.apache.camel.impl.ProcessorEndpoint.onExchange(ProcessorEndpoint.java:101)       at 
org.apache.camel.impl.ProcessorEndpoint$1.process(ProcessorEndpoint.java:71) at 
org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)   at
org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)     at 
org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:122) at 
org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:298)        at 
org.apache.camel.processor.SendProcessor.process(SendProcessor.java:117)     at 
org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)     at 
org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)        

\
{code}

when I upgrade camel to 2.10.3 and use the validator component:

{noformat}
<camel:to uri=""validator:META-INF/xsd/transactiongatetransfertransaction.xsd"" />

{noformat}

this did not happen in 2.10.2 or versions before that.",njiang,davidkarlsen@gmail.com,Major,Resolved,Fixed,10/Dec/12 14:23,26/Apr/13 09:29
Bug,CAMEL-5864,12623403,Pre-XSLT and Validator Type Conversions from byte[] may easily fail,"XsltBuilder.getSource(Exchange e, Object o) tries to convert the payload to a Source. 

If you have a byte[] payload, then the following type converter will likely be picked:

{code}
InstanceMethodTypeConverter: public javax.xml.transform.sax.SAXSource 
org.apache.camel.converter.jaxp.XmlConverter.toSAXSource(byte[],org.apache.camel.Exchange) 
throws java.io.IOException,org.xml.sax.SAXException,javax.xml.transform.TransformerException
{code}

This type converter requires the Exchange (InstanceMethodTypeConverter.useExchange == true), but line 461 of XsltBuilder doesn't pass it on.

Therefore, a NullPointerException happens because the conversion method tries to access the Exchange.",njiang,raulvk,Major,Resolved,Fixed,11/Dec/12 16:08,18/Dec/12 00:15
Bug,CAMEL-5865,12623429,camel-jms: concurrent consumers on Temp Reply Queue requires more work,"This feature requires a bit more work to get right. Currently it's a bit buggy. Here are the reasons:

- every time the DLMC initialises a new consumer task (AsyncMessageListenerInvoker), it invokes the Destination Resolver. The current code ends up creating a new temp queue and overwriting the reply queue in the Reply Manager every time.
- temp queues can only be consumed from by the same JMS connection that created the queue. If you use a connection pool and maxConsumers > 1, there's no way to guarantee that the same JMS connection is used to create the subsequent consumers, as concurrency expands. We should explicitly set cacheLevel=CACHE_CONSUMER which activates sharing the connection in the DLMC across consumers.
",raulvk,raulvk,Major,Closed,Fixed,11/Dec/12 18:38,02/May/13 02:30
Bug,CAMEL-5866,12623521,Pass Exchange param to TypeConverter.convertTo calls where possible,"Where the Exchange object is available, pass it to TypeConverter.convertTo.

Currently, when Camel happens to pick a Type Converter requiring the Exchange, conversion may fail (as there's no check for this situation).

See CAMEL-5864 as an example.",raulvk,raulvk,Major,Resolved,Fixed,12/Dec/12 10:44,18/Dec/12 00:15
Bug,CAMEL-5867,12623551,camel-jetty - Always creates a default http client even if not needed,"The jetty component creates a default jetty http client, even if its not needed.
",davsclaus,davsclaus,Major,Resolved,Fixed,12/Dec/12 13:58,15/Dec/12 13:37
Bug,CAMEL-5871,12623687,Unable to parse CSV if a field has a ' (single quote) at it's begining ,"Bindy is unable to parse correctly a CSV file if a field begin with a ' (single quote).

This case is valid for foreign names/words or french apostrophe:

{code:title=order.csv}
10,A9,Patric,'T jo,ISIN,XD12345678,BUY,Share,2500.45,USD,08-01-2009
{code}",njiang,nexus6b,Major,Resolved,Fixed,13/Dec/12 08:46,22/Dec/12 15:47
Bug,CAMEL-5875,12623830,AdviceWith not working with weaveFirst/weaveLast because of delayed camelContext start due to using isUseAdviceWith.,"It is the second bug I found related to AdviceWith, we might want to copy/paste all the test, to make sure AdviceWith can be used before the CamelContext is started with all it's functionality.

http://camel.465427.n5.nabble.com/Bug-AdviceWith-not-working-with-weaveAddFirst-weaveAddLast-td5724054.html",njiang,lleclerc,Major,Resolved,Fixed,13/Dec/12 21:13,17/Dec/12 14:49
Bug,CAMEL-5877,12623943,Sporadic Source Conversion issues with CxfPayload,"There is one concrete issue to report and there is an additional point that I would like to bring up.

First, the concrete issue is that CxfEndpoints' findName does not currently handle SAXSource. Consequently, the it fails to find the element name when the payload source is of SAXSource.

This issue can be observed sporadically, as there is a chance the Source could be SAXSource. Because of the nature of this problem explained below, this issue may be observed non-deterministically and consequently it is not often observed.

What is happening is that, when a conversion to CxfPayload is triggered, CxfPayloadConverter's convertTo method arbitrarily picks a converter that can convert the input to Source. The code goes into BaseTypeConverterRegistry and searches for a matching converter in its type mappings' entry set. As this set has no deterministic ordering and there are several matching converters, an arbitrary converter is picked up. THe chance of getting the SAXSource converter is small but not zero. Therefore, when this happens, CxfEndpoint's findName is not finding the element name as there is no handling for SAXSource. Interestingly, the DOMSource converter is the most frequently picked converter, followed by the StaxSource converter.

So, SAXSource handling needs to be added in CXFEndpoint's findName method.

The second point is about the preferred Source held in CxfPayload. I think for jdk 1.6 or above, StAXSource could be the preferred source. In that case, we can use the StAXSource converter instead of arbitrarily picking one of the Source converters. We can adjust CxfPayloadConverter's convertTo method to look up for the StAXSource converter first.

I am attaching a proposed patch (the correction for the first issue and the suggestion regarding the second point, assuming StAXSource is preferred.). Could you review it and comment on it?

Thanks.

Regards, aki


",dkulp,ay,Major,Resolved,Fixed,14/Dec/12 12:01,19/Dec/12 19:52
Bug,CAMEL-5880,12624185,camel-quickfix: QuickfixjEndpoint should adhere the InOut exchange pattern if you would ask for this MEP,See http://camel.465427.n5.nabble.com/camel-quickfix-RequestReplyExample-java-io-IOException-td5723769.html,bvahdat,bvahdat,Minor,Resolved,Fixed,16/Dec/12 12:20,01/May/13 07:04
Bug,CAMEL-5883,12624242,File consumer - When using done file name then delete the file when batch is complete,"When using doneFileName then the file is deleted on each completion of files in the batch. And you get WARN logs about the done file cannot be deleted, because the 1st already deleted the file.

We should defer and only delete the done file on the last in the batch.",njiang,davsclaus,Minor,Resolved,Fixed,17/Dec/12 08:51,27/Apr/15 14:53
Bug,CAMEL-5887,12624297,Unexpected behavior when combining onException and multiple adviceWith in camel-core,"Here is the unit test with 3 tests (expected results and the strange behavior) : http://pastebin.com/JUnLFy70

A second adviceWith() on a second route will change the behavior from the first route during an Exception.

adviceWith is only used once per route, as suggested in the recommendation : http://camel.apache.org/advicewith.html
""It is recommended to only advice a given route once (you can of course advice multiple routes). If you do it multiple times, then it may not work as expected, especially when any kind of error handling is involved. The Camel team plan for Camel 3.0 to support this as internal refactorings in the routing engine is needed to support this properly.""",davsclaus,lleclerc,Major,Resolved,Fixed,17/Dec/12 15:13,22/Dec/12 15:34
Bug,CAMEL-5888,12624342,When call removeRouteDefinition the route doesn't removed from collection of route Definitions.,"When I call removeRouteDefinition for DefaultCamelContext, the route doesn't removed from collection of routes.
   public synchronized void removeRouteDefinition(RouteDefinition routeDefinition) throws Exception {
        String id = routeDefinition.idOrCreate(nodeIdFactory);
        stopRoute(id);
        removeRoute(id);
    }
it is just remove it from running route service if context is running.
When we do restart context this route started again. 
Workaround for this issue is call  removeRouteDefinitions and wrap single route into collection.

But when I call removeRouteDefinitions - it is process correctly: remove from collections of definitions and after removed from running route services.
    public synchronized void removeRouteDefinitions(Collection<RouteDefinition> routeDefinitions) throws Exception {
        this.routeDefinitions.removeAll(routeDefinitions);
        for (RouteDefinition routeDefinition : routeDefinitions) {
            removeRouteDefinition(routeDefinition);
        }
    }

 
",davsclaus,gennady@buhsl.com,Minor,Resolved,Fixed,17/Dec/12 20:10,21/Dec/12 13:51
Bug,CAMEL-5890,12624405,NPE when jaxb fallback converter is used with RequestEntityConverter,"As the RequestEntityConverter.toRequestEntity(String str, Exchange exchange) doesn't has check if the exchange is null, it caused the NPE as the user complain in camel user mailing list[1]

[1]http://camel.465427.n5.nabble.com/Content-Enrich-Error-when-upgrading-Apache-Camel-from-2-9-2-to-any-higher-version-td5724244.html

Here is the stack trace.
{code}
CaughtExceptionType:org.apache.camel.TypeConversionException, CaughtExceptionMessage:Error during type conversion from type: com.ecc.DamBean to the required type: org.apache.commons.httpclient.methods.RequestEntity with value com.ecc.DamBean@8811a59 due Error during type conversion from type: java.lang.String to the required type: org.apache.commons.httpclient.methods.RequestEntity with value ... 
The complete Stack Trace : 
Exchange[ExchangePattern:InOnly, BodyType:String, Body:, CaughtExceptionType:org.apache.camel.TypeConversionException, CaughtExceptionMessage:Error during type conversion from type: com.ecc.DamBean to the required type: org.apache.commons.httpclient.methods.RequestEntity with value com.ecc.DamBean@8811a59 due Error during type conversion from type: java.lang.String to the required type: org.apache.commons.httpclient.methods.RequestEntity with value due java.lang.NullPointerException, StackTrace:org.apache.camel.TypeConversionException: Error during type conversion from type: com.ecc.DamBean to the required type: org.apache.commons.httpclient.methods.RequestEntity with value com.ecc.DamBean@8811a59 due Error during type conversion from type: java.lang.String to the required type: org.apache.commons.httpclient.methods.RequestEntity with value due java.lang.NullPointerException	at org.apache.camel.converter.jaxb.FallbackTypeConverter.convertTo(FallbackTypeConverter.java:98)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(BaseTypeConverterRegistry.java:289)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:111)	at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:72)	at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:47)	at org.apache.camel.component.http.HttpProducer.createRequestEntity(HttpProducer.java:390)	at org.apache.camel.component.http.HttpProducer.createMethod(HttpProducer.java:343)	at org.apache.camel.component.http.HttpProducer.process(HttpProducer.java:91)	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.Enricher.process(Enricher.java:114)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:334)	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:220)	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:61)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.Pipeline.process(Pipeline.java:148)	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:61)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)	at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:57)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:275)	at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:185)	at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:139)	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:909)	at java.lang.Thread.run(Thread.java:662)Caused by: org.apache.camel.TypeConversionException: Error during type conversion from type: java.lang.String to the required type: org.apache.commons.httpclient.methods.RequestEntity with value due java.lang.NullPointerException	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:126)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:98)	at org.apache.camel.converter.jaxb.FallbackTypeConverter.marshall(FallbackTypeConverter.java:222)	at org.apache.camel.converter.jaxb.FallbackTypeConverter.convertTo(FallbackTypeConverter.java:94)	... 44 moreCaused by: org.apache.camel.RuntimeCamelException: java.lang.NullPointerException	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1271)	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:927)	at org.apache.camel.impl.converter.StaticMethodTypeConverter.convertTo(StaticMethodTypeConverter.java:47)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(BaseTypeConverterRegistry.java:253)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:111)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:98)	at org.apache.camel.converter.jaxb.FallbackTypeConverter.marshall(FallbackTypeConverter.java:222)	at org.apache.camel.converter.jaxb.FallbackTypeConverter.convertTo(FallbackTypeConverter.java:94)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(BaseTypeConverterRegistry.java:289)	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:111)	at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:72)	at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:47)	at org.apache.camel.component.http.HttpProducer.createRequestEntity(HttpProducer.java:390)	at org.apache.camel.component.http.HttpProducer.createMethod(HttpProducer.java:343)	at org.apache.camel.component.http.HttpProducer.process(HttpProducer.java:91)	at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.Enricher.process(Enricher.java:114)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:334)	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:220)	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.Pipeline.process(Pipeline.java:117)	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150)	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117)	at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99)	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:73)	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73)	at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:275)	at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:183)	at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:139)	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)	... 1 moreCaused by: java.lang.NullPointerException	at org.apache.camel.component.http.RequestEntityConverter.toRequestEntity(RequestEntityConverter.java:51)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)	at java.lang.reflect.Method.invoke(Method.java:597)	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:923)	... 50 more]
{code}",njiang,njiang,Major,Resolved,Fixed,18/Dec/12 03:59,18/Jan/13 01:54
Bug,CAMEL-5897,12624503,Nullcheck missing in CxfPayloadConverter,"There's a null-check missing in CxfPayloadConverter which triggers a NPE if the payload is the result from a SOAP Fault, in which case the CxfPayload object contains no body sources at all.

Stack trace, in this case what triggers type converters is Stream Caching, but it could happen in other cases where the FallbackConverter is utilized.

{code}
Exception in thread ""default-workqueue-3"" org.apache.camel.TypeConversionException: Error during type conversion from type: org.apache.camel.component.cxf.CxfPayload to the required type: org.apache.camel.StreamCache with value org.apache.camel.component.cxf.CxfPayload headers: []body: null due java.lang.NullPointerException
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:126)
	at org.apache.camel.core.osgi.OsgiTypeConverter.convertTo(OsgiTypeConverter.java:102)
	at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:72)
	at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:47)
	at org.apache.camel.processor.interceptor.StreamCachingInterceptor.process(StreamCachingInterceptor.java:46)
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)
	at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:303)
[...]
        at org.apache.camel.component.cxf.CxfClientCallback.handleException(CxfClientCallback.java:82)
	at org.apache.cxf.interceptor.ClientOutFaultObserver.onMessage(ClientOutFaultObserver.java:59)
	at org.apache.cxf.transport.http.HTTPConduit$WrappedOutputStream$1.run(HTTPConduit.java:1540)
	at org.apache.cxf.workqueue.AutomaticWorkQueueImpl$3.run(AutomaticWorkQueueImpl.java:426)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at org.apache.cxf.workqueue.AutomaticWorkQueueImpl$AWQThreadFactory$1.run(AutomaticWorkQueueImpl.java:351)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.apache.camel.RuntimeCamelException: java.lang.NullPointerException
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1271)
	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:927)
	at org.apache.camel.impl.converter.StaticMethodFallbackTypeConverter.convertTo(StaticMethodFallbackTypeConverter.java:50)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.doConvertTo(BaseTypeConverterRegistry.java:289)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.convertTo(BaseTypeConverterRegistry.java:111)
	... 56 more
Caused by: java.lang.NullPointerException
	at org.apache.camel.component.cxf.converter.CxfPayloadConverter.convertTo(CxfPayloadConverter.java:150)
	at sun.reflect.GeneratedMethodAccessor191.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:923)
	... 59 more
{code}",raulvk,raulvk,Major,Resolved,Fixed,18/Dec/12 17:58,18/Dec/12 21:20
Bug,CAMEL-5899,12624667,camel-netty - Producer should avoid blocking while waiting for connection,We should at most wait for the connection timeout to trigger.,davsclaus,davsclaus,Major,Resolved,Fixed,19/Dec/12 16:38,20/Dec/12 10:04
Bug,CAMEL-5905,12624981,Negative in-flight counter,"I noticed a situation with a negative in-flight counter.

Seems related to situations with handling an exception. Will have to dig deeper. Just wanted to log a JIRA.",davsclaus,davsclaus,Minor,Resolved,Fixed,21/Dec/12 11:52,21/Jan/13 12:17
Bug,CAMEL-5907,12625093,Camel leaks memory on undeploy / redeploy in app server when JMX enabled and createConnector = true,"We have embedded Camel in an EAR that we deploy to Weblogic. The Camel context is configured via Spring:

{code}    <camelContext id=""camel"" handleFault=""true"" autoStartup=""{{autoStartup}}"" xmlns=""http://camel.apache.org/schema/spring"">
        <contextScan />
        <jmxAgent id=""camelAgent"" createConnector=""true"" registryPort=""{{jmxPort}}"" />
    </camelContext>{code}

You can see that we create a JMX connector to allow for remote management.

However, we have run into PermGen space issues, because our application is leaking class loaders when the application is undeployed or redeployed.

After digging around (and addressing some Jasper Reports ThreadLocal sloppiness), it appears that the only issue left is that the sun.rmi.transport.ObjectTable class maintains a static reference to all available RMI targets. Unfortunately, one of these targets is the JMX connector created by Camel, which was obviously loaded via our application's classloader.

Thus, ObjectTable has a static reference to the Camel JMX RMI target, which has a reference to the app's class loader, which in turn has references to all classes loaded (and generated) for that single deployment of the application -- and none of these classes can be GC'ed.

After digging through the code for Camel's DefaultManagementAgent, I'm inclined to believe that the fix is fairly simple:

# Update {{createJmxConnector(String)}} to cache the reference to the created {{Registry}} in an instance variable.

# Update {{doStop()}} to check if we have a cached {{Registry}} instance, and if we do, call {{UnicastRemoteObject.unexportObject(registry, true);}}

Some app servers have workarounds for this sort of leak (see ""RMI targets"" in Table 1 at [1]), but Weblogic doesn't seem to.

I'll also attach a screenshot of the memory analysis (more info at [2]).



[1] http://pic.dhe.ibm.com/infocenter/wasinfo/v8r5/index.jsp?topic=%2Fcom.ibm.websphere.express.doc%2Fae%2Fctrb_memleakdetection.html

[2] http://www.yourkit.com/docs/kb/class_loaders.jsp",njiang,sdanig,Minor,Resolved,Fixed,22/Dec/12 02:55,02/Jan/13 07:11
Bug,CAMEL-5908,12625137,"Websocket consumer cannot be started, because of NPE","Websocket consumer cannot be started for simple route 
        <camel:route>
            <camel:from uri=""websocket://localhost:9998/cnt""/>
            <camel:to uri=""stream:out""/>
        </camel:route>

Root cause:

Websoket component has not serlvet object. 

WebsoketComponent#line:170
// Don't provide a Servlet object as Producer/Consumer will create them later on
connectorRef = new ConnectorRef(server, connector, null);


WebsoketComponent#line:190
if (prodcon instanceof WebsocketConsumer) {
      // connect websocket consumer, to servlet
      connectorRef.servlet.connect((WebsocketConsumer) prodcon);
}


Stack trace is:
Caused by: java.lang.NullPointerException
        at org.apache.camel.component.websocket.WebsocketComponent.connect(WebsocketComponent.java:192)
        at org.apache.camel.component.websocket.WebsocketEndpoint.connect(WebsocketEndpoint.java:90)
        at org.apache.camel.component.websocket.WebsocketConsumer.start(WebsocketConsumer.java:36)
        at org.apache.camel.impl.DefaultCamelContext.startService(DefaultCamelContext.java:1763)
        at org.apache.camel.impl.DefaultCamelContext.doStartOrResumeRouteConsumers(DefaultCamelContext.java:2059)
        at org.apache.camel.impl.DefaultCamelContext.doStartRouteConsumers(DefaultCamelContext.java:1995)
        at org.apache.camel.impl.DefaultCamelContext.safelyStartRouteServices(DefaultCamelContext.java:1923)
        at org.apache.camel.impl.DefaultCamelContext.doStartOrResumeRoutes(DefaultCamelContext.java:1702)
        at org.apache.camel.impl.DefaultCamelContext.doStartCamel(DefaultCamelContext.java:1583)
        at org.apache.camel.impl.DefaultCamelContext.doStart(DefaultCamelContext.java:1444)
        at org.apache.camel.spring.SpringCamelContext.doStart(SpringCamelContext.java:179)
        at org.apache.camel.support.ServiceSupport.start(ServiceSupport.java:60)
        at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1412)
        at org.apache.camel.spring.SpringCamelContext.maybeStart(SpringCamelContext.java:228)
        at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:118)",njiang,azarny,Major,Resolved,Fixed,23/Dec/12 08:53,29/Dec/12 10:31
Bug,CAMEL-5917,12625479,camel-jms - JmsPollingConsumer use the constant fields from JmsTemplate for receiveNoWait etc,"See nabble
http://camel.465427.n5.nabble.com/JmsPollingConsumer-and-method-receiveNoWait-has-incorrect-timeout-value-tp5722817.html",davsclaus,davsclaus,Minor,Resolved,Fixed,29/Dec/12 10:44,29/Dec/12 11:56
