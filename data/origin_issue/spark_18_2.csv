Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (dependent),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Unexpected overriden of exitFn in SparkSubmitSuite,SPARK-26501,13206907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liupengcheng,liupengcheng,liupengcheng,29/Dec/18 07:42,03/Jan/19 16:28,13/Jul/23 08:48,03/Jan/19 16:28,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Deploy,Spark Core,,,0,,,,,"When I run SparkSubmitSuite of spark2.3.2 in intellij IDE, I found that some tests cannot pass when I run them one by one, but they passed when the whole SparkSubmitSuite was run.

Failed tests when ran seperately:

 
{code:java}
test(""SPARK_CONF_DIR overrides spark-defaults.conf"") {
  forConfDir(Map(""spark.executor.memory"" -> ""2.3g"")) { path =>
    val unusedJar = TestUtils.createJarWithClasses(Seq.empty)
    val args = Seq(
      ""--class"", SimpleApplicationTest.getClass.getName.stripSuffix(""$""),
      ""--name"", ""testApp"",
      ""--master"", ""local"",
      unusedJar.toString)
    val appArgs = new SparkSubmitArguments(args, Map(""SPARK_CONF_DIR"" -> path))
    assert(appArgs.defaultPropertiesFile != null)
    assert(appArgs.defaultPropertiesFile.startsWith(path))
    assert(appArgs.propertiesFile == null)
    appArgs.executorMemory should be (""2.3g"")
  }
}
{code}
Failure reason:
{code:java}
Error: Executor Memory cores must be a positive number
Run with --help for usage help or --verbose for debug output
{code}
 

After carefully checked the code, I found the exitFn of SparkSubmit is overrided by front tests in testPrematrueExit call.

Although the above test was fixed by SPARK-22941, but the overriden of exitFn might cause other problems in the future.

 ",,apachespark,liupengcheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 16:28:23 UTC 2019,,,,,,,,,,"0|u00dxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/18 07:55;apachespark;User 'liupc' has created a pull request for this issue:
https://github.com/apache/spark/pull/23404;;;","03/Jan/19 16:28;srowen;Resolved by https://github.com/apache/spark/pull/23404;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcUtils.makeGetter does not handle ByteType,SPARK-26499,13206879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdsilva,tdsilva,tdsilva,29/Dec/18 00:10,12/Dec/22 18:10,13/Jul/23 08:48,01/Jan/19 06:13,2.4.0,,,,,,,,,,,,,,,,,2.4.5,3.0.0,,,SQL,,,,0,,,,,"I am trying to use the  DataSource V2 API to read from a JDBC source. While using {{JdbcUtils.resultSetToSparkInternalRows}} to create an internal row from a ResultSet that has a column of type TINYINT I ran into the following exception
{code:java}
java.lang.IllegalArgumentException: Unsupported type tinyint
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(JdbcUtils.scala:502)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetters$1.apply(JdbcUtils.scala:379)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetters$1.apply(JdbcUtils.scala:379)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetters(JdbcUtils.scala:379)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.<init>(JdbcUtils.scala:340)
{code}
This happens because ByteType is not handled in {{JdbcUtils.makeGetter}}.
",,apachespark,dongjoon,tdsilva,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 07 15:02:43 UTC 2020,,,,,,,,,,"0|u00drk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jan/19 06:13;gurwls223;Issue resolved by pull request 23400
[https://github.com/apache/spark/pull/23400];;;","15/Nov/19 03:14;dongjoon;This is backported to branch-2.4 via https://github.com/apache/spark/pull/26531 ;;;","07/Oct/20 15:02;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29968;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid to use Random.nextString in StreamingInnerJoinSuite,SPARK-26496,13206850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,bersprockets,bersprockets,28/Dec/18 18:36,12/Dec/22 18:10,13/Jul/23 08:48,29/Dec/18 20:16,2.4.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Structured Streaming,Tests,,,0,,,,,"This is a bit esoteric and minor, but makes it difficult to run SQL unit tests successfully on High Sierra.

StreamingInnerJoinSuite.""locality preferences of StateStoreAwareZippedRDD"" generates a directory name using {{Random.nextString(10)}}, and frequently that directory name is unacceptable to High Sierra.

For example:
{noformat}
scala> val prefix = Random.nextString(10); val dir = new File(""/tmp"", ""del_"" + prefix + ""-"" + UUID.randomUUID.toString); dir.mkdirs()
prefix: String = 媈ᒢ탊渓뀟?녛ꃲ싢櫦
dir: java.io.File = /tmp/del_媈ᒢ탊渓뀟?녛ꃲ싢櫦-aff57fc6-ca38-4825-b4f3-473140edd4f6
res39: Boolean = true // this one was OK

scala> val prefix = Random.nextString(10); val dir = new File(""/tmp"", ""del_"" + prefix + ""-"" + UUID.randomUUID.toString); dir.mkdirs()
prefix: String = 窽텘⒘駖ⵚ駢⡞Ρ닋੎
dir: java.io.File = /tmp/del_窽텘⒘駖ⵚ駢⡞Ρ닋੎-a3f99855-c429-47a0-a108-47bca6905745
res40: Boolean = false  // nope, didn't like this one

scala> prefix.foreach(x => printf(""%04x "", x.toInt))
7abd d158 2498 99d6 2d5a 99e2 285e 03a1 b2cb 0a4e 

scala> prefix(9)
res46: Char = ੎

scala> val prefix = ""\u7abd""
prefix: String = 窽

scala> val dir = new File(""/tmp"", ""del_"" + prefix + ""-"" + UUID.randomUUID.toString); dir.mkdirs()
dir: java.io.File = /tmp/del_窽-d1c3af34-d34d-43fe-afed-ccef9a800ff4
res47: Boolean = true // it's OK with \u7abd

scala> val prefix = ""\u0a4e""
prefix: String = ੎

scala> val dir = new File(""/tmp"", ""del_"" + prefix + ""-"" + UUID.randomUUID.toString); dir.mkdirs()
dir: java.io.File = /tmp/del_੎-3654a34c-6f74-4591-85af-a0f28b675a6f
res50: Boolean = false // doesn't like \u0a4e
{noformat}
I thought it might have something to do with my Java 8 version, but Python is equally affected:
{noformat}
>>> f = open(u""/tmp/del_\u7abd_file"", ""wb"")
f = open(u""/tmp/del_\u7abd_file"", ""wb"")
>>> f.write(""hello\n"")
f.write(""hello\n"")
# it's OK with \u7abd
>>> f2 = open(u""/tmp/del_\u0a4e_file"", ""wb"")
f2 = open(u""/tmp/del_\u0a4e_file"", ""wb"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
IOError: [Errno 92] Illegal byte sequence: u'/tmp/del_\u0a4e_file'
# doesn't like \u0a4e
>>> f2 = open(u""/tmp/del_\ufa4e_file"", ""wb"")
f2 = open(u""/tmp/del_\ufa4e_file"", ""wb"")
# a little change and it's happy again
>>> 
{noformat}
Mac OS X Sierra is perfectly happy with these characters. This seems to be a limitation introduced by High Sierra.","Mac OS X High Sierra
",bersprockets,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19613,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 29 20:16:29 UTC 2018,,,,,,,,,,"0|u00dl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/18 09:38;gurwls223;I think we should fix it to nextFloat.toString. Similar fix was made in SPARK-19613 before.;;;","29/Dec/18 20:16;dongjoon;This is resolved via https://github.com/apache/spark/pull/23405;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change lead/lag argument name from count to offset,SPARK-26451,13206522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,deepyaman,deepyaman,deepyaman,26/Dec/18 23:54,12/Dec/22 18:10,13/Jul/23 08:48,27/Dec/18 16:04,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,SQL,,,0,release-notes,,,,,,deepyaman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,The 'lag' function in Pyspark accepted an argument 'count' which should have been called 'offset'. It has been renamed accordingly.,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 27 16:04:22 UTC 2018,,,,,,,,,,"0|u00bkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/18 16:04;gurwls223;Issue resolved by pull request 23357
[https://github.com/apache/spark/pull/23357];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage color doesn't change with it's status,SPARK-26444,13206428,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,seancxmao,seancxmao,seancxmao,26/Dec/18 08:16,28/Dec/18 14:46,13/Jul/23 08:48,28/Dec/18 13:42,2.4.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Web UI,,,,0,,,,,"On job page, in event timeline section, stage color doesn't change according to its status. See attachments for some screen shots. 

 ",,seancxmao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/18 09:04;seancxmao;active.png;https://issues.apache.org/jira/secure/attachment/12953050/active.png","26/Dec/18 09:04;seancxmao;complete.png;https://issues.apache.org/jira/secure/attachment/12953051/complete.png","26/Dec/18 09:04;seancxmao;failed.png;https://issues.apache.org/jira/secure/attachment/12953052/failed.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 28 13:42:32 UTC 2018,,,,,,,,,,"0|u00azk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/18 13:42;srowen;Issue resolved by pull request 23385
[https://github.com/apache/spark/pull/23385];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Decimal data becomes bigint to query, unable to query",SPARK-26437,13206338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zengxl,zengxl,25/Dec/18 06:19,29/Aug/22 06:48,13/Jul/23 08:48,27/Dec/18 23:34,1.6.3,2.0.2,2.1.3,2.2.2,2.3.1,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"this is my sql:

create table tmp.tmp_test_6387_1224_spark  stored  as ORCFile  as select 0.00 as a

select a from tmp.tmp_test_6387_1224_spark

CREATE TABLE `tmp.tmp_test_6387_1224_spark`(

 {color:#f79232} `a` decimal(2,2)){color}

ROW FORMAT SERDE

  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'

STORED AS INPUTFORMAT

  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'

OUTPUTFORMAT

  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'

When I query this table（use hive or sparksql，the exception is same）, I throw the following exception information

*Caused by: java.io.EOFException: Reading BigInteger past EOF from compressed stream Stream for column 1 kind DATA position: 0 length: 0 range: 0 offset: 0 limit: 0*

        *at org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readBigInteger(SerializationUtils.java:176)*

        *at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$DecimalTreeReader.next(TreeReaderFactory.java:1264)*

        *at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StructTreeReader.next(TreeReaderFactory.java:2004)*

        *at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:1039)*

 ",,dongjoon,mgaido,smilegator,viirya,zengxl,,,,,,,,,,,,,,,,,,,,SPARK-40253,SPARK-20901,,,,,,,,,,,,,,HIVE-13083,SPARK-22977,,,,SPARK-25271,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 10 04:51:04 UTC 2019,,,,,,,,,,"0|u00afs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/18 10:58;mgaido;cc [~dongjoon];;;","27/Dec/18 22:51;dongjoon;Thanks, [~mgaido].;;;","27/Dec/18 23:33;dongjoon;Hi, [~zengxl].

Thank you for reporting. This is a very old issue since Apache Spark 1.x which occurs when you use `decimal`. Please note that `CAST` and `decimal` in the following example. Since Spark 2.0, `0.0` literal interpreted as `Decimal`. So, you are hitting this issue without casting, too. This is fixed at `master` branch and will be released as Apache Spark 3.0.0.

{code}
scala> sc.version
res0: String = 1.6.3

scala> sql(""drop table spark_orc"")
scala> sql(""create table spark_orc stored as orc as select cast(0.00 as decimal(2,2)) as a"")
scala> sql(""select * from spark_orc"").show
...
Caused by: java.io.EOFException: Reading BigInteger past EOF from compressed stream Stream for column 1 kind DATA position: 0 length: 0 range: 0 offset: 0 limit: 0
{code}

If you are interested, the followings are the details.

First, the underlying ORC issue (HIVE-13083) is fixed at Hive 1.3.0, but Spark is still using embedded Hive 1.2.1. To avoid the underlying ORC issue, you can use new ORC data source (`set spark.sql.orc.impl=native`). So, in Spark 2.4.0, you can use `USING` syntax to avoid this.

{code}
scala> sql(""create table spark_orc using orc as select 0.00 as a"")
scala> sql(""select * from spark_orc"").show
+----+
|   a|
+----+
|0.00|
+----+

scala> spark.version
res2: String = 2.4.0
{code}

Second, SPARK-22977 made a regression on CTAS at Spark 2.3.0 and is fixed recently SPARK-25271 (Hive CTAS commands should use data source if it is convertible) at Apache Spark 3.0.0. In Spark 3.0.0, you can use `STORED AS ORC` syntax without this problem.
{code}
scala> sql(""create table spark_orc stored as orc as select 0.00 as a"")
scala> sql(""select * from spark_orc"").show
+----+
|   a|
+----+
|0.00|
+----+

scala> spark.version
res3: String = 3.0.0-SNAPSHOT
{code}

So, I'll close this issue since this is fixed in 3.0.0.

cc [~cloud_fan], [~viirya], [~smilegator], [~hyukjin.kwon];;;","03/Jan/19 06:23;zengxl;Thanks [~dongjoon];;;","10/May/19 04:51;smilegator;Even if we do not use our native ORC reader, Spark 3.0 will be able to read it when enabling Hadoop 3.2 profile since we upgrade Hive executive JAR from 1.2.1 too 2.3.4. See the PR https://github.com/apache/spark/pull/24391;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update availableSlots by availableCpus for barrier taskset,SPARK-26431,13206181,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,Ngone51,Ngone51,23/Dec/18 08:36,27/Sep/19 18:39,13/Jul/23 08:48,27/Sep/19 18:36,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,23/Dec/18 00:00,0,,,,,"availableCpus decrease as  tasks allocated, so, we should update availableSlots by availableCpus for barrier taskset to avoid unnecessary resourceOffer process.",,jiangxb1987,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Sep 27 18:36:17 UTC 2019,,,,,,,,,,"0|u009gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/19 18:36;jiangxb1987;Resolved by https://github.com/apache/spark/pull/25946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache ORC to 1.5.4,SPARK-26427,13206032,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,21/Dec/18 17:58,27/Jan/19 01:21,13/Jul/23 08:48,22/Dec/18 08:42,3.0.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Build,,,,0,,,,,"This issue aims to update Apache ORC dependency to the latest version 1.5.4 released at Dec. 20. ([Release Notes|https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318320&version=12344187])
{code}
[ORC-237] - OrcFile.mergeFiles Specified block size is less than configured minimum value
[ORC-409] - Changes for extending MemoryManagerImpl
[ORC-410] - Fix a locale-dependent test in TestCsvReader
[ORC-416] - Avoid opening data reader when there is no stripe
[ORC-417] - Use dynamic Apache Maven mirror link
[ORC-419] - Ensure to call `close` at RecordReaderImpl constructor exception
[ORC-432] - openjdk 8 has a bug that prevents surefire from working
[ORC-435] - Ability to read stripes that are greater than 2GB
[ORC-437] - Make acid schema checks case insensitive
[ORC-411] - Update build to work with Java 10.
[ORC-418] - Fix broken docker build script
{code}",,cloud_fan,dongjoon,githubbot,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,SPARK-23458,,,SPARK-24417,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 23 02:30:50 UTC 2019,,,,,,,,,,"0|u008sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/18 18:01;githubbot;dongjoon-hyun opened a new pull request #23364: [SPARK-26427][BUILD] Upgrade Apache ORC to 1.5.4
URL: https://github.com/apache/spark/pull/23364
 
 
   ## What changes were proposed in this pull request?
   
   This PR aims to update Apache ORC dependency to the latest version 1.5.4 released at Dec. 20. ([Release Notes](https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318320&version=12344187]))
   ```
   [ORC-237] - OrcFile.mergeFiles Specified block size is less than configured minimum value
   [ORC-409] - Changes for extending MemoryManagerImpl
   [ORC-410] - Fix a locale-dependent test in TestCsvReader
   [ORC-416] - Avoid opening data reader when there is no stripe
   [ORC-417] - Use dynamic Apache Maven mirror link
   [ORC-419] - Ensure to call `close` at RecordReaderImpl constructor exception
   [ORC-432] - openjdk 8 has a bug that prevents surefire from working
   [ORC-435] - Ability to read stripes that are greater than 2GB
   [ORC-437] - Make acid schema checks case insensitive
   [ORC-411] - Update build to work with Java 10.
   [ORC-418] - Fix broken docker build script
   ```
   
   ## How was this patch tested?
   
   Build and pass Jenkins.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Dec/18 08:42;dongjoon;This is resolved via https://github.com/apache/spark/pull/23364;;;","22/Dec/18 08:44;githubbot;asfgit closed pull request #23364: [SPARK-26427][BUILD] Upgrade Apache ORC to 1.5.4
URL: https://github.com/apache/spark/pull/23364
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/dev/deps/spark-deps-hadoop-2.7 b/dev/deps/spark-deps-hadoop-2.7
index 71423af0789c6..1af29fcaff2aa 100644
--- a/dev/deps/spark-deps-hadoop-2.7
+++ b/dev/deps/spark-deps-hadoop-2.7
@@ -155,9 +155,9 @@ objenesis-2.5.1.jar
 okhttp-3.8.1.jar
 okio-1.13.0.jar
 opencsv-2.3.jar
-orc-core-1.5.3-nohive.jar
-orc-mapreduce-1.5.3-nohive.jar
-orc-shims-1.5.3.jar
+orc-core-1.5.4-nohive.jar
+orc-mapreduce-1.5.4-nohive.jar
+orc-shims-1.5.4.jar
 oro-2.0.8.jar
 osgi-resource-locator-1.0.1.jar
 paranamer-2.8.jar
diff --git a/dev/deps/spark-deps-hadoop-3.1 b/dev/deps/spark-deps-hadoop-3.1
index 93eafef045330..05f180b17a588 100644
--- a/dev/deps/spark-deps-hadoop-3.1
+++ b/dev/deps/spark-deps-hadoop-3.1
@@ -172,9 +172,9 @@ okhttp-2.7.5.jar
 okhttp-3.8.1.jar
 okio-1.13.0.jar
 opencsv-2.3.jar
-orc-core-1.5.3-nohive.jar
-orc-mapreduce-1.5.3-nohive.jar
-orc-shims-1.5.3.jar
+orc-core-1.5.4-nohive.jar
+orc-mapreduce-1.5.4-nohive.jar
+orc-shims-1.5.4.jar
 oro-2.0.8.jar
 osgi-resource-locator-1.0.1.jar
 paranamer-2.8.jar
diff --git a/pom.xml b/pom.xml
index 310d7de955125..de9421419edc2 100644
--- a/pom.xml
+++ b/pom.xml
@@ -132,7 +132,7 @@
     <kafka.version>2.1.0</kafka.version>
     <derby.version>10.12.1.1</derby.version>
     <parquet.version>1.10.0</parquet.version>
-    <orc.version>1.5.3</orc.version>
+    <orc.version>1.5.4</orc.version>
     <orc.classifier>nohive</orc.classifier>
     <hive.parquet.version>1.6.0</hive.parquet.version>
     <jetty.version>9.4.12.v20180830</jetty.version>
@@ -1740,6 +1740,10 @@
         <classifier>${orc.classifier}</classifier>
         <scope>${orc.deps.scope}</scope>
         <exclusions>
+          <exclusion>
+            <groupId>javax.xml.bind</groupId>
+            <artifactId>jaxb-api</artifactId>
+          </exclusion>
           <exclusion>
             <groupId>org.apache.hadoop</groupId>
             <artifactId>hadoop-common</artifactId>


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Jan/19 22:36;dongjoon;Hi, [~smilegator] and [~cloud_fan].
In general, we don't change the dependency at the maintenance release.
But, for this case, I'm wondering if we can have this to reduce the resource leakage in `branch-2.4` for next Spark 2.4.1.
Can I proceed to make a backport PR?;;;","23/Jan/19 02:12;cloud_fan;does it include other transitive dependences upgrade?;;;","23/Jan/19 02:30;dongjoon;It's only ORC dependency changes.
{code}
-orc-core-1.5.2-nohive.jar
-orc-mapreduce-1.5.2-nohive.jar
-orc-shims-1.5.2.jar
+orc-core-1.5.4-nohive.jar
+orc-mapreduce-1.5.4-nohive.jar
+orc-shims-1.5.4.jar
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpressionInfo related unit tests fail in Windows,SPARK-26426,13206011,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanlin-Lynn,yanlin-Lynn,yanlin-Lynn,21/Dec/18 16:27,12/Dec/22 18:10,13/Jul/23 08:48,25/Dec/18 07:54,2.4.0,3.0.0,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,"Using Windows 7 OS, after run""mvn install"" for latest spark version, and then  run  ""mvn test -Dtest=none -DwildcardSuites=org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite -pl :spark-sql_2.12"", it fails with Assertion error as follows

ForeachBatchSinkSuite:
- foreachBatch with non-stateful query *** FAILED ***
  java.lang.AssertionError:
  at org.apache.spark.sql.catalyst.expressions.ExpressionInfo.<init>(ExpressionInfo.java:82)
  at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expressionInfo(FunctionRegistry.scala:636)
  at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression(FunctionRegistry.scala:595)
  at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>(FunctionRegistry.scala:193)
  at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>(FunctionRegistry.scala)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$functionRegistry$2(BaseSessionStateBuilder.scala:99)
  at scala.Option.getOrElse(Option.scala:138)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute(BaseSessionStateBuilder.scala:99)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry(BaseSessionStateBuilder.scala:97)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:290)
  ...
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.analysis.FunctionRegistry$
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$functionRegistry$2(BaseSessionStateBuilder.scala:99)
  at scala.Option.getOrElse(Option.scala:138)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute(BaseSessionStateBuilder.scala:99)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry(BaseSessionStateBuilder.scala:97)
  at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:290)
  at org.apache.spark.sql.test.TestSparkSession.sessionState$lzycompute(TestSQLContext.scala:42)
  at org.apache.spark.sql.test.TestSparkSession.sessionState(TestSQLContext.scala:41)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:77)
  at org.apache.spark.sql.execution.streaming.MemoryStreamBase.toDF(memory.scala:60)
  at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite.$anonfun$new$5(ForeachBatchSinkSuite.scala:48)
","Windows 7 Operation System

===== maven version info ======
Apache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-25T02:41:47+08:00)
Maven home: D:\apache-maven-3.6.0
Java version: 1.8.0_121, vendor: Oracle Corporation, runtime: D:\java\jdk1.8.0_121\jre
Default locale: zh_CN, platform encoding: GBK
OS name: ""windows 7"", version: ""6.1"", arch: ""amd64"", family: ""windows""

===== java version info =====
java version ""1.8.0_121""
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)
",githubbot,yanlin-Lynn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/18 16:30;yanlin-Lynn;unit-test.log;https://issues.apache.org/jira/secure/attachment/12952703/unit-test.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 25 07:54:31 UTC 2018,,,,,,,,,,"0|u008ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/18 16:31;yanlin-Lynn;see  attached file ""unit-test.log"" for more details.;;;","21/Dec/18 16:41;githubbot;yanlin-Lynn opened a new pull request #23363: [SPARK-26426]fix ExpresionInfo assert error in windows operation system.
URL: https://github.com/apache/spark/pull/23363
 
 
   ## What changes were proposed in this pull request?
   fix ExpresionInfo assert error in windows operation system, when running unit tests.
   
   ## How was this patch tested?
   unit tests

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Dec/18 07:54;gurwls223;Issue resolved by pull request 23363
[https://github.com/apache/spark/pull/23363];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to disable Hive support in SparkR when Hadoop version is unsupported,SPARK-26422,13205759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,20/Dec/18 15:26,12/Dec/22 18:11,13/Jul/23 08:48,21/Dec/18 08:11,3.0.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SparkR,,,,0,,,,,"When we make a Spark session as below:

{code}
sparkSession <- sparkR.session(""local[4]"", ""SparkR"", Sys.getenv(""SPARK_HOME""),
                               list(spark.driver.extraClassPath = jarpaths,
                                    spark.executor.extraClassPath = jarpaths),
                               enableHiveSupport = FALSE)
{code}

I faced an issue that it's unable to disable Hive support explicitly with the error below:

{code}
java.lang.reflect.InvocationTargetException
...
Caused by: java.lang.IllegalArgumentException: Unrecognized Hadoop major version number: 3.1.1.3.1.0.0-78
	at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:174)
	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:139)
	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:100)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:368)
	... 43 more
Error in handleErrors(returnStatus, conn) :
  java.lang.ExceptionInInitializerError
	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:105)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:193)
	at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:1116)
	at org.apache.spark.sql.api.r.SQLUtils$.getOrCreateSparkSession(SQLUtils.scala:52)
	at org.apache.spark.sql.api.r.SQLUtils.getOrCreateSparkSession(SQLUtils.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:167)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:108)
...
{code} ",,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 21 08:13:37 UTC 2018,,,,,,,,,,"0|u0073s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/18 15:38;githubbot;HyukjinKwon opened a new pull request #23356: [SPARK-26422][R] Support to disable Hive support in SparkR even for Hadoop versions unsupported by Hive fork
URL: https://github.com/apache/spark/pull/23356
 
 
   ## What changes were proposed in this pull request?
   
   Currently,  even if I explicitly disable Hive support in SparkR session as below:
   
   ```r
   sparkSession <- sparkR.session(""local[4]"", ""SparkR"", Sys.getenv(""SPARK_HOME""),
                                  enableHiveSupport = FALSE)
   ```
   
   produces when the Hadoop version is not supported by our Hive fork:
   
   ```
   java.lang.reflect.InvocationTargetException
   ...
   Caused by: java.lang.IllegalArgumentException: Unrecognized Hadoop major version number: 3.1.1.3.1.0.0-78
   	at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:174)
   	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:139)
   	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:100)
   	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:368)
   	... 43 more
   Error in handleErrors(returnStatus, conn) :
     java.lang.ExceptionInInitializerError
   	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:105)
   	at java.lang.Class.forName0(Native Method)
   	at java.lang.Class.forName(Class.java:348)
   	at org.apache.spark.util.Utils$.classForName(Utils.scala:193)
   	at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:1116)
   	at org.apache.spark.sql.api.r.SQLUtils$.getOrCreateSparkSession(SQLUtils.scala:52)
   	at org.apache.spark.sql.api.r.SQLUtils.getOrCreateSparkSession(SQLUtils.scala)
   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   ```
   
   The root cause is that:
   
   ```
   SparkSession.hiveClassesArePresent
   ```
   
   check if the class is loadable or not to check if that's in classpath but `org.apache.hadoop.hive.conf.HiveConf` has a check for Hadoop version as static logic which is executed right away. This throws an `IllegalArgumentException` and that's not caught:
   
   https://github.com/apache/spark/blob/36edbac1c8337a4719f90e4abd58d38738b2e1fb/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L1113-L1121
   
   So, currently, if users have a Hive built-in Spark with unsupported Hadoop version by our fork (namely 3+), there's no way to use SparkR even thought it could work.
   
   This PR just propose to change the order of bool comparison so that we can don't execute `SparkSession.hiveClassesArePresent` when:
   
     1. `enableHiveSupport` is explicitly disabled
     2. `spark.sql.catalogImplementation` is `in-memory`
   
   so that we **only** check `SparkSession.hiveClassesArePresent` when Hive support is explicitly enabled by short short circuiting.
   
   ## How was this patch tested?
   
   It's difficult to write a test since we don't run tests against Hadoop 3 yet. See https://github.com/apache/spark/pull/21588. Manually tested.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Dec/18 08:11;gurwls223;Issue resolved by pull request 23356
[https://github.com/apache/spark/pull/23356];;;","21/Dec/18 08:13;githubbot;asfgit closed pull request #23356: [SPARK-26422][R] Support to disable Hive support in SparkR even for Hadoop versions unsupported by Hive fork
URL: https://github.com/apache/spark/pull/23356
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala b/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala
index becb05cf72aba..e98cab8b56d13 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala
@@ -49,9 +49,17 @@ private[sql] object SQLUtils extends Logging {
       sparkConfigMap: JMap[Object, Object],
       enableHiveSupport: Boolean): SparkSession = {
     val spark =
-      if (SparkSession.hiveClassesArePresent && enableHiveSupport &&
+      if (enableHiveSupport &&
           jsc.sc.conf.get(CATALOG_IMPLEMENTATION.key, ""hive"").toLowerCase(Locale.ROOT) ==
-            ""hive"") {
+            ""hive"" &&
+          // Note that the order of conditions here are on purpose.
+          // `SparkSession.hiveClassesArePresent` checks if Hive's `HiveConf` is loadable or not;
+          // however, `HiveConf` itself has some static logic to check if Hadoop version is
+          // supported or not, which throws an `IllegalArgumentException` if unsupported.
+          // If this is checked first, there's no way to disable Hive support in the case above.
+          // So, we intentionally check if Hive classes are loadable or not only when
+          // Hive support is explicitly enabled by short-circuiting. See also SPARK-26422.
+          SparkSession.hiveClassesArePresent) {
         SparkSession.builder().sparkContext(withHiveExternalCatalog(jsc.sc)).getOrCreate()
       } else {
         if (enableHiveSupport) {


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race between SparkContext and YARN AM can cause NPE in UI setup code,SPARK-26414,13205474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,19/Dec/18 19:12,17/May/20 18:14,13/Jul/23 08:48,09/Jan/19 19:37,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,YARN,,,0,,,,,"There's a super narrow race between the SparkContext and the AM startup code:

- SC starts the AM and waits for it to go into running state
- AM goes into running state, unblocking SC
- AM sends AmIpFilter config to SC, adds the filter to the list and then the filter configs
- unblocked SC is in the middle of setting up the UI and sees only the filter, but not the configs

Then you get this:

{noformat}
ERROR org.apache.spark.SparkContext  - Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.init(AmIpFilter.java:81)
	at org.spark_project.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)
	at org.spark_project.jetty.servlet.ServletHandler.initialize(ServletHandler.java:881)
	at org.spark_project.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349)
	at org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778)
	at org.spark_project.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:520)
	at org.apache.spark.ui.WebUI$$anonfun$attachHandler$1.apply(WebUI.scala:96)
	at org.apache.spark.ui.WebUI$$anonfun$attachHandler$1.apply(WebUI.scala:96)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.ui.WebUI.attachHandler(WebUI.scala:96)
	at org.apache.spark.SparkContext$$anonfun$22$$anonfun$apply$8.apply(SparkContext.scala:522)
	at org.apache.spark.SparkContext$$anonfun$22$$anonfun$apply$8.apply(SparkContext.scala:522)
	at scala.Option.foreach(Option.scala:257)
{noformat}
",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 19:37:01 UTC 2019,,,,,,,,,,"0|u005co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/19 19:37;vanzin;I actually ended up fixing this in the change for SPARK-24522, since it required adding some thread-safety to the code in question here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DataFrame pivot using array column fails with ""Unsupported literal type class""",SPARK-26403,13205285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,huonw,huonw,19/Dec/18 05:39,12/Dec/22 18:10,13/Jul/23 08:48,03/Jan/19 03:02,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Doing a pivot (using the {{pivot(pivotColumn: Column)}} overload) on a column containing arrays results in a runtime error:

{code:none}
scala> val df = Seq((1, Seq(""a"", ""x""), 2), (1, Seq(""b""), 3), (2, Seq(""a"", ""x""), 10), (3, Seq(), 100)).toDF(""x"", ""s"", ""y"")
df: org.apache.spark.sql.DataFrame = [x: int, s: array<string> ... 1 more field]

scala> df.show
+---+------+---+
|  x|     s|  y|
+---+------+---+
|  1|[a, x]|  2|
|  1|   [b]|  3|
|  2|[a, x]| 10|
|  3|    []|100|
+---+------+---+


scala> df.groupBy(""x"").pivot(""s"").agg(collect_list($""y"")).show
java.lang.RuntimeException: Unsupported literal type class scala.collection.mutable.WrappedArray$ofRef WrappedArray()
  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:78)
  at org.apache.spark.sql.RelationalGroupedDataset$$anonfun$pivot$1.apply(RelationalGroupedDataset.scala:419)
  at org.apache.spark.sql.RelationalGroupedDataset$$anonfun$pivot$1.apply(RelationalGroupedDataset.scala:419)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:419)
  at org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:397)
  at org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:317)
  ... 49 elided
{code}

However, this doesn't seem to be a fundamental limitation with {{pivot}}, as it works fine using the {{pivot(pivotColumn: Column, values: Seq[Any])}} overload, as long as the arrays are mapped to the {{Array}} type:

{code:none}
scala> val rawValues = df.select(""s"").distinct.sort(""s"").collect
rawValues: Array[org.apache.spark.sql.Row] = Array([WrappedArray()], [WrappedArray(a, x)], [WrappedArray(b)])

scala> val values = rawValues.map(_.getSeq[String](0).to[Array])
values: Array[Array[String]] = Array(Array(), Array(a, x), Array(b))

scala> df.groupBy(""x"").pivot(""s"", values).agg(collect_list($""y"")).show
+---+-----+------+---+
|  x|   []|[a, x]|[b]|
+---+-----+------+---+
|  1|   []|   [2]|[3]|
|  3|[100]|    []| []|
|  2|   []|  [10]| []|
+---+-----+------+---+
{code}

It would be nice if {{pivot}} was more resilient to Spark's own representation of array columns, and so the first version worked.",,githubbot,huonw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 03:02:58 UTC 2019,,,,,,,,,,"0|u0046o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/18 09:43;githubbot;HyukjinKwon opened a new pull request #23349: [SPARK-26403][SQL] Support pivoting using array column for `pivot(column)` API
URL: https://github.com/apache/spark/pull/23349
 
 
   ## What changes were proposed in this pull request?
   
   This PR fixes `Literal(..: Any)` can accepts `collection.mutable.WrappedArray` in order to `pivot(Column)` can accepts array column as well.
   
   We can unwrap the array and use it for type dispatch.
   
   ```scala
   val df = Seq(
     (2, Seq.empty[String]),
     (2, Seq(""a"", ""x"")),
     (3, Seq.empty[String]),
     (3, Seq(""a"", ""x""))).toDF(""x"", ""s"")
   df.groupBy(""x"").pivot(""s"").count().show()
   ```
   
   Before:
   
   ```
   Unsupported literal type class scala.collection.mutable.WrappedArray$ofRef WrappedArray()
   java.lang.RuntimeException: Unsupported literal type class scala.collection.mutable.WrappedArray$ofRef WrappedArray()
   	at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:80)
   	at org.apache.spark.sql.RelationalGroupedDataset.$anonfun$pivot$2(RelationalGroupedDataset.scala:427)
   	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
   	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
   	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
   	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:39)
   	at scala.collection.TraversableLike.map(TraversableLike.scala:237)
   	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
   	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
   	at org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:425)
   	at org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:406)
   	at org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:317)
   	at org.apache.spark.sql.DataFramePivotSuite.$anonfun$new$1(DataFramePivotSuite.scala:341)
   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
   ```
   
   After:
   
   ```
   +---+---+------+
   |  x| []|[a, x]|
   +---+---+------+
   |  3|  1|     1|
   |  2|  1|     1|
   +---+---+------+
   ```
   
   ## How was this patch tested?
   
   Manually tested and unittests were added.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Jan/19 03:02;gurwls223;Issue resolved by pull request 23349
[https://github.com/apache/spark/pull/23349];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotation error for Utils.timeStringAsMs,SPARK-26394,13205115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,Jackey Lee,Jackey Lee,Jackey Lee,18/Dec/18 12:05,18/Dec/18 18:18,13/Jul/23 08:48,18/Dec/18 18:17,2.4.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Documentation,Spark Core,,,0,,,,,"Utils.timeStringAsMs() is parsing time to milliseconds, but in annotation, it says ""Convert a time parameter such as (50s, 100ms, or 250us) to microseconds for internal use.""

Thus, microseconds should be changed to milliseconds.",,githubbot,Jackey Lee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 18 18:18:59 UTC 2018,,,,,,,,,,"0|u0034w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/18 13:00;githubbot;stczwd opened a new pull request #23346: [SPARK-26394][core] Fix annotation error for Utils.timeStringAsMs
URL: https://github.com/apache/spark/pull/23346
 
 
   ## What changes were proposed in this pull request?
   
   Change microseconds to milliseconds in annotation of Utils.timeStringAsMs.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Dec/18 18:15;srowen;This doesn't need a JIRA if the 'what' and 'how' are the same;;;","18/Dec/18 18:17;srowen;Issue resolved by pull request 23346
[https://github.com/apache/spark/pull/23346];;;","18/Dec/18 18:18;githubbot;srowen closed pull request #23346: [SPARK-26394][core] Fix annotation error for Utils.timeStringAsMs
URL: https://github.com/apache/spark/pull/23346
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/scala/org/apache/spark/util/Utils.scala b/core/src/main/scala/org/apache/spark/util/Utils.scala
index c8b148be84536..8f86b472b9373 100644
--- a/core/src/main/scala/org/apache/spark/util/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala
@@ -1085,7 +1085,7 @@ private[spark] object Utils extends Logging {
   }
 
   /**
-   * Convert a time parameter such as (50s, 100ms, or 250us) to microseconds for internal use. If
+   * Convert a time parameter such as (50s, 100ms, or 250us) to milliseconds for internal use. If
    * no suffix is provided, the passed number is assumed to be in ms.
    */
   def timeStringAsMs(str: String): Long = {


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
prefix sorter should handle -0.0,SPARK-26382,13204839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Dec/18 08:46,18/Dec/18 18:13,13/Jul/23 08:48,18/Dec/18 18:11,3.0.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,,,cloud_fan,dongjoon,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 18 18:13:55 UTC 2018,,,,,,,,,,"0|u001g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/18 08:47;githubbot;cloud-fan opened a new pull request #23334: [SPARK-26382][] prefix sorter should handle -0.0
URL: https://github.com/apache/spark/pull/23334
 
 
   ## What changes were proposed in this pull request?
   
   (Please fill in changes proposed in this fix)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review http://spark.apache.org/contributing.html before opening a pull request.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Dec/18 18:11;dongjoon;This is resolved via https://github.com/apache/spark/pull/23334 ;;;","18/Dec/18 18:13;githubbot;asfgit closed pull request #23334: [SPARK-26382][CORE] prefix comparator should handle -0.0
URL: https://github.com/apache/spark/pull/23334
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java
index 0910db22af004..bef1bdadb27aa 100644
--- a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java
@@ -69,6 +69,8 @@ public static long computePrefix(byte[] bytes) {
      * details see http://stereopsis.com/radix.html.
      */
     public static long computePrefix(double value) {
+      // normalize -0.0 to 0.0, as they should be equal
+      value = value == -0.0 ? 0.0 : value;
       // Java's doubleToLongBits already canonicalizes all NaN values to the smallest possible
       // positive NaN, so there's nothing special we need to do for NaNs.
       long bits = Double.doubleToLongBits(value);
diff --git a/core/src/test/scala/org/apache/spark/util/collection/unsafe/sort/PrefixComparatorsSuite.scala b/core/src/test/scala/org/apache/spark/util/collection/unsafe/sort/PrefixComparatorsSuite.scala
index 73546ef1b7a60..38cb37c524594 100644
--- a/core/src/test/scala/org/apache/spark/util/collection/unsafe/sort/PrefixComparatorsSuite.scala
+++ b/core/src/test/scala/org/apache/spark/util/collection/unsafe/sort/PrefixComparatorsSuite.scala
@@ -125,6 +125,7 @@ class PrefixComparatorsSuite extends SparkFunSuite with PropertyChecks {
     val nan2Prefix = PrefixComparators.DoublePrefixComparator.computePrefix(nan2)
     assert(nan1Prefix === nan2Prefix)
     val doubleMaxPrefix = PrefixComparators.DoublePrefixComparator.computePrefix(Double.MaxValue)
+    // NaN is greater than the max double value.
     assert(PrefixComparators.DOUBLE.compare(nan1Prefix, doubleMaxPrefix) === 1)
   }
 
@@ -134,22 +135,34 @@ class PrefixComparatorsSuite extends SparkFunSuite with PropertyChecks {
     assert(java.lang.Double.doubleToRawLongBits(negativeNan) < 0)
     val prefix = PrefixComparators.DoublePrefixComparator.computePrefix(negativeNan)
     val doubleMaxPrefix = PrefixComparators.DoublePrefixComparator.computePrefix(Double.MaxValue)
+    // -NaN is greater than the max double value.
     assert(PrefixComparators.DOUBLE.compare(prefix, doubleMaxPrefix) === 1)
   }
 
   test(""double prefix comparator handles other special values properly"") {
-    val nullValue = 0L
+    // See `SortPrefix.nullValue` for how we deal with nulls for float/double type
+    val smallestNullPrefix = 0L
+    val largestNullPrefix = -1L
     val nan = PrefixComparators.DoublePrefixComparator.computePrefix(Double.NaN)
     val posInf = PrefixComparators.DoublePrefixComparator.computePrefix(Double.PositiveInfinity)
     val negInf = PrefixComparators.DoublePrefixComparator.computePrefix(Double.NegativeInfinity)
     val minValue = PrefixComparators.DoublePrefixComparator.computePrefix(Double.MinValue)
     val maxValue = PrefixComparators.DoublePrefixComparator.computePrefix(Double.MaxValue)
     val zero = PrefixComparators.DoublePrefixComparator.computePrefix(0.0)
+    val minusZero = PrefixComparators.DoublePrefixComparator.computePrefix(-0.0)
+
+    // null is greater than everything including NaN, when we need to treat it as the largest value.
+    assert(PrefixComparators.DOUBLE.compare(largestNullPrefix, nan) === 1)
+    // NaN is greater than the positive infinity.
     assert(PrefixComparators.DOUBLE.compare(nan, posInf) === 1)
     assert(PrefixComparators.DOUBLE.compare(posInf, maxValue) === 1)
     assert(PrefixComparators.DOUBLE.compare(maxValue, zero) === 1)
     assert(PrefixComparators.DOUBLE.compare(zero, minValue) === 1)
     assert(PrefixComparators.DOUBLE.compare(minValue, negInf) === 1)
-    assert(PrefixComparators.DOUBLE.compare(negInf, nullValue) === 1)
+    // null is smaller than everything including negative infinity, when we need to treat it as
+    // the smallest value.
+    assert(PrefixComparators.DOUBLE.compare(negInf, smallestNullPrefix) === 1)
+    // 0.0 should be equal to -0.0.
+    assert(PrefixComparators.DOUBLE.compare(zero, minusZero) === 0)
   }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use dummy TimeZoneId for CurrentTimestamp to avoid UnresolvedException in CurrentBatchTimestamp,SPARK-26379,13204739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kailashgupta1012,kailashgupta1012,16/Dec/18 16:36,29/Jan/19 01:04,13/Jul/23 08:48,27/Jan/19 21:47,2.3.0,2.3.1,2.3.2,2.4.0,3.0.0,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Structured Streaming,,,,0,,,,,"While using withColumn to add a column to a structured streaming Dataset, I am getting following exception: 

org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'timestamp

Following is sample code
{code:java}
final String path = ""path_to_input_directory"";

final StructType schema = new StructType(new StructField[] { new StructField(""word"", StringType, false, Metadata.empty()), new StructField(""count"", DataTypes.IntegerType, false, Metadata.empty()) });

SparkSession sparkSession = SparkSession.builder().appName(""StructuredStreamingIssue"").master(""local"").getOrCreate();
Dataset<Row> words = sparkSession.readStream().option(""sep"", "","").schema(schema).csv(path);

Dataset<Row> wordsWithTimestamp = words.withColumn(""timestamp"", functions.current_timestamp());

// wordsWithTimestamp.explain(true);

StreamingQuery query = wordsWithTimestamp.writeStream().outputMode(""update"").option(""truncate"", ""false"").format(""console"").trigger(Trigger.ProcessingTime(""2 seconds"")).start();

query.awaitTermination();{code}
Following are the contents of the file present at _path_
{code:java}
a,2
c,4
d,2
r,1
t,9
{code}
This seems working with 2.2.0 release, but not with 2.3.0 and 2.4.0",,dongjoon,iamhumanbeing,kabhwan,kailashgupta1012,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 25 23:03:48 UTC 2019,,,,,,,,,,"0|u000ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/18 16:43;kailashgupta1012;Full exception stack trace
{code:java}
18/12/16 22:09:32 ERROR MicroBatchExecution: Query [id = ca9aaed4-e750-453a-88f9-2807365efcf2, runId = e346ac1d-3873-4922-a327-d539dfdc44bc] terminated with error
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'timestamp
at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:105)
at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:435)
at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:435)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:381)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.immutable.List.map(List.scala:285)
at org.apache.spark.sql.types.StructType$.fromAttributes(StructType.scala:435)
at org.apache.spark.sql.catalyst.plans.QueryPlan.schema$lzycompute(QueryPlan.scala:157)
at org.apache.spark.sql.catalyst.plans.QueryPlan.schema(QueryPlan.scala:157)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:447)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:133)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:117)
at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
Exception in thread ""main"" org.apache.spark.sql.streaming.StreamingQueryException: Invalid call to dataType on unresolved object, tree: 'timestamp
=== Streaming Query ===
Identifier: [id = ca9aaed4-e750-453a-88f9-2807365efcf2, runId = e346ac1d-3873-4922-a327-d539dfdc44bc]
Current Committed Offsets: {}
Current Available Offsets: {FileStreamSource[file:/input]: {""logOffset"":0}}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
Project [word#0, count#1, current_timestamp() AS timestamp#4]
+- StreamingExecutionRelation FileStreamSource[file:/input], [word#0, count#1]

at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295)
at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
Caused by: org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'timestamp
at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:105)
at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:435)
at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:435)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:381)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.immutable.List.map(List.scala:285)
at org.apache.spark.sql.types.StructType$.fromAttributes(StructType.scala:435)
at org.apache.spark.sql.catalyst.plans.QueryPlan.schema$lzycompute(QueryPlan.scala:157)
at org.apache.spark.sql.catalyst.plans.QueryPlan.schema(QueryPlan.scala:157)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:447)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:133)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:117)
at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
... 1 more
18/12/16 22:09:32 INFO SparkContext: Invoking stop() from shutdown hook{code};;;","16/Dec/18 16:55;kailashgupta1012;Explain result
{code:java}
== Parsed Logical Plan ==
Project [word#0, count#1, current_timestamp() AS timestamp#4]
+- AnalysisBarrier
+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7c11d32,csv,List(),Some(StructType(StructField(word,StringType,false), StructField(count,IntegerType,false))),List(),None,Map(sep -> ,, path -> /input),None), FileSource[/input], [word#0, count#1]

== Analyzed Logical Plan ==
word: string, count: int, timestamp: timestamp
Project [word#0, count#1, current_timestamp() AS timestamp#4]
+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7c11d32,csv,List(),Some(StructType(StructField(word,StringType,false), StructField(count,IntegerType,false))),List(),None,Map(sep -> ,, path -> /input),None), FileSource[/input], [word#0, count#1]

== Optimized Logical Plan ==
Project [word#0, count#1, 1544979190083000 AS timestamp#4]
+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7c11d32,csv,List(),Some(StructType(StructField(word,StringType,false), StructField(count,IntegerType,false))),List(),None,Map(sep -> ,, path -> /input),None), FileSource[/input], [word#0, count#1]

== Physical Plan ==
*(1) Project [word#0, count#1, 1544979190083000 AS timestamp#4]
+- StreamingRelation FileSource[/input], [word#0, count#1]{code};;;","22/Jan/19 07:36;kabhwan;This looks like also occurred on the master branch. Simpler to reproduce.

{code}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession
import spark.implicits._
val lines = spark.readStream.format(""socket"").option(""host"", ""localhost"").option(""port"", 9999).load()
val final_df = lines.withColumn(""ts"",lit(current_timestamp())) 
val query = final_df.writeStream.format(""console"").start()
{code}

I'll quickly submit a patch soon.;;;","23/Jan/19 18:34;dongjoon;Thank you, [~kailashgupta1012] and [~kabhwan].;;;","25/Jan/19 23:03;dongjoon;This is resolved via https://github.com/apache/spark/pull/23609;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV parsing uses previous good value for bad input field,SPARK-26372,13204519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,14/Dec/18 16:38,12/Dec/22 18:10,13/Jul/23 08:48,16/Dec/18 03:03,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"For example:
{noformat}
bash-3.2$ cat test.csv 
""hello"",1999-08-01
""there"",""bad date""
""again"",""2017-11-22""
bash-3.2$ bin/spark-shell
..etc..
scala> import org.apache.spark.sql.types._
scala> import org.apache.spark.sql.SaveMode
scala> var schema = StructType(StructField(""col1"", StringType) ::
     |   StructField(""col2"", DateType) ::
     |   Nil)
schema: org.apache.spark.sql.types.StructType = StructType(StructField(col1,StringType,true), StructField(col2,DateType,true))
scala> val df = spark.read.schema(schema).csv(""test.csv"")
df: org.apache.spark.sql.DataFrame = [col1: string, col2: date]
scala> df.show
+-----+----------+                                                              
| col1|      col2|
+-----+----------+
|hello|1999-08-01|
|there|1999-08-01|
|again|2017-11-22|
+-----+----------+
scala> 
{noformat}
col2 from the second row contains ""1999-08-01"", when it should contain null.

This is because UnivocityParser reuses the same Row object for each input record. If there is an exception converting an input field, the code simply skips over that field, leaving the existing value in the Row object.

The simple fix is to set the column to null in the Row object whenever there is a badRecordException while converting the input field.",,bersprockets,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 16 03:05:33 UTC 2018,,,,,,,,,,"0|s01j8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/18 16:45;bersprockets;I can prep a PR, unless someone thinks this needs a different solution than the one I proposed.;;;","14/Dec/18 23:02;githubbot;bersprockets opened a new pull request #23323: [SPARK-26372][SQL] Don't reuse value from previous row when parsing bad CSV input field
URL: https://github.com/apache/spark/pull/23323
 
 
   ## What changes were proposed in this pull request?
   
   CSV parsing accidentally uses the previous good value for a bad input field. See example in Jira.
   
   This PR ensures that the associated column is set to null when an input field cannot be converted.
   
   ## How was this patch tested?
   
   Added new test.
   Ran all SQL unit tests (testOnly org.apache.spark.sql.*).
   Ran pyspark tests for pyspark-sql
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Dec/18 03:03;gurwls223;Issue resolved by pull request 23323
[https://github.com/apache/spark/pull/23323];;;","16/Dec/18 03:05;githubbot;asfgit closed pull request #23323: [SPARK-26372][SQL] Don't reuse value from previous row when parsing bad CSV input field
URL: https://github.com/apache/spark/pull/23323
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityParser.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityParser.scala
index 0f375e036029c..aafc9ebdcaa12 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityParser.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityParser.scala
@@ -239,6 +239,7 @@ class UnivocityParser(
         } catch {
           case NonFatal(e) =>
             badRecordException = badRecordException.orElse(Some(e))
+            row.setNullAt(i)
         }
         i += 1
       }
diff --git a/sql/core/src/test/resources/test-data/bad_after_good.csv b/sql/core/src/test/resources/test-data/bad_after_good.csv
new file mode 100644
index 0000000000000..4621a7d23714d
--- /dev/null
+++ b/sql/core/src/test/resources/test-data/bad_after_good.csv
@@ -0,0 +1,2 @@
+""good record"",1999-08-01
+""bad record"",1999-088-01
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala
index 3b977d74053e6..d9e5d7af19671 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala
@@ -63,6 +63,7 @@ class CSVSuite extends QueryTest with SharedSQLContext with SQLTestUtils with Te
   private val datesFile = ""test-data/dates.csv""
   private val unescapedQuotesFile = ""test-data/unescaped-quotes.csv""
   private val valueMalformedFile = ""test-data/value-malformed.csv""
+  private val badAfterGoodFile = ""test-data/bad_after_good.csv""
 
   /** Verifies data and schema. */
   private def verifyCars(
@@ -2012,4 +2013,22 @@ class CSVSuite extends QueryTest with SharedSQLContext with SQLTestUtils with Te
       assert(!files.exists(_.getName.endsWith(""csv"")))
     }
   }
+
+  test(""Do not reuse last good value for bad input field"") {
+    val schema = StructType(
+      StructField(""col1"", StringType) ::
+      StructField(""col2"", DateType) ::
+      Nil
+    )
+    val rows = spark.read
+      .schema(schema)
+      .format(""csv"")
+      .load(testFile(badAfterGoodFile))
+
+    val expectedRows = Seq(
+      Row(""good record"", java.sql.Date.valueOf(""1999-08-01"")),
+      Row(""bad record"", null))
+
+    checkAnswer(rows, expectedRows)
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix resolution of higher-order function for the same identifier.,SPARK-26370,13204417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,14/Dec/18 07:34,14/Dec/18 16:30,13/Jul/23 08:48,14/Dec/18 16:30,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,"When using a higher-order function with the same variable name as the existing columns in {{Filter}} or something which uses {{Analyzer.resolveExpressionBottomUp}} during the resolution, e.g.,:
{code}
val df = Seq(
  (Seq(1, 9, 8, 7), 1, 2),
  (Seq(5, 9, 7), 2, 2),
  (Seq.empty, 3, 2),
  (null, 4, 2)
).toDF(""i"", ""x"", ""d"")

checkAnswer(df.filter(""exists(i, x -> x % d == 0)""),
  Seq(Row(Seq(1, 9, 8, 7), 1, 2)))
checkAnswer(df.select(""x"").filter(""exists(i, x -> x % d == 0)""),
  Seq(Row(1)))
{code}
the following exception happens:
{code:java}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.BoundReference cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:237)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.$anonfun$functionsForEval$1(higherOrderFunctions.scala:147)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:237)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval(higherOrderFunctions.scala:145)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval$(higherOrderFunctions.scala:145)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval$lzycompute(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval(higherOrderFunctions.scala:176)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval$(higherOrderFunctions.scala:176)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionForEval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.nullSafeEval(higherOrderFunctions.scala:387)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:190)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:185)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.eval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
  at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:216)
  at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:215)

...
{code}
because the {{UnresolvedAttribute}} s in {{LambdaFunction}} are unexpectedly resolved by the rule.",,cloud_fan,githubbot,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 14 16:30:18 UTC 2018,,,,,,,,,,"0|s01im8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/18 07:44;githubbot;ueshin opened a new pull request #23320: [SPARK-26370][SQL] Fix resolution of higher-order function for the same identifier.
URL: https://github.com/apache/spark/pull/23320
 
 
   ## What changes were proposed in this pull request?
   
   When using a higher-order function with the same variable name as the existing columns in `Filter` or something which uses `Analyzer.resolveExpressionBottomUp` during the resolution, e.g.,:
   
   ```scala
   val df = Seq(
     (Seq(1, 9, 8, 7), 1, 2),
     (Seq(5, 9, 7), 2, 2),
     (Seq.empty, 3, 2),
     (null, 4, 2)
   ).toDF(""i"", ""x"", ""d"")
   
   checkAnswer(df.filter(""exists(i, x -> x % d == 0)""),
     Seq(Row(Seq(1, 9, 8, 7), 1, 2)))
   checkAnswer(df.select(""x"").filter(""exists(i, x -> x % d == 0)""),
     Seq(Row(1)))
   ```
   
   the following exception happens:
   
   ```
   java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.BoundReference cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression
     at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
     at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
     at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
     at scala.collection.TraversableLike.map(TraversableLike.scala:237)
     at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
     at scala.collection.AbstractTraversable.map(Traversable.scala:108)
     at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.$anonfun$functionsForEval$1(higherOrderFunctions.scala:147)
     at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
     at scala.collection.immutable.List.foreach(List.scala:392)
     at scala.collection.TraversableLike.map(TraversableLike.scala:237)
     at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
     at scala.collection.immutable.List.map(List.scala:298)
     at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval(higherOrderFunctions.scala:145)
     at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval$(higherOrderFunctions.scala:145)
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval$lzycompute(higherOrderFunctions.scala:369)
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval(higherOrderFunctions.scala:369)
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval(higherOrderFunctions.scala:176)
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval$(higherOrderFunctions.scala:176)
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionForEval(higherOrderFunctions.scala:369)
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.nullSafeEval(higherOrderFunctions.scala:387)
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:190)
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:185)
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.eval(higherOrderFunctions.scala:369)
     at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
     at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:216)
     at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:215)
   
   ...
   ```
   
   because the `UnresolvedAttribute`s in `LambdaFunction` are unexpectedly resolved by the rule.
   
   This pr modified to use a placeholder `UnresolvedNamedLambdaVariable` to prevent unexpected resolution.
   
   ## How was this patch tested?
   
   Added a test and modified some tests.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Dec/18 16:27;githubbot;asfgit closed pull request #23320: [SPARK-26370][SQL] Fix resolution of higher-order function for the same identifier.
URL: https://github.com/apache/spark/pull/23320
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala
index a8a7bbd9f9cd0..1cd7f412bb678 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala
@@ -150,13 +150,14 @@ case class ResolveLambdaVariables(conf: SQLConf) extends Rule[LogicalPlan] {
       val lambdaMap = l.arguments.map(v => canonicalizer(v.name) -> v).toMap
       l.mapChildren(resolve(_, parentLambdaMap ++ lambdaMap))
 
-    case u @ UnresolvedAttribute(name +: nestedFields) =>
+    case u @ UnresolvedNamedLambdaVariable(name +: nestedFields) =>
       parentLambdaMap.get(canonicalizer(name)) match {
         case Some(lambda) =>
           nestedFields.foldLeft(lambda: Expression) { (expr, fieldName) =>
             ExtractValue(expr, Literal(fieldName), conf.resolver)
           }
-        case None => u
+        case None =>
+          UnresolvedAttribute(u.nameParts)
       }
 
     case _ =>
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala
index a8639d29f964d..7141b6e996389 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala
@@ -22,12 +22,34 @@ import java.util.concurrent.atomic.AtomicReference
 import scala.collection.mutable
 
 import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.catalyst.analysis.{TypeCheckResult, TypeCoercion, UnresolvedAttribute}
+import org.apache.spark.sql.catalyst.analysis.{TypeCheckResult, TypeCoercion, UnresolvedAttribute, UnresolvedException}
 import org.apache.spark.sql.catalyst.expressions.codegen._
 import org.apache.spark.sql.catalyst.util._
 import org.apache.spark.sql.types._
 import org.apache.spark.unsafe.array.ByteArrayMethods
 
+/**
+ * A placeholder of lambda variables to prevent unexpected resolution of [[LambdaFunction]].
+ */
+case class UnresolvedNamedLambdaVariable(nameParts: Seq[String])
+  extends LeafExpression with NamedExpression with Unevaluable {
+
+  override def name: String =
+    nameParts.map(n => if (n.contains(""."")) s""`$n`"" else n).mkString(""."")
+
+  override def exprId: ExprId = throw new UnresolvedException(this, ""exprId"")
+  override def dataType: DataType = throw new UnresolvedException(this, ""dataType"")
+  override def nullable: Boolean = throw new UnresolvedException(this, ""nullable"")
+  override def qualifier: Seq[String] = throw new UnresolvedException(this, ""qualifier"")
+  override def toAttribute: Attribute = throw new UnresolvedException(this, ""toAttribute"")
+  override def newInstance(): NamedExpression = throw new UnresolvedException(this, ""newInstance"")
+  override lazy val resolved = false
+
+  override def toString: String = s""lambda '$name""
+
+  override def sql: String = name
+}
+
 /**
  * A named lambda variable.
  */
@@ -79,7 +101,7 @@ case class LambdaFunction(
 
 object LambdaFunction {
   val identity: LambdaFunction = {
-    val id = UnresolvedAttribute.quoted(""id"")
+    val id = UnresolvedNamedLambdaVariable(Seq(""id""))
     LambdaFunction(id, Seq(id))
   }
 }
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala
index 672bffcfc0cad..8959f78b656d2 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala
@@ -1338,9 +1338,12 @@ class AstBuilder(conf: SQLConf) extends SqlBaseBaseVisitor[AnyRef] with Logging
    */
   override def visitLambda(ctx: LambdaContext): Expression = withOrigin(ctx) {
     val arguments = ctx.IDENTIFIER().asScala.map { name =>
-      UnresolvedAttribute.quoted(name.getText)
+      UnresolvedNamedLambdaVariable(UnresolvedAttribute.quoted(name.getText).nameParts)
     }
-    LambdaFunction(expression(ctx.expression), arguments)
+    val function = expression(ctx.expression).transformUp {
+      case a: UnresolvedAttribute => UnresolvedNamedLambdaVariable(a.nameParts)
+    }
+    LambdaFunction(function, arguments)
   }
 
   /**
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala
index c4171c75ecd03..a5847ba7c522d 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala
@@ -49,19 +49,21 @@ class ResolveLambdaVariablesSuite extends PlanTest {
     comparePlans(Analyzer.execute(plan(e1)), plan(e2))
   }
 
+  private def lv(s: Symbol) = UnresolvedNamedLambdaVariable(Seq(s.name))
+
   test(""resolution - no op"") {
     checkExpression(key, key)
   }
 
   test(""resolution - simple"") {
-    val in = ArrayTransform(values1, LambdaFunction('x.attr + 1, 'x.attr :: Nil))
+    val in = ArrayTransform(values1, LambdaFunction(lv('x) + 1, lv('x) :: Nil))
     val out = ArrayTransform(values1, LambdaFunction(lvInt + 1, lvInt :: Nil))
     checkExpression(in, out)
   }
 
   test(""resolution - nested"") {
     val in = ArrayTransform(values2, LambdaFunction(
-      ArrayTransform('x.attr, LambdaFunction('x.attr + 1, 'x.attr :: Nil)), 'x.attr :: Nil))
+      ArrayTransform(lv('x), LambdaFunction(lv('x) + 1, lv('x) :: Nil)), lv('x) :: Nil))
     val out = ArrayTransform(values2, LambdaFunction(
       ArrayTransform(lvArray, LambdaFunction(lvInt + 1, lvInt :: Nil)), lvArray :: Nil))
     checkExpression(in, out)
@@ -75,14 +77,14 @@ class ResolveLambdaVariablesSuite extends PlanTest {
 
   test(""fail - name collisions"") {
     val p = plan(ArrayTransform(values1,
-      LambdaFunction('x.attr + 'X.attr, 'x.attr :: 'X.attr :: Nil)))
+      LambdaFunction(lv('x) + lv('X), lv('x) :: lv('X) :: Nil)))
     val msg = intercept[AnalysisException](Analyzer.execute(p)).getMessage
     assert(msg.contains(""arguments should not have names that are semantically the same""))
   }
 
   test(""fail - lambda arguments"") {
     val p = plan(ArrayTransform(values1,
-      LambdaFunction('x.attr + 'y.attr + 'z.attr, 'x.attr :: 'y.attr :: 'z.attr :: Nil)))
+      LambdaFunction(lv('x) + lv('y) + lv('z), lv('x) :: lv('y) :: lv('z) :: Nil)))
     val msg = intercept[AnalysisException](Analyzer.execute(p)).getMessage
     assert(msg.contains(""does not match the number of arguments expected""))
   }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala
index ee0d04da3e46c..748075bfd6a68 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala
@@ -20,7 +20,7 @@ package org.apache.spark.sql.catalyst.optimizer
 import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
 import org.apache.spark.sql.catalyst.dsl.expressions._
 import org.apache.spark.sql.catalyst.dsl.plans._
-import org.apache.spark.sql.catalyst.expressions.{And, ArrayExists, ArrayFilter, ArrayTransform, CaseWhen, Expression, GreaterThan, If, LambdaFunction, Literal, MapFilter, NamedExpression, Or}
+import org.apache.spark.sql.catalyst.expressions.{And, ArrayExists, ArrayFilter, ArrayTransform, CaseWhen, Expression, GreaterThan, If, LambdaFunction, Literal, MapFilter, NamedExpression, Or, UnresolvedNamedLambdaVariable}
 import org.apache.spark.sql.catalyst.expressions.Literal.{FalseLiteral, TrueLiteral}
 import org.apache.spark.sql.catalyst.plans.{Inner, PlanTest}
 import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan}
@@ -306,22 +306,24 @@ class ReplaceNullWithFalseInPredicateSuite extends PlanTest {
     testProjection(originalExpr = column, expectedExpr = column)
   }
 
+  private def lv(s: Symbol) = UnresolvedNamedLambdaVariable(Seq(s.name))
+
   test(""replace nulls in lambda function of ArrayFilter"") {
-    testHigherOrderFunc('a, ArrayFilter, Seq('e))
+    testHigherOrderFunc('a, ArrayFilter, Seq(lv('e)))
   }
 
   test(""replace nulls in lambda function of ArrayExists"") {
-    testHigherOrderFunc('a, ArrayExists, Seq('e))
+    testHigherOrderFunc('a, ArrayExists, Seq(lv('e)))
   }
 
   test(""replace nulls in lambda function of MapFilter"") {
-    testHigherOrderFunc('m, MapFilter, Seq('k, 'v))
+    testHigherOrderFunc('m, MapFilter, Seq(lv('k), lv('v)))
   }
 
   test(""inability to replace nulls in arbitrary higher-order function"") {
     val lambdaFunc = LambdaFunction(
-      function = If('e > 0, Literal(null, BooleanType), TrueLiteral),
-      arguments = Seq[NamedExpression]('e))
+      function = If(lv('e) > 0, Literal(null, BooleanType), TrueLiteral),
+      arguments = Seq[NamedExpression](lv('e)))
     val column = ArrayTransform('a, lambdaFunc)
     testProjection(originalExpr = column, expectedExpr = column)
   }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
index b4df22c5b29fa..8bcc69d580d83 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
@@ -246,9 +246,11 @@ class ExpressionParserSuite extends PlanTest {
     intercept(""foo(a x)"", ""extraneous input 'x'"")
   }
 
+  private def lv(s: Symbol) = UnresolvedNamedLambdaVariable(Seq(s.name))
+
   test(""lambda functions"") {
-    assertEqual(""x -> x + 1"", LambdaFunction('x + 1, Seq('x.attr)))
-    assertEqual(""(x, y) -> x + y"", LambdaFunction('x + 'y, Seq('x.attr, 'y.attr)))
+    assertEqual(""x -> x + 1"", LambdaFunction(lv('x) + 1, Seq(lv('x))))
+    assertEqual(""(x, y) -> x + y"", LambdaFunction(lv('x) + lv('y), Seq(lv('x), lv('y))))
   }
 
   test(""window function expressions"") {
diff --git a/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out b/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out
index 35740094ba53e..86a578ca013df 100644
--- a/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out
@@ -85,7 +85,7 @@ FROM various_maps
 struct<>
 -- !query 5 output
 org.apache.spark.sql.AnalysisException
-cannot resolve 'map_zip_with(various_maps.`decimal_map1`, various_maps.`decimal_map2`, lambdafunction(named_struct(NamePlaceholder(), `k`, NamePlaceholder(), `v1`, NamePlaceholder(), `v2`), `k`, `v1`, `v2`))' due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are [decimal(36,0), decimal(36,35)].; line 1 pos 7
+cannot resolve 'map_zip_with(various_maps.`decimal_map1`, various_maps.`decimal_map2`, lambdafunction(named_struct(NamePlaceholder(), k, NamePlaceholder(), v1, NamePlaceholder(), v2), k, v1, v2))' due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are [decimal(36,0), decimal(36,35)].; line 1 pos 7
 
 
 -- !query 6
@@ -113,7 +113,7 @@ FROM various_maps
 struct<>
 -- !query 8 output
 org.apache.spark.sql.AnalysisException
-cannot resolve 'map_zip_with(various_maps.`decimal_map2`, various_maps.`int_map`, lambdafunction(named_struct(NamePlaceholder(), `k`, NamePlaceholder(), `v1`, NamePlaceholder(), `v2`), `k`, `v1`, `v2`))' due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are [decimal(36,35), int].; line 1 pos 7
+cannot resolve 'map_zip_with(various_maps.`decimal_map2`, various_maps.`int_map`, lambdafunction(named_struct(NamePlaceholder(), k, NamePlaceholder(), v1, NamePlaceholder(), v2), k, v1, v2))' due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are [decimal(36,35), int].; line 1 pos 7
 
 
 -- !query 9
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala
index e6d1a038a5918..b7fc9570af919 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala
@@ -2908,6 +2908,26 @@ class DataFrameFunctionsSuite extends QueryTest with SharedSQLContext {
     }
     assert(ex.getMessage.contains(""Cannot use null as map key""))
   }
+
+  test(""SPARK-26370: Fix resolution of higher-order function for the same identifier"") {
+    val df = Seq(
+      (Seq(1, 9, 8, 7), 1, 2),
+      (Seq(5, 9, 7), 2, 2),
+      (Seq.empty, 3, 2),
+      (null, 4, 2)
+    ).toDF(""i"", ""x"", ""d"")
+
+    checkAnswer(df.selectExpr(""x"", ""exists(i, x -> x % d == 0)""),
+      Seq(
+        Row(1, true),
+        Row(2, false),
+        Row(3, false),
+        Row(4, null)))
+    checkAnswer(df.filter(""exists(i, x -> x % d == 0)""),
+      Seq(Row(Seq(1, 9, 8, 7), 1, 2)))
+    checkAnswer(df.select(""x"").filter(""exists(i, x -> x % d == 0)""),
+      Seq(Row(1)))
+  }
 }
 
 object DataFrameFunctionsSuite {


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Dec/18 16:30;cloud_fan;Issue resolved by pull request 23320
[https://github.com/apache/spark/pull/23320];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Except with transform regression,SPARK-26366,13204289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,danospv,danospv,13/Dec/18 16:20,02/Mar/20 09:00,13/Jul/23 08:48,19/Dec/18 07:26,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Spark Core,SQL,,,0,correctness,,,,"There appears to be a regression between Spark 2.2 and 2.3. Below is the code to reproduce it:

 
{code:java}
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._


val inputDF = spark.sqlContext.createDataFrame(
  spark.sparkContext.parallelize(Seq(
    Row(""0"", ""john"", ""smith"", ""john@smith.com""),
    Row(""1"", ""jane"", ""doe"", ""jane@doe.com""),
    Row(""2"", ""apache"", ""spark"", ""spark@apache.org""),
    Row(""3"", ""foo"", ""bar"", null)
  )),
  StructType(List(
    StructField(""id"", StringType, nullable=true),
    StructField(""first_name"", StringType, nullable=true),
    StructField(""last_name"", StringType, nullable=true),
    StructField(""email"", StringType, nullable=true)
  ))
)

val exceptDF = inputDF.transform( toProcessDF =>
  toProcessDF.filter(
      (
        col(""first_name"").isin(Seq(""john"", ""jane""): _*)
          and col(""last_name"").isin(Seq(""smith"", ""doe""): _*)
      )
      or col(""email"").isin(List(): _*)
  )
)

inputDF.except(exceptDF).show()
{code}
Output with Spark 2.2:
{noformat}
+---+----------+---------+----------------+
| id|first_name|last_name| email|
+---+----------+---------+----------------+
| 2| apache| spark|spark@apache.org|
| 3| foo| bar| null|
+---+----------+---------+----------------+{noformat}
Output with Spark 2.3:
{noformat}
+---+----------+---------+----------------+
| id|first_name|last_name| email|
+---+----------+---------+----------------+
| 2| apache| spark|spark@apache.org|
+---+----------+---------+----------------+{noformat}
Note, changing the last line to 
{code:java}
inputDF.except(exceptDF.cache()).show()
{code}
produces identical output for both Spark 2.3 and 2.2

 ",,danospv,githubbot,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 04 17:25:56 UTC 2019,,,,,,,,,,"0|s01hts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/19 17:25;rxin;Please make sure you guys tag these tickets with correctness label!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a workaround for PyArrow 0.11.,SPARK-26355,13204145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,13/Dec/18 03:55,12/Dec/22 18:10,13/Jul/23 08:48,13/Dec/18 05:28,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,PySpark,SQL,,,0,,,,,"In PyArrow 0.11, there is a API breaking change.

- ARROW-1949 - [Python/C++] Add option to Array.from_pandas and pyarrow.array to perform unsafe casts.

We should add a workaround to support PyArrow 0.11.",,githubbot,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 13 05:28:11 UTC 2018,,,,,,,,,,"0|s01gxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/18 04:13;githubbot;ueshin opened a new pull request #23305: [SPARK-26355][PYSPARK] Add a workaround for PyArrow 0.11.
URL: https://github.com/apache/spark/pull/23305
 
 
   ## What changes were proposed in this pull request?
   
   In PyArrow 0.11, there is a API breaking change.
   
   - [ARROW-1949](https://issues.apache.org/jira/browse/ARROW-1949) - [Python/C++] Add option to Array.from_pandas and pyarrow.array to perform unsafe casts.
   
   We should add a workaround to support PyArrow 0.11.
   
   ## How was this patch tested?
   
   In my local environment.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Dec/18 05:18;githubbot;asfgit closed pull request #23305: [SPARK-26355][PYSPARK] Add a workaround for PyArrow 0.11.
URL: https://github.com/apache/spark/pull/23305
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/python/pyspark/serializers.py b/python/pyspark/serializers.py
index f3ebd3767a0a1..fd4695210fb7c 100644
--- a/python/pyspark/serializers.py
+++ b/python/pyspark/serializers.py
@@ -281,7 +281,10 @@ def create_array(s, t):
             # TODO: see ARROW-2432. Remove when the minimum PyArrow version becomes 0.10.0.
             return pa.Array.from_pandas(s.apply(
                 lambda v: decimal.Decimal('NaN') if v is None else v), mask=mask, type=t)
-        return pa.Array.from_pandas(s, mask=mask, type=t)
+        elif LooseVersion(pa.__version__) < LooseVersion(""0.11.0""):
+            # TODO: see ARROW-1949. Remove when the minimum PyArrow version becomes 0.11.0.
+            return pa.Array.from_pandas(s, mask=mask, type=t)
+        return pa.Array.from_pandas(s, mask=mask, type=t, safe=False)
 
     arrs = [create_array(s, t) for s, t in series]
     return pa.RecordBatch.from_arrays(arrs, [""_%d"" % i for i in xrange(len(arrs))])
diff --git a/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py b/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py
index bfecc071386e9..a12c608dff9dd 100644
--- a/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py
+++ b/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py
@@ -468,8 +468,15 @@ def invalid_positional_types(pdf):
         with QuietTest(self.sc):
             with self.assertRaisesRegexp(Exception, ""KeyError: 'id'""):
                 grouped_df.apply(column_name_typo).collect()
-            with self.assertRaisesRegexp(Exception, ""No cast implemented""):
-                grouped_df.apply(invalid_positional_types).collect()
+            from distutils.version import LooseVersion
+            import pyarrow as pa
+            if LooseVersion(pa.__version__) < LooseVersion(""0.11.0""):
+                # TODO: see ARROW-1949. Remove when the minimum PyArrow version becomes 0.11.0.
+                with self.assertRaisesRegexp(Exception, ""No cast implemented""):
+                    grouped_df.apply(invalid_positional_types).collect()
+            else:
+                with self.assertRaisesRegexp(Exception, ""an integer is required""):
+                    grouped_df.apply(invalid_positional_types).collect()
 
     def test_positional_assignment_conf(self):
         import pandas as pd


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Dec/18 05:28;gurwls223;Fixed in https://github.com/apache/spark/pull/23305;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join reordering should not change the order of output attributes,SPARK-26352,13204131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,13/Dec/18 01:17,29/May/20 11:03,13/Jul/23 08:48,17/Dec/18 05:52,2.2.0,2.3.0,2.4.0,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,0,correctness,,,,"The optimizer rule {{org.apache.spark.sql.catalyst.optimizer.ReorderJoin}} performs join reordering on inner joins. This was introduced from SPARK-12032 in 2015-12.

After it had reordered the joins, though, it didn't check whether or not the column order (in terms of the {{output}} attribute list) is still the same as before. Thus, it's possible to have a mismatch between the reordered column order vs the schema that a DataFrame thinks it has.

This can be demonstrated with the example:
{code:none}
spark.sql(""create table table_a (x int, y int) using parquet"")
spark.sql(""create table table_b (i int, j int) using parquet"")
spark.sql(""create table table_c (a int, b int) using parquet"")
val df = spark.sql(""with df1 as (select * from table_a cross join table_b) select * from df1 join table_c on a = x and b = i"")
{code}
here's what the DataFrame thinks:
{code:none}
scala> df.printSchema
root
 |-- x: integer (nullable = true)
 |-- y: integer (nullable = true)
 |-- i: integer (nullable = true)
 |-- j: integer (nullable = true)
 |-- a: integer (nullable = true)
 |-- b: integer (nullable = true)
{code}
here's what the optimized plan thinks, after join reordering:
{code:none}
scala> df.queryExecution.optimizedPlan.output.foreach(a => println(s""|-- ${a.name}: ${a.dataType.typeName}""))
|-- x: integer
|-- y: integer
|-- a: integer
|-- b: integer
|-- i: integer
|-- j: integer
{code}

If we exclude the {{ReorderJoin}} rule (using Spark 2.4's optimizer rule exclusion feature), it's back to normal:
{code:none}
scala> spark.conf.set(""spark.sql.optimizer.excludedRules"", ""org.apache.spark.sql.catalyst.optimizer.ReorderJoin"")

scala> val df = spark.sql(""with df1 as (select * from table_a cross join table_b) select * from df1 join table_c on a = x and b = i"")
df: org.apache.spark.sql.DataFrame = [x: int, y: int ... 4 more fields]

scala> df.queryExecution.optimizedPlan.output.foreach(a => println(s""|-- ${a.name}: ${a.dataType.typeName}""))
|-- x: integer
|-- y: integer
|-- i: integer
|-- j: integer
|-- a: integer
|-- b: integer
{code}

Note that this column ordering problem leads to data corruption, and can manifest itself in various symptoms:
* Silently corrupting data, if the reordered columns happen to either have matching types or have sufficiently-compatible types (e.g. all fixed length primitive types are considered as ""sufficiently compatible"" in an UnsafeRow), then only the resulting data is going to be wrong but it might not trigger any alarms immediately. Or
* Weird Java-level exceptions like {{java.lang.NegativeArraySizeException}}, or even SIGSEGVs.",,cloud_fan,githubbot,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12032,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 17 15:01:29 UTC 2018,,,,,,,,,,"0|s01guo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/18 01:22;githubbot;rednaxelafx opened a new pull request #23303: [SPARK-26352][SQL] ReorderJoin should not change the order of columns
URL: https://github.com/apache/spark/pull/23303
 
 
   ## What changes were proposed in this pull request?
   
   The optimizer rule `org.apache.spark.sql.catalyst.optimizer.ReorderJoin` performs join reordering on inner joins. This was introduced from SPARK-12032 (https://github.com/apache/spark/pull/10073) in 2015-12.
   
   After it had reordered the joins, though, it didn't check whether or not the column order (in terms of the `output` attribute list) is still the same as before. Thus, it's possible to have a mismatch between the reordered column order vs the schema that a DataFrame thinks it has.
   
   This can be demonstrated with the example:
   ```scala
   spark.sql(""create table table_a (x int, y int) using parquet"")
   spark.sql(""create table table_b (i int, j int) using parquet"")
   spark.sql(""create table table_c (a int, b int) using parquet"")
   val df = spark.sql(""""""
     with df1 as (select * from table_a cross join table_b)
     select * from df1 join table_c on a = x and b = i
   """""")
   ```
   here's what the DataFrame thinks:
   ```
   scala> df.printSchema
   root
    |-- x: integer (nullable = true)
    |-- y: integer (nullable = true)
    |-- i: integer (nullable = true)
    |-- j: integer (nullable = true)
    |-- a: integer (nullable = true)
    |-- b: integer (nullable = true)
   ```
   here's what the optimized plan thinks, after join reordering:
   ```
   scala> df.queryExecution.optimizedPlan.output.foreach(a => println(s""|-- ${a.name}: ${a.dataType.typeName}""))
   |-- x: integer
   |-- y: integer
   |-- a: integer
   |-- b: integer
   |-- i: integer
   |-- j: integer
   ```
   
   If we exclude the `ReorderJoin` rule (using Spark 2.4's optimizer rule exclusion feature), it's back to normal:
   ```
   scala> spark.conf.set(""spark.sql.optimizer.excludedRules"", ""org.apache.spark.sql.catalyst.optimizer.ReorderJoin"")
   
   scala> val df = spark.sql(""with df1 as (select * from table_a cross join table_b) select * from df1 join table_c on a = x and b = i"")
   df: org.apache.spark.sql.DataFrame = [x: int, y: int ... 4 more fields]
   
   scala> df.queryExecution.optimizedPlan.output.foreach(a => println(s""|-- ${a.name}: ${a.dataType.typeName}""))
   |-- x: integer
   |-- y: integer
   |-- i: integer
   |-- j: integer
   |-- a: integer
   |-- b: integer
   ```
   
   Note that this column ordering problem leads to data corruption, and can manifest itself in various symptoms:
   * Silently corrupting data, if the reordered columns happen to either have matching types or have sufficiently-compatible types (e.g. all fixed length primitive types are considered as ""sufficiently compatible"" in an `UnsafeRow`), then only the resulting data is going to be wrong but it might not trigger any alarms immediately. Or
   * Weird Java-level exceptions like `java.lang.NegativeArraySizeException`, or even SIGSEGVs.
   
   ## How was this patch tested?
   
   Added new unit test in `JoinReorderSuite` and new end-to-end test in `JoinSuite`.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Dec/18 05:46;githubbot;asfgit closed pull request #23303: [SPARK-26352][SQL] join reorder should not change the order of output attributes
URL: https://github.com/apache/spark/pull/23303
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/CostBasedJoinReorder.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/CostBasedJoinReorder.scala
index 064ca68b7a628..01634a9d852c6 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/CostBasedJoinReorder.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/CostBasedJoinReorder.scala
@@ -48,6 +48,7 @@ object CostBasedJoinReorder extends Rule[LogicalPlan] with PredicateHelper {
           if projectList.forall(_.isInstanceOf[Attribute]) =>
           reorder(p, p.output)
       }
+
       // After reordering is finished, convert OrderedJoin back to Join
       result transformDown {
         case OrderedJoin(left, right, jt, cond) => Join(left, right, jt, cond)
@@ -175,11 +176,20 @@ object JoinReorderDP extends PredicateHelper with Logging {
         assert(topOutputSet == p.outputSet)
         // Keep the same order of final output attributes.
         p.copy(projectList = output)
+      case finalPlan if !sameOutput(finalPlan, output) =>
+        Project(output, finalPlan)
       case finalPlan =>
         finalPlan
     }
   }
 
+  private def sameOutput(plan: LogicalPlan, expectedOutput: Seq[Attribute]): Boolean = {
+    val thisOutput = plan.output
+    thisOutput.length == expectedOutput.length && thisOutput.zip(expectedOutput).forall {
+      case (a1, a2) => a1.semanticEquals(a2)
+    }
+  }
+
   /** Find all possible plans at the next level, based on existing levels. */
   private def searchLevel(
       existingLevels: Seq[JoinPlanMap],
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
index 6ebb194d71c2e..0b6471289a471 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
@@ -86,9 +86,9 @@ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
   }
 
   def apply(plan: LogicalPlan): LogicalPlan = plan transform {
-    case ExtractFiltersAndInnerJoins(input, conditions)
+    case p @ ExtractFiltersAndInnerJoins(input, conditions)
         if input.size > 2 && conditions.nonEmpty =>
-      if (SQLConf.get.starSchemaDetection && !SQLConf.get.cboEnabled) {
+      val reordered = if (SQLConf.get.starSchemaDetection && !SQLConf.get.cboEnabled) {
         val starJoinPlan = StarSchemaDetection.reorderStarJoins(input, conditions)
         if (starJoinPlan.nonEmpty) {
           val rest = input.filterNot(starJoinPlan.contains(_))
@@ -99,6 +99,14 @@ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
       } else {
         createOrderedJoin(input, conditions)
       }
+
+      if (p.sameOutput(reordered)) {
+        reordered
+      } else {
+        // Reordering the joins have changed the order of the columns.
+        // Inject a projection to make sure we restore to the expected ordering.
+        Project(p.output, reordered)
+      }
   }
 }
 
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinOptimizationSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinOptimizationSuite.scala
index ccd9d8dd4d213..e9438b2eee550 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinOptimizationSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinOptimizationSuite.scala
@@ -102,16 +102,19 @@ class JoinOptimizationSuite extends PlanTest {
         x.join(y).join(z).where((""x.b"".attr === ""z.b"".attr) && (""y.d"".attr === ""z.a"".attr)),
         x.join(z, condition = Some(""x.b"".attr === ""z.b"".attr))
           .join(y, condition = Some(""y.d"".attr === ""z.a"".attr))
+          .select(Seq(""x.a"", ""x.b"", ""x.c"", ""y.d"", ""z.a"", ""z.b"", ""z.c"").map(_.attr): _*)
       ),
       (
         x.join(y, Cross).join(z, Cross)
           .where((""x.b"".attr === ""z.b"".attr) && (""y.d"".attr === ""z.a"".attr)),
         x.join(z, Cross, Some(""x.b"".attr === ""z.b"".attr))
           .join(y, Cross, Some(""y.d"".attr === ""z.a"".attr))
+          .select(Seq(""x.a"", ""x.b"", ""x.c"", ""y.d"", ""z.a"", ""z.b"", ""z.c"").map(_.attr): _*)
       ),
       (
         x.join(y, Inner).join(z, Cross).where(""x.b"".attr === ""z.a"".attr),
         x.join(z, Cross, Some(""x.b"".attr === ""z.a"".attr)).join(y, Inner)
+          .select(Seq(""x.a"", ""x.b"", ""x.c"", ""y.d"", ""z.a"", ""z.b"", ""z.c"").map(_.attr): _*)
       )
     )
 
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
index 565b0a10154a8..c94a8b9e318f6 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
@@ -20,8 +20,8 @@ package org.apache.spark.sql.catalyst.optimizer
 import org.apache.spark.sql.catalyst.dsl.expressions._
 import org.apache.spark.sql.catalyst.dsl.plans._
 import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap}
-import org.apache.spark.sql.catalyst.plans.{Inner, PlanTest}
-import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, LogicalPlan}
+import org.apache.spark.sql.catalyst.plans.{Cross, Inner, PlanTest}
+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, LocalRelation, LogicalPlan}
 import org.apache.spark.sql.catalyst.rules.RuleExecutor
 import org.apache.spark.sql.catalyst.statsEstimation.{StatsEstimationTestBase, StatsTestPlan}
 import org.apache.spark.sql.internal.SQLConf.{CBO_ENABLED, JOIN_REORDER_ENABLED}
@@ -124,7 +124,8 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
     // the original order (t1 J t2) J t3.
     val bestPlan =
       t1.join(t3, Inner, Some(nameToAttr(""t1.v-1-10"") === nameToAttr(""t3.v-1-100"")))
-      .join(t2, Inner, Some(nameToAttr(""t1.k-1-2"") === nameToAttr(""t2.k-1-5"")))
+        .join(t2, Inner, Some(nameToAttr(""t1.k-1-2"") === nameToAttr(""t2.k-1-5"")))
+        .select(outputsOf(t1, t2, t3): _*)
 
     assertEqualPlans(originalPlan, bestPlan)
   }
@@ -139,7 +140,9 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
     val bestPlan =
       t1.join(t3, Inner, Some(nameToAttr(""t1.v-1-10"") === nameToAttr(""t3.v-1-100"")))
         .join(t2, Inner, Some(nameToAttr(""t1.k-1-2"") === nameToAttr(""t2.k-1-5"")))
+        .select(outputsOf(t1, t2, t3): _*) // this is redundant but we'll take it for now
         .join(t4)
+        .select(outputsOf(t1, t2, t4, t3): _*)
 
     assertEqualPlans(originalPlan, bestPlan)
   }
@@ -202,6 +205,7 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
       t1.join(t2, Inner, Some(nameToAttr(""t1.k-1-2"") === nameToAttr(""t2.k-1-5"")))
         .join(t4.join(t3, Inner, Some(nameToAttr(""t4.v-1-10"") === nameToAttr(""t3.v-1-100""))),
           Inner, Some(nameToAttr(""t1.k-1-2"") === nameToAttr(""t4.k-1-2"")))
+        .select(outputsOf(t1, t4, t2, t3): _*)
 
     assertEqualPlans(originalPlan, bestPlan)
   }
@@ -219,6 +223,23 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
     }
   }
 
+  test(""SPARK-26352: join reordering should not change the order of attributes"") {
+    // This test case does not rely on CBO.
+    // It's similar to the test case above, but catches a reordering bug that the one above doesn't
+    val tab1 = LocalRelation('x.int, 'y.int)
+    val tab2 = LocalRelation('i.int, 'j.int)
+    val tab3 = LocalRelation('a.int, 'b.int)
+    val original =
+      tab1.join(tab2, Cross)
+          .join(tab3, Inner, Some('a === 'x && 'b === 'i))
+    val expected =
+      tab1.join(tab3, Inner, Some('a === 'x))
+          .join(tab2, Cross, Some('b === 'i))
+          .select(outputsOf(tab1, tab2, tab3): _*)
+
+    assertEqualPlans(original, expected)
+  }
+
   test(""reorder recursively"") {
     // Original order:
     //          Join
@@ -266,8 +287,17 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
   private def assertEqualPlans(
       originalPlan: LogicalPlan,
       groundTruthBestPlan: LogicalPlan): Unit = {
-    val optimized = Optimize.execute(originalPlan.analyze)
+    val analyzed = originalPlan.analyze
+    val optimized = Optimize.execute(analyzed)
     val expected = groundTruthBestPlan.analyze
+
+    assert(analyzed.sameOutput(expected)) // if this fails, the expected plan itself is incorrect
+    assert(analyzed.sameOutput(optimized))
+
     compareJoinOrder(optimized, expected)
   }
+
+  private def outputsOf(plans: LogicalPlan*): Seq[Attribute] = {
+    plans.map(_.output).reduce(_ ++ _)
+  }
 }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinCostBasedReorderSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinCostBasedReorderSuite.scala
index d4d23ad69b2c2..baae934e1e4fe 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinCostBasedReorderSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinCostBasedReorderSuite.scala
@@ -218,6 +218,7 @@ class StarJoinCostBasedReorderSuite extends PlanTest with StatsEstimationTestBas
         .join(d1, Inner, Some(nameToAttr(""f1_fk1"") === nameToAttr(""d1_pk"")))
         .join(t2, Inner, Some(nameToAttr(""f1_c2"") === nameToAttr(""t2_c1"")))
         .join(t1, Inner, Some(nameToAttr(""f1_c1"") === nameToAttr(""t1_c1"")))
+        .select(outputsOf(f1, t1, t2, d1, d2): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -256,6 +257,7 @@ class StarJoinCostBasedReorderSuite extends PlanTest with StatsEstimationTestBas
         .join(t3.join(t2, Inner, Some(nameToAttr(""t2_c2"") === nameToAttr(""t3_c1""))), Inner,
           Some(nameToAttr(""d1_c2"") === nameToAttr(""t2_c1"")))
         .join(t1, Inner, Some(nameToAttr(""t1_c1"") === nameToAttr(""f1_c1"")))
+        .select(outputsOf(d1, t1, t2, f1, d2, t3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -297,6 +299,7 @@ class StarJoinCostBasedReorderSuite extends PlanTest with StatsEstimationTestBas
           Some(nameToAttr(""t3_c1"") === nameToAttr(""t4_c1"")))
         .join(t1.join(t2, Inner, Some(nameToAttr(""t1_c1"") === nameToAttr(""t2_c1""))), Inner,
           Some(nameToAttr(""t1_c2"") === nameToAttr(""t4_c2"")))
+        .select(outputsOf(d1, t1, t2, t3, t4, f1, d2): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -347,6 +350,7 @@ class StarJoinCostBasedReorderSuite extends PlanTest with StatsEstimationTestBas
           Some(nameToAttr(""d3_c2"") === nameToAttr(""t1_c1"")))
         .join(t5.join(t6, Inner, Some(nameToAttr(""t5_c2"") === nameToAttr(""t6_c2""))), Inner,
           Some(nameToAttr(""d2_c2"") === nameToAttr(""t5_c1"")))
+        .select(outputsOf(d1, t3, t4, f1, d2, t5, t6, d3, t1, t2): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -375,6 +379,7 @@ class StarJoinCostBasedReorderSuite extends PlanTest with StatsEstimationTestBas
       f1.join(d3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""d3_pk"")))
         .join(d2, Inner, Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_pk"")))
         .join(d1, Inner, Some(nameToAttr(""f1_fk1"") === nameToAttr(""d1_pk"")))
+        .select(outputsOf(d1, d2, f1, d3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -400,13 +405,27 @@ class StarJoinCostBasedReorderSuite extends PlanTest with StatsEstimationTestBas
       f1.join(t3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""t3_c1"")))
         .join(t2, Inner, Some(nameToAttr(""f1_fk2"") === nameToAttr(""t2_c1"")))
         .join(t1, Inner, Some(nameToAttr(""f1_fk1"") === nameToAttr(""t1_c1"")))
+        .select(outputsOf(t1, f1, t2, t3): _*)
 
     assertEqualPlans(query, expected)
   }
 
   private def assertEqualPlans( plan1: LogicalPlan, plan2: LogicalPlan): Unit = {
-    val optimized = Optimize.execute(plan1.analyze)
+    val analyzed = plan1.analyze
+    val optimized = Optimize.execute(analyzed)
     val expected = plan2.analyze
+
+    assert(equivalentOutput(analyzed, expected)) // if this fails, the expected itself is incorrect
+    assert(equivalentOutput(analyzed, optimized))
+
     compareJoinOrder(optimized, expected)
   }
+
+  private def outputsOf(plans: LogicalPlan*): Seq[Attribute] = {
+    plans.map(_.output).reduce(_ ++ _)
+  }
+
+  private def equivalentOutput(plan1: LogicalPlan, plan2: LogicalPlan): Boolean = {
+    normalizeExprIds(plan1).output == normalizeExprIds(plan2).output
+  }
 }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinReorderSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinReorderSuite.scala
index 4e0883e91e84a..9dc653b9d6c44 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinReorderSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinReorderSuite.scala
@@ -182,6 +182,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d1, Inner, Some(nameToAttr(""f1_fk1"") === nameToAttr(""d1_pk1"")))
         .join(d3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""d3_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, d2, f1, d3, s3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -220,6 +221,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""d3_pk1"")))
         .join(d2, Inner, Some(nameToAttr(""f1_fk2"") < nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, f1, d2, s3, d3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -255,7 +257,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
         .join(d2, Inner, Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""s3_c2"")))
-
+        .select(outputsOf(d1, f1, d2, s3, d3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -292,6 +294,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""d3_pk1"")))
         .join(d2, Inner, Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_c2"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") < nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, f1, d2, s3, d3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -395,6 +398,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d2.where(nameToAttr(""d2_c2"") === 2), Inner,
           Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""f11_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, f11, f1, d2, s3): _*)
 
     assertEqualPlans(query, equivQuery)
   }
@@ -430,6 +434,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d2.where(nameToAttr(""d2_c2"") === 2), Inner,
           Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_c4"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, d3, f1, d2, s3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -465,6 +470,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d2.where(nameToAttr(""d2_c2"") === 2), Inner,
           Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, d3, f1, d2, s3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -499,6 +505,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d2.where(nameToAttr(""d2_c2"") === 2),
           Inner, Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, d3, f1, d2, s3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -532,6 +539,7 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d3, Inner, Some(nameToAttr(""f1_fk3"") < nameToAttr(""d3_pk1"")))
         .join(d2, Inner, Some(nameToAttr(""f1_fk2"") < nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") < nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, d3, f1, d2, s3): _*)
 
     assertEqualPlans(query, expected)
   }
@@ -565,13 +573,27 @@ class StarJoinReorderSuite extends PlanTest with StatsEstimationTestBase {
         .join(d3, Inner, Some(nameToAttr(""f1_fk3"") === nameToAttr(""d3_pk1"")))
         .join(d2, Inner, Some(nameToAttr(""f1_fk2"") === nameToAttr(""d2_pk1"")))
         .join(s3, Inner, Some(nameToAttr(""d3_fk1"") === nameToAttr(""s3_pk1"")))
+        .select(outputsOf(d1, d3, f1, d2, s3): _*)
 
     assertEqualPlans(query, expected)
   }
 
-  private def assertEqualPlans( plan1: LogicalPlan, plan2: LogicalPlan): Unit = {
-    val optimized = Optimize.execute(plan1.analyze)
+  private def assertEqualPlans(plan1: LogicalPlan, plan2: LogicalPlan): Unit = {
+    val analyzed = plan1.analyze
+    val optimized = Optimize.execute(analyzed)
     val expected = plan2.analyze
+
+    assert(equivalentOutput(analyzed, expected)) // if this fails, the expected itself is incorrect
+    assert(equivalentOutput(analyzed, optimized))
+
     compareJoinOrder(optimized, expected)
   }
+
+  private def outputsOf(plans: LogicalPlan*): Seq[Attribute] = {
+    plans.map(_.output).reduce(_ ++ _)
+  }
+
+  private def equivalentOutput(plan1: LogicalPlan, plan2: LogicalPlan): Boolean = {
+    normalizeExprIds(plan1).output == normalizeExprIds(plan2).output
+  }
 }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
index aa2162c9d2cda..91445c8d96d85 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
@@ -895,4 +895,18 @@ class JoinSuite extends QueryTest with SharedSQLContext {
       checkAnswer(res, Row(0, 0, 0))
     }
   }
+
+  test(""SPARK-26352: join reordering should not change the order of columns"") {
+    withTable(""tab1"", ""tab2"", ""tab3"") {
+      spark.sql(""select 1 as x, 100 as y"").write.saveAsTable(""tab1"")
+      spark.sql(""select 42 as i, 200 as j"").write.saveAsTable(""tab2"")
+      spark.sql(""select 1 as a, 42 as b"").write.saveAsTable(""tab3"")
+
+      val df = spark.sql(""""""
+        with tmp as (select * from tab1 cross join tab2)
+        select * from tmp join tab3 on a = x and b = i
+      """""")
+      checkAnswer(df, Row(1, 100, 42, 200, 1, 42))
+    }
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Dec/18 05:52;cloud_fan;Issue resolved by pull request 23303
[https://github.com/apache/spark/pull/23303];;;","17/Dec/18 06:56;githubbot;rednaxelafx opened a new pull request #23330: [SPARK-26352][SQL][FOLLOWUP-2.4] Fix missing sameOutput in branch-2.4
URL: https://github.com/apache/spark/pull/23330
 
 
   ## What changes were proposed in this pull request?
   
   After https://github.com/apache/spark/pull/23303 was merged to branch-2.3/2.4, the builds on those branches were broken due to missing a `LogicalPlan.sameOutput` function which came from https://github.com/apache/spark/pull/22713 only available on master.
   
   This PR is to follow-up with the broken 2.3/2.4 branches and make a copy of the new `LogicalPlan.sameOutput` into `ReorderJoin` to make it locally available.
   
   ## How was this patch tested?
   
   Fix the build of 2.3/2.4.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Dec/18 08:26;githubbot;rednaxelafx opened a new pull request #23333: [SPARK-26352][SQL][FOLLOWUP-2.3] Fix missing sameOutput in branch-2.3
URL: https://github.com/apache/spark/pull/23333
 
 
   ## What changes were proposed in this pull request?
   
   This is the branch-2.3 equivalent of https://github.com/apache/spark/pull/23330.
   
   After https://github.com/apache/spark/pull/23303 was merged to branch-2.3/2.4, the builds on those branches were broken due to missing a `LogicalPlan.sameOutput` function which came from https://github.com/apache/spark/pull/22713 only available on master.
   
   This PR is to follow-up with the broken 2.3/2.4 branches and make a copy of the new `LogicalPlan.sameOutput` into `ReorderJoin` to make it locally available.
   
   ## How was this patch tested?
   
   Fix the build of 2.3/2.4.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Dec/18 14:55;githubbot;cloud-fan closed pull request #23330: [SPARK-26352][SQL][FOLLOWUP-2.4] Fix missing sameOutput in branch-2.4
URL: https://github.com/apache/spark/pull/23330
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
index 0b6471289a471..2feb4720f9f92 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
@@ -100,7 +100,7 @@ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
         createOrderedJoin(input, conditions)
       }
 
-      if (p.sameOutput(reordered)) {
+      if (sameOutput(p, reordered)) {
         reordered
       } else {
         // Reordering the joins have changed the order of the columns.
@@ -108,6 +108,21 @@ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
         Project(p.output, reordered)
       }
   }
+
+  /**
+   * Returns true iff output of both plans are semantically the same, ie.:
+   *  - they contain the same number of `Attribute`s;
+   *  - references are the same;
+   *  - the order is equal too.
+   * NOTE: this is copied over from SPARK-25691 from master.
+   */
+  def sameOutput(plan1: LogicalPlan, plan2: LogicalPlan): Boolean = {
+    val output1 = plan1.output
+    val output2 = plan2.output
+    output1.length == output2.length && output1.zip(output2).forall {
+      case (a1, a2) => a1.semanticEquals(a2)
+    }
+  }
 }
 
 /**
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
index c94a8b9e318f6..38a70f0691dd4 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
@@ -291,8 +291,8 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
     val optimized = Optimize.execute(analyzed)
     val expected = groundTruthBestPlan.analyze
 
-    assert(analyzed.sameOutput(expected)) // if this fails, the expected plan itself is incorrect
-    assert(analyzed.sameOutput(optimized))
+    assert(sameOutput(analyzed, expected)) // if this fails, the expected plan itself is incorrect
+    assert(sameOutput(analyzed, optimized))
 
     compareJoinOrder(optimized, expected)
   }
@@ -300,4 +300,19 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
   private def outputsOf(plans: LogicalPlan*): Seq[Attribute] = {
     plans.map(_.output).reduce(_ ++ _)
   }
+
+  /**
+   * Returns true iff output of both plans are semantically the same, ie.:
+   *  - they contain the same number of `Attribute`s;
+   *  - references are the same;
+   *  - the order is equal too.
+   * NOTE: this is copied over from SPARK-25691 from master.
+   */
+  def sameOutput(plan1: LogicalPlan, plan2: LogicalPlan): Boolean = {
+    val output1 = plan1.output
+    val output2 = plan2.output
+    output1.length == output2.length && output1.zip(output2).forall {
+      case (a1, a2) => a1.semanticEquals(a2)
+    }
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Dec/18 15:01;githubbot;cloud-fan closed pull request #23333: [SPARK-26352][SQL][FOLLOWUP-2.3] Fix missing sameOutput in branch-2.3
URL: https://github.com/apache/spark/pull/23333
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
index fedef68bf8513..503e20490a92c 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
@@ -99,7 +99,7 @@ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
         createOrderedJoin(input, conditions)
       }
 
-      if (p.sameOutput(reordered)) {
+      if (sameOutput(p, reordered)) {
         reordered
       } else {
         // Reordering the joins have changed the order of the columns.
@@ -107,6 +107,21 @@ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
         Project(p.output, reordered)
       }
   }
+
+  /**
+   * Returns true iff output of both plans are semantically the same, ie.:
+   *  - they contain the same number of `Attribute`s;
+   *  - references are the same;
+   *  - the order is equal too.
+   * NOTE: this is copied over from SPARK-25691 from master.
+   */
+  def sameOutput(plan1: LogicalPlan, plan2: LogicalPlan): Boolean = {
+    val output1 = plan1.output
+    val output2 = plan2.output
+    output1.length == output2.length && output1.zip(output2).forall {
+      case (a1, a2) => a1.semanticEquals(a2)
+    }
+  }
 }
 
 /**
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
index c8a4b6da4fcd0..9526cbca77094 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala
@@ -300,8 +300,8 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
     val optimized = Optimize.execute(analyzed)
     val expected = groundTruthBestPlan.analyze
 
-    assert(analyzed.sameOutput(expected)) // if this fails, the expected plan itself is incorrect
-    assert(analyzed.sameOutput(optimized))
+    assert(sameOutput(analyzed, expected)) // if this fails, the expected plan itself is incorrect
+    assert(sameOutput(analyzed, optimized))
 
     compareJoinOrder(optimized, expected)
   }
@@ -309,4 +309,19 @@ class JoinReorderSuite extends PlanTest with StatsEstimationTestBase {
   private def outputsOf(plans: LogicalPlan*): Seq[Attribute] = {
     plans.map(_.output).reduce(_ ++ _)
   }
+
+  /**
+   * Returns true iff output of both plans are semantically the same, ie.:
+   *  - they contain the same number of `Attribute`s;
+   *  - references are the same;
+   *  - the order is equal too.
+   * NOTE: this is copied over from SPARK-25691 from master.
+   */
+  def sameOutput(plan1: LogicalPlan, plan2: LogicalPlan): Boolean = {
+    val output1 = plan1.output
+    val output2 = plan2.output
+    output1.length == output2.length && output1.zip(output2).forall {
+      case (a1, a2) => a1.semanticEquals(a2)
+    }
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documented formula of precision at k does not match the actual code,SPARK-26351,13204123,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,olbapjose,olbapjose,12/Dec/18 23:48,06/Feb/19 16:52,13/Jul/23 08:48,21/Jan/19 00:14,2.4.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Documentation,MLlib,,,0,,,,,"The formula of the *precision @ k* for measuring the quality of the recommendations:

[https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#ranking-systems]

says that j goes from 0 to *min(|D|, k)* , but according to the code, 

[https://github.com/apache/spark/blob/a63e7b2a212bab94d080b00cf1c5f397800a276a/mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala#L65]

 
{code:java}
val n = math.min(pred.length, k){code}
 

The notation of Spark documentation defines

D_i as the set of ground truth relevant documents for user i

R_i as the set of recommended documents (i.e. predictions) given for user i .

According to the code, the documentation should say j goes from 0 to *min( | R_i |, k )*",,olbapjose,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 21 00:14:40 UTC 2019,,,,,,,,,,"0|s01gsw:",9223372036854775807,,,,,sowen,,,,,,,,,,,,,,,,,,"18/Jan/19 20:45;shahid;I am working on it.;;;","21/Jan/19 00:14;srowen;Issue resolved by pull request 23589
[https://github.com/apache/spark/pull/23589];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Behavior of reading files that start with underscore is confusing,SPARK-26339,13203718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Keiichi Hirobe,Keiichi Hirobe,Keiichi Hirobe,11/Dec/18 13:27,08/Jan/19 03:27,13/Jul/23 08:48,06/Jan/19 14:52,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Behavior of reading files that start with underscore is as follows.
 # spark.read (no schema) throws exception which message is confusing.
 # spark.read (userSpecificationSchema) succesfully reads, but content is emtpy.

Example of files are as follows.
 The same behavior occured when I read json files.
{code:bash}
$ cat test.csv
test1,10
test2,20
$ cp test.csv _test.csv
$ ./bin/spark-shell  --master local[2]
{code}
Spark shell snippet for reproduction:
{code:java}
scala> val df=spark.read.csv(""test.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]

scala> df.show()
+-----+---+
|  _c0|_c1|
+-----+---+
|test1| 10|
|test2| 20|
+-----+---+

scala> val df = spark.read.schema(""test STRING, number INT"").csv(""test.csv"")
df: org.apache.spark.sql.DataFrame = [test: string, number: int]
scala> df.show()
+-----+------+
| test|number|
+-----+------+
|test1|    10|
|test2|    20|
+-----+------+

scala> val df=spark.read.csv(""_test.csv"")
org.apache.spark.sql.AnalysisException: Unable to infer schema for CSV. It must be specified manually.;
  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$13(DataSource.scala:185)
  at scala.Option.getOrElse(Option.scala:138)
  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:185)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)
  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:231)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:219)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:625)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:478)
  ... 49 elided

scala> val df=spark.read.schema(""test STRING, number INT"").csv(""_test.csv"")
df: org.apache.spark.sql.DataFrame = [test: string, number: int]

scala> df.show()
+----+------+
|test|number|
+----+------+
+----+------+
{code}
I noticed that spark cannot read files that start with underscore after I read some codes.(I could not find any documents about file name limitation)

Above behavior is not good especially userSpecificationSchema case, I think.

I suggest to throw exception which message is ""Path does not exist"" in both cases.



",,apachespark,githubbot,Keiichi Hirobe,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 06 14:52:24 UTC 2019,,,,,,,,,,"0|s01ec0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/18 13:37;mgaido;The point is: files starting with underscores are hidden files in Hadoop FS. So what you are doing is the same as reading an empty folder:

 - if you set the schema, an empty dataframe is returned;
 - if you don't set it, the schema will be inferred from the data, but since there is no data the exception occurs.

I don't think this is a bug.;;;","11/Dec/18 13:51;apachespark;User 'KeiichiHirobe' has created a pull request for this issue:
https://github.com/apache/spark/pull/23288;;;","11/Dec/18 20:34;githubbot;srowen commented on a change in pull request #23288: [SPARK-26339][SQL]Throws better exception when reading files that start with underscore
URL: https://github.com/apache/spark/pull/23288#discussion_r240781003
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala
 ##########
 @@ -554,7 +554,8 @@ case class DataSource(
 
       // Sufficient to check head of the globPath seq for non-glob scenario
       // Don't need to check once again if files exist in streaming mode
-      if (checkFilesExist && !fs.exists(globPath.head)) {
+      if (checkFilesExist &&
+          (!fs.exists(globPath.head) || InMemoryFileIndex.shouldFilterOut(globPath.head.getName))) {
 
 Review comment:
   I'm probably misunderstanding, but doesn't this still cause it to throw a 'Path does not exist' exception?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","31/Dec/18 16:16;srowen;Issue resolved by pull request 23288
[https://github.com/apache/spark/pull/23288];;;","04/Jan/19 06:03;apachespark;User 'KeiichiHirobe' has created a pull request for this issue:
https://github.com/apache/spark/pull/23446;;;","04/Jan/19 06:03;apachespark;User 'KeiichiHirobe' has created a pull request for this issue:
https://github.com/apache/spark/pull/23446;;;","06/Jan/19 14:52;srowen;Issue resolved by pull request 23446
[https://github.com/apache/spark/pull/23446];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
check input types of ScalaUDF even if some inputs are of Any type,SPARK-26323,13203440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Dec/18 14:14,12/Dec/22 18:10,13/Jul/23 08:48,08/Jan/19 14:45,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 08 14:45:21 UTC 2019,,,,,,,,,,"0|s01cmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/18 14:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23275;;;","10/Dec/18 14:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23275;;;","10/Dec/18 15:20;githubbot;mgaido91 commented on a change in pull request #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#discussion_r240250904
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
 ##########
 @@ -88,68 +88,49 @@ sealed trait UserDefinedFunction {
 private[sql] case class SparkUserDefinedFunction(
     f: AnyRef,
     dataType: DataType,
-    inputTypes: Option[Seq[DataType]],
-    nullableTypes: Option[Seq[Boolean]],
+    inputSchemas: Seq[Option[ScalaReflection.Schema]],
     name: Option[String] = None,
     nullable: Boolean = true,
     deterministic: Boolean = true) extends UserDefinedFunction {
 
   @scala.annotation.varargs
-  override def apply(exprs: Column*): Column = {
-    // TODO: make sure this class is only instantiated through `SparkUserDefinedFunction.create()`
-    // and `nullableTypes` is always set.
-    if (inputTypes.isDefined) {
-      assert(inputTypes.get.length == nullableTypes.get.length)
-    }
-
-    val inputsNullSafe = nullableTypes.getOrElse {
-      ScalaReflection.getParameterTypeNullability(f)
-    }
+  override def apply(cols: Column*): Column = {
+    Column(createScalaUDF(cols.map(_.expr)))
+  }
 
-    Column(ScalaUDF(
+  private[sql] def createScalaUDF(exprs: Seq[Expression]): ScalaUDF = {
+    // It's possible that some of the inputs don't have a specific type(e.g. `Any`),  skip type
+    // check and null check for them.
+    val inputTypes = inputSchemas.map(_.map(_.dataType).getOrElse(AnyDataType))
+    val inputsNullSafe = inputSchemas.map(_.map(_.nullable).getOrElse(true))
 
 Review comment:
   nit: forall?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:12;githubbot;srowen commented on a change in pull request #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#discussion_r240275041
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
 ##########
 @@ -88,68 +88,49 @@ sealed trait UserDefinedFunction {
 private[sql] case class SparkUserDefinedFunction(
     f: AnyRef,
     dataType: DataType,
-    inputTypes: Option[Seq[DataType]],
-    nullableTypes: Option[Seq[Boolean]],
+    inputSchemas: Seq[Option[ScalaReflection.Schema]],
     name: Option[String] = None,
     nullable: Boolean = true,
     deterministic: Boolean = true) extends UserDefinedFunction {
 
   @scala.annotation.varargs
-  override def apply(exprs: Column*): Column = {
-    // TODO: make sure this class is only instantiated through `SparkUserDefinedFunction.create()`
-    // and `nullableTypes` is always set.
-    if (inputTypes.isDefined) {
-      assert(inputTypes.get.length == nullableTypes.get.length)
-    }
-
-    val inputsNullSafe = nullableTypes.getOrElse {
-      ScalaReflection.getParameterTypeNullability(f)
-    }
+  override def apply(cols: Column*): Column = {
+    Column(createScalaUDF(cols.map(_.expr)))
+  }
 
-    Column(ScalaUDF(
+  private[sql] def createScalaUDF(exprs: Seq[Expression]): ScalaUDF = {
+    // It's possible that some of the inputs don't have a specific type(e.g. `Any`),  skip type
+    // check and null check for them.
+    val inputTypes = inputSchemas.map(_.map(_.dataType).getOrElse(AnyDataType))
+    val inputsNullSafe = inputSchemas.map(_.map(_.nullable).getOrElse(true))
 
 Review comment:
   I'm missing it, how could you write this more simply with `forall` to get from `Option[Schema]` to `Boolean`?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:18;githubbot;mgaido91 commented on a change in pull request #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#discussion_r240278143
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
 ##########
 @@ -88,68 +88,49 @@ sealed trait UserDefinedFunction {
 private[sql] case class SparkUserDefinedFunction(
     f: AnyRef,
     dataType: DataType,
-    inputTypes: Option[Seq[DataType]],
-    nullableTypes: Option[Seq[Boolean]],
+    inputSchemas: Seq[Option[ScalaReflection.Schema]],
     name: Option[String] = None,
     nullable: Boolean = true,
     deterministic: Boolean = true) extends UserDefinedFunction {
 
   @scala.annotation.varargs
-  override def apply(exprs: Column*): Column = {
-    // TODO: make sure this class is only instantiated through `SparkUserDefinedFunction.create()`
-    // and `nullableTypes` is always set.
-    if (inputTypes.isDefined) {
-      assert(inputTypes.get.length == nullableTypes.get.length)
-    }
-
-    val inputsNullSafe = nullableTypes.getOrElse {
-      ScalaReflection.getParameterTypeNullability(f)
-    }
+  override def apply(cols: Column*): Column = {
+    Column(createScalaUDF(cols.map(_.expr)))
+  }
 
-    Column(ScalaUDF(
+  private[sql] def createScalaUDF(exprs: Seq[Expression]): ScalaUDF = {
+    // It's possible that some of the inputs don't have a specific type(e.g. `Any`),  skip type
+    // check and null check for them.
+    val inputTypes = inputSchemas.map(_.map(_.dataType).getOrElse(AnyDataType))
+    val inputsNullSafe = inputSchemas.map(_.map(_.nullable).getOrElse(true))
 
 Review comment:
   I mean `inputSchemas.map(_.forall(_.nullable))`

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:29;githubbot;SparkQA commented on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445878673
 
 
   **[Test build #99919 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99919/testReport)** for PR 23275 at commit [`8582607`](https://github.com/apache/spark/commit/8582607195f12a4c133fb28b59e8a7fce7a97fbb).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:29;githubbot;AmplabJenkins commented on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445878926
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:29;githubbot;AmplabJenkins commented on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445878940
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99919/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:30;githubbot;SparkQA removed a comment on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445836177
 
 
   **[Test build #99919 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99919/testReport)** for PR 23275 at commit [`8582607`](https://github.com/apache/spark/commit/8582607195f12a4c133fb28b59e8a7fce7a97fbb).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:30;githubbot;AmplabJenkins removed a comment on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445878926
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:31;githubbot;AmplabJenkins removed a comment on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445878940
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99919/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:38;githubbot;SparkQA commented on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445882225
 
 
   **[Test build #99920 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99920/testReport)** for PR 23275 at commit [`92466d4`](https://github.com/apache/spark/commit/92466d486734f3904be31e45b85e49654eb39255).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:39;githubbot;SparkQA removed a comment on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445840083
 
 
   **[Test build #99920 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99920/testReport)** for PR 23275 at commit [`92466d4`](https://github.com/apache/spark/commit/92466d486734f3904be31e45b85e49654eb39255).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:39;githubbot;AmplabJenkins commented on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445882594
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:39;githubbot;AmplabJenkins commented on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445882601
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99920/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:40;githubbot;AmplabJenkins removed a comment on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445882594
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 16:41;githubbot;AmplabJenkins removed a comment on issue #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#issuecomment-445882601
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99920/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:08;githubbot;srowen commented on a change in pull request #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#discussion_r240298939
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
 ##########
 @@ -88,68 +88,49 @@ sealed trait UserDefinedFunction {
 private[sql] case class SparkUserDefinedFunction(
     f: AnyRef,
     dataType: DataType,
-    inputTypes: Option[Seq[DataType]],
-    nullableTypes: Option[Seq[Boolean]],
+    inputSchemas: Seq[Option[ScalaReflection.Schema]],
     name: Option[String] = None,
     nullable: Boolean = true,
     deterministic: Boolean = true) extends UserDefinedFunction {
 
   @scala.annotation.varargs
-  override def apply(exprs: Column*): Column = {
-    // TODO: make sure this class is only instantiated through `SparkUserDefinedFunction.create()`
-    // and `nullableTypes` is always set.
-    if (inputTypes.isDefined) {
-      assert(inputTypes.get.length == nullableTypes.get.length)
-    }
-
-    val inputsNullSafe = nullableTypes.getOrElse {
-      ScalaReflection.getParameterTypeNullability(f)
-    }
+  override def apply(cols: Column*): Column = {
+    Column(createScalaUDF(cols.map(_.expr)))
+  }
 
-    Column(ScalaUDF(
+  private[sql] def createScalaUDF(exprs: Seq[Expression]): ScalaUDF = {
+    // It's possible that some of the inputs don't have a specific type(e.g. `Any`),  skip type
+    // check and null check for them.
+    val inputTypes = inputSchemas.map(_.map(_.dataType).getOrElse(AnyDataType))
+    val inputsNullSafe = inputSchemas.map(_.map(_.nullable).getOrElse(true))
 
 Review comment:
   Ah right. I'm neutral on whether it's clearer than getOrElse; I think we end up using the latter in the code more. I know IJ suggests forall though.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 01:58;githubbot;cloud-fan commented on a change in pull request #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#discussion_r240449840
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala
 ##########
 @@ -47,25 +47,13 @@ case class ScalaUDF(
     function: AnyRef,
     dataType: DataType,
     children: Seq[Expression],
-    inputsNullSafe: Seq[Boolean],
-    inputTypes: Seq[DataType] = Nil,
+    @transient inputsNullSafe: Seq[Boolean],
 
 Review comment:
   expressions are usually serialized to executor side. Previously it's fine, as all data types are case class, which is serializable. But `AnyDataType` is normal scala object, which is not serializable.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 02:13;githubbot;cloud-fan commented on a change in pull request #23275: [SPARK-26323][SQL] Scala UDF should still check input types even if some inputs are of type Any
URL: https://github.com/apache/spark/pull/23275#discussion_r240451835
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
 ##########
 @@ -88,68 +88,49 @@ sealed trait UserDefinedFunction {
 private[sql] case class SparkUserDefinedFunction(
     f: AnyRef,
     dataType: DataType,
-    inputTypes: Option[Seq[DataType]],
-    nullableTypes: Option[Seq[Boolean]],
+    inputSchemas: Seq[Option[ScalaReflection.Schema]],
     name: Option[String] = None,
     nullable: Boolean = true,
     deterministic: Boolean = true) extends UserDefinedFunction {
 
   @scala.annotation.varargs
-  override def apply(exprs: Column*): Column = {
-    // TODO: make sure this class is only instantiated through `SparkUserDefinedFunction.create()`
-    // and `nullableTypes` is always set.
-    if (inputTypes.isDefined) {
-      assert(inputTypes.get.length == nullableTypes.get.length)
-    }
-
-    val inputsNullSafe = nullableTypes.getOrElse {
-      ScalaReflection.getParameterTypeNullability(f)
-    }
+  override def apply(cols: Column*): Column = {
+    Column(createScalaUDF(cols.map(_.expr)))
+  }
 
-    Column(ScalaUDF(
+  private[sql] def createScalaUDF(exprs: Seq[Expression]): ScalaUDF = {
+    // It's possible that some of the inputs don't have a specific type(e.g. `Any`),  skip type
+    // check and null check for them.
+    val inputTypes = inputSchemas.map(_.map(_.dataType).getOrElse(AnyDataType))
+    val inputsNullSafe = inputSchemas.map(_.map(_.nullable).getOrElse(true))
 
 Review comment:
   Here `getOrElse` maybe better, as it matches the previous line.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","08/Jan/19 14:45;gurwls223;Issue resolved by pull request 23275
[https://github.com/apache/spark/pull/23275];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade SBT to 0.13.18,SPARK-26317,13203366,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Dec/18 07:47,10/Dec/18 20:09,13/Jul/23 08:48,10/Dec/18 20:06,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Build,,,,0,,,,,SBT 0.13.14 ~ 1.1.1 has a bug on accessing `java.util.Base64.getDecoder` on JDK9+. It's fixed at 1.1.2 and backported to 0.13.18 (released last week Nov 28th). This issue aims to update SBT.,,apachespark,dongjoon,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 10 20:09:27 UTC 2018,,,,,,,,,,"0|s01c6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/18 07:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/23270;;;","10/Dec/18 18:21;githubbot;SparkQA commented on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445918152
 
 
   **[Test build #99918 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99918/testReport)** for PR 23270 at commit [`0885c94`](https://github.com/apache/spark/commit/0885c947d3b4561df2d39f1bc9a35a06d7f0ed0c).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 18:22;githubbot;SparkQA removed a comment on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445826720
 
 
   **[Test build #99918 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99918/testReport)** for PR 23270 at commit [`0885c94`](https://github.com/apache/spark/commit/0885c947d3b4561df2d39f1bc9a35a06d7f0ed0c).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 18:23;githubbot;AmplabJenkins commented on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445918808
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 18:23;githubbot;AmplabJenkins commented on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445918815
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99918/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 18:24;githubbot;AmplabJenkins removed a comment on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445918808
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 18:24;githubbot;AmplabJenkins removed a comment on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445918815
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99918/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 19:07;githubbot;dongjoon-hyun commented on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445933945
 
 
   cc @srowen and @dbtsai 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 19:50;githubbot;dbtsai commented on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445948821
 
 
   LGTM.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 19:58;githubbot;dongjoon-hyun commented on issue #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270#issuecomment-445951570
 
 
   Thank you, @srowen and @dbtsai and @HyukjinKwon . Merged to master.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 20:06;dongjoon;This is resolved via https://github.com/apache/spark/pull/23270;;;","10/Dec/18 20:09;githubbot;dongjoon-hyun closed pull request #23270: [SPARK-26317][BUILD] Upgrade SBT to 0.13.18
URL: https://github.com/apache/spark/pull/23270
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/project/build.properties b/project/build.properties
index d03985d980ec8..23aa187fb35a7 100644
--- a/project/build.properties
+++ b/project/build.properties
@@ -14,4 +14,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-sbt.version=0.13.17
+sbt.version=0.13.18


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto cast threshold from Integer to Float in approxSimilarityJoin of BucketedRandomProjectionLSHModel,SPARK-26315,13203361,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cinqsong,cinqsong,cinqsong,10/Dec/18 06:02,17/Dec/18 07:33,13/Jul/23 08:48,15/Dec/18 14:43,2.3.2,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,MLlib,PySpark,,,0,,,,,"when I was using 
{code:java}
// code placeholder
BucketedRandomProjectionLSHModel.approxSimilarityJoin(dt_features, dt_features, distCol=""EuclideanDistance"", threshold=20.)
{code}
I was confused then that this method reported an exception some java method (dataset, dataset, integer, string) fingerprint can not be found.... I think if I give an integer, and the python method of pyspark should be auto-cast this to float if needed. ",,bryanc,cinqsong,githubbot,jinghe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 15 14:44:46 UTC 2018,,,,,,,,,,"0|s01c5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/18 19:45;bryanc;I believe {{def approxSimilarityJoin(...)}} in LSHModelf in feature.py, should have {{threshold = TypeConverters.toFloat(threshold)}} before the call to {{_call_java(...)}}

I can help guide if anyone would like to submit a PR for the fix.;;;","13/Dec/18 05:46;jinghe;Hi, [~bryanc]  I will try to provide a PR. Thanks!;;;","13/Dec/18 16:32;githubbot;jerryjch opened a new pull request #23313: [SPARK-26315][PYSPARk] auto cast threshold from Integer to Float in approxSimilarityJoin of BucketedRandomProjectionLSHModel
URL: https://github.com/apache/spark/pull/23313
 
 
   
   ## What changes were proposed in this pull request?
   
   If the input parameter 'threshold' to the function approxSimilarityJoin is not a float, we would get an exception.  The fix is to convert the 'threshold' into a float before calling the java implementation method.
   
   ## How was this patch tested?
   
   Added a new test case.  Without this fix, the test will throw an exception as reported in the JIRA. With the fix, the test passes.
   
   Please review http://spark.apache.org/contributing.html before opening a pull request.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Dec/18 14:43;srowen;Issue resolved by pull request 23313
[https://github.com/apache/spark/pull/23313];;;","15/Dec/18 14:44;githubbot;srowen closed pull request #23313: [SPARK-26315][PYSPARk] auto cast threshold from Integer to Float in approxSimilarityJoin of BucketedRandomProjectionLSHModel
URL: https://github.com/apache/spark/pull/23313
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/python/pyspark/ml/feature.py b/python/pyspark/ml/feature.py
index c9507c20918e3..08ae58246adb6 100755
--- a/python/pyspark/ml/feature.py
+++ b/python/pyspark/ml/feature.py
@@ -192,6 +192,7 @@ def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=""distCol"")
                  ""datasetA"" and ""datasetB"", and a column ""distCol"" is added to show the distance
                  between each pair.
         """"""
+        threshold = TypeConverters.toFloat(threshold)
         return self._call_java(""approxSimilarityJoin"", datasetA, datasetB, threshold, distCol)
 
 
@@ -239,6 +240,16 @@ class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutp
     |  3|  6| 2.23606797749979|
     +---+---+-----------------+
     ...
+    >>> model.approxSimilarityJoin(df, df2, 3, distCol=""EuclideanDistance"").select(
+    ...     col(""datasetA.id"").alias(""idA""),
+    ...     col(""datasetB.id"").alias(""idB""),
+    ...     col(""EuclideanDistance"")).show()
+    +---+---+-----------------+
+    |idA|idB|EuclideanDistance|
+    +---+---+-----------------+
+    |  3|  6| 2.23606797749979|
+    +---+---+-----------------+
+    ...
     >>> brpPath = temp_path + ""/brp""
     >>> brp.save(brpPath)
     >>> brp2 = BucketedRandomProjectionLSH.load(brpPath)


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large BigDecimal value is converted to null when passed into a UDF,SPARK-26308,13203122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,jay.pranavamurthi,jay.pranavamurthi,07/Dec/18 18:49,12/Dec/22 18:10,13/Jul/23 08:48,20/Dec/18 06:20,2.3.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"We are loading a Hive table into a Spark DataFrame. The Hive table has a decimal(30, 0) column with values greater than Long.MAX_VALUE. The DataFrame loads correctly.

We then use a UDF to convert the decimal type to a String value. For decimal values < Long.MAX_VALUE, this works fine, but when the decimal value > Long.MAX_VALUE, the input to the UDF is a *null*.

Hive table schema and data:
{code:java}
create table decimal_test (col1 decimal(30, 0), col2 decimal(10, 0), col3 int, col4 string);
insert into decimal_test values(2011000000000002456556, 123456789, 10, 'test1');
{code}
 

Execution in spark-shell:

_(Note that the first column in the final output is null, it should have been ""2011000000000002456556"")_
{code:java}
scala> val df1 = spark.sqlContext.sql(""select * from decimal_test"")
df1: org.apache.spark.sql.DataFrame = [col1: decimal(30,0), col2: decimal(10,0) ... 2 more fields]

scala> df1.show
+--------------------+---------+----+-----+
| col1| col2|col3| col4|
+--------------------+---------+----+-----+
|20110000000000024...|123456789| 10|test1|
+--------------------+---------+----+-----+


scala> val decimalToString = (value: java.math.BigDecimal) => if (value == null) null else { value.toBigInteger().toString }
decimalToString: java.math.BigDecimal => String = <function1>

scala> val udf1 = org.apache.spark.sql.functions.udf(decimalToString)
udf1: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(DecimalType(38,18))))

scala> val df2 = df1.withColumn(""col1"", udf1(df1.col(""col1"")))
df2: org.apache.spark.sql.DataFrame = [col1: string, col2: decimal(10,0) ... 2 more fields]

scala> df2.show
+----+---------+----+-----+
|col1| col2|col3| col4|
+----+---------+----+-----+
|null|123456789| 10|test1|
+----+---------+----+-----+
{code}
Oddly this works if we change the ""decimalToString"" udf to take an ""Any"" instead of a ""java.math.BigDecimal""
{code:java}
scala> val decimalToString = (value: Any) => if (value == null) null else { if (value.isInstanceOf[java.math.BigDecimal]) value.asInstanceOf[java.math.BigDecimal].toBigInteger().toString else null }
decimalToString: Any => String = <function1>

scala> val udf1 = org.apache.spark.sql.functions.udf(decimalToString)
udf1: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,None)

scala> val df2 = df1.withColumn(""col1"", udf1(df1.col(""col1"")))
df2: org.apache.spark.sql.DataFrame = [col1: string, col2: decimal(10,0) ... 2 more fields]

scala> df2.show
+--------------------+---------+----+-----+
| col1| col2|col3| col4|
+--------------------+---------+----+-----+
|20110000000000024...|123456789| 10|test1|
+--------------------+---------+----+-----+
{code}",,cloud_fan,dongjoon,githubbot,jay.pranavamurthi,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 20 06:22:25 UTC 2018,,,,,,,,,,"0|s01aoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/18 19:44;dongjoon;cc [~mgaido];;;","10/Dec/18 09:32;mgaido;Thanks for pinging me [~dongjoon], I'll take a look at this asap.;;;","10/Dec/18 10:46;mgaido;So the problem here is that the type inferred for decimal types in UDF is {{decimal(38, 18)}}. Since the value doesn't fit in this range, it is converted to {{null}}. In your ""workaround"", you're basically turning off automatic inference for the input types, hence the problem is ""solved"". Since the UDF are not related to a column/schema, I don't have a better suggestion how to deal with this case. Another ""workaround"" is to use the overloaded {{udf}} method which gets the output data type too as parameter (in this method there is no input inference either).

The only ""solution"" I can think of is to disable type inference for decimal types. If we agree on this I can submit a PR for it. What do you think [~cloud_fan] [~smilegator] [~ueshin]?;;;","10/Dec/18 11:16;cloud_fan;`ScalaUDF.inputTypes` is only used to check input types, I think we should put a general `DecimalType` there, instead of a specific one like `DecimalType(38, 18)`.;;;","10/Dec/18 11:24;mgaido;[~cloud_fan] what do you mean by a general {{DecimalType}}? A {{DecimalType}} can be instantiated only with a related precision and scale. And whatever we choose cannot fit all the cases. Are you suggesting making optional the precision and scale in {{DecimalType}}?;;;","10/Dec/18 11:40;cloud_fan;there is a `object DecimalType extends AbstractDataType`.;;;","10/Dec/18 13:18;mgaido;Yes, but it is an {{AbstractDataType}}, not a {{DataType}}. This can be done (it may require binaries breaking changes, but may be fine for 3.0), but alleviates only the issue: let's think to the case of an array of decimals: the array type can't be an abstract datatype, so the problem would still be there.;;;","10/Dec/18 14:13;cloud_fan;How about we override `ScalaUDF.checkInputTypes` and special-case decimal type?;;;","10/Dec/18 14:25;mgaido;I think that works. Maybe it is not a ""perfect"" solution but for Scala UDF it should be safe to do that. I'll work on it, thanks.;;;","10/Dec/18 15:12;mgaido;I checked but it doesn't work because the cast is added anyway in {{ImplicitTypeCasts}}. So I don't have a good solution which works for any scenario. An idea may be to create an abstract version of Array, Strict and Map types which allow abstract types, but I am not sure this is acceptable just to fix this issue... We may start here tackling the case of decimals only and leave a TODO for decimals in complex types. How does this sound?;;;","13/Dec/18 05:00;gurwls223;^ I think that sounds okay.;;;","13/Dec/18 11:43;githubbot;mgaido91 opened a new pull request #23308: [SPARK-26308][SQL] Infer abstract decimal type for java/scala BigDecimal
URL: https://github.com/apache/spark/pull/23308
 
 
   ## What changes were proposed in this pull request?
   
   Currently, when we infer the schema for scala/java decimals, we return as data type the `SYSTEM_DEFAULT` implementation, ie. the decimal type with precision 38 and scale 18. But this is not right, as we know nothing about the right precision and scale and these values can be not enough to store the data. This problem arises in particular with UDF, where we cast all the input of type `DecimalType` to a `DecimalType(38, 18)`: in case this is not enough, null is returned as input for the UDF.
   
   The PR changes the resolution of `BigDecimal`/`Decimal` to the abstract `DecimalType`. Please notice that the same problem is still present for Decimals in arrays, structs and maps, because the related types don't accept an `AbstractDataType`. In a followup work, we can introduce abstract types for them accepting abstract types in order to fix the issue for them too.
   
   ## How was this patch tested?
   
   added UT
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Dec/18 06:20;cloud_fan;Issue resolved by pull request 23308
[https://github.com/apache/spark/pull/23308];;;","20/Dec/18 06:22;githubbot;asfgit closed pull request #23308: [SPARK-26308][SQL] Avoid cast of decimals for ScalaUDF
URL: https://github.com/apache/spark/pull/23308
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala
index 133fa119b7aa6..1706b3eece6d7 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala
@@ -879,6 +879,37 @@ object TypeCoercion {
           }
         }
         e.withNewChildren(children)
+
+      case udf: ScalaUDF if udf.inputTypes.nonEmpty =>
+        val children = udf.children.zip(udf.inputTypes).map { case (in, expected) =>
+          implicitCast(in, udfInputToCastType(in.dataType, expected)).getOrElse(in)
+        }
+        udf.withNewChildren(children)
+    }
+
+    private def udfInputToCastType(input: DataType, expectedType: DataType): DataType = {
+      (input, expectedType) match {
+        // SPARK-26308: avoid casting to an arbitrary precision and scale for decimals. Please note
+        // that precision and scale cannot be inferred properly for a ScalaUDF because, when it is
+        // created, it is not bound to any column. So here the precision and scale of the input
+        // column is used.
+        case (in: DecimalType, _: DecimalType) => in
+        case (ArrayType(dtIn, _), ArrayType(dtExp, nullableExp)) =>
+          ArrayType(udfInputToCastType(dtIn, dtExp), nullableExp)
+        case (MapType(keyDtIn, valueDtIn, _), MapType(keyDtExp, valueDtExp, nullableExp)) =>
+          MapType(udfInputToCastType(keyDtIn, keyDtExp),
+            udfInputToCastType(valueDtIn, valueDtExp),
+            nullableExp)
+        case (StructType(fieldsIn), StructType(fieldsExp)) =>
+          val fieldTypes =
+            fieldsIn.map(_.dataType).zip(fieldsExp.map(_.dataType)).map { case (dtIn, dtExp) =>
+              udfInputToCastType(dtIn, dtExp)
+            }
+          StructType(fieldsExp.zip(fieldTypes).map { case (field, newDt) =>
+            field.copy(dataType = newDt)
+          })
+        case (_, other) => other
+      }
     }
 
     /**
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala
index fae90caebf96c..a23aaa3a0b3ef 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala
@@ -52,7 +52,7 @@ case class ScalaUDF(
     udfName: Option[String] = None,
     nullable: Boolean = true,
     udfDeterministic: Boolean = true)
-  extends Expression with ImplicitCastInputTypes with NonSQLExpression with UserDefinedExpression {
+  extends Expression with NonSQLExpression with UserDefinedExpression {
 
   // The constructor for SPARK 2.1 and 2.2
   def this(
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala
index 20dcefa7e3cad..a26d306cff6b5 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala
@@ -17,6 +17,8 @@
 
 package org.apache.spark.sql
 
+import java.math.BigDecimal
+
 import org.apache.spark.sql.api.java._
 import org.apache.spark.sql.catalyst.plans.logical.Project
 import org.apache.spark.sql.execution.QueryExecution
@@ -26,7 +28,7 @@ import org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationComm
 import org.apache.spark.sql.functions.{lit, udf}
 import org.apache.spark.sql.test.SharedSQLContext
 import org.apache.spark.sql.test.SQLTestData._
-import org.apache.spark.sql.types.{DataTypes, DoubleType}
+import org.apache.spark.sql.types._
 import org.apache.spark.sql.util.QueryExecutionListener
 
 
@@ -420,4 +422,32 @@ class UDFSuite extends QueryTest with SharedSQLContext {
       checkAnswer(df, Seq(Row(""null1x""), Row(null), Row(""N3null"")))
     }
   }
+
+  test(""SPARK-26308: udf with decimal"") {
+    val df1 = spark.createDataFrame(
+      sparkContext.parallelize(Seq(Row(new BigDecimal(""2011000000000002456556"")))),
+      StructType(Seq(StructField(""col1"", DecimalType(30, 0)))))
+    val udf1 = org.apache.spark.sql.functions.udf((value: BigDecimal) => {
+      if (value == null) null else value.toBigInteger.toString
+    })
+    checkAnswer(df1.select(udf1(df1.col(""col1""))), Seq(Row(""2011000000000002456556"")))
+  }
+
+  test(""SPARK-26308: udf with complex types of decimal"") {
+    val df1 = spark.createDataFrame(
+      sparkContext.parallelize(Seq(Row(Array(new BigDecimal(""2011000000000002456556""))))),
+      StructType(Seq(StructField(""col1"", ArrayType(DecimalType(30, 0))))))
+    val udf1 = org.apache.spark.sql.functions.udf((arr: Seq[BigDecimal]) => {
+      arr.map(value => if (value == null) null else value.toBigInteger.toString)
+    })
+    checkAnswer(df1.select(udf1($""col1"")), Seq(Row(Array(""2011000000000002456556""))))
+
+    val df2 = spark.createDataFrame(
+      sparkContext.parallelize(Seq(Row(Map(""a"" -> new BigDecimal(""2011000000000002456556""))))),
+      StructType(Seq(StructField(""col1"", MapType(StringType, DecimalType(30, 0))))))
+    val udf2 = org.apache.spark.sql.functions.udf((map: Map[String, BigDecimal]) => {
+      map.mapValues(value => if (value == null) null else value.toBigInteger.toString)
+    })
+    checkAnswer(df2.select(udf2($""col1"")), Seq(Row(Map(""a"" -> ""2011000000000002456556""))))
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CTAS when INSERT a partitioned table using Hive serde,SPARK-26307,13203103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Dec/18 17:38,10/Dec/18 07:01,13/Jul/23 08:48,10/Dec/18 07:01,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,0,,,,,"{code:java}
    withTable(""hive_test"") {
      withSQLConf(
          ""hive.exec.dynamic.partition.mode"" -> ""nonstrict"") {
        val df = Seq((""a"", 100)).toDF(""part"", ""id"")
        df.write.format(""hive"").partitionBy(""part"")
          .mode(""overwrite"").saveAsTable(""hive_test"")
        df.write.format(""hive"").partitionBy(""part"")
          .mode(""append"").saveAsTable(""hive_test"")
      }
    }{code}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 10 07:01:45 UTC 2018,,,,,,,,,,"0|s01akg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/18 18:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/23255;;;","10/Dec/18 07:01;cloud_fan;Issue resolved by pull request 23255
[https://github.com/apache/spark/pull/23255];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.util.collection.SorterSuite,SPARK-26306,13203057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,gsomogyi,gsomogyi,07/Dec/18 14:41,23/Apr/20 20:35,13/Jul/23 08:48,04/Jan/19 21:35,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Tests,,,,0,,,,,"TestTimSort causes OOM frequently.

{code:java}
[info] org.apache.spark.util.collection.SorterSuite *** ABORTED *** (3 seconds, 225 milliseconds)
[info]   java.lang.OutOfMemoryError: Java heap space
[info]   at org.apache.spark.util.collection.TestTimSort.createArray(TestTimSort.java:56)
[info]   at org.apache.spark.util.collection.TestTimSort.getTimSortBugTestSet(TestTimSort.java:43)
[info]   at org.apache.spark.util.collection.SorterSuite.$anonfun$new$8(SorterSuite.scala:70)
[info]   at org.apache.spark.util.collection.SorterSuite$$Lambda$11365/360747485.apply$mcV$sp(Unknown Source)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
[info]   at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
[info]   at org.scalatest.FunSuiteLike$$Lambda$132/1886906768.apply(Unknown Source)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
[info]   at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
[info]   at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
[info]   at org.scalatest.FunSuiteLike$$Lambda$128/398936629.apply(Unknown Source)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
[info]   at org.scalatest.SuperEngine$$Lambda$129/1905082148.apply(Unknown Source)
[info]   at scala.collection.immutable.List.foreach(List.scala:388)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
[info]   at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
[info]   at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
[info]   at org.scalatest.Suite.run(Suite.scala:1147)
[info]   at org.scalatest.Suite.run$(Suite.scala:1129)
[error] Uncaught exception when running org.apache.spark.util.collection.SorterSuite: java.lang.OutOfMemoryError: Java heap space
sbt.ForkMain$ForkError: java.lang.OutOfMemoryError: Java heap space
	at org.apache.spark.util.collection.TestTimSort.createArray(TestTimSort.java:56)
	at org.apache.spark.util.collection.TestTimSort.getTimSortBugTestSet(TestTimSort.java:43)
	at org.apache.spark.util.collection.SorterSuite.$anonfun$new$8(SorterSuite.scala:70)
	at org.apache.spark.util.collection.SorterSuite$$Lambda$11365/360747485.apply$mcV$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$Lambda$132/1886906768.apply(Unknown Source)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$Lambda$128/398936629.apply(Unknown Source)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at org.scalatest.SuperEngine$$Lambda$129/1905082148.apply(Unknown Source)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
{code}

https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99826/


- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5325/testReport/junit/org.apache.spark.util.collection/SorterSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5329/testReport/junit/org.apache.spark.util.collection/SorterSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5320/testReport/junit/org.apache.spark.util.collection/SorterSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/",,dongjoon,gsomogyi,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31543,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 04 21:35:59 UTC 2019,,,,,,,,,,"0|s01aa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/18 15:22;viirya;Besides above build, is there any build that this test fails too?;;;","07/Dec/18 15:31;gsomogyi;No idea, I've seen it only in PR builder and thought file it to help others.;;;","07/Dec/18 19:59;gsomogyi;Tested it on my local machine in a loop and never appeared.;;;","10/Dec/18 02:31;viirya;I have not noticed the evidence that this test is flaky for now. Maybe we can close this and you can always reopen it if this is really an issue.;;;","10/Dec/18 08:14;gsomogyi;I'm fine to close this and reopen if comes more often. Closing...

Thanks for taking a look at it.;;;","02/Jan/19 02:20;dongjoon;I'm reopening this because this is a real flakiniess issue.
cc [~srowen];;;","04/Jan/19 21:35;srowen;Issue resolved by pull request 23425
[https://github.com/apache/spark/pull/23425];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cast exception when having python udf in subquery,SPARK-26293,13202760,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,06/Dec/18 11:18,20/Mar/20 03:18,13/Jul/23 08:48,11/Dec/18 06:35,2.4.0,,,,,,,,,,,,,,,,,2.4.6,3.0.0,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 11 08:40:42 UTC 2018,,,,,,,,,,"0|s018go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/18 12:10;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23248;;;","10/Dec/18 15:35;githubbot;AdolphKK commented on issue #23248: [SPARK-26293][SQL] Cast exception when having python udf in subquery
URL: https://github.com/apache/spark/pull/23248#issuecomment-445858370
 
 
   looks good for me, +1 :+1: 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 06:21;githubbot;asfgit closed pull request #23248: [SPARK-26293][SQL] Cast exception when having python udf in subquery
URL: https://github.com/apache/spark/pull/23248
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/python/pyspark/sql/tests/test_udf.py b/python/pyspark/sql/tests/test_udf.py
index ed298f724d551..12cf8c7de1dad 100644
--- a/python/pyspark/sql/tests/test_udf.py
+++ b/python/pyspark/sql/tests/test_udf.py
@@ -23,7 +23,7 @@
 
 from pyspark import SparkContext
 from pyspark.sql import SparkSession, Column, Row
-from pyspark.sql.functions import UserDefinedFunction
+from pyspark.sql.functions import UserDefinedFunction, udf
 from pyspark.sql.types import *
 from pyspark.sql.utils import AnalysisException
 from pyspark.testing.sqlutils import ReusedSQLTestCase, test_compiled, test_not_compiled_message
@@ -102,7 +102,6 @@ def test_udf_registration_return_type_not_none(self):
 
     def test_nondeterministic_udf(self):
         # Test that nondeterministic UDFs are evaluated only once in chained UDF evaluations
-        from pyspark.sql.functions import udf
         import random
         udf_random_col = udf(lambda: int(100 * random.random()), IntegerType()).asNondeterministic()
         self.assertEqual(udf_random_col.deterministic, False)
@@ -113,7 +112,6 @@ def test_nondeterministic_udf(self):
 
     def test_nondeterministic_udf2(self):
         import random
-        from pyspark.sql.functions import udf
         random_udf = udf(lambda: random.randint(6, 6), IntegerType()).asNondeterministic()
         self.assertEqual(random_udf.deterministic, False)
         random_udf1 = self.spark.catalog.registerFunction(""randInt"", random_udf)
@@ -132,7 +130,6 @@ def test_nondeterministic_udf2(self):
 
     def test_nondeterministic_udf3(self):
         # regression test for SPARK-23233
-        from pyspark.sql.functions import udf
         f = udf(lambda x: x)
         # Here we cache the JVM UDF instance.
         self.spark.range(1).select(f(""id""))
@@ -144,7 +141,7 @@ def test_nondeterministic_udf3(self):
         self.assertFalse(deterministic)
 
     def test_nondeterministic_udf_in_aggregate(self):
-        from pyspark.sql.functions import udf, sum
+        from pyspark.sql.functions import sum
         import random
         udf_random_col = udf(lambda: int(100 * random.random()), 'int').asNondeterministic()
         df = self.spark.range(10)
@@ -181,7 +178,6 @@ def test_multiple_udfs(self):
         self.assertEqual(tuple(row), (6, 5))
 
     def test_udf_in_filter_on_top_of_outer_join(self):
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1)])
         right = self.spark.createDataFrame([Row(a=1)])
         df = left.join(right, on='a', how='left_outer')
@@ -190,7 +186,6 @@ def test_udf_in_filter_on_top_of_outer_join(self):
 
     def test_udf_in_filter_on_top_of_join(self):
         # regression test for SPARK-18589
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1)])
         right = self.spark.createDataFrame([Row(b=1)])
         f = udf(lambda a, b: a == b, BooleanType())
@@ -199,7 +194,6 @@ def test_udf_in_filter_on_top_of_join(self):
 
     def test_udf_in_join_condition(self):
         # regression test for SPARK-25314
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1)])
         right = self.spark.createDataFrame([Row(b=1)])
         f = udf(lambda a, b: a == b, BooleanType())
@@ -211,7 +205,7 @@ def test_udf_in_join_condition(self):
 
     def test_udf_in_left_outer_join_condition(self):
         # regression test for SPARK-26147
-        from pyspark.sql.functions import udf, col
+        from pyspark.sql.functions import col
         left = self.spark.createDataFrame([Row(a=1)])
         right = self.spark.createDataFrame([Row(b=1)])
         f = udf(lambda a: str(a), StringType())
@@ -223,7 +217,6 @@ def test_udf_in_left_outer_join_condition(self):
 
     def test_udf_in_left_semi_join_condition(self):
         # regression test for SPARK-25314
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)])
         right = self.spark.createDataFrame([Row(b=1, b1=1, b2=1)])
         f = udf(lambda a, b: a == b, BooleanType())
@@ -236,7 +229,6 @@ def test_udf_in_left_semi_join_condition(self):
     def test_udf_and_common_filter_in_join_condition(self):
         # regression test for SPARK-25314
         # test the complex scenario with both udf and common filter
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)])
         right = self.spark.createDataFrame([Row(b=1, b1=1, b2=1), Row(b=1, b1=3, b2=1)])
         f = udf(lambda a, b: a == b, BooleanType())
@@ -247,7 +239,6 @@ def test_udf_and_common_filter_in_join_condition(self):
     def test_udf_and_common_filter_in_left_semi_join_condition(self):
         # regression test for SPARK-25314
         # test the complex scenario with both udf and common filter
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)])
         right = self.spark.createDataFrame([Row(b=1, b1=1, b2=1), Row(b=1, b1=3, b2=1)])
         f = udf(lambda a, b: a == b, BooleanType())
@@ -258,7 +249,6 @@ def test_udf_and_common_filter_in_left_semi_join_condition(self):
     def test_udf_not_supported_in_join_condition(self):
         # regression test for SPARK-25314
         # test python udf is not supported in join type besides left_semi and inner join.
-        from pyspark.sql.functions import udf
         left = self.spark.createDataFrame([Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)])
         right = self.spark.createDataFrame([Row(b=1, b1=1, b2=1), Row(b=1, b1=3, b2=1)])
         f = udf(lambda a, b: a == b, BooleanType())
@@ -301,7 +291,7 @@ def test_broadcast_in_udf(self):
 
     def test_udf_with_filter_function(self):
         df = self.spark.createDataFrame([(1, ""1""), (2, ""2""), (1, ""2""), (1, ""2"")], [""key"", ""value""])
-        from pyspark.sql.functions import udf, col
+        from pyspark.sql.functions import col
         from pyspark.sql.types import BooleanType
 
         my_filter = udf(lambda a: a < 2, BooleanType())
@@ -310,7 +300,7 @@ def test_udf_with_filter_function(self):
 
     def test_udf_with_aggregate_function(self):
         df = self.spark.createDataFrame([(1, ""1""), (2, ""2""), (1, ""2""), (1, ""2"")], [""key"", ""value""])
-        from pyspark.sql.functions import udf, col, sum
+        from pyspark.sql.functions import col, sum
         from pyspark.sql.types import BooleanType
 
         my_filter = udf(lambda a: a == 1, BooleanType())
@@ -326,7 +316,7 @@ def test_udf_with_aggregate_function(self):
         self.assertEqual(sel.collect(), [Row(t=4), Row(t=3)])
 
     def test_udf_in_generate(self):
-        from pyspark.sql.functions import udf, explode
+        from pyspark.sql.functions import explode
         df = self.spark.range(5)
         f = udf(lambda x: list(range(x)), ArrayType(LongType()))
         row = df.select(explode(f(*df))).groupBy().sum().first()
@@ -353,7 +343,6 @@ def test_udf_in_generate(self):
         self.assertEqual(res[3][1], 1)
 
     def test_udf_with_order_by_and_limit(self):
-        from pyspark.sql.functions import udf
         my_copy = udf(lambda x: x, IntegerType())
         df = self.spark.range(10).orderBy(""id"")
         res = df.select(df.id, my_copy(df.id).alias(""copy"")).limit(1)
@@ -394,14 +383,14 @@ def test_non_existed_udaf(self):
                                 lambda: spark.udf.registerJavaUDAF(""udaf1"", ""non_existed_udaf""))
 
     def test_udf_with_input_file_name(self):
-        from pyspark.sql.functions import udf, input_file_name
+        from pyspark.sql.functions import input_file_name
         sourceFile = udf(lambda path: path, StringType())
         filePath = ""python/test_support/sql/people1.json""
         row = self.spark.read.json(filePath).select(sourceFile(input_file_name())).first()
         self.assertTrue(row[0].find(""people1.json"") != -1)
 
     def test_udf_with_input_file_name_for_hadooprdd(self):
-        from pyspark.sql.functions import udf, input_file_name
+        from pyspark.sql.functions import input_file_name
 
         def filename(path):
             return path
@@ -427,9 +416,6 @@ def test_udf_defers_judf_initialization(self):
         # This is separate of  UDFInitializationTests
         # to avoid context initialization
         # when udf is called
-
-        from pyspark.sql.functions import UserDefinedFunction
-
         f = UserDefinedFunction(lambda x: x, StringType())
 
         self.assertIsNone(
@@ -445,8 +431,6 @@ def test_udf_defers_judf_initialization(self):
         )
 
     def test_udf_with_string_return_type(self):
-        from pyspark.sql.functions import UserDefinedFunction
-
         add_one = UserDefinedFunction(lambda x: x + 1, ""integer"")
         make_pair = UserDefinedFunction(lambda x: (-x, x), ""struct<x:integer,y:integer>"")
         make_array = UserDefinedFunction(
@@ -460,13 +444,11 @@ def test_udf_with_string_return_type(self):
         self.assertTupleEqual(expected, actual)
 
     def test_udf_shouldnt_accept_noncallable_object(self):
-        from pyspark.sql.functions import UserDefinedFunction
-
         non_callable = None
         self.assertRaises(TypeError, UserDefinedFunction, non_callable, StringType())
 
     def test_udf_with_decorator(self):
-        from pyspark.sql.functions import lit, udf
+        from pyspark.sql.functions import lit
         from pyspark.sql.types import IntegerType, DoubleType
 
         @udf(IntegerType())
@@ -523,7 +505,6 @@ def as_double(x):
         )
 
     def test_udf_wrapper(self):
-        from pyspark.sql.functions import udf
         from pyspark.sql.types import IntegerType
 
         def f(x):
@@ -569,7 +550,7 @@ def test_nonparam_udf_with_aggregate(self):
     # SPARK-24721
     @unittest.skipIf(not test_compiled, test_not_compiled_message)
     def test_datasource_with_udf(self):
-        from pyspark.sql.functions import udf, lit, col
+        from pyspark.sql.functions import lit, col
 
         path = tempfile.mkdtemp()
         shutil.rmtree(path)
@@ -609,8 +590,6 @@ def test_datasource_with_udf(self):
 
     # SPARK-25591
     def test_same_accumulator_in_udfs(self):
-        from pyspark.sql.functions import udf
-
         data_schema = StructType([StructField(""a"", IntegerType(), True),
                                   StructField(""b"", IntegerType(), True)])
         data = self.spark.createDataFrame([[1, 2]], schema=data_schema)
@@ -632,6 +611,15 @@ def second_udf(x):
         data.collect()
         self.assertEqual(test_accum.value, 101)
 
+    # SPARK-26293
+    def test_udf_in_subquery(self):
+        f = udf(lambda x: x, ""long"")
+        with self.tempView(""v""):
+            self.spark.range(1).filter(f(""id"") >= 0).createTempView(""v"")
+            sql = self.spark.sql
+            result = sql(""select i from values(0L) as data(i) where i in (select id from v)"")
+            self.assertEqual(result.collect(), [Row(i=0)])
+
 
 class UDFInitializationTests(unittest.TestCase):
     def tearDown(self):
@@ -642,8 +630,6 @@ def tearDown(self):
             SparkContext._active_spark_context.stop()
 
     def test_udf_init_shouldnt_initialize_context(self):
-        from pyspark.sql.functions import UserDefinedFunction
-
         UserDefinedFunction(lambda x: x, StringType())
 
         self.assertIsNone(
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala
index 2b87796dc6833..a5203daea9cd0 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala
@@ -60,8 +60,12 @@ private class BatchIterator[T](iter: Iterator[T], batchSize: Int)
 /**
  * A logical plan that evaluates a [[PythonUDF]].
  */
-case class ArrowEvalPython(udfs: Seq[PythonUDF], output: Seq[Attribute], child: LogicalPlan)
-  extends UnaryNode
+case class ArrowEvalPython(
+    udfs: Seq[PythonUDF],
+    output: Seq[Attribute],
+    child: LogicalPlan) extends UnaryNode {
+  override def producedAttributes: AttributeSet = AttributeSet(output.drop(child.output.length))
+}
 
 /**
  * A physical plan that evaluates a [[PythonUDF]].
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala
index b08b7e60e130b..d3736d24e5019 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala
@@ -32,8 +32,12 @@ import org.apache.spark.sql.types.{StructField, StructType}
 /**
  * A logical plan that evaluates a [[PythonUDF]]
  */
-case class BatchEvalPython(udfs: Seq[PythonUDF], output: Seq[Attribute], child: LogicalPlan)
-  extends UnaryNode
+case class BatchEvalPython(
+    udfs: Seq[PythonUDF],
+    output: Seq[Attribute],
+    child: LogicalPlan) extends UnaryNode {
+  override def producedAttributes: AttributeSet = AttributeSet(output.drop(child.output.length))
+}
 
 /**
  * A physical plan that evaluates a [[PythonUDF]]
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala
index 90b5325919e96..380c31baa6213 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala
@@ -24,7 +24,7 @@ import org.apache.spark.api.python.PythonEvalType
 import org.apache.spark.sql.AnalysisException
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
-import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Filter, LogicalPlan, Project}
+import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.catalyst.rules.Rule
 
 
@@ -131,8 +131,20 @@ object ExtractPythonUDFs extends Rule[LogicalPlan] with PredicateHelper {
     expressions.flatMap(collectEvaluableUDFs)
   }
 
-  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {
-    case plan: LogicalPlan => extract(plan)
+  def apply(plan: LogicalPlan): LogicalPlan = plan match {
+    // SPARK-26293: A subquery will be rewritten into join later, and will go through this rule
+    // eventually. Here we skip subquery, as Python UDF only needs to be extracted once.
+    case _: Subquery => plan
+
+    case _ => plan transformUp {
+      // A safe guard. `ExtractPythonUDFs` only runs once, so we will not hit `BatchEvalPython` and
+      // `ArrowEvalPython` in the input plan. However if we hit them, we must skip them, as we can't
+      // extract Python UDFs from them.
+      case p: BatchEvalPython => p
+      case p: ArrowEvalPython => p
+
+      case plan: LogicalPlan => extract(plan)
+    }
   }
 
   /**


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 06:22;githubbot;cloud-fan commented on issue #23248: [SPARK-26293][SQL] Cast exception when having python udf in subquery
URL: https://github.com/apache/spark/pull/23248#issuecomment-446086659
 
 
   thanks, merging to master/2.4!

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 06:35;cloud_fan;Issue resolved by pull request 23248
[https://github.com/apache/spark/pull/23248];;;","11/Dec/18 08:32;githubbot;HyukjinKwon commented on issue #23248: [SPARK-26293][SQL] Cast exception when having python udf in subquery
URL: https://github.com/apache/spark/pull/23248#issuecomment-446115111
 
 
   BTW, @cloud-fan, I think it's going to be a considerable conflict against branch-2.4 ... If the conflict is considerable, might better to open a PR.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:36;githubbot;cloud-fan commented on issue #23248: [SPARK-26293][SQL] Cast exception when having python udf in subquery
URL: https://github.com/apache/spark/pull/23248#issuecomment-446116345
 
 
   @HyukjinKwon the conflict is only the test. I just moved the test (without those cleanups) to the giant `tests.py` in 2.4. 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:40;githubbot;HyukjinKwon commented on issue #23248: [SPARK-26293][SQL] Cast exception when having python udf in subquery
URL: https://github.com/apache/spark/pull/23248#issuecomment-446117533
 
 
   Ah, sounds good!

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When zstd compression enabled, Inprogress application in the history server appUI showing finished job as running",SPARK-26283,13202608,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,abhishek.akg,abhishek.akg,05/Dec/18 18:17,02/Oct/19 05:53,13/Jul/23 08:48,09/Dec/18 17:44,2.4.0,3.0.0,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,Web UI,,,0,,,,,"When zstd compression enabled, Inprogress application in the history server appUI showing finished job as running",,abhishek.akg,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29322,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 09 17:44:35 UTC 2018,,,,,,,,,,"0|s017iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/18 18:17;shahid;Thanks. I am working on it.;;;","05/Dec/18 19:10;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23241;;;","05/Dec/18 19:11;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23241;;;","09/Dec/18 17:44;srowen;Issue resolved by pull request 23241
[https://github.com/apache/spark/pull/23241];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duration column of task table should be executor run time instead of real duration,SPARK-26281,13202598,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,Gengliang.Wang,Gengliang.Wang,05/Dec/18 17:43,07/Dec/18 20:35,13/Jul/23 08:48,07/Dec/18 20:35,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Web UI,,,,0,,,,,"In PR https://github.com/apache/spark/pull/23081/ , the duration column is changed to executor run time. The behavior is consistent with the summary metrics table and previous Spark version.

However, after PR https://github.com/apache/spark/pull/21688, the issue can be reproduced again.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 07 20:35:15 UTC 2018,,,,,,,,,,"0|s017go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/18 17:49;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23240;;;","05/Dec/18 20:54;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23160;;;","07/Dec/18 20:35;srowen;Resolved by https://github.com/apache/spark/pull/23160;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V2 Streaming sources cannot be written to V1 sinks,SPARK-26278,13202538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jpolchlo,jpolchlo,05/Dec/18 13:25,14/May/19 12:46,13/Jul/23 08:48,14/May/19 12:46,2.3.2,,,,,,,,,,,,,,,,,,,,,Input/Output,Structured Streaming,,,0,,,,,"Starting from a streaming DataFrame derived from a custom v2 MicroBatch reader, we have
{code:java}
val df: DataFrame = ... 
assert(df.isStreaming)

val outputFormat = ""orc"" // also applies to ""csv"" and ""json"" but not ""console"" 

df.writeStream
  .format(outputFormat)
  .option(""checkpointLocation"", ""/tmp/checkpoints"")
  .option(""path"", ""/tmp/result"")
  .start
{code}
This code fails with the following stack trace:
{code:java}
2018-12-04 08:24:27 ERROR MicroBatchExecution:91 - Query [id = 193f97bf-8064-4658-8aa6-0f481919eafe, runId = e96ed7e5-aaf4-4ef4-a3f3-05fe0b01a715] terminated with error
java.lang.ClassCastException: org.apache.spark.sql.execution.streaming.SerializedOffset cannot be cast to org.apache.spark.sql.sources.v2.reader.streaming.Offset
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$1$$anonfun$apply$9.apply(MicroBatchExecution.scala:405)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$1$$anonfun$apply$9.apply(MicroBatchExecution.scala:390)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    at scala.collection.Iterator$class.foreach(Iterator.scala:893)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)
    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    at org.apache.spark.sql.execution.streaming.StreamProgress.flatMap(StreamProgress.scala:25)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$1.apply(MicroBatchExecution.scala:390)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$1.apply(MicroBatchExecution.scala:390)
    at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:389)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:133)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
    at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:121)
    at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:117)
    at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
    at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189){code}
I'm filing this issue on the suggestion of [~mojodna] who suggests that this problem could be resolved by backporting streaming sinks from spark 2.4.0.",,jpolchlo,mojodna,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 12:46:15 UTC 2019,,,,,,,,,,"0|s0173c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/18 17:43;mojodna;I was thinking specifically of the SerializedOffset / Offset incompatibility referenced in SPARK-25257 and fixed in SPARK-23092 (but just the part that affects v2 source -> v1 sinks).;;;","14/May/19 09:20;uncleGen;[~jpolchlo] Could you please close this jira? This issue has been fixed.;;;","14/May/19 12:46;jpolchlo;Will take you at your word that it's all set, [~uncleGen].  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download page must link to https://www.apache.org/dist/spark for current releases,SPARK-26274,13202483,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,sebb,sebb,05/Dec/18 10:24,03/Mar/19 15:41,13/Jul/23 08:48,03/Mar/19 15:41,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.3,,,,Documentation,,,,0,,,,,"The download page currently uses the archive server:
https://archive.apache.org/dist/spark/...
for all sigs and hashes.
This is fine for archived releases, however current ones must link to the mirror system, i.e.
https://www.apache.org/dist/spark/...

Also, the page does not link directly to the hash or sig.
This makes it very difficult for the user, as they have to choose the correct file.
The download page must link directly to the actual sig or hash.

Ideally do so for the archived releases as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 02 15:59:03 UTC 2019,,,,,,,,,,"0|s016r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/19 15:59;srowen;Just saw this one -- sounds fine to me.
See https://github.com/apache/spark-website/pull/184;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please delete old releases from mirroring system,SPARK-26272,13202479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sebb,sebb,05/Dec/18 10:13,02/Mar/19 19:34,13/Jul/23 08:48,02/Mar/19 19:34,2.3.1,,,,,,,,,,,,,,,,,2.3.3,,,,Deploy,Documentation,Web UI,,0,,,,,"The release notes say 2.3.2 is the latest release for the 2.3.x line.

As such, earlier releases such as 2.3.1 should no longer be hosted on the mirrors.

Please drop https://www.apache.org/dist/spark/spark-2.3.1/ and adjust any remaining download links accordingly",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1449,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 02 19:34:38 UTC 2019,,,,,,,,,,"0|s016q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/19 19:34;srowen;Yep, we do this regularly, and do it alongside releases now, and 2.3.1 was zapped a while ago. 2.3.2 should be removed; not sure why it wasn't with 2.3.3's release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource,SPARK-26269,13202435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ngone51,Ngone51,Ngone51,05/Dec/18 06:09,17/May/20 18:13,13/Jul/23 08:48,21/Dec/18 19:28,2.3.1,2.3.2,2.4.0,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,YARN,,10/Dec/18 00:00,0,,,,,"Currently, YarnAllocator may put a node with a completed container whose exit status is not one of SUCCESS, PREEMPTED, KILLED_EXCEEDED_VMEM, KILLED_EXCEEDED_PMEM into blacklist. Howerver, for other exit status, e.g. KILLED_BY_RESOURCEMANAGER, Yarn do not consider its related nodes shoule be added into blacklist(see YARN's explaination for detail https://github.com/apache/hadoop/blob/228156cfd1b474988bc4fedfbf7edddc87db41e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Apps.java#L273). So, relaxing the current blacklist rule and having the same blacklist behaviour with YARN would maxmize use of cluster resources.

 ",,AK2019,apachespark,githubbot,Ngone51,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 22 02:24:13 UTC 2018,,,,,,,,,,"0|s016gg:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"05/Dec/18 06:22;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/23223;;;","05/Dec/18 06:23;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/23223;;;","10/Dec/18 19:34;githubbot;tgravescs commented on issue #23223: [SPARK-26269][YARN]Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource
URL: https://github.com/apache/spark/pull/23223#issuecomment-445943499
 
 
   ok thanks for trying.  if I get a chance I can try later in the week, but that doesn't have to block this now if someone else has time to review before I get to it. We can always pull it back later.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Dec/18 19:27;githubbot;asfgit closed pull request #23223: [SPARK-26269][YARN]Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource
URL: https://github.com/apache/spark/pull/23223
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala
index 9497530805c1a..e158d96149622 100644
--- a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala
@@ -612,13 +612,23 @@ private[yarn] class YarnAllocator(
             val message = ""Container killed by YARN for exceeding physical memory limits. "" +
               s""$diag Consider boosting ${EXECUTOR_MEMORY_OVERHEAD.key}.""
             (true, message)
-          case _ =>
-            // all the failures which not covered above, like:
-            // disk failure, kill by app master or resource manager, ...
-            allocatorBlacklistTracker.handleResourceAllocationFailure(hostOpt)
-            (true, ""Container marked as failed: "" + containerId + onHostStr +
-              "". Exit status: "" + completedContainer.getExitStatus +
-              "". Diagnostics: "" + completedContainer.getDiagnostics)
+          case other_exit_status =>
+            // SPARK-26269: follow YARN's blacklisting behaviour(see https://github
+            // .com/apache/hadoop/blob/228156cfd1b474988bc4fedfbf7edddc87db41e3/had
+            // oop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/ap
+            // ache/hadoop/yarn/util/Apps.java#L273 for details)
+            if (NOT_APP_AND_SYSTEM_FAULT_EXIT_STATUS.contains(other_exit_status)) {
+              (false, s""Container marked as failed: $containerId$onHostStr"" +
+                s"". Exit status: ${completedContainer.getExitStatus}"" +
+                s"". Diagnostics: ${completedContainer.getDiagnostics}."")
+            } else {
+              // completed container from a bad node
+              allocatorBlacklistTracker.handleResourceAllocationFailure(hostOpt)
+              (true, s""Container from a bad node: $containerId$onHostStr"" +
+                s"". Exit status: ${completedContainer.getExitStatus}"" +
+                s"". Diagnostics: ${completedContainer.getDiagnostics}."")
+            }
+
 
         }
         if (exitCausedByApp) {
@@ -744,4 +754,12 @@ private object YarnAllocator {
   val MEM_REGEX = ""[0-9.]+ [KMG]B""
   val VMEM_EXCEEDED_EXIT_CODE = -103
   val PMEM_EXCEEDED_EXIT_CODE = -104
+
+  val NOT_APP_AND_SYSTEM_FAULT_EXIT_STATUS = Set(
+    ContainerExitStatus.KILLED_BY_RESOURCEMANAGER,
+    ContainerExitStatus.KILLED_BY_APPMASTER,
+    ContainerExitStatus.KILLED_AFTER_APP_COMPLETION,
+    ContainerExitStatus.ABORTED,
+    ContainerExitStatus.DISKS_FAILED
+  )
 }
diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala
index ceac7cda5f8be..268976b629507 100644
--- a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala
@@ -120,7 +120,9 @@ private[spark] class YarnAllocatorBlacklistTracker(
     if (removals.nonEmpty) {
       logInfo(s""removing nodes from YARN application master's blacklist: $removals"")
     }
-    amClient.updateBlacklist(additions.asJava, removals.asJava)
+    if (additions.nonEmpty || removals.nonEmpty) {
+      amClient.updateBlacklist(additions.asJava, removals.asJava)
+    }
     currentBlacklistedYarnNodes = nodesToBlacklist
   }
 
diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala
index aeac68e6ed330..201910731e934 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala
@@ -87,7 +87,7 @@ class YarnAllocatorBlacklistTrackerSuite extends SparkFunSuite with Matchers
     // expired blacklisted nodes (simulating a resource request)
     yarnBlacklistTracker.setSchedulerBlacklistedNodes(Set(""host1"", ""host2""))
     // no change is communicated to YARN regarding the blacklisting
-    verify(amClientMock).updateBlacklist(Collections.emptyList(), Collections.emptyList())
+    verify(amClientMock, times(0)).updateBlacklist(Collections.emptyList(), Collections.emptyList())
   }
 
   test(""combining scheduler and allocation blacklist"") {
diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
index b61e7df4420ef..53a538dc1de29 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
@@ -17,6 +17,8 @@
 
 package org.apache.spark.deploy.yarn
 
+import java.util.Collections
+
 import scala.collection.JavaConverters._
 
 import org.apache.hadoop.conf.Configuration
@@ -114,13 +116,29 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with BeforeAndAfter
       clock)
   }
 
-  def createContainer(host: String, resource: Resource = containerResource): Container = {
-    val containerId = ContainerId.newContainerId(appAttemptId, containerNum)
+  def createContainer(
+      host: String,
+      containerNumber: Int = containerNum,
+      resource: Resource = containerResource): Container = {
+    val  containerId: ContainerId = ContainerId.newContainerId(appAttemptId, containerNum)
     containerNum += 1
     val nodeId = NodeId.newInstance(host, 1000)
     Container.newInstance(containerId, nodeId, """", resource, RM_REQUEST_PRIORITY, null)
   }
 
+  def createContainers(hosts: Seq[String], containerIds: Seq[Int]): Seq[Container] = {
+    hosts.zip(containerIds).map{case (host, id) => createContainer(host, id)}
+  }
+
+  def createContainerStatus(
+      containerId: ContainerId,
+      exitStatus: Int,
+      containerState: ContainerState = ContainerState.COMPLETE,
+      diagnostics: String = ""diagnostics""): ContainerStatus = {
+    ContainerStatus.newInstance(containerId, containerState, diagnostics, exitStatus)
+  }
+
+
   test(""single container allocated"") {
     // request a single container and receive it
     val handler = createAllocator(1)
@@ -148,7 +166,7 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with BeforeAndAfter
       Map(YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + ""gpu"" -> ""2G""))
 
     handler.updateResourceRequests()
-    val container = createContainer(""host1"", handler.resource)
+    val container = createContainer(""host1"", resource = handler.resource)
     handler.handleAllocatedContainers(Array(container))
 
     // get amount of memory and vcores from resource, so effectively skipping their validation
@@ -417,4 +435,55 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with BeforeAndAfter
     clock.advance(50 * 1000L)
     handler.getNumExecutorsFailed should be (0)
   }
+
+  test(""SPARK-26269: YarnAllocator should have same blacklist behaviour with YARN"") {
+    val rmClientSpy = spy(rmClient)
+    val maxExecutors = 11
+
+    val handler = createAllocator(
+      maxExecutors,
+      rmClientSpy,
+      Map(
+        ""spark.yarn.blacklist.executor.launch.blacklisting.enabled"" -> ""true"",
+        ""spark.blacklist.application.maxFailedExecutorsPerNode"" -> ""0""))
+    handler.updateResourceRequests()
+
+    val hosts = (0 until maxExecutors).map(i => s""host$i"")
+    val ids = 0 to maxExecutors
+    val containers = createContainers(hosts, ids)
+
+    val nonBlacklistedStatuses = Seq(
+      ContainerExitStatus.SUCCESS,
+      ContainerExitStatus.PREEMPTED,
+      ContainerExitStatus.KILLED_EXCEEDED_VMEM,
+      ContainerExitStatus.KILLED_EXCEEDED_PMEM,
+      ContainerExitStatus.KILLED_BY_RESOURCEMANAGER,
+      ContainerExitStatus.KILLED_BY_APPMASTER,
+      ContainerExitStatus.KILLED_AFTER_APP_COMPLETION,
+      ContainerExitStatus.ABORTED,
+      ContainerExitStatus.DISKS_FAILED)
+
+    val nonBlacklistedContainerStatuses = nonBlacklistedStatuses.zipWithIndex.map {
+      case (exitStatus, idx) => createContainerStatus(containers(idx).getId, exitStatus)
+    }
+
+    val BLACKLISTED_EXIT_CODE = 1
+    val blacklistedStatuses = Seq(ContainerExitStatus.INVALID, BLACKLISTED_EXIT_CODE)
+
+    val blacklistedContainerStatuses = blacklistedStatuses.zip(9 until maxExecutors).map {
+      case (exitStatus, idx) => createContainerStatus(containers(idx).getId, exitStatus)
+    }
+
+    handler.handleAllocatedContainers(containers.slice(0, 9))
+    handler.processCompletedContainers(nonBlacklistedContainerStatuses)
+    verify(rmClientSpy, never())
+      .updateBlacklist(hosts.slice(0, 9).asJava, Collections.emptyList())
+
+    handler.handleAllocatedContainers(containers.slice(9, 11))
+    handler.processCompletedContainers(blacklistedContainerStatuses)
+    verify(rmClientSpy)
+      .updateBlacklist(hosts.slice(9, 10).asJava, Collections.emptyList())
+    verify(rmClientSpy)
+      .updateBlacklist(hosts.slice(10, 11).asJava, Collections.emptyList())
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Dec/18 01:29;githubbot;Ngone51 opened a new pull request #23366: [SPARK-26269][YARN][BRANCH-2.4]Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource
URL: https://github.com/apache/spark/pull/23366
 
 
   ## What changes were proposed in this pull request?
   
   As I mentioned in jira [SPARK-26269](https://issues.apache.org/jira/browse/SPARK-26269), in order to maxmize the use of cluster resource,  this pr try to make `YarnAllocator` have the same blacklist behaviour with YARN.
   
   ## How was this patch tested?
   
   Added.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Dec/18 01:42;githubbot;Ngone51 closed pull request #23366: [SPARK-26269][YARN][BRANCH-2.4]Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource
URL: https://github.com/apache/spark/pull/23366
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/.gitignore b/.gitignore
index 19db7ac277944..e4c44d0590d59 100644
--- a/.gitignore
+++ b/.gitignore
@@ -77,7 +77,6 @@ target/
 unit-tests.log
 work/
 docs/.jekyll-metadata
-*.crc
 
 # For Hive
 TempStatsStore/
diff --git a/.travis.yml b/.travis.yml
deleted file mode 100644
index 05b94adeeb93b..0000000000000
--- a/.travis.yml
+++ /dev/null
@@ -1,50 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements. See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the ""License""); you may not use this file except in compliance with
-# the License. You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an ""AS IS"" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# Spark provides this Travis CI configuration file to help contributors
-# check Scala/Java style conformance and JDK7/8 compilation easily
-# during their preparing pull requests.
-#   - Scalastyle is executed during `maven install` implicitly.
-#   - Java Checkstyle is executed by `lint-java`.
-# See the related discussion here.
-# https://github.com/apache/spark/pull/12980
-
-# 1. Choose OS (Ubuntu 14.04.3 LTS Server Edition 64bit, ~2 CORE, 7.5GB RAM)
-sudo: required
-dist: trusty
-
-# 2. Choose language and target JDKs for parallel builds.
-language: java
-jdk:
-  - oraclejdk8
-
-# 3. Setup cache directory for SBT and Maven.
-cache:
-  directories:
-  - $HOME/.sbt
-  - $HOME/.m2
-
-# 4. Turn off notifications.
-notifications:
-  email: false
-
-# 5. Run maven install before running lint-java.
-install:
-  - export MAVEN_SKIP_RC=1
-  - build/mvn -T 4 -q -DskipTests -Pkubernetes -Pmesos -Pyarn -Pkinesis-asl -Phive -Phive-thriftserver install
-
-# 6. Run lint-java.
-script:
-  - dev/lint-java
diff --git a/R/WINDOWS.md b/R/WINDOWS.md
index da668a69b8679..33a4c850cfdac 100644
--- a/R/WINDOWS.md
+++ b/R/WINDOWS.md
@@ -3,7 +3,7 @@
 To build SparkR on Windows, the following steps are required
 
 1. Install R (>= 3.1) and [Rtools](http://cran.r-project.org/bin/windows/Rtools/). Make sure to
-include Rtools and R in `PATH`.
+include Rtools and R in `PATH`. Note that support for R prior to version 3.4 is deprecated as of Spark 3.0.0.
 
 2. Install
 [JDK8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) and set
diff --git a/R/pkg/DESCRIPTION b/R/pkg/DESCRIPTION
index f52d785e05cdd..736da46eaa8d3 100644
--- a/R/pkg/DESCRIPTION
+++ b/R/pkg/DESCRIPTION
@@ -1,6 +1,6 @@
 Package: SparkR
 Type: Package
-Version: 2.4.0
+Version: 3.0.0
 Title: R Frontend for Apache Spark
 Description: Provides an R Frontend for Apache Spark.
 Authors@R: c(person(""Shivaram"", ""Venkataraman"", role = c(""aut"", ""cre""),
@@ -15,7 +15,7 @@ URL: http://www.apache.org/ http://spark.apache.org/
 BugReports: http://spark.apache.org/contributing.html
 SystemRequirements: Java (== 8)
 Depends:
-    R (>= 3.0),
+    R (>= 3.1),
     methods
 Suggests:
     knitr,
diff --git a/R/pkg/NAMESPACE b/R/pkg/NAMESPACE
index 96ff389faf4a0..1f8ba0bcf1cf5 100644
--- a/R/pkg/NAMESPACE
+++ b/R/pkg/NAMESPACE
@@ -28,9 +28,8 @@ importFrom(""utils"", ""download.file"", ""object.size"", ""packageVersion"", ""tail"", ""u
 
 # S3 methods exported
 export(""sparkR.session"")
-export(""sparkR.init"")
-export(""sparkR.stop"")
 export(""sparkR.session.stop"")
+export(""sparkR.stop"")
 export(""sparkR.conf"")
 export(""sparkR.version"")
 export(""sparkR.uiWebUrl"")
@@ -42,9 +41,6 @@ export(""sparkR.callJStatic"")
 
 export(""install.spark"")
 
-export(""sparkRSQL.init"",
-       ""sparkRHive.init"")
-
 # MLlib integration
 exportMethods(""glm"",
               ""spark.glm"",
@@ -70,7 +66,8 @@ exportMethods(""glm"",
               ""spark.svmLinear"",
               ""spark.fpGrowth"",
               ""spark.freqItemsets"",
-              ""spark.associationRules"")
+              ""spark.associationRules"",
+              ""spark.findFrequentSequentialPatterns"")
 
 # Job group lifecycle management methods
 export(""setJobGroup"",
@@ -150,7 +147,6 @@ exportMethods(""arrange"",
               ""printSchema"",
               ""randomSplit"",
               ""rbind"",
-              ""registerTempTable"",
               ""rename"",
               ""repartition"",
               ""repartitionByRange"",
@@ -158,7 +154,6 @@ exportMethods(""arrange"",
               ""sample"",
               ""sample_frac"",
               ""sampleBy"",
-              ""saveAsParquetFile"",
               ""saveAsTable"",
               ""saveDF"",
               ""schema"",
@@ -200,6 +195,7 @@ exportMethods(""%<=>%"",
               ""acos"",
               ""add_months"",
               ""alias"",
+              ""approx_count_distinct"",
               ""approxCountDistinct"",
               ""approxQuantile"",
               ""array_contains"",
@@ -258,6 +254,7 @@ exportMethods(""%<=>%"",
               ""dayofweek"",
               ""dayofyear"",
               ""decode"",
+              ""degrees"",
               ""dense_rank"",
               ""desc"",
               ""element_at"",
@@ -274,6 +271,7 @@ exportMethods(""%<=>%"",
               ""floor"",
               ""format_number"",
               ""format_string"",
+              ""from_csv"",
               ""from_json"",
               ""from_unixtime"",
               ""from_utc_timestamp"",
@@ -339,6 +337,7 @@ exportMethods(""%<=>%"",
               ""posexplode"",
               ""posexplode_outer"",
               ""quarter"",
+              ""radians"",
               ""rand"",
               ""randn"",
               ""rank"",
@@ -352,6 +351,8 @@ exportMethods(""%<=>%"",
               ""row_number"",
               ""rpad"",
               ""rtrim"",
+              ""schema_of_csv"",
+              ""schema_of_json"",
               ""second"",
               ""sha1"",
               ""sha2"",
@@ -385,6 +386,7 @@ exportMethods(""%<=>%"",
               ""tanh"",
               ""toDegrees"",
               ""toRadians"",
+              ""to_csv"",
               ""to_date"",
               ""to_json"",
               ""to_timestamp"",
@@ -413,18 +415,14 @@ export(""as.DataFrame"",
        ""cacheTable"",
        ""clearCache"",
        ""createDataFrame"",
-       ""createExternalTable"",
        ""createTable"",
        ""currentDatabase"",
-       ""dropTempTable"",
        ""dropTempView"",
-       ""jsonFile"",
        ""listColumns"",
        ""listDatabases"",
        ""listFunctions"",
        ""listTables"",
        ""loadDF"",
-       ""parquetFile"",
        ""read.df"",
        ""read.jdbc"",
        ""read.json"",
diff --git a/R/pkg/R/DataFrame.R b/R/pkg/R/DataFrame.R
index 4f2d4c7c002d4..24ed449f2a7d1 100644
--- a/R/pkg/R/DataFrame.R
+++ b/R/pkg/R/DataFrame.R
@@ -226,7 +226,9 @@ setMethod(""showDF"",
 
 #' show
 #'
-#' Print class and type information of a Spark object.
+#' If eager evaluation is enabled and the Spark object is a SparkDataFrame, evaluate the
+#' SparkDataFrame and print top rows of the SparkDataFrame, otherwise, print the class
+#' and type information of the Spark object.
 #'
 #' @param object a Spark object. Can be a SparkDataFrame, Column, GroupedData, WindowSpec.
 #'
@@ -244,11 +246,33 @@ setMethod(""showDF"",
 #' @note show(SparkDataFrame) since 1.4.0
 setMethod(""show"", ""SparkDataFrame"",
           function(object) {
-            cols <- lapply(dtypes(object), function(l) {
-              paste(l, collapse = "":"")
-            })
-            s <- paste(cols, collapse = "", "")
-            cat(paste(class(object), ""["", s, ""]\n"", sep = """"))
+            allConf <- sparkR.conf()
+            prop <- allConf[[""spark.sql.repl.eagerEval.enabled""]]
+            if (!is.null(prop) && identical(prop, ""true"")) {
+              argsList <- list()
+              argsList$x <- object
+              prop <- allConf[[""spark.sql.repl.eagerEval.maxNumRows""]]
+              if (!is.null(prop)) {
+                numRows <- as.integer(prop)
+                if (numRows > 0) {
+                  argsList$numRows <- numRows
+                }
+              }
+              prop <- allConf[[""spark.sql.repl.eagerEval.truncate""]]
+              if (!is.null(prop)) {
+                truncate <- as.integer(prop)
+                if (truncate > 0) {
+                  argsList$truncate <- truncate
+                }
+              }
+              do.call(showDF, argsList)
+            } else {
+              cols <- lapply(dtypes(object), function(l) {
+                paste(l, collapse = "":"")
+              })
+              s <- paste(cols, collapse = "", "")
+              cat(paste(class(object), ""["", s, ""]\n"", sep = """"))
+            }
           })
 
 #' DataTypes
@@ -497,33 +521,6 @@ setMethod(""createOrReplaceTempView"",
               invisible(callJMethod(x@sdf, ""createOrReplaceTempView"", viewName))
           })
 
-#' (Deprecated) Register Temporary Table
-#'
-#' Registers a SparkDataFrame as a Temporary Table in the SparkSession
-#' @param x A SparkDataFrame
-#' @param tableName A character vector containing the name of the table
-#'
-#' @family SparkDataFrame functions
-#' @seealso \link{createOrReplaceTempView}
-#' @rdname registerTempTable-deprecated
-#' @name registerTempTable
-#' @aliases registerTempTable,SparkDataFrame,character-method
-#' @examples
-#'\dontrun{
-#' sparkR.session()
-#' path <- ""path/to/file.json""
-#' df <- read.json(path)
-#' registerTempTable(df, ""json_df"")
-#' new_df <- sql(""SELECT * FROM json_df"")
-#'}
-#' @note registerTempTable since 1.4.0
-setMethod(""registerTempTable"",
-          signature(x = ""SparkDataFrame"", tableName = ""character""),
-          function(x, tableName) {
-              .Deprecated(""createOrReplaceTempView"")
-              invisible(callJMethod(x@sdf, ""createOrReplaceTempView"", tableName))
-          })
-
 #' insertInto
 #'
 #' Insert the contents of a SparkDataFrame into a table registered in the current SparkSession.
@@ -769,6 +766,13 @@ setMethod(""repartition"",
 #'  \item{2.} {Return a new SparkDataFrame range partitioned by the given column(s),
 #'                      using \code{spark.sql.shuffle.partitions} as number of partitions.}
 #'}
+#' At least one partition-by expression must be specified.
+#' When no explicit sort order is specified, ""ascending nulls first"" is assumed.
+#'
+#' Note that due to performance reasons this method uses sampling to estimate the ranges.
+#' Hence, the output may not be consistent, since sampling can return different values.
+#' The sample size can be controlled by the config
+#' \code{spark.sql.execution.rangeExchange.sampleSizePerPartition}.
 #'
 #' @param x a SparkDataFrame.
 #' @param numPartitions the number of partitions to use.
@@ -823,7 +827,6 @@ setMethod(""repartitionByRange"",
 #' toJSON
 #'
 #' Converts a SparkDataFrame into a SparkDataFrame of JSON string.
-#'
 #' Each row is turned into a JSON document with columns as different fields.
 #' The returned SparkDataFrame has a single character column with the name \code{value}
 #'
@@ -933,7 +936,6 @@ setMethod(""write.orc"",
 #' path <- ""path/to/file.json""
 #' df <- read.json(path)
 #' write.parquet(df, ""/tmp/sparkr-tmp1/"")
-#' saveAsParquetFile(df, ""/tmp/sparkr-tmp2/"")
 #'}
 #' @note write.parquet since 1.6.0
 setMethod(""write.parquet"",
@@ -944,17 +946,6 @@ setMethod(""write.parquet"",
             invisible(handledCallJMethod(write, ""parquet"", path))
           })
 
-#' @rdname write.parquet
-#' @name saveAsParquetFile
-#' @aliases saveAsParquetFile,SparkDataFrame,character-method
-#' @note saveAsParquetFile since 1.4.0
-setMethod(""saveAsParquetFile"",
-          signature(x = ""SparkDataFrame"", path = ""character""),
-          function(x, path) {
-            .Deprecated(""write.parquet"")
-            write.parquet(x, path)
-          })
-
 #' Save the content of SparkDataFrame in a text file at the specified path.
 #'
 #' Save the content of the SparkDataFrame in a text file at the specified path.
@@ -2739,15 +2730,29 @@ setMethod(""union"",
             dataFrame(unioned)
           })
 
-#' unionAll is deprecated - use union instead
-#' @rdname union
-#' @name unionAll
+#' Return a new SparkDataFrame containing the union of rows.
+#'
+#' This is an alias for \code{union}.
+#'
+#' @param x a SparkDataFrame.
+#' @param y a SparkDataFrame.
+#' @return A SparkDataFrame containing the result of the unionAll operation.
+#' @family SparkDataFrame functions
 #' @aliases unionAll,SparkDataFrame,SparkDataFrame-method
+#' @rdname unionAll
+#' @name unionAll
+#' @seealso \link{union}
+#' @examples
+#'\dontrun{
+#' sparkR.session()
+#' df1 <- read.json(path)
+#' df2 <- read.json(path2)
+#' unionAllDF <- unionAll(df1, df2)
+#' }
 #' @note unionAll since 1.4.0
 setMethod(""unionAll"",
           signature(x = ""SparkDataFrame"", y = ""SparkDataFrame""),
           function(x, y) {
-            .Deprecated(""union"")
             union(x, y)
           })
 
@@ -2955,6 +2960,9 @@ setMethod(""exceptAll"",
 #' @param source a name for external data source.
 #' @param mode one of 'append', 'overwrite', 'error', 'errorifexists', 'ignore'
 #'             save mode (it is 'error' by default)
+#' @param partitionBy a name or a list of names of columns to partition the output by on the file
+#'                    system. If specified, the output is laid out on the file system similar
+#'                    to Hive's partitioning scheme.
 #' @param ... additional argument(s) passed to the method.
 #'
 #' @family SparkDataFrame functions
@@ -2966,13 +2974,13 @@ setMethod(""exceptAll"",
 #' sparkR.session()
 #' path <- ""path/to/file.json""
 #' df <- read.json(path)
-#' write.df(df, ""myfile"", ""parquet"", ""overwrite"")
+#' write.df(df, ""myfile"", ""parquet"", ""overwrite"", partitionBy = c(""col1"", ""col2""))
 #' saveDF(df, parquetPath2, ""parquet"", mode = ""append"", mergeSchema = TRUE)
 #' }
 #' @note write.df since 1.4.0
 setMethod(""write.df"",
           signature(df = ""SparkDataFrame""),
-          function(df, path = NULL, source = NULL, mode = ""error"", ...) {
+          function(df, path = NULL, source = NULL, mode = ""error"", partitionBy = NULL, ...) {
             if (!is.null(path) && !is.character(path)) {
               stop(""path should be character, NULL or omitted."")
             }
@@ -2986,8 +2994,18 @@ setMethod(""write.df"",
             if (is.null(source)) {
               source <- getDefaultSqlSource()
             }
+            cols <- NULL
+            if (!is.null(partitionBy)) {
+              if (!all(sapply(partitionBy, function(c) is.character(c)))) {
+                stop(""All partitionBy column names should be characters."")
+              }
+              cols <- as.list(partitionBy)
+            }
             write <- callJMethod(df@sdf, ""write"")
             write <- callJMethod(write, ""format"", source)
+            if (!is.null(cols)) {
+              write <- callJMethod(write, ""partitionBy"", cols)
+            }
             write <- setWriteOptions(write, path = path, mode = mode, ...)
             write <- handledCallJMethod(write, ""save"")
           })
@@ -3986,7 +4004,17 @@ setMethod(""hint"",
           signature(x = ""SparkDataFrame"", name = ""character""),
           function(x, name, ...) {
             parameters <- list(...)
-            stopifnot(all(sapply(parameters, is.character)))
+            if (!all(sapply(parameters, function(y) {
+              if (is.character(y) || is.numeric(y)) {
+                TRUE
+              } else if (is.list(y)) {
+                all(sapply(y, function(z) { is.character(z) || is.numeric(z) }))
+              } else {
+                FALSE
+              }
+            }))) {
+              stop(""sql hint should be character, numeric, or list with character or numeric."")
+            }
             jdf <- callJMethod(x@sdf, ""hint"", name, parameters)
             dataFrame(jdf)
           })
diff --git a/R/pkg/R/SQLContext.R b/R/pkg/R/SQLContext.R
index c819a7d14ae98..afcdd6faa849d 100644
--- a/R/pkg/R/SQLContext.R
+++ b/R/pkg/R/SQLContext.R
@@ -37,37 +37,6 @@ getInternalType <- function(x) {
          stop(paste(""Unsupported type for SparkDataFrame:"", class(x))))
 }
 
-#' Temporary function to reroute old S3 Method call to new
-#' This function is specifically implemented to remove SQLContext from the parameter list.
-#' It determines the target to route the call by checking the parent of this callsite (say 'func').
-#' The target should be called 'func.default'.
-#' We need to check the class of x to ensure it is SQLContext/HiveContext before dispatching.
-#' @param newFuncSig name of the function the user should call instead in the deprecation message
-#' @param x the first parameter of the original call
-#' @param ... the rest of parameter to pass along
-#' @return whatever the target returns
-#' @noRd
-dispatchFunc <- function(newFuncSig, x, ...) {
-  # When called with SparkR::createDataFrame, sys.call()[[1]] returns c(::, SparkR, createDataFrame)
-  callsite <- as.character(sys.call(sys.parent())[[1]])
-  funcName <- callsite[[length(callsite)]]
-  f <- get(paste0(funcName, "".default""))
-  # Strip sqlContext from list of parameters and then pass the rest along.
-  contextNames <- c(""org.apache.spark.sql.SQLContext"",
-                    ""org.apache.spark.sql.hive.HiveContext"",
-                    ""org.apache.spark.sql.hive.test.TestHiveContext"",
-                    ""org.apache.spark.sql.SparkSession"")
-  if (missing(x) && length(list(...)) == 0) {
-    f()
-  } else if (class(x) == ""jobj"" &&
-            any(grepl(paste(contextNames, collapse = ""|""), getClassName.jobj(x)))) {
-    .Deprecated(newFuncSig, old = paste0(funcName, ""(sqlContext...)""))
-    f(...)
-  } else {
-    f(x, ...)
-  }
-}
-
 #' return the SparkSession
 #' @noRd
 getSparkSession <- function() {
@@ -198,11 +167,10 @@ getDefaultSqlSource <- function() {
 #' df4 <- createDataFrame(cars, numPartitions = 2)
 #' }
 #' @name createDataFrame
-#' @method createDataFrame default
 #' @note createDataFrame since 1.4.0
 # TODO(davies): support sampling and infer type from NA
-createDataFrame.default <- function(data, schema = NULL, samplingRatio = 1.0,
-                                    numPartitions = NULL) {
+createDataFrame <- function(data, schema = NULL, samplingRatio = 1.0,
+                            numPartitions = NULL) {
   sparkSession <- getSparkSession()
 
   if (is.data.frame(data)) {
@@ -285,31 +253,18 @@ createDataFrame.default <- function(data, schema = NULL, samplingRatio = 1.0,
   dataFrame(sdf)
 }
 
-createDataFrame <- function(x, ...) {
-  dispatchFunc(""createDataFrame(data, schema = NULL)"", x, ...)
-}
-
 #' @rdname createDataFrame
 #' @aliases createDataFrame
-#' @method as.DataFrame default
 #' @note as.DataFrame since 1.6.0
-as.DataFrame.default <- function(data, schema = NULL, samplingRatio = 1.0, numPartitions = NULL) {
+as.DataFrame <- function(data, schema = NULL, samplingRatio = 1.0, numPartitions = NULL) {
   createDataFrame(data, schema, samplingRatio, numPartitions)
 }
 
-#' @param ... additional argument(s).
-#' @rdname createDataFrame
-#' @aliases as.DataFrame
-as.DataFrame <- function(data, ...) {
-  dispatchFunc(""as.DataFrame(data, schema = NULL)"", data, ...)
-}
-
 #' toDF
 #'
 #' Converts an RDD to a SparkDataFrame by infer the types.
 #'
 #' @param x An RDD
-#'
 #' @rdname SparkDataFrame
 #' @noRd
 #' @examples
@@ -343,12 +298,10 @@ setMethod(""toDF"", signature(x = ""RDD""),
 #' path <- ""path/to/file.json""
 #' df <- read.json(path)
 #' df <- read.json(path, multiLine = TRUE)
-#' df <- jsonFile(path)
 #' }
 #' @name read.json
-#' @method read.json default
 #' @note read.json since 1.6.0
-read.json.default <- function(path, ...) {
+read.json <- function(path, ...) {
   sparkSession <- getSparkSession()
   options <- varargsToStrEnv(...)
   # Allow the user to have a more flexible definition of the text file path
@@ -359,55 +312,6 @@ read.json.default <- function(path, ...) {
   dataFrame(sdf)
 }
 
-read.json <- function(x, ...) {
-  dispatchFunc(""read.json(path)"", x, ...)
-}
-
-#' @rdname read.json
-#' @name jsonFile
-#' @method jsonFile default
-#' @note jsonFile since 1.4.0
-jsonFile.default <- function(path) {
-  .Deprecated(""read.json"")
-  read.json(path)
-}
-
-jsonFile <- function(x, ...) {
-  dispatchFunc(""jsonFile(path)"", x, ...)
-}
-
-#' JSON RDD
-#'
-#' Loads an RDD storing one JSON object per string as a SparkDataFrame.
-#'
-#' @param sqlContext SQLContext to use
-#' @param rdd An RDD of JSON string
-#' @param schema A StructType object to use as schema
-#' @param samplingRatio The ratio of simpling used to infer the schema
-#' @return A SparkDataFrame
-#' @noRd
-#' @examples
-#'\dontrun{
-#' sparkR.session()
-#' rdd <- texFile(sc, ""path/to/json"")
-#' df <- jsonRDD(sqlContext, rdd)
-#'}
-
-# TODO: remove - this method is no longer exported
-# TODO: support schema
-jsonRDD <- function(sqlContext, rdd, schema = NULL, samplingRatio = 1.0) {
-  .Deprecated(""read.json"")
-  rdd <- serializeToString(rdd)
-  if (is.null(schema)) {
-    read <- callJMethod(sqlContext, ""read"")
-    # samplingRatio is deprecated
-    sdf <- callJMethod(read, ""json"", callJMethod(getJRDD(rdd), ""rdd""))
-    dataFrame(sdf)
-  } else {
-    stop(""not implemented"")
-  }
-}
-
 #' Create a SparkDataFrame from an ORC file.
 #'
 #' Loads an ORC file, returning the result as a SparkDataFrame.
@@ -434,12 +338,12 @@ read.orc <- function(path, ...) {
 #' Loads a Parquet file, returning the result as a SparkDataFrame.
 #'
 #' @param path path of file to read. A vector of multiple paths is allowed.
+#' @param ... additional data source specific named properties.
 #' @return SparkDataFrame
 #' @rdname read.parquet
 #' @name read.parquet
-#' @method read.parquet default
 #' @note read.parquet since 1.6.0
-read.parquet.default <- function(path, ...) {
+read.parquet <- function(path, ...) {
   sparkSession <- getSparkSession()
   options <- varargsToStrEnv(...)
   # Allow the user to have a more flexible definition of the Parquet file path
@@ -450,24 +354,6 @@ read.parquet.default <- function(path, ...) {
   dataFrame(sdf)
 }
 
-read.parquet <- function(x, ...) {
-  dispatchFunc(""read.parquet(...)"", x, ...)
-}
-
-#' @param ... argument(s) passed to the method.
-#' @rdname read.parquet
-#' @name parquetFile
-#' @method parquetFile default
-#' @note parquetFile since 1.4.0
-parquetFile.default <- function(...) {
-  .Deprecated(""read.parquet"")
-  read.parquet(unlist(list(...)))
-}
-
-parquetFile <- function(x, ...) {
-  dispatchFunc(""parquetFile(...)"", x, ...)
-}
-
 #' Create a SparkDataFrame from a text file.
 #'
 #' Loads text files and returns a SparkDataFrame whose schema starts with
@@ -487,9 +373,8 @@ parquetFile <- function(x, ...) {
 #' df <- read.text(path)
 #' }
 #' @name read.text
-#' @method read.text default
 #' @note read.text since 1.6.1
-read.text.default <- function(path, ...) {
+read.text <- function(path, ...) {
   sparkSession <- getSparkSession()
   options <- varargsToStrEnv(...)
   # Allow the user to have a more flexible definition of the text file path
@@ -500,10 +385,6 @@ read.text.default <- function(path, ...) {
   dataFrame(sdf)
 }
 
-read.text <- function(x, ...) {
-  dispatchFunc(""read.text(path)"", x, ...)
-}
-
 #' SQL Query
 #'
 #' Executes a SQL query using Spark, returning the result as a SparkDataFrame.
@@ -520,18 +401,13 @@ read.text <- function(x, ...) {
 #' new_df <- sql(""SELECT * FROM table"")
 #' }
 #' @name sql
-#' @method sql default
 #' @note sql since 1.4.0
-sql.default <- function(sqlQuery) {
+sql <- function(sqlQuery) {
   sparkSession <- getSparkSession()
   sdf <- callJMethod(sparkSession, ""sql"", sqlQuery)
   dataFrame(sdf)
 }
 
-sql <- function(x, ...) {
-  dispatchFunc(""sql(sqlQuery)"", x, ...)
-}
-
 #' Create a SparkDataFrame from a SparkSQL table or view
 #'
 #' Returns the specified table or view as a SparkDataFrame. The table or view must already exist or
@@ -590,9 +466,8 @@ tableToDF <- function(tableName) {
 #' df4 <- read.df(mapTypeJsonPath, ""json"", stringSchema, multiLine = TRUE)
 #' }
 #' @name read.df
-#' @method read.df default
 #' @note read.df since 1.4.0
-read.df.default <- function(path = NULL, source = NULL, schema = NULL, na.strings = ""NA"", ...) {
+read.df <- function(path = NULL, source = NULL, schema = NULL, na.strings = ""NA"", ...) {
   if (!is.null(path) && !is.character(path)) {
     stop(""path should be character, NULL or omitted."")
   }
@@ -627,22 +502,13 @@ read.df.default <- function(path = NULL, source = NULL, schema = NULL, na.string
   dataFrame(sdf)
 }
 
-read.df <- function(x = NULL, ...) {
-  dispatchFunc(""read.df(path = NULL, source = NULL, schema = NULL, ...)"", x, ...)
-}
-
 #' @rdname read.df
 #' @name loadDF
-#' @method loadDF default
 #' @note loadDF since 1.6.0
-loadDF.default <- function(path = NULL, source = NULL, schema = NULL, ...) {
+loadDF <- function(path = NULL, source = NULL, schema = NULL, ...) {
   read.df(path, source, schema, ...)
 }
 
-loadDF <- function(x = NULL, ...) {
-  dispatchFunc(""loadDF(path = NULL, source = NULL, schema = NULL, ...)"", x, ...)
-}
-
 #' Create a SparkDataFrame representing the database table accessible via JDBC URL
 #'
 #' Additional JDBC database connection properties can be set (...)
diff --git a/R/pkg/R/catalog.R b/R/pkg/R/catalog.R
index baf4d861fcf86..7641f8a7a0432 100644
--- a/R/pkg/R/catalog.R
+++ b/R/pkg/R/catalog.R
@@ -17,40 +17,6 @@
 
 # catalog.R: SparkSession catalog functions
 
-#' (Deprecated) Create an external table
-#'
-#' Creates an external table based on the dataset in a data source,
-#' Returns a SparkDataFrame associated with the external table.
-#'
-#' The data source is specified by the \code{source} and a set of options(...).
-#' If \code{source} is not specified, the default data source configured by
-#' ""spark.sql.sources.default"" will be used.
-#'
-#' @param tableName a name of the table.
-#' @param path the path of files to load.
-#' @param source the name of external data source.
-#' @param schema the schema of the data required for some data sources.
-#' @param ... additional argument(s) passed to the method.
-#' @return A SparkDataFrame.
-#' @rdname createExternalTable-deprecated
-#' @seealso \link{createTable}
-#' @examples
-#'\dontrun{
-#' sparkR.session()
-#' df <- createExternalTable(""myjson"", path=""path/to/json"", source=""json"", schema)
-#' }
-#' @name createExternalTable
-#' @method createExternalTable default
-#' @note createExternalTable since 1.4.0
-createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {
-  .Deprecated(""createTable"", old = ""createExternalTable"")
-  createTable(tableName, path, source, schema, ...)
-}
-
-createExternalTable <- function(x, ...) {
-  dispatchFunc(""createExternalTable(tableName, path = NULL, source = NULL, ...)"", x, ...)
-}
-
 #' Creates a table based on the dataset in a data source
 #'
 #' Creates a table based on the dataset in a data source. Returns a SparkDataFrame associated with
@@ -69,7 +35,6 @@ createExternalTable <- function(x, ...) {
 #' @param ... additional named parameters as options for the data source.
 #' @return A SparkDataFrame.
 #' @rdname createTable
-#' @seealso \link{createExternalTable}
 #' @examples
 #'\dontrun{
 #' sparkR.session()
@@ -117,18 +82,13 @@ createTable <- function(tableName, path = NULL, source = NULL, schema = NULL, ..
 #' cacheTable(""table"")
 #' }
 #' @name cacheTable
-#' @method cacheTable default
 #' @note cacheTable since 1.4.0
-cacheTable.default <- function(tableName) {
+cacheTable <- function(tableName) {
   sparkSession <- getSparkSession()
   catalog <- callJMethod(sparkSession, ""catalog"")
   invisible(handledCallJMethod(catalog, ""cacheTable"", tableName))
 }
 
-cacheTable <- function(x, ...) {
-  dispatchFunc(""cacheTable(tableName)"", x, ...)
-}
-
 #' Uncache Table
 #'
 #' Removes the specified table from the in-memory cache.
@@ -146,18 +106,13 @@ cacheTable <- function(x, ...) {
 #' uncacheTable(""table"")
 #' }
 #' @name uncacheTable
-#' @method uncacheTable default
 #' @note uncacheTable since 1.4.0
-uncacheTable.default <- function(tableName) {
+uncacheTable <- function(tableName) {
   sparkSession <- getSparkSession()
   catalog <- callJMethod(sparkSession, ""catalog"")
   invisible(handledCallJMethod(catalog, ""uncacheTable"", tableName))
 }
 
-uncacheTable <- function(x, ...) {
-  dispatchFunc(""uncacheTable(tableName)"", x, ...)
-}
-
 #' Clear Cache
 #'
 #' Removes all cached tables from the in-memory cache.
@@ -168,48 +123,13 @@ uncacheTable <- function(x, ...) {
 #' clearCache()
 #' }
 #' @name clearCache
-#' @method clearCache default
 #' @note clearCache since 1.4.0
-clearCache.default <- function() {
+clearCache <- function() {
   sparkSession <- getSparkSession()
   catalog <- callJMethod(sparkSession, ""catalog"")
   invisible(callJMethod(catalog, ""clearCache""))
 }
 
-clearCache <- function() {
-  dispatchFunc(""clearCache()"")
-}
-
-#' (Deprecated) Drop Temporary Table
-#'
-#' Drops the temporary table with the given table name in the catalog.
-#' If the table has been cached/persisted before, it's also unpersisted.
-#'
-#' @param tableName The name of the SparkSQL table to be dropped.
-#' @seealso \link{dropTempView}
-#' @rdname dropTempTable-deprecated
-#' @examples
-#' \dontrun{
-#' sparkR.session()
-#' df <- read.df(path, ""parquet"")
-#' createOrReplaceTempView(df, ""table"")
-#' dropTempTable(""table"")
-#' }
-#' @name dropTempTable
-#' @method dropTempTable default
-#' @note dropTempTable since 1.4.0
-dropTempTable.default <- function(tableName) {
-  .Deprecated(""dropTempView"", old = ""dropTempTable"")
-  if (class(tableName) != ""character"") {
-    stop(""tableName must be a string."")
-  }
-  dropTempView(tableName)
-}
-
-dropTempTable <- function(x, ...) {
-  dispatchFunc(""dropTempView(viewName)"", x, ...)
-}
-
 #' Drops the temporary view with the given view name in the catalog.
 #'
 #' Drops the temporary view with the given view name in the catalog.
@@ -250,17 +170,12 @@ dropTempView <- function(viewName) {
 #' tables(""hive"")
 #' }
 #' @name tables
-#' @method tables default
 #' @note tables since 1.4.0
-tables.default <- function(databaseName = NULL) {
+tables <- function(databaseName = NULL) {
   # rename column to match previous output schema
   withColumnRenamed(listTables(databaseName), ""name"", ""tableName"")
 }
 
-tables <- function(x, ...) {
-  dispatchFunc(""tables(databaseName = NULL)"", x, ...)
-}
-
 #' Table Names
 #'
 #' Returns the names of tables in the given database as an array.
@@ -274,9 +189,8 @@ tables <- function(x, ...) {
 #' tableNames(""hive"")
 #' }
 #' @name tableNames
-#' @method tableNames default
 #' @note tableNames since 1.4.0
-tableNames.default <- function(databaseName = NULL) {
+tableNames <- function(databaseName = NULL) {
   sparkSession <- getSparkSession()
   callJStatic(""org.apache.spark.sql.api.r.SQLUtils"",
               ""getTableNames"",
@@ -284,10 +198,6 @@ tableNames.default <- function(databaseName = NULL) {
               databaseName)
 }
 
-tableNames <- function(x, ...) {
-  dispatchFunc(""tableNames(databaseName = NULL)"", x, ...)
-}
-
 #' Returns the current default database
 #'
 #' Returns the current default database.
diff --git a/R/pkg/R/context.R b/R/pkg/R/context.R
index f168ca76b6007..e99136723f65b 100644
--- a/R/pkg/R/context.R
+++ b/R/pkg/R/context.R
@@ -167,18 +167,30 @@ parallelize <- function(sc, coll, numSlices = 1) {
   # 2-tuples of raws
   serializedSlices <- lapply(slices, serialize, connection = NULL)
 
-  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)
+  # The RPC backend cannot handle arguments larger than 2GB (INT_MAX)
   # If serialized data is safely less than that threshold we send it over the PRC channel.
   # Otherwise, we write it to a file and send the file name
   if (objectSize < sizeLimit) {
     jrdd <- callJStatic(""org.apache.spark.api.r.RRDD"", ""createRDDFromArray"", sc, serializedSlices)
   } else {
-    fileName <- writeToTempFile(serializedSlices)
-    jrdd <- tryCatch(callJStatic(
-        ""org.apache.spark.api.r.RRDD"", ""createRDDFromFile"", sc, fileName, as.integer(numSlices)),
-      finally = {
-        file.remove(fileName)
-    })
+    if (callJStatic(""org.apache.spark.api.r.RUtils"", ""getEncryptionEnabled"", sc)) {
+      # the length of slices here is the parallelism to use in the jvm's sc.parallelize()
+      parallelism <- as.integer(numSlices)
+      jserver <- newJObject(""org.apache.spark.api.r.RParallelizeServer"", sc, parallelism)
+      authSecret <- callJMethod(jserver, ""secret"")
+      port <- callJMethod(jserver, ""port"")
+      conn <- socketConnection(port = port, blocking = TRUE, open = ""wb"", timeout = 1500)
+      doServerAuth(conn, authSecret)
+      writeToConnection(serializedSlices, conn)
+      jrdd <- callJMethod(jserver, ""getResult"")
+    } else {
+      fileName <- writeToTempFile(serializedSlices)
+      jrdd <- tryCatch(callJStatic(
+          ""org.apache.spark.api.r.RRDD"", ""createRDDFromFile"", sc, fileName, as.integer(numSlices)),
+        finally = {
+          file.remove(fileName)
+      })
+    }
   }
 
   RDD(jrdd, ""byte"")
@@ -194,14 +206,21 @@ getMaxAllocationLimit <- function(sc) {
   ))
 }
 
+writeToConnection <- function(serializedSlices, conn) {
+  tryCatch({
+    for (slice in serializedSlices) {
+      writeBin(as.integer(length(slice)), conn, endian = ""big"")
+      writeBin(slice, conn, endian = ""big"")
+    }
+  }, finally = {
+    close(conn)
+  })
+}
+
 writeToTempFile <- function(serializedSlices) {
   fileName <- tempfile()
   conn <- file(fileName, ""wb"")
-  for (slice in serializedSlices) {
-    writeBin(as.integer(length(slice)), conn, endian = ""big"")
-    writeBin(slice, conn, endian = ""big"")
-  }
-  close(conn)
+  writeToConnection(serializedSlices, conn)
   fileName
 }
 
diff --git a/R/pkg/R/functions.R b/R/pkg/R/functions.R
index 572dee50127b8..f568a931ae1fe 100644
--- a/R/pkg/R/functions.R
+++ b/R/pkg/R/functions.R
@@ -112,7 +112,7 @@ NULL
 #' df <- createDataFrame(cbind(model = rownames(mtcars), mtcars))
 #' tmp <- mutate(df, v1 = log(df$mpg), v2 = cbrt(df$disp),
 #'                   v3 = bround(df$wt, 1), v4 = bin(df$cyl),
-#'                   v5 = hex(df$wt), v6 = toDegrees(df$gear),
+#'                   v5 = hex(df$wt), v6 = degrees(df$gear),
 #'                   v7 = atan2(df$cyl, df$am), v8 = hypot(df$cyl, df$am),
 #'                   v9 = pmod(df$hp, df$cyl), v10 = shiftLeft(df$disp, 1),
 #'                   v11 = conv(df$hp, 10, 16), v12 = sign(df$vs - 0.5),
@@ -187,7 +187,9 @@ NULL
 #'          \itemize{
 #'          \item \code{to_json}: it is the column containing the struct, array of the structs,
 #'              the map or array of maps.
+#'          \item \code{to_csv}: it is the column containing the struct.
 #'          \item \code{from_json}: it is the column containing the JSON string.
+#'          \item \code{from_csv}: it is the column containing the CSV string.
 #'          }
 #' @param y Column to compute on.
 #' @param value A value to compute on.
@@ -196,10 +198,25 @@ NULL
 #'          \item \code{array_position}: a value to locate in the given array.
 #'          \item \code{array_remove}: a value to remove in the given array.
 #'          }
-#' @param ... additional argument(s). In \code{to_json} and \code{from_json}, this contains
-#'            additional named properties to control how it is converted, accepts the same
-#'            options as the JSON data source.  In \code{arrays_zip}, this contains additional
-#'            Columns of arrays to be merged.
+#' @param schema
+#'          \itemize{
+#'          \item \code{from_json}: a structType object to use as the schema to use
+#'              when parsing the JSON string. Since Spark 2.3, the DDL-formatted string is
+#'              also supported for the schema.
+#'          \item \code{from_csv}: a DDL-formatted string
+#'          }
+#' @param ... additional argument(s).
+#'          \itemize{
+#'          \item \code{to_json}, \code{from_json} and \code{schema_of_json}: this contains
+#'              additional named properties to control how it is converted and accepts the
+#'              same options as the JSON data source.
+#'          \item \code{to_json}: it supports the ""pretty"" option which enables pretty
+#'              JSON generation.
+#'          \item \code{to_csv}, \code{from_csv} and \code{schema_of_csv}: this contains
+#'              additional named properties to control how it is converted and accepts the
+#'              same options as the CSV data source.
+#'          \item \code{arrays_zip}, this contains additional Columns of arrays to be merged.
+#'          }
 #' @name column_collection_functions
 #' @rdname column_collection_functions
 #' @family collection functions
@@ -310,23 +327,37 @@ setMethod(""acos"",
           })
 
 #' @details
-#' \code{approxCountDistinct}: Returns the approximate number of distinct items in a group.
+#' \code{approx_count_distinct}: Returns the approximate number of distinct items in a group.
 #'
 #' @rdname column_aggregate_functions
-#' @aliases approxCountDistinct approxCountDistinct,Column-method
+#' @aliases approx_count_distinct approx_count_distinct,Column-method
 #' @examples
 #'
 #' \dontrun{
-#' head(select(df, approxCountDistinct(df$gear)))
-#' head(select(df, approxCountDistinct(df$gear, 0.02)))
+#' head(select(df, approx_count_distinct(df$gear)))
+#' head(select(df, approx_count_distinct(df$gear, 0.02)))
 #' head(select(df, countDistinct(df$gear, df$cyl)))
 #' head(select(df, n_distinct(df$gear)))
 #' head(distinct(select(df, ""gear"")))}
+#' @note approx_count_distinct(Column) since 3.0.0
+setMethod(""approx_count_distinct"",
+          signature(x = ""Column""),
+          function(x) {
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""approx_count_distinct"", x@jc)
+            column(jc)
+          })
+
+#' @details
+#' \code{approxCountDistinct}: Returns the approximate number of distinct items in a group.
+#'
+#' @rdname column_aggregate_functions
+#' @aliases approxCountDistinct approxCountDistinct,Column-method
 #' @note approxCountDistinct(Column) since 1.4.0
 setMethod(""approxCountDistinct"",
           signature(x = ""Column""),
           function(x) {
-            jc <- callJStatic(""org.apache.spark.sql.functions"", ""approxCountDistinct"", x@jc)
+            .Deprecated(""approx_count_distinct"")
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""approx_count_distinct"", x@jc)
             column(jc)
           })
 
@@ -1641,7 +1672,22 @@ setMethod(""tanh"",
 setMethod(""toDegrees"",
           signature(x = ""Column""),
           function(x) {
-            jc <- callJStatic(""org.apache.spark.sql.functions"", ""toDegrees"", x@jc)
+            .Deprecated(""degrees"")
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""degrees"", x@jc)
+            column(jc)
+          })
+
+#' @details
+#' \code{degrees}: Converts an angle measured in radians to an approximately equivalent angle
+#' measured in degrees.
+#'
+#' @rdname column_math_functions
+#' @aliases degrees degrees,Column-method
+#' @note degrees since 3.0.0
+setMethod(""degrees"",
+          signature(x = ""Column""),
+          function(x) {
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""degrees"", x@jc)
             column(jc)
           })
 
@@ -1655,7 +1701,22 @@ setMethod(""toDegrees"",
 setMethod(""toRadians"",
           signature(x = ""Column""),
           function(x) {
-            jc <- callJStatic(""org.apache.spark.sql.functions"", ""toRadians"", x@jc)
+            .Deprecated(""radians"")
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""radians"", x@jc)
+            column(jc)
+          })
+
+#' @details
+#' \code{radians}: Converts an angle measured in degrees to an approximately equivalent angle
+#' measured in radians.
+#'
+#' @rdname column_math_functions
+#' @aliases radians radians,Column-method
+#' @note radians since 3.0.0
+setMethod(""radians"",
+          signature(x = ""Column""),
+          function(x) {
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""radians"", x@jc)
             column(jc)
           })
 
@@ -1717,12 +1778,16 @@ setMethod(""to_date"",
 #' df2 <- mutate(df2, people_json = to_json(df2$people))
 #'
 #' # Converts a map into a JSON object
-#' df2 <- sql(""SELECT map('name', 'Bob')) as people"")
+#' df2 <- sql(""SELECT map('name', 'Bob') as people"")
 #' df2 <- mutate(df2, people_json = to_json(df2$people))
 #'
 #' # Converts an array of maps into a JSON array
 #' df2 <- sql(""SELECT array(map('name', 'Bob'), map('name', 'Alice')) as people"")
-#' df2 <- mutate(df2, people_json = to_json(df2$people))}
+#' df2 <- mutate(df2, people_json = to_json(df2$people))
+#'
+#' # Converts a map into a pretty JSON object
+#' df2 <- sql(""SELECT map('name', 'Bob') as people"")
+#' df2 <- mutate(df2, people_json = to_json(df2$people, pretty = TRUE))}
 #' @note to_json since 2.2.0
 setMethod(""to_json"", signature(x = ""Column""),
           function(x, ...) {
@@ -1731,6 +1796,26 @@ setMethod(""to_json"", signature(x = ""Column""),
             column(jc)
           })
 
+#' @details
+#' \code{to_csv}: Converts a column containing a \code{structType} into a Column of CSV string.
+#' Resolving the Column can fail if an unsupported type is encountered.
+#'
+#' @rdname column_collection_functions
+#' @aliases to_csv to_csv,Column-method
+#' @examples
+#'
+#' \dontrun{
+#' # Converts a struct into a CSV string
+#' df2 <- sql(""SELECT named_struct('date', cast('2000-01-01' as date)) as d"")
+#' select(df2, to_csv(df2$d, dateFormat = 'dd/MM/yyyy'))}
+#' @note to_csv since 3.0.0
+setMethod(""to_csv"", signature(x = ""Column""),
+          function(x, ...) {
+            options <- varargsToStrEnv(...)
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""to_csv"", x@jc, options)
+            column(jc)
+          })
+
 #' @details
 #' \code{to_timestamp}: Converts the column into a TimestampType. You may optionally specify
 #' a format according to the rules in:
@@ -2035,13 +2120,24 @@ setMethod(""pmod"", signature(y = ""Column""),
 
 #' @param rsd maximum estimation error allowed (default = 0.05).
 #'
+#' @rdname column_aggregate_functions
+#' @aliases approx_count_distinct,Column-method
+#' @note approx_count_distinct(Column, numeric) since 3.0.0
+setMethod(""approx_count_distinct"",
+          signature(x = ""Column""),
+          function(x, rsd = 0.05) {
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""approx_count_distinct"", x@jc, rsd)
+            column(jc)
+          })
+
 #' @rdname column_aggregate_functions
 #' @aliases approxCountDistinct,Column-method
 #' @note approxCountDistinct(Column, numeric) since 1.4.0
 setMethod(""approxCountDistinct"",
           signature(x = ""Column""),
           function(x, rsd = 0.05) {
-            jc <- callJStatic(""org.apache.spark.sql.functions"", ""approxCountDistinct"", x@jc, rsd)
+            .Deprecated(""approx_count_distinct"")
+            jc <- callJStatic(""org.apache.spark.sql.functions"", ""approx_count_distinct"", x@jc, rsd)
             column(jc)
           })
 
@@ -2164,8 +2260,6 @@ setMethod(""date_format"", signature(y = ""Column"", x = ""character""),
 #' to \code{TRUE}. If the string is unparseable, the Column will contain the value NA.
 #'
 #' @rdname column_collection_functions
-#' @param schema a structType object to use as the schema to use when parsing the JSON string.
-#'               Since Spark 2.3, the DDL-formatted string is also supported for the schema.
 #' @param as.json.array indicating if input string is JSON array of objects or a single object.
 #' @aliases from_json from_json,Column,characterOrstructType-method
 #' @examples
@@ -2203,9 +2297,98 @@ setMethod(""from_json"", signature(x = ""Column"", schema = ""characterOrstructType"")
           })
 
 #' @details
-#' \code{from_utc_timestamp}: Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a
-#' time in UTC, and renders that time as a timestamp in the given time zone. For example, 'GMT+1'
-#' would yield '2017-07-14 03:40:00.0'.
+#' \code{schema_of_json}: Parses a JSON string and infers its schema in DDL format.
+#'
+#' @rdname column_collection_functions
+#' @aliases schema_of_json schema_of_json,characterOrColumn-method
+#' @examples
+#'
+#' \dontrun{
+#' json <- ""{\""name\"":\""Bob\""}""
+#' df <- sql(""SELECT * FROM range(1)"")
+#' head(select(df, schema_of_json(json)))}
+#' @note schema_of_json since 3.0.0
+setMethod(""schema_of_json"", signature(x = ""characterOrColumn""),
+          function(x, ...) {
+            if (class(x) == ""character"") {
+              col <- callJStatic(""org.apache.spark.sql.functions"", ""lit"", x)
+            } else {
+              col <- x@jc
+            }
+            options <- varargsToStrEnv(...)
+            jc <- callJStatic(""org.apache.spark.sql.functions"",
+                              ""schema_of_json"",
+                              col, options)
+            column(jc)
+          })
+
+#' @details
+#' \code{from_csv}: Parses a column containing a CSV string into a Column of \code{structType}
+#' with the specified \code{schema}.
+#' If the string is unparseable, the Column will contain the value NA.
+#'
+#' @rdname column_collection_functions
+#' @aliases from_csv from_csv,Column,character-method
+#' @examples
+#'
+#' \dontrun{
+#' df <- sql(""SELECT 'Amsterdam,2018' as csv"")
+#' schema <- ""city STRING, year INT""
+#' head(select(df, from_csv(df$csv, schema)))}
+#' @note from_csv since 3.0.0
+setMethod(""from_csv"", signature(x = ""Column"", schema = ""characterOrColumn""),
+          function(x, schema, ...) {
+            if (class(schema) == ""Column"") {
+              jschema <- schema@jc
+            } else if (is.character(schema)) {
+              jschema <- callJStatic(""org.apache.spark.sql.functions"", ""lit"", schema)
+            } else {
+              stop(""schema argument should be a column or character"")
+            }
+            options <- varargsToStrEnv(...)
+            jc <- callJStatic(""org.apache.spark.sql.functions"",
+                              ""from_csv"",
+                              x@jc, jschema, options)
+            column(jc)
+          })
+
+#' @details
+#' \code{schema_of_csv}: Parses a CSV string and infers its schema in DDL format.
+#'
+#' @rdname column_collection_functions
+#' @aliases schema_of_csv schema_of_csv,characterOrColumn-method
+#' @examples
+#'
+#' \dontrun{
+#' csv <- ""Amsterdam,2018""
+#' df <- sql(""SELECT * FROM range(1)"")
+#' head(select(df, schema_of_csv(csv)))}
+#' @note schema_of_csv since 3.0.0
+setMethod(""schema_of_csv"", signature(x = ""characterOrColumn""),
+          function(x, ...) {
+            if (class(x) == ""character"") {
+              col <- callJStatic(""org.apache.spark.sql.functions"", ""lit"", x)
+            } else {
+              col <- x@jc
+            }
+            options <- varargsToStrEnv(...)
+            jc <- callJStatic(""org.apache.spark.sql.functions"",
+                              ""schema_of_csv"",
+                              col, options)
+            column(jc)
+          })
+
+#' @details
+#' \code{from_utc_timestamp}: This is a common function for databases supporting TIMESTAMP WITHOUT
+#' TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a
+#' timestamp in UTC, and renders that timestamp as a timestamp in the given time zone.
+#' However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not
+#' timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to
+#' the given timezone.
+#' This function may return confusing result if the input is a string with timezone, e.g.
+#' (\code{2018-03-13T06:18:23+00:00}). The reason is that, Spark firstly cast the string to
+#' timestamp according to the timezone in the string, and finally display the result by converting
+#' the timestamp to string according to the session local timezone.
 #'
 #' @rdname column_datetime_diff_functions
 #'
@@ -2261,9 +2444,16 @@ setMethod(""next_day"", signature(y = ""Column"", x = ""character""),
           })
 
 #' @details
-#' \code{to_utc_timestamp}: Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a
-#' time in the given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1'
-#' would yield '2017-07-14 01:40:00.0'.
+#' \code{to_utc_timestamp}: This is a common function for databases supporting TIMESTAMP WITHOUT
+#' TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a
+#' timestamp in the given timezone, and renders that timestamp as a timestamp in UTC.
+#' However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not
+#' timezone-agnostic. So in Spark this function just shift the timestamp value from the given
+#' timezone to UTC timezone.
+#' This function may return confusing result if the input is a string with timezone, e.g.
+#' (\code{2018-03-13T06:18:23+00:00}). The reason is that, Spark firstly cast the string to
+#' timestamp according to the timezone in the string, and finally display the result by converting
+#' the timestamp to string according to the session local timezone.
 #'
 #' @rdname column_datetime_diff_functions
 #' @aliases to_utc_timestamp to_utc_timestamp,Column,character-method
@@ -3243,7 +3433,7 @@ setMethod(""flatten"",
 #'
 #' @rdname column_collection_functions
 #' @aliases map_entries map_entries,Column-method
-#' @note map_entries since 2.4.0
+#' @note map_entries since 3.0.0
 setMethod(""map_entries"",
           signature(x = ""Column""),
           function(x) {
@@ -3458,13 +3648,21 @@ setMethod(""collect_set"",
 
 #' @details
 #' \code{split_string}: Splits string on regular expression.
-#' Equivalent to \code{split} SQL function.
+#' Equivalent to \code{split} SQL function. Optionally a
+#' \code{limit} can be specified
 #'
 #' @rdname column_string_functions
+#' @param limit determines the length of the returned array.
+#'              \itemize{
+#'              \item \code{limit > 0}: length of the array will be at most \code{limit}
+#'              \item \code{limit <= 0}: the returned array can have any length
+#'              }
+#'
 #' @aliases split_string split_string,Column-method
 #' @examples
 #'
 #' \dontrun{
+#' head(select(df, split_string(df$Class, ""\\d"", 2)))
 #' head(select(df, split_string(df$Sex, ""a"")))
 #' head(select(df, split_string(df$Class, ""\\d"")))
 #' # This is equivalent to the following SQL expression
@@ -3472,8 +3670,9 @@ setMethod(""collect_set"",
 #' @note split_string 2.3.0
 setMethod(""split_string"",
           signature(x = ""Column"", pattern = ""character""),
-          function(x, pattern) {
-            jc <- callJStatic(""org.apache.spark.sql.functions"", ""split"", x@jc, pattern)
+          function(x, pattern, limit = -1) {
+            jc <- callJStatic(""org.apache.spark.sql.functions"",
+                              ""split"", x@jc, pattern, as.integer(limit))
             column(jc)
           })
 
diff --git a/R/pkg/R/generics.R b/R/pkg/R/generics.R
index 27c1b312d645c..eed76465221c6 100644
--- a/R/pkg/R/generics.R
+++ b/R/pkg/R/generics.R
@@ -528,9 +528,6 @@ setGeneric(""persist"", function(x, newLevel) { standardGeneric(""persist"") })
 #' @rdname printSchema
 setGeneric(""printSchema"", function(x) { standardGeneric(""printSchema"") })
 
-#' @rdname registerTempTable-deprecated
-setGeneric(""registerTempTable"", function(x, tableName) { standardGeneric(""registerTempTable"") })
-
 #' @rdname rename
 setGeneric(""rename"", function(x, ...) { standardGeneric(""rename"") })
 
@@ -595,9 +592,6 @@ setGeneric(""write.parquet"", function(x, path, ...) {
   standardGeneric(""write.parquet"")
 })
 
-#' @rdname write.parquet
-setGeneric(""saveAsParquetFile"", function(x, path) { standardGeneric(""saveAsParquetFile"") })
-
 #' @rdname write.stream
 setGeneric(""write.stream"", function(df, source = NULL, outputMode = NULL, ...) {
   standardGeneric(""write.stream"")
@@ -637,7 +631,7 @@ setGeneric(""toRDD"", function(x) { standardGeneric(""toRDD"") })
 #' @rdname union
 setGeneric(""union"", function(x, y) { standardGeneric(""union"") })
 
-#' @rdname union
+#' @rdname unionAll
 setGeneric(""unionAll"", function(x, y) { standardGeneric(""unionAll"") })
 
 #' @rdname unionByName
@@ -755,6 +749,10 @@ setGeneric(""windowOrderBy"", function(col, ...) { standardGeneric(""windowOrderBy""
 #' @name NULL
 setGeneric(""add_months"", function(y, x) { standardGeneric(""add_months"") })
 
+#' @rdname column_aggregate_functions
+#' @name NULL
+setGeneric(""approx_count_distinct"", function(x, ...) { standardGeneric(""approx_count_distinct"") })
+
 #' @rdname column_aggregate_functions
 #' @name NULL
 setGeneric(""approxCountDistinct"", function(x, ...) { standardGeneric(""approxCountDistinct"") })
@@ -984,6 +982,10 @@ setGeneric(""format_string"", function(format, x, ...) { standardGeneric(""format_s
 #' @name NULL
 setGeneric(""from_json"", function(x, schema, ...) { standardGeneric(""from_json"") })
 
+#' @rdname column_collection_functions
+#' @name NULL
+setGeneric(""from_csv"", function(x, schema, ...) { standardGeneric(""from_csv"") })
+
 #' @rdname column_datetime_functions
 #' @name NULL
 setGeneric(""from_unixtime"", function(x, ...) { standardGeneric(""from_unixtime"") })
@@ -1204,6 +1206,14 @@ setGeneric(""rpad"", function(x, len, pad) { standardGeneric(""rpad"") })
 #' @name NULL
 setGeneric(""rtrim"", function(x, trimString) { standardGeneric(""rtrim"") })
 
+#' @rdname column_collection_functions
+#' @name NULL
+setGeneric(""schema_of_csv"", function(x, ...) { standardGeneric(""schema_of_csv"") })
+
+#' @rdname column_collection_functions
+#' @name NULL
+setGeneric(""schema_of_json"", function(x, ...) { standardGeneric(""schema_of_json"") })
+
 #' @rdname column_aggregate_functions
 #' @name NULL
 setGeneric(""sd"", function(x, na.rm = FALSE) { standardGeneric(""sd"") })
@@ -1258,7 +1268,7 @@ setGeneric(""sort_array"", function(x, asc = TRUE) { standardGeneric(""sort_array"")
 
 #' @rdname column_string_functions
 #' @name NULL
-setGeneric(""split_string"", function(x, pattern) { standardGeneric(""split_string"") })
+setGeneric(""split_string"", function(x, pattern, ...) { standardGeneric(""split_string"") })
 
 #' @rdname column_string_functions
 #' @name NULL
@@ -1292,10 +1302,18 @@ setGeneric(""substring_index"", function(x, delim, count) { standardGeneric(""subst
 #' @name NULL
 setGeneric(""sumDistinct"", function(x) { standardGeneric(""sumDistinct"") })
 
+#' @rdname column_math_functions
+#' @name NULL
+setGeneric(""degrees"", function(x) { standardGeneric(""degrees"") })
+
 #' @rdname column_math_functions
 #' @name NULL
 setGeneric(""toDegrees"", function(x) { standardGeneric(""toDegrees"") })
 
+#' @rdname column_math_functions
+#' @name NULL
+setGeneric(""radians"", function(x) { standardGeneric(""radians"") })
+
 #' @rdname column_math_functions
 #' @name NULL
 setGeneric(""toRadians"", function(x) { standardGeneric(""toRadians"") })
@@ -1308,6 +1326,10 @@ setGeneric(""to_date"", function(x, format) { standardGeneric(""to_date"") })
 #' @name NULL
 setGeneric(""to_json"", function(x, ...) { standardGeneric(""to_json"") })
 
+#' @rdname column_collection_functions
+#' @name NULL
+setGeneric(""to_csv"", function(x, ...) { standardGeneric(""to_csv"") })
+
 #' @rdname column_datetime_functions
 #' @name NULL
 setGeneric(""to_timestamp"", function(x, format) { standardGeneric(""to_timestamp"") })
@@ -1453,6 +1475,10 @@ setGeneric(""spark.freqItemsets"", function(object) { standardGeneric(""spark.freqI
 #' @rdname spark.fpGrowth
 setGeneric(""spark.associationRules"", function(object) { standardGeneric(""spark.associationRules"") })
 
+#' @rdname spark.prefixSpan
+setGeneric(""spark.findFrequentSequentialPatterns"",
+            function(data, ...) { standardGeneric(""spark.findFrequentSequentialPatterns"") })
+
 #' @param object a fitted ML model object.
 #' @param path the directory where the model is saved.
 #' @param ... additional argument(s) passed to the method.
diff --git a/R/pkg/R/mllib_fpm.R b/R/pkg/R/mllib_fpm.R
index 4ad34fe82328f..ac37580c6b373 100644
--- a/R/pkg/R/mllib_fpm.R
+++ b/R/pkg/R/mllib_fpm.R
@@ -23,6 +23,12 @@
 #' @note FPGrowthModel since 2.2.0
 setClass(""FPGrowthModel"", slots = list(jobj = ""jobj""))
 
+#' S4 class that represents a PrefixSpan
+#'
+#' @param jobj a Java object reference to the backing Scala PrefixSpan
+#' @note PrefixSpan since 3.0.0
+setClass(""PrefixSpan"", slots = list(jobj = ""jobj""))
+
 #' FP-growth
 #'
 #' A parallel FP-growth algorithm to mine frequent itemsets.
@@ -155,3 +161,61 @@ setMethod(""write.ml"", signature(object = ""FPGrowthModel"", path = ""character""),
           function(object, path, overwrite = FALSE) {
             write_internal(object, path, overwrite)
           })
+
+#' PrefixSpan
+#'
+#' A parallel PrefixSpan algorithm to mine frequent sequential patterns.
+#' \code{spark.findFrequentSequentialPatterns} returns a complete set of frequent sequential
+#' patterns.
+#' For more details, see
+#' \href{https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#prefixspan}{
+#' PrefixSpan}.
+#'
+#  Find frequent sequential patterns.
+#' @param data A SparkDataFrame.
+#' @param minSupport Minimal support level.
+#' @param maxPatternLength Maximal pattern length.
+#' @param maxLocalProjDBSize Maximum number of items (including delimiters used in the internal
+#'                           storage format) allowed in a projected database before local
+#'                           processing.
+#' @param sequenceCol name of the sequence column in dataset.
+#' @param ... additional argument(s) passed to the method.
+#' @return A complete set of frequent sequential patterns in the input sequences of itemsets.
+#'         The returned \code{SparkDataFrame} contains columns of sequence and corresponding
+#'         frequency. The schema of it will be:
+#'         \code{sequence: ArrayType(ArrayType(T))} (T is the item type)
+#'         \code{freq: Long}
+#' @rdname spark.prefixSpan
+#' @aliases findFrequentSequentialPatterns,PrefixSpan,SparkDataFrame-method
+#' @examples
+#' \dontrun{
+#' df <- createDataFrame(list(list(list(list(1L, 2L), list(3L))),
+#'                       list(list(list(1L), list(3L, 2L), list(1L, 2L))),
+#'                       list(list(list(1L, 2L), list(5L))),
+#'                       list(list(list(6L)))), schema = c(""sequence""))
+#' frequency <- spark.findFrequentSequentialPatterns(df, minSupport = 0.5, maxPatternLength = 5L,
+#'                                                   maxLocalProjDBSize = 32000000L)
+#' showDF(frequency)
+#' }
+#' @note spark.findFrequentSequentialPatterns(SparkDataFrame) since 3.0.0
+setMethod(""spark.findFrequentSequentialPatterns"",
+          signature(data = ""SparkDataFrame""),
+          function(data, minSupport = 0.1, maxPatternLength = 10L,
+            maxLocalProjDBSize = 32000000L, sequenceCol = ""sequence"") {
+              if (!is.numeric(minSupport) || minSupport < 0) {
+                stop(""minSupport should be a number with value >= 0."")
+              }
+              if (!is.integer(maxPatternLength) || maxPatternLength <= 0) {
+                stop(""maxPatternLength should be a number with value > 0."")
+              }
+              if (!is.numeric(maxLocalProjDBSize) || maxLocalProjDBSize <= 0) {
+                stop(""maxLocalProjDBSize should be a number with value > 0."")
+              }
+
+              jobj <- callJStatic(""org.apache.spark.ml.r.PrefixSpanWrapper"", ""getPrefixSpan"",
+                                  as.numeric(minSupport), as.integer(maxPatternLength),
+                                  as.numeric(maxLocalProjDBSize), as.character(sequenceCol))
+              object <- new(""PrefixSpan"", jobj = jobj)
+              dataFrame(callJMethod(object@jobj, ""findFrequentSequentialPatterns"", data@sdf))
+            }
+          )
diff --git a/R/pkg/R/sparkR.R b/R/pkg/R/sparkR.R
index d3a9cbae7d808..ac289d38d01bd 100644
--- a/R/pkg/R/sparkR.R
+++ b/R/pkg/R/sparkR.R
@@ -88,49 +88,6 @@ sparkR.stop <- function() {
   sparkR.session.stop()
 }
 
-#' (Deprecated) Initialize a new Spark Context
-#'
-#' This function initializes a new SparkContext.
-#'
-#' @param master The Spark master URL
-#' @param appName Application name to register with cluster manager
-#' @param sparkHome Spark Home directory
-#' @param sparkEnvir Named list of environment variables to set on worker nodes
-#' @param sparkExecutorEnv Named list of environment variables to be used when launching executors
-#' @param sparkJars Character vector of jar files to pass to the worker nodes
-#' @param sparkPackages Character vector of package coordinates
-#' @seealso \link{sparkR.session}
-#' @rdname sparkR.init-deprecated
-#' @examples
-#'\dontrun{
-#' sc <- sparkR.init(""local[2]"", ""SparkR"", ""/home/spark"")
-#' sc <- sparkR.init(""local[2]"", ""SparkR"", ""/home/spark"",
-#'                  list(spark.executor.memory=""1g""))
-#' sc <- sparkR.init(""yarn-client"", ""SparkR"", ""/home/spark"",
-#'                  list(spark.executor.memory=""4g""),
-#'                  list(LD_LIBRARY_PATH=""/directory of JVM libraries (libjvm.so) on workers/""),
-#'                  c(""one.jar"", ""two.jar"", ""three.jar""),
-#'                  c(""com.databricks:spark-avro_2.11:2.0.1""))
-#'}
-#' @note sparkR.init since 1.4.0
-sparkR.init <- function(
-  master = """",
-  appName = ""SparkR"",
-  sparkHome = Sys.getenv(""SPARK_HOME""),
-  sparkEnvir = list(),
-  sparkExecutorEnv = list(),
-  sparkJars = """",
-  sparkPackages = """") {
-  .Deprecated(""sparkR.session"")
-  sparkR.sparkContext(master,
-     appName,
-     sparkHome,
-     convertNamedListToEnv(sparkEnvir),
-     convertNamedListToEnv(sparkExecutorEnv),
-     sparkJars,
-     sparkPackages)
-}
-
 # Internal function to handle creating the SparkContext.
 sparkR.sparkContext <- function(
   master = """",
@@ -272,61 +229,6 @@ sparkR.sparkContext <- function(
   sc
 }
 
-#' (Deprecated) Initialize a new SQLContext
-#'
-#' This function creates a SparkContext from an existing JavaSparkContext and
-#' then uses it to initialize a new SQLContext
-#'
-#' Starting SparkR 2.0, a SparkSession is initialized and returned instead.
-#' This API is deprecated and kept for backward compatibility only.
-#'
-#' @param jsc The existing JavaSparkContext created with SparkR.init()
-#' @seealso \link{sparkR.session}
-#' @rdname sparkRSQL.init-deprecated
-#' @examples
-#'\dontrun{
-#' sc <- sparkR.init()
-#' sqlContext <- sparkRSQL.init(sc)
-#'}
-#' @note sparkRSQL.init since 1.4.0
-sparkRSQL.init <- function(jsc = NULL) {
-  .Deprecated(""sparkR.session"")
-
-  if (exists("".sparkRsession"", envir = .sparkREnv)) {
-    return(get("".sparkRsession"", envir = .sparkREnv))
-  }
-
-  # Default to without Hive support for backward compatibility.
-  sparkR.session(enableHiveSupport = FALSE)
-}
-
-#' (Deprecated) Initialize a new HiveContext
-#'
-#' This function creates a HiveContext from an existing JavaSparkContext
-#'
-#' Starting SparkR 2.0, a SparkSession is initialized and returned instead.
-#' This API is deprecated and kept for backward compatibility only.
-#'
-#' @param jsc The existing JavaSparkContext created with SparkR.init()
-#' @seealso \link{sparkR.session}
-#' @rdname sparkRHive.init-deprecated
-#' @examples
-#'\dontrun{
-#' sc <- sparkR.init()
-#' sqlContext <- sparkRHive.init(sc)
-#'}
-#' @note sparkRHive.init since 1.4.0
-sparkRHive.init <- function(jsc = NULL) {
-  .Deprecated(""sparkR.session"")
-
-  if (exists("".sparkRsession"", envir = .sparkREnv)) {
-    return(get("".sparkRsession"", envir = .sparkREnv))
-  }
-
-  # Default to without Hive support for backward compatibility.
-  sparkR.session(enableHiveSupport = TRUE)
-}
-
 #' Get the existing SparkSession or initialize a new SparkSession.
 #'
 #' SparkSession is the entry point into SparkR. \code{sparkR.session} gets the existing
@@ -482,26 +384,11 @@ sparkR.uiWebUrl <- function() {
 #' setJobGroup(""myJobGroup"", ""My job group description"", TRUE)
 #'}
 #' @note setJobGroup since 1.5.0
-#' @method setJobGroup default
-setJobGroup.default <- function(groupId, description, interruptOnCancel) {
+setJobGroup <- function(groupId, description, interruptOnCancel) {
   sc <- getSparkContext()
   invisible(callJMethod(sc, ""setJobGroup"", groupId, description, interruptOnCancel))
 }
 
-setJobGroup <- function(sc, groupId, description, interruptOnCancel) {
-  if (class(sc) == ""jobj"" && any(grepl(""JavaSparkContext"", getClassName.jobj(sc)))) {
-    .Deprecated(""setJobGroup(groupId, description, interruptOnCancel)"",
-                old = ""setJobGroup(sc, groupId, description, interruptOnCancel)"")
-    setJobGroup.default(groupId, description, interruptOnCancel)
-  } else {
-    # Parameter order is shifted
-    groupIdToUse <- sc
-    descriptionToUse <- groupId
-    interruptOnCancelToUse <- description
-    setJobGroup.default(groupIdToUse, descriptionToUse, interruptOnCancelToUse)
-  }
-}
-
 #' Clear current job group ID and its description
 #'
 #' @rdname clearJobGroup
@@ -512,22 +399,11 @@ setJobGroup <- function(sc, groupId, description, interruptOnCancel) {
 #' clearJobGroup()
 #'}
 #' @note clearJobGroup since 1.5.0
-#' @method clearJobGroup default
-clearJobGroup.default <- function() {
+clearJobGroup <- function() {
   sc <- getSparkContext()
   invisible(callJMethod(sc, ""clearJobGroup""))
 }
 
-clearJobGroup <- function(sc) {
-  if (!missing(sc) &&
-      class(sc) == ""jobj"" &&
-      any(grepl(""JavaSparkContext"", getClassName.jobj(sc)))) {
-    .Deprecated(""clearJobGroup()"", old = ""clearJobGroup(sc)"")
-  }
-  clearJobGroup.default()
-}
-
-
 #' Cancel active jobs for the specified group
 #'
 #' @param groupId the ID of job group to be cancelled
@@ -539,23 +415,11 @@ clearJobGroup <- function(sc) {
 #' cancelJobGroup(""myJobGroup"")
 #'}
 #' @note cancelJobGroup since 1.5.0
-#' @method cancelJobGroup default
-cancelJobGroup.default <- function(groupId) {
+cancelJobGroup <- function(groupId) {
   sc <- getSparkContext()
   invisible(callJMethod(sc, ""cancelJobGroup"", groupId))
 }
 
-cancelJobGroup <- function(sc, groupId) {
-  if (class(sc) == ""jobj"" && any(grepl(""JavaSparkContext"", getClassName.jobj(sc)))) {
-    .Deprecated(""cancelJobGroup(groupId)"", old = ""cancelJobGroup(sc, groupId)"")
-    cancelJobGroup.default(groupId)
-  } else {
-    # Parameter order is shifted
-    groupIdToUse <- sc
-    cancelJobGroup.default(groupIdToUse)
-  }
-}
-
 #' Set a human readable description of the current job.
 #'
 #' Set a description that is shown as a job description in UI.
@@ -626,6 +490,8 @@ sparkConfToSubmitOps[[""spark.driver.extraLibraryPath""]] <- ""--driver-library-pat
 sparkConfToSubmitOps[[""spark.master""]] <- ""--master""
 sparkConfToSubmitOps[[""spark.yarn.keytab""]] <- ""--keytab""
 sparkConfToSubmitOps[[""spark.yarn.principal""]] <- ""--principal""
+sparkConfToSubmitOps[[""spark.kerberos.keytab""]] <- ""--keytab""
+sparkConfToSubmitOps[[""spark.kerberos.principal""]] <- ""--principal""
 
 
 # Utility function that returns Spark Submit arguments as a string
diff --git a/R/pkg/R/stats.R b/R/pkg/R/stats.R
index 497f18c763048..7252351ebebb2 100644
--- a/R/pkg/R/stats.R
+++ b/R/pkg/R/stats.R
@@ -109,7 +109,7 @@ setMethod(""corr"",
 #'
 #' Finding frequent items for columns, possibly with false positives.
 #' Using the frequent element count algorithm described in
-#' \url{http://dx.doi.org/10.1145/762471.762473}, proposed by Karp, Schenker, and Papadimitriou.
+#' \url{https://doi.org/10.1145/762471.762473}, proposed by Karp, Schenker, and Papadimitriou.
 #'
 #' @param x A SparkDataFrame.
 #' @param cols A vector column names to search frequent items in.
@@ -143,7 +143,7 @@ setMethod(""freqItems"", signature(x = ""SparkDataFrame"", cols = ""character""),
 #' *exact* rank of x is close to (p * N). More precisely,
 #'   floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).
 #' This method implements a variation of the Greenwald-Khanna algorithm (with some speed
-#' optimizations). The algorithm was first present in [[http://dx.doi.org/10.1145/375663.375670
+#' optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670
 #' Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.
 #' Note that NA values will be ignored in numerical columns before calculation. For
 #'   columns only containing NA values, an empty list is returned.
diff --git a/R/pkg/inst/profile/general.R b/R/pkg/inst/profile/general.R
index 8c75c19ca7ac3..3efb460846fc2 100644
--- a/R/pkg/inst/profile/general.R
+++ b/R/pkg/inst/profile/general.R
@@ -16,6 +16,10 @@
 #
 
 .First <- function() {
+  if (utils::compareVersion(paste0(R.version$major, ""."", R.version$minor), ""3.4.0"") == -1) {
+    warning(""Support for R prior to version 3.4 is deprecated since Spark 3.0.0"")
+  }
+
   packageDir <- Sys.getenv(""SPARKR_PACKAGE_DIR"")
   dirs <- strsplit(packageDir, "","")[[1]]
   .libPaths(c(dirs, .libPaths()))
diff --git a/R/pkg/inst/profile/shell.R b/R/pkg/inst/profile/shell.R
index 8a8111a8c5419..32eb3671b5941 100644
--- a/R/pkg/inst/profile/shell.R
+++ b/R/pkg/inst/profile/shell.R
@@ -16,6 +16,10 @@
 #
 
 .First <- function() {
+  if (utils::compareVersion(paste0(R.version$major, ""."", R.version$minor), ""3.4.0"") == -1) {
+    warning(""Support for R prior to version 3.4 is deprecated since Spark 3.0.0"")
+  }
+
   home <- Sys.getenv(""SPARK_HOME"")
   .libPaths(c(file.path(home, ""R"", ""lib""), .libPaths()))
   Sys.setenv(NOAWT = 1)
diff --git a/R/pkg/tests/fulltests/test_Serde.R b/R/pkg/tests/fulltests/test_Serde.R
index 3577929323b8b..1525bdb2f5c8b 100644
--- a/R/pkg/tests/fulltests/test_Serde.R
+++ b/R/pkg/tests/fulltests/test_Serde.R
@@ -124,3 +124,35 @@ test_that(""SerDe of list of lists"", {
 })
 
 sparkR.session.stop()
+
+# Note that this test should be at the end of tests since the configruations used here are not
+# specific to sessions, and the Spark context is restarted.
+test_that(""createDataFrame large objects"", {
+  for (encryptionEnabled in list(""true"", ""false"")) {
+    # To simulate a large object scenario, we set spark.r.maxAllocationLimit to a smaller value
+    conf <- list(spark.r.maxAllocationLimit = ""100"",
+                 spark.io.encryption.enabled = encryptionEnabled)
+
+    suppressWarnings(sparkR.session(master = sparkRTestMaster,
+                                    sparkConfig = conf,
+                                    enableHiveSupport = FALSE))
+
+    sc <- getSparkContext()
+    actual <- callJStatic(""org.apache.spark.api.r.RUtils"", ""getEncryptionEnabled"", sc)
+    expected <- as.logical(encryptionEnabled)
+    expect_equal(actual, expected)
+
+    tryCatch({
+      # suppress warnings from dot in the field names. See also SPARK-21536.
+      df <- suppressWarnings(createDataFrame(iris, numPartitions = 3))
+      expect_equal(getNumPartitions(df), 3)
+      expect_equal(dim(df), dim(iris))
+
+      df <- createDataFrame(cars, numPartitions = 3)
+      expect_equal(collect(df), cars)
+    },
+    finally = {
+      sparkR.stop()
+    })
+  }
+})
diff --git a/R/pkg/tests/fulltests/test_context.R b/R/pkg/tests/fulltests/test_context.R
index 288a2714a554e..eb8d2a700e1ea 100644
--- a/R/pkg/tests/fulltests/test_context.R
+++ b/R/pkg/tests/fulltests/test_context.R
@@ -54,15 +54,6 @@ test_that(""Check masked functions"", {
                sort(namesOfMaskedCompletely, na.last = TRUE))
 })
 
-test_that(""repeatedly starting and stopping SparkR"", {
-  for (i in 1:4) {
-    sc <- suppressWarnings(sparkR.init(master = sparkRTestMaster))
-    rdd <- parallelize(sc, 1:20, 2L)
-    expect_equal(countRDD(rdd), 20)
-    suppressWarnings(sparkR.stop())
-  }
-})
-
 test_that(""repeatedly starting and stopping SparkSession"", {
   for (i in 1:4) {
     sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE)
@@ -101,9 +92,6 @@ test_that(""job group functions can be called"", {
   cancelJobGroup(""groupId"")
   clearJobGroup()
 
-  suppressWarnings(setJobGroup(sc, ""groupId"", ""job description"", TRUE))
-  suppressWarnings(cancelJobGroup(sc, ""groupId""))
-  suppressWarnings(clearJobGroup(sc))
   sparkR.session.stop()
 })
 
diff --git a/R/pkg/tests/fulltests/test_mllib_fpm.R b/R/pkg/tests/fulltests/test_mllib_fpm.R
index d80f66a25de1c..daf9ff97a8216 100644
--- a/R/pkg/tests/fulltests/test_mllib_fpm.R
+++ b/R/pkg/tests/fulltests/test_mllib_fpm.R
@@ -83,4 +83,20 @@ test_that(""spark.fpGrowth"", {
 
 })
 
+test_that(""spark.prefixSpan"", {
+    df <- createDataFrame(list(list(list(list(1L, 2L), list(3L))),
+                          list(list(list(1L), list(3L, 2L), list(1L, 2L))),
+                          list(list(list(1L, 2L), list(5L))),
+                          list(list(list(6L)))), schema = c(""sequence""))
+    result1 <- spark.findFrequentSequentialPatterns(df, minSupport = 0.5, maxPatternLength = 5L,
+                                                    maxLocalProjDBSize = 32000000L)
+
+    expected_result <- createDataFrame(list(list(list(list(1L)), 3L),
+                                            list(list(list(3L)), 2L),
+                                            list(list(list(2L)), 3L),
+                                            list(list(list(1L, 2L)), 3L),
+                                            list(list(list(1L), list(3L)), 2L)),
+                                            schema = c(""sequence"", ""freq""))
+  })
+
 sparkR.session.stop()
diff --git a/R/pkg/tests/fulltests/test_sparkSQL.R b/R/pkg/tests/fulltests/test_sparkSQL.R
index 0c4bdb31b027b..0d5118c127f2b 100644
--- a/R/pkg/tests/fulltests/test_sparkSQL.R
+++ b/R/pkg/tests/fulltests/test_sparkSQL.R
@@ -106,15 +106,6 @@ if (is_windows()) {
   Sys.setenv(TZ = ""GMT"")
 }
 
-test_that(""calling sparkRSQL.init returns existing SQL context"", {
-  sqlContext <- suppressWarnings(sparkRSQL.init(sc))
-  expect_equal(suppressWarnings(sparkRSQL.init(sc)), sqlContext)
-})
-
-test_that(""calling sparkRSQL.init returns existing SparkSession"", {
-  expect_equal(suppressWarnings(sparkRSQL.init(sc)), sparkSession)
-})
-
 test_that(""calling sparkR.session returns existing SparkSession"", {
   expect_equal(sparkR.session(), sparkSession)
 })
@@ -221,7 +212,7 @@ test_that(""structField type strings"", {
 
 test_that(""create DataFrame from RDD"", {
   rdd <- lapply(parallelize(sc, 1:10), function(x) { list(x, as.character(x)) })
-  df <- createDataFrame(rdd, list(""a"", ""b""))
+  df <- SparkR::createDataFrame(rdd, list(""a"", ""b""))
   dfAsDF <- as.DataFrame(rdd, list(""a"", ""b""))
   expect_is(df, ""SparkDataFrame"")
   expect_is(dfAsDF, ""SparkDataFrame"")
@@ -287,7 +278,7 @@ test_that(""create DataFrame from RDD"", {
 
   df <- as.DataFrame(cars, numPartitions = 2)
   expect_equal(getNumPartitions(df), 2)
-  df <- createDataFrame(cars, numPartitions = 3)
+  df <- SparkR::createDataFrame(cars, numPartitions = 3)
   expect_equal(getNumPartitions(df), 3)
   # validate limit by num of rows
   df <- createDataFrame(cars, numPartitions = 60)
@@ -308,7 +299,7 @@ test_that(""create DataFrame from RDD"", {
   sql(""CREATE TABLE people (name string, age double, height float)"")
   df <- read.df(jsonPathNa, ""json"", schema)
   insertInto(df, ""people"")
-  expect_equal(collect(sql(""SELECT age from people WHERE name = 'Bob'""))$age,
+  expect_equal(collect(SparkR::sql(""SELECT age from people WHERE name = 'Bob'""))$age,
                c(16))
   expect_equal(collect(sql(""SELECT height from people WHERE name ='Bob'""))$height,
                c(176.5))
@@ -316,18 +307,6 @@ test_that(""create DataFrame from RDD"", {
   unsetHiveContext()
 })
 
-test_that(""createDataFrame uses files for large objects"", {
-  # To simulate a large file scenario, we set spark.r.maxAllocationLimit to a smaller value
-  conf <- callJMethod(sparkSession, ""conf"")
-  callJMethod(conf, ""set"", ""spark.r.maxAllocationLimit"", ""100"")
-  df <- suppressWarnings(createDataFrame(iris, numPartitions = 3))
-  expect_equal(getNumPartitions(df), 3)
-
-  # Resetting the conf back to default value
-  callJMethod(conf, ""set"", ""spark.r.maxAllocationLimit"", toString(.Machine$integer.max / 10))
-  expect_equal(dim(df), dim(iris))
-})
-
 test_that(""read/write csv as DataFrame"", {
   if (windows_with_hadoop()) {
     csvPath <- tempfile(pattern = ""sparkr-test"", fileext = "".csv"")
@@ -640,14 +619,10 @@ test_that(""read/write json files"", {
     jsonPath3 <- tempfile(pattern = ""jsonPath3"", fileext = "".json"")
     write.json(df, jsonPath3)
 
-    # Test read.json()/jsonFile() works with multiple input paths
+    # Test read.json() works with multiple input paths
     jsonDF1 <- read.json(c(jsonPath2, jsonPath3))
     expect_is(jsonDF1, ""SparkDataFrame"")
     expect_equal(count(jsonDF1), 6)
-    # Suppress warnings because jsonFile is deprecated
-    jsonDF2 <- suppressWarnings(jsonFile(c(jsonPath2, jsonPath3)))
-    expect_is(jsonDF2, ""SparkDataFrame"")
-    expect_equal(count(jsonDF2), 6)
 
     unlink(jsonPath2)
     unlink(jsonPath3)
@@ -667,20 +642,6 @@ test_that(""read/write json files - compression option"", {
   unlink(jsonPath)
 })
 
-test_that(""jsonRDD() on a RDD with json string"", {
-  sqlContext <- suppressWarnings(sparkRSQL.init(sc))
-  rdd <- parallelize(sc, mockLines)
-  expect_equal(countRDD(rdd), 3)
-  df <- suppressWarnings(jsonRDD(sqlContext, rdd))
-  expect_is(df, ""SparkDataFrame"")
-  expect_equal(count(df), 3)
-
-  rdd2 <- flatMap(rdd, function(x) c(x, x))
-  df <- suppressWarnings(jsonRDD(sqlContext, rdd2))
-  expect_is(df, ""SparkDataFrame"")
-  expect_equal(count(df), 6)
-})
-
 test_that(""test tableNames and tables"", {
   count <- count(listTables())
 
@@ -695,10 +656,10 @@ test_that(""test tableNames and tables"", {
   expect_true(""tableName"" %in% colnames(tables()))
   expect_true(all(c(""tableName"", ""database"", ""isTemporary"") %in% colnames(tables())))
 
-  suppressWarnings(registerTempTable(df, ""table2""))
+  createOrReplaceTempView(df, ""table2"")
   tables <- listTables()
   expect_equal(count(tables), count + 2)
-  suppressWarnings(dropTempTable(""table1""))
+  dropTempView(""table1"")
   expect_true(dropTempView(""table2""))
 
   tables <- listTables()
@@ -1418,7 +1379,7 @@ test_that(""column operators"", {
 
 test_that(""column functions"", {
   c <- column(""a"")
-  c1 <- abs(c) + acos(c) + approxCountDistinct(c) + ascii(c) + asin(c) + atan(c)
+  c1 <- abs(c) + acos(c) + approx_count_distinct(c) + ascii(c) + asin(c) + atan(c)
   c2 <- avg(c) + base64(c) + bin(c) + bitwiseNOT(c) + cbrt(c) + ceil(c) + cos(c)
   c3 <- cosh(c) + count(c) + crc32(c) + hash(c) + exp(c)
   c4 <- explode(c) + expm1(c) + factorial(c) + first(c) + floor(c) + hex(c)
@@ -1427,7 +1388,7 @@ test_that(""column functions"", {
   c7 <- mean(c) + min(c) + month(c) + negate(c) + posexplode(c) + quarter(c)
   c8 <- reverse(c) + rint(c) + round(c) + rtrim(c) + sha1(c) + monotonically_increasing_id()
   c9 <- signum(c) + sin(c) + sinh(c) + size(c) + stddev(c) + soundex(c) + sqrt(c) + sum(c)
-  c10 <- sumDistinct(c) + tan(c) + tanh(c) + toDegrees(c) + toRadians(c)
+  c10 <- sumDistinct(c) + tan(c) + tanh(c) + degrees(c) + radians(c)
   c11 <- to_date(c) + trim(c) + unbase64(c) + unhex(c) + upper(c)
   c12 <- variance(c) + ltrim(c, ""a"") + rtrim(c, ""b"") + trim(c, ""c"")
   c13 <- lead(""col"", 1) + lead(c, 1) + lag(""col"", 1) + lag(c, 1)
@@ -1659,7 +1620,20 @@ test_that(""column functions"", {
   expect_equal(collect(select(df, bround(df$x, 0)))[[1]][1], 2)
   expect_equal(collect(select(df, bround(df$x, 0)))[[1]][2], 4)
 
-  # Test to_json(), from_json()
+  # Test from_csv(), schema_of_csv()
+  df <- as.DataFrame(list(list(""col"" = ""1"")))
+  c <- collect(select(df, alias(from_csv(df$col, ""a INT""), ""csv"")))
+  expect_equal(c[[1]][[1]]$a, 1)
+  c <- collect(select(df, alias(from_csv(df$col, lit(""a INT"")), ""csv"")))
+  expect_equal(c[[1]][[1]]$a, 1)
+
+  df <- as.DataFrame(list(list(""col"" = ""1"")))
+  c <- collect(select(df, schema_of_csv(""Amsterdam,2018"")))
+  expect_equal(c[[1]], ""struct<_c0:string,_c1:int>"")
+  c <- collect(select(df, schema_of_csv(lit(""Amsterdam,2018""))))
+  expect_equal(c[[1]], ""struct<_c0:string,_c1:int>"")
+
+  # Test to_json(), from_json(), schema_of_json()
   df <- sql(""SELECT array(named_struct('name', 'Bob'), named_struct('name', 'Alice')) as people"")
   j <- collect(select(df, alias(to_json(df$people), ""json"")))
   expect_equal(j[order(j$json), ][1], ""[{\""name\"":\""Bob\""},{\""name\"":\""Alice\""}]"")
@@ -1686,6 +1660,12 @@ test_that(""column functions"", {
     expect_true(any(apply(s, 1, function(x) { x[[1]]$age == 16 })))
   }
 
+  df <- as.DataFrame(list(list(""col"" = ""1"")))
+  c <- collect(select(df, schema_of_json('{""name"":""Bob""}')))
+  expect_equal(c[[1]], ""struct<name:string>"")
+  c <- collect(select(df, schema_of_json(lit('{""name"":""Bob""}'))))
+  expect_equal(c[[1]], ""struct<name:string>"")
+
   # Test to_json() supports arrays of primitive types and arrays
   df <- sql(""SELECT array(19, 42, 70) as age"")
   j <- collect(select(df, alias(to_json(df$age), ""json"")))
@@ -1699,14 +1679,14 @@ test_that(""column functions"", {
   df <- as.DataFrame(list(list(""col"" = ""{\""date\"":\""21/10/2014\""}"")))
   schema2 <- structType(structField(""date"", ""date""))
   s <- collect(select(df, from_json(df$col, schema2)))
-  expect_equal(s[[1]][[1]], NA)
+  expect_equal(s[[1]][[1]]$date, NA)
   s <- collect(select(df, from_json(df$col, schema2, dateFormat = ""dd/MM/yyyy"")))
   expect_is(s[[1]][[1]]$date, ""Date"")
   expect_equal(as.character(s[[1]][[1]]$date), ""2014-10-21"")
 
   # check for unparseable
   df <- as.DataFrame(list(list(""a"" = """")))
-  expect_equal(collect(select(df, from_json(df$a, schema)))[[1]][[1]], NA)
+  expect_equal(collect(select(df, from_json(df$a, schema)))[[1]][[1]]$a, NA)
 
   # check if array type in string is correctly supported.
   jsonArr <- ""[{\""name\"":\""Bob\""}, {\""name\"":\""Alice\""}]""
@@ -1721,6 +1701,11 @@ test_that(""column functions"", {
     expect_equal(arr$arrcol[[1]][[2]]$name, ""Alice"")
   }
 
+  # Test to_csv()
+  df <- sql(""SELECT named_struct('name', 'Bob') as people"")
+  j <- collect(select(df, alias(to_csv(df$people), ""csv"")))
+  expect_equal(j[order(j$csv), ][1], ""Bob"")
+
   # Test create_array() and create_map()
   df <- as.DataFrame(data.frame(
     x = c(1.0, 2.0), y = c(-1.0, 3.0), z = c(-2.0, 5.0)
@@ -1831,6 +1816,14 @@ test_that(""string operators"", {
     collect(select(df4, split_string(df4$a, ""\\\\"")))[1, 1],
     list(list(""a.b@c.d   1"", ""b""))
   )
+  expect_equal(
+    collect(select(df4, split_string(df4$a, ""\\."", 2)))[1, 1],
+    list(list(""a"", ""b@c.d   1\\b""))
+  )
+  expect_equal(
+    collect(select(df4, split_string(df4$a, ""b"", 0)))[1, 1],
+    list(list(""a."", ""@c.d   1\\"", """"))
+  )
 
   l5 <- list(list(a = ""abc""))
   df5 <- createDataFrame(l5)
@@ -2419,6 +2412,15 @@ test_that(""join(), crossJoin() and merge() on a DataFrame"", {
   expect_true(any(grepl(""BroadcastHashJoin"", execution_plan_broadcast)))
 })
 
+test_that(""test hint"", {
+  df <- sql(""SELECT * FROM range(10e10)"")
+  hintList <- list(""hint2"", ""hint3"", ""hint4"")
+  execution_plan_hint <- capture.output(
+    explain(hint(df, ""hint1"", 1.23456, ""aaaaaaaaaa"", hintList), TRUE)
+  )
+  expect_true(any(grepl(""1.23456, aaaaaaaaaa"", execution_plan_hint)))
+})
+
 test_that(""toJSON() on DataFrame"", {
   df <- as.DataFrame(cars)
   df_json <- toJSON(df)
@@ -2467,6 +2469,7 @@ test_that(""union(), unionByName(), rbind(), except(), and intersect() on a DataF
   expect_is(unioned, ""SparkDataFrame"")
   expect_equal(count(unioned), 6)
   expect_equal(first(unioned)$name, ""Michael"")
+  expect_equal(count(arrange(suppressWarnings(union(df, df2)), df$age)), 6)
   expect_equal(count(arrange(suppressWarnings(unionAll(df, df2)), df$age)), 6)
 
   df1 <- select(df2, ""age"", ""name"")
@@ -2646,17 +2649,14 @@ test_that(""read/write Parquet files"", {
     expect_is(df2, ""SparkDataFrame"")
     expect_equal(count(df2), 3)
 
-    # Test write.parquet/saveAsParquetFile and read.parquet/parquetFile
+    # Test write.parquet and read.parquet
     parquetPath2 <- tempfile(pattern = ""parquetPath2"", fileext = "".parquet"")
     write.parquet(df, parquetPath2)
     parquetPath3 <- tempfile(pattern = ""parquetPath3"", fileext = "".parquet"")
-    suppressWarnings(saveAsParquetFile(df, parquetPath3))
+    write.parquet(df, parquetPath3)
     parquetDF <- read.parquet(c(parquetPath2, parquetPath3))
     expect_is(parquetDF, ""SparkDataFrame"")
     expect_equal(count(parquetDF), count(df) * 2)
-    parquetDF2 <- suppressWarnings(parquetFile(parquetPath2, parquetPath3))
-    expect_is(parquetDF2, ""SparkDataFrame"")
-    expect_equal(count(parquetDF2), count(df) * 2)
 
     # Test if varargs works with variables
     saveMode <- ""overwrite""
@@ -2704,8 +2704,16 @@ test_that(""read/write text files"", {
   expect_equal(colnames(df2), c(""value""))
   expect_equal(count(df2), count(df) * 2)
 
+  df3 <- createDataFrame(list(list(1L, ""1""), list(2L, ""2""), list(1L, ""1""), list(2L, ""2"")),
+                         schema = c(""key"", ""value""))
+  textPath3 <- tempfile(pattern = ""textPath3"", fileext = "".txt"")
+  write.df(df3, textPath3, ""text"", mode = ""overwrite"", partitionBy = ""key"")
+  df4 <- read.df(textPath3, ""text"")
+  expect_equal(count(df3), count(df4))
+
   unlink(textPath)
   unlink(textPath2)
+  unlink(textPath3)
 })
 
 test_that(""read/write text files - compression option"", {
@@ -3457,39 +3465,6 @@ test_that(""Window functions on a DataFrame"", {
   expect_equal(result, expected)
 })
 
-test_that(""createDataFrame sqlContext parameter backward compatibility"", {
-  sqlContext <- suppressWarnings(sparkRSQL.init(sc))
-  a <- 1:3
-  b <- c(""a"", ""b"", ""c"")
-  ldf <- data.frame(a, b)
-  # Call function with namespace :: operator - SPARK-16538
-  df <- suppressWarnings(SparkR::createDataFrame(sqlContext, ldf))
-  expect_equal(columns(df), c(""a"", ""b""))
-  expect_equal(dtypes(df), list(c(""a"", ""int""), c(""b"", ""string"")))
-  expect_equal(count(df), 3)
-  ldf2 <- collect(df)
-  expect_equal(ldf$a, ldf2$a)
-
-  df2 <- suppressWarnings(createDataFrame(sqlContext, iris))
-  expect_equal(count(df2), 150)
-  expect_equal(ncol(df2), 5)
-
-  df3 <- suppressWarnings(read.df(sqlContext, jsonPath, ""json""))
-  expect_is(df3, ""SparkDataFrame"")
-  expect_equal(count(df3), 3)
-
-  before <- suppressWarnings(createDataFrame(sqlContext, iris))
-  after <- suppressWarnings(createDataFrame(iris))
-  expect_equal(collect(before), collect(after))
-
-  # more tests for SPARK-16538
-  createOrReplaceTempView(df, ""table"")
-  SparkR::listTables()
-  SparkR::sql(""SELECT 1"")
-  suppressWarnings(SparkR::sql(sqlContext, ""SELECT * FROM table""))
-  suppressWarnings(SparkR::dropTempTable(sqlContext, ""table""))
-})
-
 test_that(""randomSplit"", {
   num <- 4000
   df <- createDataFrame(data.frame(id = 1:num))
@@ -3676,7 +3651,7 @@ test_that(""catalog APIs, listTables, listColumns, listFunctions"", {
 
   createOrReplaceTempView(as.DataFrame(cars), ""cars"")
 
-  tb <- listTables()
+  tb <- SparkR::listTables()
   expect_equal(nrow(tb), count + 1)
   tbs <- collect(tb)
   expect_true(nrow(tbs[tbs$name == ""cars"", ]) > 0)
diff --git a/R/pkg/tests/fulltests/test_sparkSQL_eager.R b/R/pkg/tests/fulltests/test_sparkSQL_eager.R
new file mode 100644
index 0000000000000..9b4489a47b655
--- /dev/null
+++ b/R/pkg/tests/fulltests/test_sparkSQL_eager.R
@@ -0,0 +1,72 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+library(testthat)
+
+context(""test show SparkDataFrame when eager execution is enabled."")
+
+test_that(""eager execution is not enabled"", {
+  # Start Spark session without eager execution enabled
+  sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE)
+
+  df <- createDataFrame(faithful)
+  expect_is(df, ""SparkDataFrame"")
+  expected <- ""eruptions:double, waiting:double""
+  expect_output(show(df), expected)
+
+  # Stop Spark session
+  sparkR.session.stop()
+})
+
+test_that(""eager execution is enabled"", {
+  # Start Spark session with eager execution enabled
+  sparkConfig <- list(spark.sql.repl.eagerEval.enabled = ""true"")
+
+  sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE, sparkConfig = sparkConfig)
+
+  df <- createDataFrame(faithful)
+  expect_is(df, ""SparkDataFrame"")
+  expected <- paste0(""(+---------+-------+\n"",
+                     ""|eruptions|waiting|\n"",
+                     ""+---------+-------+\n)*"",
+                     ""(only showing top 20 rows)"")
+  expect_output(show(df), expected)
+
+  # Stop Spark session
+  sparkR.session.stop()
+})
+
+test_that(""eager execution is enabled with maxNumRows and truncate set"", {
+  # Start Spark session with eager execution enabled
+  sparkConfig <- list(spark.sql.repl.eagerEval.enabled = ""true"",
+                      spark.sql.repl.eagerEval.maxNumRows = as.integer(5),
+                      spark.sql.repl.eagerEval.truncate = as.integer(2))
+
+  sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE, sparkConfig = sparkConfig)
+
+  df <- arrange(createDataFrame(faithful), ""waiting"")
+  expect_is(df, ""SparkDataFrame"")
+  expected <- paste0(""(+---------+-------+\n"",
+                     ""|eruptions|waiting|\n"",
+                     ""+---------+-------+\n"",
+                     ""|       1.|     43|\n)*"",
+                     ""(only showing top 5 rows)"")
+  expect_output(show(df), expected)
+
+  # Stop Spark session
+  sparkR.session.stop()
+})
diff --git a/R/pkg/tests/fulltests/test_streaming.R b/R/pkg/tests/fulltests/test_streaming.R
index bfb1a046490ec..6f0d2aefee886 100644
--- a/R/pkg/tests/fulltests/test_streaming.R
+++ b/R/pkg/tests/fulltests/test_streaming.R
@@ -127,6 +127,7 @@ test_that(""Specify a schema by using a DDL-formatted string when reading"", {
   expect_false(awaitTermination(q, 5 * 1000))
   callJMethod(q@ssq, ""processAllAvailable"")
   expect_equal(head(sql(""SELECT count(*) FROM people3""))[[1]], 3)
+  stopQuery(q)
 
   expect_error(read.stream(path = parquetPath, schema = ""name stri""),
                ""DataType stri is not supported."")
diff --git a/R/pkg/tests/run-all.R b/R/pkg/tests/run-all.R
index 94d75188fb948..1e96418558883 100644
--- a/R/pkg/tests/run-all.R
+++ b/R/pkg/tests/run-all.R
@@ -18,50 +18,55 @@
 library(testthat)
 library(SparkR)
 
-# Turn all warnings into errors
-options(""warn"" = 2)
+# SPARK-25572
+if (identical(Sys.getenv(""NOT_CRAN""), ""true"")) {
 
-if (.Platform$OS.type == ""windows"") {
-  Sys.setenv(TZ = ""GMT"")
-}
+  # Turn all warnings into errors
+  options(""warn"" = 2)
 
-# Setup global test environment
-# Install Spark first to set SPARK_HOME
+  if (.Platform$OS.type == ""windows"") {
+    Sys.setenv(TZ = ""GMT"")
+  }
 
-# NOTE(shivaram): We set overwrite to handle any old tar.gz files or directories left behind on
-# CRAN machines. For Jenkins we should already have SPARK_HOME set.
-install.spark(overwrite = TRUE)
+  # Setup global test environment
+  # Install Spark first to set SPARK_HOME
 
-sparkRDir <- file.path(Sys.getenv(""SPARK_HOME""), ""R"")
-sparkRWhitelistSQLDirs <- c(""spark-warehouse"", ""metastore_db"")
-invisible(lapply(sparkRWhitelistSQLDirs,
-                 function(x) { unlink(file.path(sparkRDir, x), recursive = TRUE, force = TRUE)}))
-sparkRFilesBefore <- list.files(path = sparkRDir, all.files = TRUE)
+  # NOTE(shivaram): We set overwrite to handle any old tar.gz files or directories left behind on
+  # CRAN machines. For Jenkins we should already have SPARK_HOME set.
+  install.spark(overwrite = TRUE)
 
-sparkRTestMaster <- ""local[1]""
-sparkRTestConfig <- list()
-if (identical(Sys.getenv(""NOT_CRAN""), ""true"")) {
-  sparkRTestMaster <- """"
-} else {
-  # Disable hsperfdata on CRAN
-  old_java_opt <- Sys.getenv(""_JAVA_OPTIONS"")
-  Sys.setenv(""_JAVA_OPTIONS"" = paste(""-XX:-UsePerfData"", old_java_opt))
-  tmpDir <- tempdir()
-  tmpArg <- paste0(""-Djava.io.tmpdir="", tmpDir)
-  sparkRTestConfig <- list(spark.driver.extraJavaOptions = tmpArg,
-                           spark.executor.extraJavaOptions = tmpArg)
-}
+  sparkRDir <- file.path(Sys.getenv(""SPARK_HOME""), ""R"")
+  sparkRWhitelistSQLDirs <- c(""spark-warehouse"", ""metastore_db"")
+  invisible(lapply(sparkRWhitelistSQLDirs,
+                   function(x) { unlink(file.path(sparkRDir, x), recursive = TRUE, force = TRUE)}))
+  sparkRFilesBefore <- list.files(path = sparkRDir, all.files = TRUE)
 
-test_package(""SparkR"")
+  sparkRTestMaster <- ""local[1]""
+  sparkRTestConfig <- list()
+  if (identical(Sys.getenv(""NOT_CRAN""), ""true"")) {
+    sparkRTestMaster <- """"
+  } else {
+    # Disable hsperfdata on CRAN
+    old_java_opt <- Sys.getenv(""_JAVA_OPTIONS"")
+    Sys.setenv(""_JAVA_OPTIONS"" = paste(""-XX:-UsePerfData"", old_java_opt))
+    tmpDir <- tempdir()
+    tmpArg <- paste0(""-Djava.io.tmpdir="", tmpDir)
+    sparkRTestConfig <- list(spark.driver.extraJavaOptions = tmpArg,
+                             spark.executor.extraJavaOptions = tmpArg)
+  }
 
-if (identical(Sys.getenv(""NOT_CRAN""), ""true"")) {
-  # set random seed for predictable results. mostly for base's sample() in tree and classification
-  set.seed(42)
-  # for testthat 1.0.2 later, change reporter from ""summary"" to default_reporter()
-  testthat:::run_tests(""SparkR"",
-                       file.path(sparkRDir, ""pkg"", ""tests"", ""fulltests""),
-                       NULL,
-                       ""summary"")
-}
+  test_package(""SparkR"")
+
+  if (identical(Sys.getenv(""NOT_CRAN""), ""true"")) {
+    # set random seed for predictable results. mostly for base's sample() in tree and classification
+    set.seed(42)
+    # for testthat 1.0.2 later, change reporter from ""summary"" to default_reporter()
+    testthat:::run_tests(""SparkR"",
+                         file.path(sparkRDir, ""pkg"", ""tests"", ""fulltests""),
+                         NULL,
+                         ""summary"")
+  }
 
-SparkR:::uninstallDownloadedSpark()
+  SparkR:::uninstallDownloadedSpark()
+
+}
diff --git a/R/pkg/vignettes/sparkr-vignettes.Rmd b/R/pkg/vignettes/sparkr-vignettes.Rmd
index 090363c5f8a3e..f80b45b4f36a8 100644
--- a/R/pkg/vignettes/sparkr-vignettes.Rmd
+++ b/R/pkg/vignettes/sparkr-vignettes.Rmd
@@ -57,6 +57,20 @@ First, let's load and attach the package.
 library(SparkR)
 ```
 
+```{r, include=FALSE}
+# disable eval if java version not supported
+override_eval <- tryCatch(!is.numeric(SparkR:::checkJavaVersion()),
+          error = function(e) { TRUE },
+          warning = function(e) { TRUE })
+
+if (override_eval) {
+  opts_hooks$set(eval = function(options) {
+    options$eval = FALSE
+    options
+  })
+}
+```
+
 `SparkSession` is the entry point into SparkR which connects your R program to a Spark cluster. You can create a `SparkSession` using `sparkR.session` and pass in options such as the application name, any Spark packages depended on, etc.
 
 We use default settings in which it runs in local mode. It auto downloads Spark package in the background if no previous installation is found. For more details about setup, see [Spark Session](#SetupSparkSession).
@@ -157,8 +171,8 @@ Property Name | Property group | spark-submit equivalent
 `spark.driver.extraClassPath` | Runtime Environment | `--driver-class-path`
 `spark.driver.extraJavaOptions` | Runtime Environment | `--driver-java-options`
 `spark.driver.extraLibraryPath` | Runtime Environment | `--driver-library-path`
-`spark.yarn.keytab` | Application Properties | `--keytab`
-`spark.yarn.principal` | Application Properties | `--principal`
+`spark.kerberos.keytab` | Application Properties | `--keytab`
+`spark.kerberos.principal` | Application Properties | `--principal`
 
 **For Windows users**: Due to different file prefixes across operating systems, to avoid the issue of potential wrong prefix, a current workaround is to specify `spark.sql.warehouse.dir` when starting the `SparkSession`.
 
@@ -542,6 +556,7 @@ SparkR supports the following machine learning models and algorithms.
 #### Frequent Pattern Mining
 
 * FP-growth
+* PrefixSpan
 
 #### Statistics
 
@@ -998,6 +1013,18 @@ We can make predictions based on the `antecedent`.
 head(predict(fpm, df))
 ```
 
+#### PrefixSpan
+
+`spark.findFrequentSequentialPatterns` method can be used to find the complete set of frequent sequential patterns in the input sequences of itemsets.
+
+```{r}
+df <- createDataFrame(list(list(list(list(1L, 2L), list(3L))),
+                      list(list(list(1L), list(3L, 2L), list(1L, 2L))),
+                      list(list(list(1L, 2L), list(5L))),
+                      list(list(list(6L)))), schema = c(""sequence""))
+head(spark.findFrequentSequentialPatterns(df, minSupport = 0.5, maxPatternLength = 5L))
+```
+
 #### Kolmogorov-Smirnov Test
 
 `spark.kstest` runs a two-sided, one-sample [Kolmogorov-Smirnov (KS) test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).
diff --git a/assembly/README b/assembly/README
index affd281a1385c..1fd6d8858348c 100644
--- a/assembly/README
+++ b/assembly/README
@@ -9,4 +9,4 @@ This module is off by default. To activate it specify the profile in the command
 
 If you need to build an assembly for a different version of Hadoop the
 hadoop-version system property needs to be set as in this example:
-  -Dhadoop.version=2.7.7
+  -Dhadoop.version=2.7.4
diff --git a/assembly/pom.xml b/assembly/pom.xml
index 9608c96fd5369..68ebfadb668ab 100644
--- a/assembly/pom.xml
+++ b/assembly/pom.xml
@@ -20,12 +20,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-assembly_2.11</artifactId>
+  <artifactId>spark-assembly_2.12</artifactId>
   <name>Spark Project Assembly</name>
   <url>http://spark.apache.org/</url>
   <packaging>pom</packaging>
diff --git a/bin/docker-image-tool.sh b/bin/docker-image-tool.sh
index d6371051ef7fb..fbf9c9e448fd1 100755
--- a/bin/docker-image-tool.sh
+++ b/bin/docker-image-tool.sh
@@ -29,6 +29,20 @@ if [ -z ""${SPARK_HOME}"" ]; then
 fi
 . ""${SPARK_HOME}/bin/load-spark-env.sh""
 
+CTX_DIR=""$SPARK_HOME/target/tmp/docker""
+
+function is_dev_build {
+  [ ! -f ""$SPARK_HOME/RELEASE"" ]
+}
+
+function cleanup_ctx_dir {
+  if is_dev_build; then
+    rm -rf ""$CTX_DIR""
+  fi
+}
+
+trap cleanup_ctx_dir EXIT
+
 function image_ref {
   local image=""$1""
   local add_repo=""${2:-1}""
@@ -41,55 +55,142 @@ function image_ref {
   echo ""$image""
 }
 
+function docker_push {
+  local image_name=""$1""
+  if [ ! -z $(docker images -q ""$(image_ref ${image_name})"") ]; then
+    docker push ""$(image_ref ${image_name})""
+    if [ $? -ne 0 ]; then
+      error ""Failed to push $image_name Docker image.""
+    fi
+  else
+    echo ""$(image_ref ${image_name}) image not found. Skipping push for this image.""
+  fi
+}
+
+# Create a smaller build context for docker in dev builds to make the build faster. Docker
+# uploads all of the current directory to the daemon, and it can get pretty big with dev
+# builds that contain test log files and other artifacts.
+#
+# Three build contexts are created, one for each image: base, pyspark, and sparkr. For them
+# to have the desired effect, the docker command needs to be executed inside the appropriate
+# context directory.
+#
+# Note: docker does not support symlinks in the build context.
+function create_dev_build_context {(
+  set -e
+  local BASE_CTX=""$CTX_DIR/base""
+  mkdir -p ""$BASE_CTX/kubernetes""
+  cp -r ""resource-managers/kubernetes/docker/src/main/dockerfiles"" \
+    ""$BASE_CTX/kubernetes/dockerfiles""
+
+  cp -r ""assembly/target/scala-$SPARK_SCALA_VERSION/jars"" ""$BASE_CTX/jars""
+  cp -r ""resource-managers/kubernetes/integration-tests/tests"" \
+    ""$BASE_CTX/kubernetes/tests""
+
+  mkdir ""$BASE_CTX/examples""
+  cp -r ""examples/src"" ""$BASE_CTX/examples/src""
+  # Copy just needed examples jars instead of everything.
+  mkdir ""$BASE_CTX/examples/jars""
+  for i in examples/target/scala-$SPARK_SCALA_VERSION/jars/*; do
+    if [ ! -f ""$BASE_CTX/jars/$(basename $i)"" ]; then
+      cp $i ""$BASE_CTX/examples/jars""
+    fi
+  done
+
+  for other in bin sbin data; do
+    cp -r ""$other"" ""$BASE_CTX/$other""
+  done
+
+  local PYSPARK_CTX=""$CTX_DIR/pyspark""
+  mkdir -p ""$PYSPARK_CTX/kubernetes""
+  cp -r ""resource-managers/kubernetes/docker/src/main/dockerfiles"" \
+    ""$PYSPARK_CTX/kubernetes/dockerfiles""
+  mkdir ""$PYSPARK_CTX/python""
+  cp -r ""python/lib"" ""$PYSPARK_CTX/python/lib""
+
+  local R_CTX=""$CTX_DIR/sparkr""
+  mkdir -p ""$R_CTX/kubernetes""
+  cp -r ""resource-managers/kubernetes/docker/src/main/dockerfiles"" \
+    ""$R_CTX/kubernetes/dockerfiles""
+  cp -r ""R"" ""$R_CTX/R""
+)}
+
+function img_ctx_dir {
+  if is_dev_build; then
+    echo ""$CTX_DIR/$1""
+  else
+    echo ""$SPARK_HOME""
+  fi
+}
+
 function build {
   local BUILD_ARGS
-  local IMG_PATH
-
-  if [ ! -f ""$SPARK_HOME/RELEASE"" ]; then
-    # Set image build arguments accordingly if this is a source repo and not a distribution archive.
-    IMG_PATH=resource-managers/kubernetes/docker/src/main/dockerfiles
-    BUILD_ARGS=(
-      ${BUILD_PARAMS}
-      --build-arg
-      img_path=$IMG_PATH
-      --build-arg
-      spark_jars=assembly/target/scala-$SPARK_SCALA_VERSION/jars
-    )
-  else
-    # Not passed as an argument to docker, but used to validate the Spark directory.
-    IMG_PATH=""kubernetes/dockerfiles""
-    BUILD_ARGS=(${BUILD_PARAMS})
+  local SPARK_ROOT=""$SPARK_HOME""
+
+  if is_dev_build; then
+    create_dev_build_context || error ""Failed to create docker build context.""
+    SPARK_ROOT=""$CTX_DIR/base""
   fi
 
-  if [ ! -d ""$IMG_PATH"" ]; then
+  # Verify that the Docker image content directory is present
+  if [ ! -d ""$SPARK_ROOT/kubernetes/dockerfiles"" ]; then
     error ""Cannot find docker image. This script must be run from a runnable distribution of Apache Spark.""
   fi
+
+  # Verify that Spark has actually been built/is a runnable distribution
+  # i.e. the Spark JARs that the Docker files will place into the image are present
+  local TOTAL_JARS=$(ls $SPARK_ROOT/jars/spark-* | wc -l)
+  TOTAL_JARS=$(( $TOTAL_JARS ))
+  if [ ""${TOTAL_JARS}"" -eq 0 ]; then
+    error ""Cannot find Spark JARs. This script assumes that Apache Spark has first been built locally or this is a runnable distribution.""
+  fi
+
+  local BUILD_ARGS=(${BUILD_PARAMS})
+
+  # If a custom SPARK_UID was set add it to build arguments
+  if [ -n ""$SPARK_UID"" ]; then
+    BUILD_ARGS+=(--build-arg spark_uid=$SPARK_UID)
+  fi
+
   local BINDING_BUILD_ARGS=(
     ${BUILD_PARAMS}
     --build-arg
     base_img=$(image_ref spark)
   )
-  local BASEDOCKERFILE=${BASEDOCKERFILE:-""$IMG_PATH/spark/Dockerfile""}
-  local PYDOCKERFILE=${PYDOCKERFILE:-""$IMG_PATH/spark/bindings/python/Dockerfile""}
-  local RDOCKERFILE=${RDOCKERFILE:-""$IMG_PATH/spark/bindings/R/Dockerfile""}
+  local BASEDOCKERFILE=${BASEDOCKERFILE:-""kubernetes/dockerfiles/spark/Dockerfile""}
+  local PYDOCKERFILE=${PYDOCKERFILE:-false}
+  local RDOCKERFILE=${RDOCKERFILE:-false}
 
-  docker build $NOCACHEARG ""${BUILD_ARGS[@]}"" \
+  (cd $(img_ctx_dir base) && docker build $NOCACHEARG ""${BUILD_ARGS[@]}"" \
     -t $(image_ref spark) \
-    -f ""$BASEDOCKERFILE"" .
+    -f ""$BASEDOCKERFILE"" .)
+  if [ $? -ne 0 ]; then
+    error ""Failed to build Spark JVM Docker image, please refer to Docker build output for details.""
+  fi
 
-  docker build $NOCACHEARG ""${BINDING_BUILD_ARGS[@]}"" \
-    -t $(image_ref spark-py) \
-    -f ""$PYDOCKERFILE"" .
+  if [ ""${PYDOCKERFILE}"" != ""false"" ]; then
+    (cd $(img_ctx_dir pyspark) && docker build $NOCACHEARG ""${BINDING_BUILD_ARGS[@]}"" \
+      -t $(image_ref spark-py) \
+      -f ""$PYDOCKERFILE"" .)
+      if [ $? -ne 0 ]; then
+        error ""Failed to build PySpark Docker image, please refer to Docker build output for details.""
+      fi
+  fi
 
-  docker build $NOCACHEARG ""${BINDING_BUILD_ARGS[@]}"" \
-    -t $(image_ref spark-r) \
-    -f ""$RDOCKERFILE"" .
+  if [ ""${RDOCKERFILE}"" != ""false"" ]; then
+    (cd $(img_ctx_dir sparkr) && docker build $NOCACHEARG ""${BINDING_BUILD_ARGS[@]}"" \
+      -t $(image_ref spark-r) \
+      -f ""$RDOCKERFILE"" .)
+    if [ $? -ne 0 ]; then
+      error ""Failed to build SparkR Docker image, please refer to Docker build output for details.""
+    fi
+  fi
 }
 
 function push {
-  docker push ""$(image_ref spark)""
-  docker push ""$(image_ref spark-py)""
-  docker push ""$(image_ref spark-r)""
+  docker_push ""spark""
+  docker_push ""spark-py""
+  docker_push ""spark-r""
 }
 
 function usage {
@@ -104,14 +205,18 @@ Commands:
 
 Options:
   -f file               Dockerfile to build for JVM based Jobs. By default builds the Dockerfile shipped with Spark.
-  -p file               Dockerfile to build for PySpark Jobs. Builds Python dependencies and ships with Spark.
-  -R file               Dockerfile to build for SparkR Jobs. Builds R dependencies and ships with Spark.
+  -p file               (Optional) Dockerfile to build for PySpark Jobs. Builds Python dependencies and ships with Spark.
+                        Skips building PySpark docker image if not specified.
+  -R file               (Optional) Dockerfile to build for SparkR Jobs. Builds R dependencies and ships with Spark.
+                        Skips building SparkR docker image if not specified.
   -r repo               Repository address.
   -t tag                Tag to apply to the built image, or to identify the image to be pushed.
   -m                    Use minikube's Docker daemon.
   -n                    Build docker image with --no-cache
-  -b arg      Build arg to build or push the image. For multiple build args, this option needs to
-              be used separately for each build arg.
+  -u uid                UID to use in the USER directive to set the user the main Spark process runs as inside the
+                        resulting container
+  -b arg                Build arg to build or push the image. For multiple build args, this option needs to
+                        be used separately for each build arg.
 
 Using minikube when building images will do so directly into minikube's Docker daemon.
 There is no need to push the images into minikube in that case, they'll be automatically
@@ -125,6 +230,9 @@ Examples:
   - Build image in minikube with tag ""testing""
     $0 -m -t testing build
 
+  - Build PySpark docker image
+    $0 -r docker.io/myrepo -t v2.3.0 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
+
   - Build and push image with tag ""v2.3.0"" to docker.io/myrepo
     $0 -r docker.io/myrepo -t v2.3.0 build
     $0 -r docker.io/myrepo -t v2.3.0 push
@@ -143,7 +251,8 @@ PYDOCKERFILE=
 RDOCKERFILE=
 NOCACHEARG=
 BUILD_PARAMS=
-while getopts f:p:R:mr:t:n:b: option
+SPARK_UID=
+while getopts f:p:R:mr:t:nb:u: option
 do
  case ""${option}""
  in
@@ -158,8 +267,12 @@ do
    if ! which minikube 1>/dev/null; then
      error ""Cannot find minikube.""
    fi
+   if ! minikube status 1>/dev/null; then
+     error ""Cannot contact minikube. Make sure it's running.""
+   fi
    eval $(minikube docker-env)
    ;;
+  u) SPARK_UID=${OPTARG};;
  esac
 done
 
diff --git a/bin/load-spark-env.sh b/bin/load-spark-env.sh
index 0b5006dbd63ac..0ada5d8d0fc1d 100644
--- a/bin/load-spark-env.sh
+++ b/bin/load-spark-env.sh
@@ -26,15 +26,17 @@ if [ -z ""${SPARK_HOME}"" ]; then
   source ""$(dirname ""$0"")""/find-spark-home
 fi
 
+SPARK_ENV_SH=""spark-env.sh""
 if [ -z ""$SPARK_ENV_LOADED"" ]; then
   export SPARK_ENV_LOADED=1
 
   export SPARK_CONF_DIR=""${SPARK_CONF_DIR:-""${SPARK_HOME}""/conf}""
 
-  if [ -f ""${SPARK_CONF_DIR}/spark-env.sh"" ]; then
+  SPARK_ENV_SH=""${SPARK_CONF_DIR}/${SPARK_ENV_SH}""
+  if [[ -f ""${SPARK_ENV_SH}"" ]]; then
     # Promote all variable declarations to environment (exported) variables
     set -a
-    . ""${SPARK_CONF_DIR}/spark-env.sh""
+    . ${SPARK_ENV_SH}
     set +a
   fi
 fi
@@ -42,19 +44,22 @@ fi
 # Setting SPARK_SCALA_VERSION if not already set.
 
 if [ -z ""$SPARK_SCALA_VERSION"" ]; then
+  SCALA_VERSION_1=2.12
+  SCALA_VERSION_2=2.11
 
-  ASSEMBLY_DIR2=""${SPARK_HOME}/assembly/target/scala-2.11""
-  ASSEMBLY_DIR1=""${SPARK_HOME}/assembly/target/scala-2.12""
-
-  if [[ -d ""$ASSEMBLY_DIR2"" && -d ""$ASSEMBLY_DIR1"" ]]; then
-    echo -e ""Presence of build for multiple Scala versions detected."" 1>&2
-    echo -e 'Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.' 1>&2
+  ASSEMBLY_DIR_1=""${SPARK_HOME}/assembly/target/scala-${SCALA_VERSION_1}""
+  ASSEMBLY_DIR_2=""${SPARK_HOME}/assembly/target/scala-${SCALA_VERSION_2}""
+  ENV_VARIABLE_DOC=""https://spark.apache.org/docs/latest/configuration.html#environment-variables""
+  if [[ -d ""$ASSEMBLY_DIR_1"" && -d ""$ASSEMBLY_DIR_2"" ]]; then
+    echo ""Presence of build for multiple Scala versions detected ($ASSEMBLY_DIR_1 and $ASSEMBLY_DIR_2)."" 1>&2
+    echo ""Remove one of them or, export SPARK_SCALA_VERSION=$SCALA_VERSION_1 in ${SPARK_ENV_SH}."" 1>&2
+    echo ""Visit ${ENV_VARIABLE_DOC} for more details about setting environment variables in spark-env.sh."" 1>&2
     exit 1
   fi
 
-  if [ -d ""$ASSEMBLY_DIR2"" ]; then
-    export SPARK_SCALA_VERSION=""2.11""
+  if [[ -d ""$ASSEMBLY_DIR_1"" ]]; then
+    export SPARK_SCALA_VERSION=${SCALA_VERSION_1}
   else
-    export SPARK_SCALA_VERSION=""2.12""
+    export SPARK_SCALA_VERSION=${SCALA_VERSION_2}
   fi
 fi
diff --git a/bin/pyspark b/bin/pyspark
index 5d5affb1f97c3..1dcddcc6196b8 100755
--- a/bin/pyspark
+++ b/bin/pyspark
@@ -57,7 +57,7 @@ export PYSPARK_PYTHON
 
 # Add the PySpark classes to the Python path:
 export PYTHONPATH=""${SPARK_HOME}/python/:$PYTHONPATH""
-export PYTHONPATH=""${SPARK_HOME}/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH""
+export PYTHONPATH=""${SPARK_HOME}/python/lib/py4j-0.10.8.1-src.zip:$PYTHONPATH""
 
 # Load the PySpark shell.py script when ./pyspark is used interactively:
 export OLD_PYTHONSTARTUP=""$PYTHONSTARTUP""
diff --git a/bin/pyspark2.cmd b/bin/pyspark2.cmd
index 15fa910c277b3..479fd464c7d3e 100644
--- a/bin/pyspark2.cmd
+++ b/bin/pyspark2.cmd
@@ -30,7 +30,7 @@ if ""x%PYSPARK_DRIVER_PYTHON%""==""x"" (
 )
 
 set PYTHONPATH=%SPARK_HOME%\python;%PYTHONPATH%
-set PYTHONPATH=%SPARK_HOME%\python\lib\py4j-0.10.7-src.zip;%PYTHONPATH%
+set PYTHONPATH=%SPARK_HOME%\python\lib\py4j-0.10.8.1-src.zip;%PYTHONPATH%
 
 set OLD_PYTHONSTARTUP=%PYTHONSTARTUP%
 set PYTHONSTARTUP=%SPARK_HOME%\python\pyspark\shell.py
diff --git a/bin/spark-shell b/bin/spark-shell
index 421f36cac3d47..e920137974980 100755
--- a/bin/spark-shell
+++ b/bin/spark-shell
@@ -32,7 +32,10 @@ if [ -z ""${SPARK_HOME}"" ]; then
   source ""$(dirname ""$0"")""/find-spark-home
 fi
 
-export _SPARK_CMD_USAGE=""Usage: ./bin/spark-shell [options]""
+export _SPARK_CMD_USAGE=""Usage: ./bin/spark-shell [options]
+
+Scala REPL options:
+  -I <file>                   preload <file>, enforcing line-by-line interpretation""
 
 # SPARK-4161: scala does not assume use of the java classpath,
 # so we need to add the ""-Dscala.usejavacp=true"" flag manually. We
diff --git a/bin/spark-shell2.cmd b/bin/spark-shell2.cmd
index aaf71906c6526..549bf43bb6078 100644
--- a/bin/spark-shell2.cmd
+++ b/bin/spark-shell2.cmd
@@ -20,7 +20,13 @@ rem
 rem Figure out where the Spark framework is installed
 call ""%~dp0find-spark-home.cmd""
 
-set _SPARK_CMD_USAGE=Usage: .\bin\spark-shell.cmd [options]
+set LF=^
+
+
+rem two empty lines are required
+set _SPARK_CMD_USAGE=Usage: .\bin\spark-shell.cmd [options]^%LF%%LF%^%LF%%LF%^
+Scala REPL options:^%LF%%LF%^
+  -I ^<file^>                   preload ^<file^>, enforcing line-by-line interpretation
 
 rem SPARK-4161: scala does not assume use of the java classpath,
 rem so we need to add the ""-Dscala.usejavacp=true"" flag manually. We
diff --git a/build/mvn b/build/mvn
index 2487b81abb4ea..4cb10e0d03fa4 100755
--- a/build/mvn
+++ b/build/mvn
@@ -116,7 +116,8 @@ install_zinc() {
 # the build/ folder
 install_scala() {
   # determine the Scala version used in Spark
-  local scala_version=`grep ""scala.version"" ""${_DIR}/../pom.xml"" | head -n1 | awk -F '[<>]' '{print $3}'`
+  local scala_binary_version=`grep ""scala.binary.version"" ""${_DIR}/../pom.xml"" | head -n1 | awk -F '[<>]' '{print $3}'`
+  local scala_version=`grep ""scala.version"" ""${_DIR}/../pom.xml"" | grep ${scala_binary_version} | head -n1 | awk -F '[<>]' '{print $3}'`
   local scala_bin=""${_DIR}/scala-${scala_version}/bin/scala""
   local TYPESAFE_MIRROR=${TYPESAFE_MIRROR:-https://downloads.lightbend.com}
 
@@ -153,6 +154,7 @@ if [ -n ""${ZINC_INSTALL_FLAG}"" -o -z ""`""${ZINC_BIN}"" -status -port ${ZINC_PORT}`
   export ZINC_OPTS=${ZINC_OPTS:-""$_COMPILE_JVM_OPTS""}
   ""${ZINC_BIN}"" -shutdown -port ${ZINC_PORT}
   ""${ZINC_BIN}"" -start -port ${ZINC_PORT} \
+    -server 127.0.0.1 -idle-timeout 3h \
     -scala-compiler ""${SCALA_COMPILER}"" \
     -scala-library ""${SCALA_LIBRARY}"" &>/dev/null
 fi
@@ -162,5 +164,12 @@ export MAVEN_OPTS=${MAVEN_OPTS:-""$_COMPILE_JVM_OPTS""}
 
 echo ""Using \`mvn\` from path: $MVN_BIN"" 1>&2
 
-# Last, call the `mvn` command as usual
+# call the `mvn` command as usual
+# SPARK-25854
 ""${MVN_BIN}"" -DzincPort=${ZINC_PORT} ""$@""
+MVN_RETCODE=$?
+
+# Try to shut down zinc explicitly if the server is still running.
+""${ZINC_BIN}"" -shutdown -port ${ZINC_PORT}
+
+exit $MVN_RETCODE
diff --git a/common/kvstore/pom.xml b/common/kvstore/pom.xml
index 8c148359c3029..f042a12fda3d2 100644
--- a/common/kvstore/pom.xml
+++ b/common/kvstore/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-kvstore_2.11</artifactId>
+  <artifactId>spark-kvstore_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project Local DB</name>
   <url>http://spark.apache.org/</url>
diff --git a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java
index bd8d9486acde5..771a9541bb349 100644
--- a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java
+++ b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java
@@ -54,11 +54,8 @@ public KVStoreSerializer() {
       return ((String) o).getBytes(UTF_8);
     } else {
       ByteArrayOutputStream bytes = new ByteArrayOutputStream();
-      GZIPOutputStream out = new GZIPOutputStream(bytes);
-      try {
+      try (GZIPOutputStream out = new GZIPOutputStream(bytes)) {
         mapper.writeValue(out, o);
-      } finally {
-        out.close();
       }
       return bytes.toByteArray();
     }
@@ -69,11 +66,8 @@ public KVStoreSerializer() {
     if (klass.equals(String.class)) {
       return (T) new String(data, UTF_8);
     } else {
-      GZIPInputStream in = new GZIPInputStream(new ByteArrayInputStream(data));
-      try {
+      try (GZIPInputStream in = new GZIPInputStream(new ByteArrayInputStream(data))) {
         return mapper.readValue(in, klass);
-      } finally {
-        in.close();
       }
     }
   }
diff --git a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java
index f62e85d435318..e3efc92c4a54a 100644
--- a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java
+++ b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java
@@ -196,6 +196,7 @@ public synchronized void close() throws IOException {
    * when Scala wrappers are used, this makes sure that, hopefully, the JNI resources held by
    * the iterator will eventually be released.
    */
+  @SuppressWarnings(""deprecation"")
   @Override
   protected void finalize() throws Throwable {
     db.closeIterator(this);
diff --git a/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java b/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java
index 205f7df87c5bc..39a952f2b0df9 100644
--- a/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java
+++ b/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java
@@ -217,7 +217,7 @@ public void testSkip() throws Exception {
   public void testNegativeIndexValues() throws Exception {
     List<Integer> expected = Arrays.asList(-100, -50, 0, 50, 100);
 
-    expected.stream().forEach(i -> {
+    expected.forEach(i -> {
       try {
         db.write(createCustomType1(i));
       } catch (Exception e) {
diff --git a/common/network-common/pom.xml b/common/network-common/pom.xml
index 8ca7733507f1b..56d01fa0e8b3d 100644
--- a/common/network-common/pom.xml
+++ b/common/network-common/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-network-common_2.11</artifactId>
+  <artifactId>spark-network-common_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project Networking</name>
   <url>http://spark.apache.org/</url>
diff --git a/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java b/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java
index ae91bc9cfdd08..480b52652de53 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java
@@ -21,6 +21,8 @@
 import java.util.List;
 
 import io.netty.channel.Channel;
+import io.netty.channel.ChannelPipeline;
+import io.netty.channel.EventLoopGroup;
 import io.netty.channel.socket.SocketChannel;
 import io.netty.handler.timeout.IdleStateHandler;
 import org.slf4j.Logger;
@@ -32,11 +34,13 @@
 import org.apache.spark.network.client.TransportResponseHandler;
 import org.apache.spark.network.protocol.MessageDecoder;
 import org.apache.spark.network.protocol.MessageEncoder;
+import org.apache.spark.network.server.ChunkFetchRequestHandler;
 import org.apache.spark.network.server.RpcHandler;
 import org.apache.spark.network.server.TransportChannelHandler;
 import org.apache.spark.network.server.TransportRequestHandler;
 import org.apache.spark.network.server.TransportServer;
 import org.apache.spark.network.server.TransportServerBootstrap;
+import org.apache.spark.network.util.IOMode;
 import org.apache.spark.network.util.NettyUtils;
 import org.apache.spark.network.util.TransportConf;
 import org.apache.spark.network.util.TransportFrameDecoder;
@@ -61,6 +65,7 @@
   private final TransportConf conf;
   private final RpcHandler rpcHandler;
   private final boolean closeIdleConnections;
+  private final boolean isClientOnly;
 
   /**
    * Force to create MessageEncoder and MessageDecoder so that we can make sure they will be created
@@ -77,17 +82,54 @@
   private static final MessageEncoder ENCODER = MessageEncoder.INSTANCE;
   private static final MessageDecoder DECODER = MessageDecoder.INSTANCE;
 
+  // Separate thread pool for handling ChunkFetchRequest. This helps to enable throttling
+  // max number of TransportServer worker threads that are blocked on writing response
+  // of ChunkFetchRequest message back to the client via the underlying channel.
+  private static EventLoopGroup chunkFetchWorkers;
+
   public TransportContext(TransportConf conf, RpcHandler rpcHandler) {
-    this(conf, rpcHandler, false);
+    this(conf, rpcHandler, false, false);
   }
 
   public TransportContext(
       TransportConf conf,
       RpcHandler rpcHandler,
       boolean closeIdleConnections) {
+    this(conf, rpcHandler, closeIdleConnections, false);
+  }
+
+  /**
+   * Enables TransportContext initialization for underlying client and server.
+   *
+   * @param conf TransportConf
+   * @param rpcHandler RpcHandler responsible for handling requests and responses.
+   * @param closeIdleConnections Close idle connections if it is set to true.
+   * @param isClientOnly This config indicates the TransportContext is only used by a client.
+   *                     This config is more important when external shuffle is enabled.
+   *                     It stops creating extra event loop and subsequent thread pool
+   *                     for shuffle clients to handle chunked fetch requests.
+   */
+  public TransportContext(
+      TransportConf conf,
+      RpcHandler rpcHandler,
+      boolean closeIdleConnections,
+      boolean isClientOnly) {
     this.conf = conf;
     this.rpcHandler = rpcHandler;
     this.closeIdleConnections = closeIdleConnections;
+    this.isClientOnly = isClientOnly;
+
+    synchronized(TransportContext.class) {
+      if (chunkFetchWorkers == null &&
+          conf.getModuleName() != null &&
+          conf.getModuleName().equalsIgnoreCase(""shuffle"") &&
+          !isClientOnly) {
+        chunkFetchWorkers = NettyUtils.createEventLoop(
+            IOMode.valueOf(conf.ioMode()),
+            conf.chunkFetchHandlerThreads(),
+            ""shuffle-chunk-fetch-handler"");
+      }
+    }
   }
 
   /**
@@ -144,14 +186,23 @@ public TransportChannelHandler initializePipeline(
       RpcHandler channelRpcHandler) {
     try {
       TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler);
-      channel.pipeline()
+      ChunkFetchRequestHandler chunkFetchHandler =
+        createChunkFetchHandler(channelHandler, channelRpcHandler);
+      ChannelPipeline pipeline = channel.pipeline()
         .addLast(""encoder"", ENCODER)
         .addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder())
         .addLast(""decoder"", DECODER)
-        .addLast(""idleStateHandler"", new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000))
+        .addLast(""idleStateHandler"",
+          new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000))
         // NOTE: Chunks are currently guaranteed to be returned in the order of request, but this
         // would require more logic to guarantee if this were not part of the same event loop.
         .addLast(""handler"", channelHandler);
+      // Use a separate EventLoopGroup to handle ChunkFetchRequest messages for shuffle rpcs.
+      if (conf.getModuleName() != null &&
+          conf.getModuleName().equalsIgnoreCase(""shuffle"")
+          && !isClientOnly) {
+        pipeline.addLast(chunkFetchWorkers, ""chunkFetchHandler"", chunkFetchHandler);
+      }
       return channelHandler;
     } catch (RuntimeException e) {
       logger.error(""Error while initializing Netty pipeline"", e);
@@ -173,5 +224,14 @@ private TransportChannelHandler createChannelHandler(Channel channel, RpcHandler
       conf.connectionTimeoutMs(), closeIdleConnections);
   }
 
+  /**
+   * Creates the dedicated ChannelHandler for ChunkFetchRequest messages.
+   */
+  private ChunkFetchRequestHandler createChunkFetchHandler(TransportChannelHandler channelHandler,
+      RpcHandler rpcHandler) {
+    return new ChunkFetchRequestHandler(channelHandler.getClient(),
+      rpcHandler.getStreamManager(), conf.maxChunksBeingTransferred());
+  }
+
   public TransportConf getConf() { return conf; }
 }
diff --git a/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java b/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java
index 1861f8d7fd8f3..2d573f512437e 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java
@@ -36,7 +36,10 @@
  */
 public abstract class ManagedBuffer {
 
-  /** Number of bytes of the data. */
+  /**
+   * Number of bytes of the data. If this buffer will decrypt for all of the views into the data,
+   * this is the size of the decrypted data.
+   */
   public abstract long size();
 
   /**
diff --git a/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java b/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java
index 056505ef53356..64fdb32a67ada 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java
@@ -159,15 +159,21 @@ public void close() throws IOException {
     // accurately report the errors when they happen.
     RuntimeException error = null;
     byte[] dummy = new byte[8];
-    try {
-      doCipherOp(encryptor, dummy, true);
-    } catch (Exception e) {
-      error = new RuntimeException(e);
+    if (encryptor != null) {
+      try {
+        doCipherOp(Cipher.ENCRYPT_MODE, dummy, true);
+      } catch (Exception e) {
+        error = new RuntimeException(e);
+      }
+      encryptor = null;
     }
-    try {
-      doCipherOp(decryptor, dummy, true);
-    } catch (Exception e) {
-      error = new RuntimeException(e);
+    if (decryptor != null) {
+      try {
+        doCipherOp(Cipher.DECRYPT_MODE, dummy, true);
+      } catch (Exception e) {
+        error = new RuntimeException(e);
+      }
+      decryptor = null;
     }
     random.close();
 
@@ -189,11 +195,11 @@ public void close() throws IOException {
   }
 
   private byte[] decrypt(byte[] in) throws GeneralSecurityException {
-    return doCipherOp(decryptor, in, false);
+    return doCipherOp(Cipher.DECRYPT_MODE, in, false);
   }
 
   private byte[] encrypt(byte[] in) throws GeneralSecurityException {
-    return doCipherOp(encryptor, in, false);
+    return doCipherOp(Cipher.ENCRYPT_MODE, in, false);
   }
 
   private void initializeForAuth(String cipher, byte[] nonce, SecretKeySpec key)
@@ -205,11 +211,13 @@ private void initializeForAuth(String cipher, byte[] nonce, SecretKeySpec key)
     byte[] iv = new byte[conf.ivLength()];
     System.arraycopy(nonce, 0, iv, 0, Math.min(nonce.length, iv.length));
 
-    encryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);
-    encryptor.init(Cipher.ENCRYPT_MODE, key, new IvParameterSpec(iv));
+    CryptoCipher _encryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);
+    _encryptor.init(Cipher.ENCRYPT_MODE, key, new IvParameterSpec(iv));
+    this.encryptor = _encryptor;
 
-    decryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);
-    decryptor.init(Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));
+    CryptoCipher _decryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);
+    _decryptor.init(Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));
+    this.decryptor = _decryptor;
   }
 
   /**
@@ -241,29 +249,52 @@ private SecretKeySpec generateKey(String kdf, int iterations, byte[] salt, int k
     return new SecretKeySpec(key.getEncoded(), conf.keyAlgorithm());
   }
 
-  private byte[] doCipherOp(CryptoCipher cipher, byte[] in, boolean isFinal)
+  private byte[] doCipherOp(int mode, byte[] in, boolean isFinal)
     throws GeneralSecurityException {
 
-    Preconditions.checkState(cipher != null);
+    CryptoCipher cipher;
+    switch (mode) {
+      case Cipher.ENCRYPT_MODE:
+        cipher = encryptor;
+        break;
+      case Cipher.DECRYPT_MODE:
+        cipher = decryptor;
+        break;
+      default:
+        throw new IllegalArgumentException(String.valueOf(mode));
+    }
 
-    int scale = 1;
-    while (true) {
-      int size = in.length * scale;
-      byte[] buffer = new byte[size];
-      try {
-        int outSize = isFinal ? cipher.doFinal(in, 0, in.length, buffer, 0)
-          : cipher.update(in, 0, in.length, buffer, 0);
-        if (outSize != buffer.length) {
-          byte[] output = new byte[outSize];
-          System.arraycopy(buffer, 0, output, 0, output.length);
-          return output;
-        } else {
-          return buffer;
+    Preconditions.checkState(cipher != null, ""Cipher is invalid because of previous error."");
+
+    try {
+      int scale = 1;
+      while (true) {
+        int size = in.length * scale;
+        byte[] buffer = new byte[size];
+        try {
+          int outSize = isFinal ? cipher.doFinal(in, 0, in.length, buffer, 0)
+            : cipher.update(in, 0, in.length, buffer, 0);
+          if (outSize != buffer.length) {
+            byte[] output = new byte[outSize];
+            System.arraycopy(buffer, 0, output, 0, output.length);
+            return output;
+          } else {
+            return buffer;
+          }
+        } catch (ShortBufferException e) {
+          // Try again with a bigger buffer.
+          scale *= 2;
         }
-      } catch (ShortBufferException e) {
-        // Try again with a bigger buffer.
-        scale *= 2;
       }
+    } catch (InternalError ie) {
+      // SPARK-25535. The commons-cryto library will throw InternalError if something goes wrong,
+      // and leave bad state behind in the Java wrappers, so it's not safe to use them afterwards.
+      if (mode == Cipher.ENCRYPT_MODE) {
+        this.encryptor = null;
+      } else {
+        this.decryptor = null;
+      }
+      throw ie;
     }
   }
 
diff --git a/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java b/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java
index b64e4b7a970b5..2745052265f7f 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java
@@ -107,45 +107,72 @@ public void addToChannel(Channel ch) throws IOException {
   private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {
     private final ByteArrayWritableChannel byteChannel;
     private final CryptoOutputStream cos;
+    private boolean isCipherValid;
 
     EncryptionHandler(TransportCipher cipher) throws IOException {
       byteChannel = new ByteArrayWritableChannel(STREAM_BUFFER_SIZE);
       cos = cipher.createOutputStream(byteChannel);
+      isCipherValid = true;
     }
 
     @Override
     public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)
       throws Exception {
-      ctx.write(new EncryptedMessage(cos, msg, byteChannel), promise);
+      ctx.write(new EncryptedMessage(this, cos, msg, byteChannel), promise);
     }
 
     @Override
     public void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception {
       try {
-        cos.close();
+        if (isCipherValid) {
+          cos.close();
+        }
       } finally {
         super.close(ctx, promise);
       }
     }
+
+    /**
+     * SPARK-25535. Workaround for CRYPTO-141. Avoid further interaction with the underlying cipher
+     * after an error occurs.
+     */
+    void reportError() {
+      this.isCipherValid = false;
+    }
+
+    boolean isCipherValid() {
+      return isCipherValid;
+    }
   }
 
   private static class DecryptionHandler extends ChannelInboundHandlerAdapter {
     private final CryptoInputStream cis;
     private final ByteArrayReadableChannel byteChannel;
+    private boolean isCipherValid;
 
     DecryptionHandler(TransportCipher cipher) throws IOException {
       byteChannel = new ByteArrayReadableChannel();
       cis = cipher.createInputStream(byteChannel);
+      isCipherValid = true;
     }
 
     @Override
     public void channelRead(ChannelHandlerContext ctx, Object data) throws Exception {
+      if (!isCipherValid) {
+        throw new IOException(""Cipher is in invalid state."");
+      }
       byteChannel.feedData((ByteBuf) data);
 
       byte[] decryptedData = new byte[byteChannel.readableBytes()];
       int offset = 0;
       while (offset < decryptedData.length) {
-        offset += cis.read(decryptedData, offset, decryptedData.length - offset);
+        // SPARK-25535: workaround for CRYPTO-141.
+        try {
+          offset += cis.read(decryptedData, offset, decryptedData.length - offset);
+        } catch (InternalError ie) {
+          isCipherValid = false;
+          throw ie;
+        }
       }
 
       ctx.fireChannelRead(Unpooled.wrappedBuffer(decryptedData, 0, decryptedData.length));
@@ -154,7 +181,9 @@ public void channelRead(ChannelHandlerContext ctx, Object data) throws Exception
     @Override
     public void channelInactive(ChannelHandlerContext ctx) throws Exception {
       try {
-        cis.close();
+        if (isCipherValid) {
+          cis.close();
+        }
       } finally {
         super.channelInactive(ctx);
       }
@@ -165,8 +194,9 @@ public void channelInactive(ChannelHandlerContext ctx) throws Exception {
     private final boolean isByteBuf;
     private final ByteBuf buf;
     private final FileRegion region;
+    private final CryptoOutputStream cos;
+    private final EncryptionHandler handler;
     private long transferred;
-    private CryptoOutputStream cos;
 
     // Due to streaming issue CRYPTO-125: https://issues.apache.org/jira/browse/CRYPTO-125, it has
     // to utilize two helper ByteArrayWritableChannel for streaming. One is used to receive raw data
@@ -176,9 +206,14 @@ public void channelInactive(ChannelHandlerContext ctx) throws Exception {
 
     private ByteBuffer currentEncrypted;
 
-    EncryptedMessage(CryptoOutputStream cos, Object msg, ByteArrayWritableChannel ch) {
+    EncryptedMessage(
+        EncryptionHandler handler,
+        CryptoOutputStream cos,
+        Object msg,
+        ByteArrayWritableChannel ch) {
       Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,
         ""Unrecognized message type: %s"", msg.getClass().getName());
+      this.handler = handler;
       this.isByteBuf = msg instanceof ByteBuf;
       this.buf = isByteBuf ? (ByteBuf) msg : null;
       this.region = isByteBuf ? null : (FileRegion) msg;
@@ -261,6 +296,9 @@ public long transferTo(WritableByteChannel target, long position) throws IOExcep
     }
 
     private void encryptMore() throws IOException {
+      if (!handler.isCipherValid()) {
+        throw new IOException(""Cipher is in invalid state."");
+      }
       byteRawChannel.reset();
 
       if (isByteBuf) {
@@ -269,8 +307,14 @@ private void encryptMore() throws IOException {
       } else {
         region.transferTo(byteRawChannel, region.transferred());
       }
-      cos.write(byteRawChannel.getData(), 0, byteRawChannel.length());
-      cos.flush();
+
+      try {
+        cos.write(byteRawChannel.getData(), 0, byteRawChannel.length());
+        cos.flush();
+      } catch (InternalError ie) {
+        handler.reportError();
+        throw ie;
+      }
 
       currentEncrypted = ByteBuffer.wrap(byteEncChannel.getData(),
         0, byteEncChannel.length());
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java
index 7b28a9a969486..a7afbfa8621c8 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java
@@ -33,7 +33,7 @@ public ChunkFetchFailure(StreamChunkId streamChunkId, String errorString) {
   }
 
   @Override
-  public Type type() { return Type.ChunkFetchFailure; }
+  public Message.Type type() { return Type.ChunkFetchFailure; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java
index 26d063feb5fe3..fe54fcc50dc86 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java
@@ -32,7 +32,7 @@ public ChunkFetchRequest(StreamChunkId streamChunkId) {
   }
 
   @Override
-  public Type type() { return Type.ChunkFetchRequest; }
+  public Message.Type type() { return Type.ChunkFetchRequest; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java
index 94c2ac9b20e43..d5c9a9b3202fb 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java
@@ -39,7 +39,7 @@ public ChunkFetchSuccess(StreamChunkId streamChunkId, ManagedBuffer buffer) {
   }
 
   @Override
-  public Type type() { return Type.ChunkFetchSuccess; }
+  public Message.Type type() { return Type.ChunkFetchSuccess; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java
index f7ffb1bd49bb6..1632fb9e03687 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java
@@ -34,7 +34,7 @@ public OneWayMessage(ManagedBuffer body) {
   }
 
   @Override
-  public Type type() { return Type.OneWayMessage; }
+  public Message.Type type() { return Type.OneWayMessage; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java
index a76624ef5dc96..61061903de23f 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java
@@ -31,7 +31,7 @@ public RpcFailure(long requestId, String errorString) {
   }
 
   @Override
-  public Type type() { return Type.RpcFailure; }
+  public Message.Type type() { return Type.RpcFailure; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java
index 2b30920f0598d..cc1bb95d2d566 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java
@@ -38,7 +38,7 @@ public RpcRequest(long requestId, ManagedBuffer message) {
   }
 
   @Override
-  public Type type() { return Type.RpcRequest; }
+  public Message.Type type() { return Type.RpcRequest; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java
index d73014ecd8506..c03291e9c0b23 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java
@@ -33,7 +33,7 @@ public RpcResponse(long requestId, ManagedBuffer message) {
   }
 
   @Override
-  public Type type() { return Type.RpcResponse; }
+  public Message.Type type() { return Type.RpcResponse; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java
index 258ef81c6783d..68fcfa7748611 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java
@@ -33,7 +33,7 @@ public StreamFailure(String streamId, String error) {
   }
 
   @Override
-  public Type type() { return Type.StreamFailure; }
+  public Message.Type type() { return Type.StreamFailure; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java
index dc183c043ed9a..1b135af752bd8 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java
@@ -34,7 +34,7 @@ public StreamRequest(String streamId) {
    }
 
   @Override
-  public Type type() { return Type.StreamRequest; }
+  public Message.Type type() { return Type.StreamRequest; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java
index 50b811604b84b..568108c4fe5e8 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java
@@ -40,7 +40,7 @@ public StreamResponse(String streamId, long byteCount, ManagedBuffer buffer) {
   }
 
   @Override
-  public Type type() { return Type.StreamResponse; }
+  public Message.Type type() { return Type.StreamResponse; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java
index fa1d26e76b852..7d21151e01074 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java
@@ -52,7 +52,7 @@ private UploadStream(long requestId, ManagedBuffer meta, long bodyByteCount) {
   }
 
   @Override
-  public Type type() { return Type.UploadStream; }
+  public Message.Type type() { return Type.UploadStream; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java b/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java
index 7331c2b481fb1..1b03300d948e2 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java
@@ -23,6 +23,7 @@
 import org.apache.spark.network.buffer.NettyManagedBuffer;
 import org.apache.spark.network.protocol.Encoders;
 import org.apache.spark.network.protocol.AbstractMessage;
+import org.apache.spark.network.protocol.Message;
 
 /**
  * Encodes a Sasl-related message which is attempting to authenticate using some credentials tagged
@@ -46,7 +47,7 @@
   }
 
   @Override
-  public Type type() { return Type.User; }
+  public Message.Type type() { return Type.User; }
 
   @Override
   public int encodedLength() {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/server/ChunkFetchRequestHandler.java b/common/network-common/src/main/java/org/apache/spark/network/server/ChunkFetchRequestHandler.java
new file mode 100644
index 0000000000000..f08d8b0f984cf
--- /dev/null
+++ b/common/network-common/src/main/java/org/apache/spark/network/server/ChunkFetchRequestHandler.java
@@ -0,0 +1,135 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.network.server;
+
+import java.net.SocketAddress;
+
+import com.google.common.base.Throwables;
+import io.netty.channel.Channel;
+import io.netty.channel.ChannelFuture;
+import io.netty.channel.ChannelFutureListener;
+import io.netty.channel.ChannelHandlerContext;
+import io.netty.channel.SimpleChannelInboundHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.spark.network.buffer.ManagedBuffer;
+import org.apache.spark.network.client.TransportClient;
+import org.apache.spark.network.protocol.ChunkFetchFailure;
+import org.apache.spark.network.protocol.ChunkFetchRequest;
+import org.apache.spark.network.protocol.ChunkFetchSuccess;
+import org.apache.spark.network.protocol.Encodable;
+
+import static org.apache.spark.network.util.NettyUtils.*;
+
+/**
+ * A dedicated ChannelHandler for processing ChunkFetchRequest messages. When sending response
+ * of ChunkFetchRequest messages to the clients, the thread performing the I/O on the underlying
+ * channel could potentially be blocked due to disk contentions. If several hundreds of clients
+ * send ChunkFetchRequest to the server at the same time, it could potentially occupying all
+ * threads from TransportServer's default EventLoopGroup for waiting for disk reads before it
+ * can send the block data back to the client as part of the ChunkFetchSuccess messages. As a
+ * result, it would leave no threads left to process other RPC messages, which takes much less
+ * time to process, and could lead to client timing out on either performing SASL authentication,
+ * registering executors, or waiting for response for an OpenBlocks messages.
+ */
+public class ChunkFetchRequestHandler extends SimpleChannelInboundHandler<ChunkFetchRequest> {
+  private static final Logger logger = LoggerFactory.getLogger(ChunkFetchRequestHandler.class);
+
+  private final TransportClient client;
+  private final StreamManager streamManager;
+  /** The max number of chunks being transferred and not finished yet. */
+  private final long maxChunksBeingTransferred;
+
+  public ChunkFetchRequestHandler(
+      TransportClient client,
+      StreamManager streamManager,
+      Long maxChunksBeingTransferred) {
+    this.client = client;
+    this.streamManager = streamManager;
+    this.maxChunksBeingTransferred = maxChunksBeingTransferred;
+  }
+
+  @Override
+  public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
+    logger.warn(""Exception in connection from "" + getRemoteAddress(ctx.channel()), cause);
+    ctx.close();
+  }
+
+  @Override
+  protected void channelRead0(
+      ChannelHandlerContext ctx,
+      final ChunkFetchRequest msg) throws Exception {
+    Channel channel = ctx.channel();
+    if (logger.isTraceEnabled()) {
+      logger.trace(""Received req from {} to fetch block {}"", getRemoteAddress(channel),
+        msg.streamChunkId);
+    }
+    long chunksBeingTransferred = streamManager.chunksBeingTransferred();
+    if (chunksBeingTransferred >= maxChunksBeingTransferred) {
+      logger.warn(""The number of chunks being transferred {} is above {}, close the connection."",
+        chunksBeingTransferred, maxChunksBeingTransferred);
+      channel.close();
+      return;
+    }
+    ManagedBuffer buf;
+    try {
+      streamManager.checkAuthorization(client, msg.streamChunkId.streamId);
+      streamManager.registerChannel(channel, msg.streamChunkId.streamId);
+      buf = streamManager.getChunk(msg.streamChunkId.streamId, msg.streamChunkId.chunkIndex);
+    } catch (Exception e) {
+      logger.error(String.format(""Error opening block %s for request from %s"",
+        msg.streamChunkId, getRemoteAddress(channel)), e);
+      respond(channel, new ChunkFetchFailure(msg.streamChunkId,
+        Throwables.getStackTraceAsString(e)));
+      return;
+    }
+
+    streamManager.chunkBeingSent(msg.streamChunkId.streamId);
+    respond(channel, new ChunkFetchSuccess(msg.streamChunkId, buf)).addListener(
+      (ChannelFutureListener) future -> streamManager.chunkSent(msg.streamChunkId.streamId));
+  }
+
+  /**
+   * The invocation to channel.writeAndFlush is async, and the actual I/O on the
+   * channel will be handled by the EventLoop the channel is registered to. So even
+   * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O,
+   * which is the potentially blocking call that could deplete server handler threads, is still
+   * being processed by TransportServer's default EventLoopGroup. In order to throttle the max
+   * number of threads that channel I/O for sending response to ChunkFetchRequest, the thread
+   * calling channel.writeAndFlush will wait for the completion of sending response back to
+   * client by invoking await(). This will throttle the rate at which threads from
+   * ChunkFetchRequest dedicated EventLoopGroup submit channel I/O requests to TransportServer's
+   * default EventLoopGroup, thus making sure that we can reserve some threads in
+   * TransportServer's default EventLoopGroup for handling other RPC messages.
+   */
+  private ChannelFuture respond(
+      final Channel channel,
+      final Encodable result) throws InterruptedException {
+    final SocketAddress remoteAddress = channel.remoteAddress();
+    return channel.writeAndFlush(result).await().addListener((ChannelFutureListener) future -> {
+      if (future.isSuccess()) {
+        logger.trace(""Sent result {} to client {}"", result, remoteAddress);
+      } else {
+        logger.error(String.format(""Error sending result %s to %s; closing connection"",
+          result, remoteAddress), future.cause());
+        channel.close();
+      }
+    });
+  }
+}
diff --git a/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java b/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java
index 56782a8327876..c824a7b0d4740 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java
@@ -18,7 +18,7 @@
 package org.apache.spark.network.server;
 
 import io.netty.channel.ChannelHandlerContext;
-import io.netty.channel.ChannelInboundHandlerAdapter;
+import io.netty.channel.SimpleChannelInboundHandler;
 import io.netty.handler.timeout.IdleState;
 import io.netty.handler.timeout.IdleStateEvent;
 import org.slf4j.Logger;
@@ -26,6 +26,8 @@
 
 import org.apache.spark.network.client.TransportClient;
 import org.apache.spark.network.client.TransportResponseHandler;
+import org.apache.spark.network.protocol.ChunkFetchRequest;
+import org.apache.spark.network.protocol.Message;
 import org.apache.spark.network.protocol.RequestMessage;
 import org.apache.spark.network.protocol.ResponseMessage;
 import static org.apache.spark.network.util.NettyUtils.getRemoteAddress;
@@ -47,7 +49,7 @@
  * on the channel for at least `requestTimeoutMs`. Note that this is duplex traffic; we will not
  * timeout if the client is continuously sending but getting no responses, for simplicity.
  */
-public class TransportChannelHandler extends ChannelInboundHandlerAdapter {
+public class TransportChannelHandler extends SimpleChannelInboundHandler<Message> {
   private static final Logger logger = LoggerFactory.getLogger(TransportChannelHandler.class);
 
   private final TransportClient client;
@@ -112,8 +114,21 @@ public void channelInactive(ChannelHandlerContext ctx) throws Exception {
     super.channelInactive(ctx);
   }
 
+  /**
+   * Overwrite acceptInboundMessage to properly delegate ChunkFetchRequest messages
+   * to ChunkFetchRequestHandler.
+   */
   @Override
-  public void channelRead(ChannelHandlerContext ctx, Object request) throws Exception {
+  public boolean acceptInboundMessage(Object msg) throws Exception {
+    if (msg instanceof ChunkFetchRequest) {
+      return false;
+    } else {
+      return super.acceptInboundMessage(msg);
+    }
+  }
+
+  @Override
+  public void channelRead0(ChannelHandlerContext ctx, Message request) throws Exception {
     if (request instanceof RequestMessage) {
       requestHandler.handle((RequestMessage) request);
     } else if (request instanceof ResponseMessage) {
diff --git a/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java b/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java
index 9fac96dbe450d..3e089b4cae273 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java
@@ -24,6 +24,7 @@
 import com.google.common.base.Throwables;
 import io.netty.channel.Channel;
 import io.netty.channel.ChannelFuture;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -97,9 +98,7 @@ public void channelInactive() {
 
   @Override
   public void handle(RequestMessage request) {
-    if (request instanceof ChunkFetchRequest) {
-      processFetchRequest((ChunkFetchRequest) request);
-    } else if (request instanceof RpcRequest) {
+    if (request instanceof RpcRequest) {
       processRpcRequest((RpcRequest) request);
     } else if (request instanceof OneWayMessage) {
       processOneWayMessage((OneWayMessage) request);
@@ -112,36 +111,6 @@ public void handle(RequestMessage request) {
     }
   }
 
-  private void processFetchRequest(final ChunkFetchRequest req) {
-    if (logger.isTraceEnabled()) {
-      logger.trace(""Received req from {} to fetch block {}"", getRemoteAddress(channel),
-        req.streamChunkId);
-    }
-    long chunksBeingTransferred = streamManager.chunksBeingTransferred();
-    if (chunksBeingTransferred >= maxChunksBeingTransferred) {
-      logger.warn(""The number of chunks being transferred {} is above {}, close the connection."",
-        chunksBeingTransferred, maxChunksBeingTransferred);
-      channel.close();
-      return;
-    }
-    ManagedBuffer buf;
-    try {
-      streamManager.checkAuthorization(reverseClient, req.streamChunkId.streamId);
-      streamManager.registerChannel(channel, req.streamChunkId.streamId);
-      buf = streamManager.getChunk(req.streamChunkId.streamId, req.streamChunkId.chunkIndex);
-    } catch (Exception e) {
-      logger.error(String.format(""Error opening block %s for request from %s"",
-        req.streamChunkId, getRemoteAddress(channel)), e);
-      respond(new ChunkFetchFailure(req.streamChunkId, Throwables.getStackTraceAsString(e)));
-      return;
-    }
-
-    streamManager.chunkBeingSent(req.streamChunkId.streamId);
-    respond(new ChunkFetchSuccess(req.streamChunkId, buf)).addListener(future -> {
-      streamManager.chunkSent(req.streamChunkId.streamId);
-    });
-  }
-
   private void processStreamRequest(final StreamRequest req) {
     if (logger.isTraceEnabled()) {
       logger.trace(""Received req from {} to fetch stream {}"", getRemoteAddress(channel),
diff --git a/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java b/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java
index 34e4bb5912dcb..43a6bc7dc3d06 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java
@@ -21,6 +21,7 @@
 import java.util.Properties;
 
 import com.google.common.primitives.Ints;
+import io.netty.util.NettyRuntime;
 
 /**
  * A central location that tracks all the settings we expose to users.
@@ -281,4 +282,35 @@ public Properties cryptoConf() {
   public long maxChunksBeingTransferred() {
     return conf.getLong(""spark.shuffle.maxChunksBeingTransferred"", Long.MAX_VALUE);
   }
+
+  /**
+   * Percentage of io.serverThreads used by netty to process ChunkFetchRequest.
+   * Shuffle server will use a separate EventLoopGroup to process ChunkFetchRequest messages.
+   * Although when calling the async writeAndFlush on the underlying channel to send
+   * response back to client, the I/O on the channel is still being handled by
+   * {@link org.apache.spark.network.server.TransportServer}'s default EventLoopGroup
+   * that's registered with the Channel, by waiting inside the ChunkFetchRequest handler
+   * threads for the completion of sending back responses, we are able to put a limit on
+   * the max number of threads from TransportServer's default EventLoopGroup that are
+   * going to be consumed by writing response to ChunkFetchRequest, which are I/O intensive
+   * and could take long time to process due to disk contentions. By configuring a slightly
+   * higher number of shuffler server threads, we are able to reserve some threads for
+   * handling other RPC messages, thus making the Client less likely to experience timeout
+   * when sending RPC messages to the shuffle server. The number of threads used for handling
+   * chunked fetch requests are percentage of io.serverThreads (if defined) else it is a percentage
+   * of 2 * #cores. However, a percentage of 0 means netty default number of threads which
+   * is 2 * #cores ignoring io.serverThreads. The percentage here is configured via
+   * spark.shuffle.server.chunkFetchHandlerThreadsPercent. The returned value is rounded off to
+   * ceiling of the nearest integer.
+   */
+  public int chunkFetchHandlerThreads() {
+    if (!this.getModuleName().equalsIgnoreCase(""shuffle"")) {
+      return 0;
+    }
+    int chunkFetchHandlerThreadsPercent =
+      conf.getInt(""spark.shuffle.server.chunkFetchHandlerThreadsPercent"", 100);
+    return (int)Math.ceil(
+     (this.serverThreads() > 0 ? this.serverThreads() : 2 * NettyRuntime.availableProcessors()) *
+     chunkFetchHandlerThreadsPercent/(double)100);
+  }
 }
diff --git a/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java
index 824482af08dd4..37a8664a52661 100644
--- a/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java
+++ b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java
@@ -143,37 +143,39 @@ public void releaseBuffers() {
   }
 
   private FetchResult fetchChunks(List<Integer> chunkIndices) throws Exception {
-    TransportClient client = clientFactory.createClient(TestUtils.getLocalHost(), server.getPort());
-    final Semaphore sem = new Semaphore(0);
-
     final FetchResult res = new FetchResult();
-    res.successChunks = Collections.synchronizedSet(new HashSet<Integer>());
-    res.failedChunks = Collections.synchronizedSet(new HashSet<Integer>());
-    res.buffers = Collections.synchronizedList(new LinkedList<ManagedBuffer>());
 
-    ChunkReceivedCallback callback = new ChunkReceivedCallback() {
-      @Override
-      public void onSuccess(int chunkIndex, ManagedBuffer buffer) {
-        buffer.retain();
-        res.successChunks.add(chunkIndex);
-        res.buffers.add(buffer);
-        sem.release();
-      }
+    try (TransportClient client =
+      clientFactory.createClient(TestUtils.getLocalHost(), server.getPort())) {
+      final Semaphore sem = new Semaphore(0);
+
+      res.successChunks = Collections.synchronizedSet(new HashSet<Integer>());
+      res.failedChunks = Collections.synchronizedSet(new HashSet<Integer>());
+      res.buffers = Collections.synchronizedList(new LinkedList<ManagedBuffer>());
+
+      ChunkReceivedCallback callback = new ChunkReceivedCallback() {
+        @Override
+        public void onSuccess(int chunkIndex, ManagedBuffer buffer) {
+          buffer.retain();
+          res.successChunks.add(chunkIndex);
+          res.buffers.add(buffer);
+          sem.release();
+        }
 
-      @Override
-      public void onFailure(int chunkIndex, Throwable e) {
-        res.failedChunks.add(chunkIndex);
-        sem.release();
-      }
-    };
+        @Override
+        public void onFailure(int chunkIndex, Throwable e) {
+          res.failedChunks.add(chunkIndex);
+          sem.release();
+        }
+      };
 
-    for (int chunkIndex : chunkIndices) {
-      client.fetchChunk(STREAM_ID, chunkIndex, callback);
-    }
-    if (!sem.tryAcquire(chunkIndices.size(), 5, TimeUnit.SECONDS)) {
-      fail(""Timeout getting response from the server"");
+      for (int chunkIndex : chunkIndices) {
+        client.fetchChunk(STREAM_ID, chunkIndex, callback);
+      }
+      if (!sem.tryAcquire(chunkIndices.size(), 5, TimeUnit.SECONDS)) {
+        fail(""Timeout getting response from the server"");
+      }
     }
-    client.close();
     return res;
   }
 
diff --git a/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchRequestHandlerSuite.java b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchRequestHandlerSuite.java
new file mode 100644
index 0000000000000..2c72c53a33ae8
--- /dev/null
+++ b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchRequestHandlerSuite.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.network;
+
+import io.netty.channel.ChannelHandlerContext;
+import java.util.ArrayList;
+import java.util.List;
+
+import io.netty.channel.Channel;
+import org.apache.spark.network.server.ChunkFetchRequestHandler;
+import org.junit.Test;
+
+import static org.mockito.Mockito.*;
+
+import org.apache.commons.lang3.tuple.ImmutablePair;
+import org.apache.commons.lang3.tuple.Pair;
+import org.apache.spark.network.buffer.ManagedBuffer;
+import org.apache.spark.network.client.TransportClient;
+import org.apache.spark.network.protocol.*;
+import org.apache.spark.network.server.NoOpRpcHandler;
+import org.apache.spark.network.server.OneForOneStreamManager;
+import org.apache.spark.network.server.RpcHandler;
+
+public class ChunkFetchRequestHandlerSuite {
+
+  @Test
+  public void handleChunkFetchRequest() throws Exception {
+    RpcHandler rpcHandler = new NoOpRpcHandler();
+    OneForOneStreamManager streamManager = (OneForOneStreamManager) (rpcHandler.getStreamManager());
+    Channel channel = mock(Channel.class);
+    ChannelHandlerContext context = mock(ChannelHandlerContext.class);
+    when(context.channel())
+      .thenAnswer(invocationOnMock0 -> {
+        return channel;
+      });
+    List<Pair<Object, ExtendedChannelPromise>> responseAndPromisePairs =
+      new ArrayList<>();
+    when(channel.writeAndFlush(any()))
+      .thenAnswer(invocationOnMock0 -> {
+        Object response = invocationOnMock0.getArguments()[0];
+        ExtendedChannelPromise channelFuture = new ExtendedChannelPromise(channel);
+        responseAndPromisePairs.add(ImmutablePair.of(response, channelFuture));
+        return channelFuture;
+      });
+
+    // Prepare the stream.
+    List<ManagedBuffer> managedBuffers = new ArrayList<>();
+    managedBuffers.add(new TestManagedBuffer(10));
+    managedBuffers.add(new TestManagedBuffer(20));
+    managedBuffers.add(new TestManagedBuffer(30));
+    managedBuffers.add(new TestManagedBuffer(40));
+    long streamId = streamManager.registerStream(""test-app"", managedBuffers.iterator());
+    streamManager.registerChannel(channel, streamId);
+    TransportClient reverseClient = mock(TransportClient.class);
+    ChunkFetchRequestHandler requestHandler = new ChunkFetchRequestHandler(reverseClient,
+      rpcHandler.getStreamManager(), 2L);
+
+    RequestMessage request0 = new ChunkFetchRequest(new StreamChunkId(streamId, 0));
+    requestHandler.channelRead(context, request0);
+    assert responseAndPromisePairs.size() == 1;
+    assert responseAndPromisePairs.get(0).getLeft() instanceof ChunkFetchSuccess;
+    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(0).getLeft())).body() ==
+      managedBuffers.get(0);
+
+    RequestMessage request1 = new ChunkFetchRequest(new StreamChunkId(streamId, 1));
+    requestHandler.channelRead(context, request1);
+    assert responseAndPromisePairs.size() == 2;
+    assert responseAndPromisePairs.get(1).getLeft() instanceof ChunkFetchSuccess;
+    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(1).getLeft())).body() ==
+      managedBuffers.get(1);
+
+    // Finish flushing the response for request0.
+    responseAndPromisePairs.get(0).getRight().finish(true);
+
+    RequestMessage request2 = new ChunkFetchRequest(new StreamChunkId(streamId, 2));
+    requestHandler.channelRead(context, request2);
+    assert responseAndPromisePairs.size() == 3;
+    assert responseAndPromisePairs.get(2).getLeft() instanceof ChunkFetchSuccess;
+    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(2).getLeft())).body() ==
+      managedBuffers.get(2);
+
+    RequestMessage request3 = new ChunkFetchRequest(new StreamChunkId(streamId, 3));
+    requestHandler.channelRead(context, request3);
+    verify(channel, times(1)).close();
+    assert responseAndPromisePairs.size() == 3;
+  }
+}
diff --git a/common/network-common/src/test/java/org/apache/spark/network/ExtendedChannelPromise.java b/common/network-common/src/test/java/org/apache/spark/network/ExtendedChannelPromise.java
new file mode 100644
index 0000000000000..573ffd627a2e7
--- /dev/null
+++ b/common/network-common/src/test/java/org/apache/spark/network/ExtendedChannelPromise.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.network;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import io.netty.channel.Channel;
+import io.netty.channel.ChannelPromise;
+import io.netty.channel.DefaultChannelPromise;
+import io.netty.util.concurrent.Future;
+import io.netty.util.concurrent.GenericFutureListener;
+
+class ExtendedChannelPromise extends DefaultChannelPromise {
+
+  private List<GenericFutureListener<Future<Void>>> listeners = new ArrayList<>();
+  private boolean success;
+
+  ExtendedChannelPromise(Channel channel) {
+    super(channel);
+    success = false;
+  }
+
+  @Override
+  public ChannelPromise addListener(
+      GenericFutureListener<? extends Future<? super Void>> listener) {
+    @SuppressWarnings(""unchecked"")
+    GenericFutureListener<Future<Void>> gfListener =
+        (GenericFutureListener<Future<Void>>) listener;
+    listeners.add(gfListener);
+    return super.addListener(listener);
+  }
+
+  @Override
+  public boolean isSuccess() {
+    return success;
+  }
+
+  @Override
+  public ChannelPromise await() throws InterruptedException {
+    return this;
+  }
+
+  public void finish(boolean success) {
+    this.success = success;
+    listeners.forEach(listener -> {
+      try {
+        listener.operationComplete(this);
+      } catch (Exception e) {
+        // do nothing
+      }
+    });
+  }
+}
diff --git a/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java b/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java
index 1f4d75c7e2ec5..1c0aa4da27ff9 100644
--- a/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java
+++ b/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java
@@ -371,23 +371,33 @@ private void assertErrorsContain(Set<String> errors, Set<String> contains) {
 
   private void assertErrorAndClosed(RpcResult result, String expectedError) {
     assertTrue(""unexpected success: "" + result.successMessages, result.successMessages.isEmpty());
-    // we expect 1 additional error, which contains *either* ""closed"" or ""Connection reset""
     Set<String> errors = result.errorMessages;
     assertEquals(""Expected 2 errors, got "" + errors.size() + ""errors: "" +
         errors, 2, errors.size());
 
+    // We expect 1 additional error due to closed connection and here are possible keywords in the
+    // error message.
+    Set<String> possibleClosedErrors = Sets.newHashSet(
+        ""closed"",
+        ""Connection reset"",
+        ""java.nio.channels.ClosedChannelException"",
+        ""java.io.IOException: Broken pipe""
+    );
     Set<String> containsAndClosed = Sets.newHashSet(expectedError);
-    containsAndClosed.add(""closed"");
-    containsAndClosed.add(""Connection reset"");
+    containsAndClosed.addAll(possibleClosedErrors);
 
     Pair<Set<String>, Set<String>> r = checkErrorsContain(errors, containsAndClosed);
 
-    Set<String> errorsNotFound = r.getRight();
-    assertEquals(1, errorsNotFound.size());
-    String err = errorsNotFound.iterator().next();
-    assertTrue(err.equals(""closed"") || err.equals(""Connection reset""));
+    assertTrue(""Got a non-empty set "" + r.getLeft(), r.getLeft().isEmpty());
 
-    assertTrue(r.getLeft().isEmpty());
+    Set<String> errorsNotFound = r.getRight();
+    assertEquals(
+        ""The size of "" + errorsNotFound + "" was not "" + (possibleClosedErrors.size() - 1),
+        possibleClosedErrors.size() - 1,
+        errorsNotFound.size());
+    for (String err: errorsNotFound) {
+      assertTrue(""Found a wrong error "" + err, containsAndClosed.contains(err));
+    }
   }
 
   private Pair<Set<String>, Set<String>> checkErrorsContain(
diff --git a/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java b/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java
index 2656cbee95a20..ad640415a8e6d 100644
--- a/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java
+++ b/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java
@@ -21,10 +21,6 @@
 import java.util.List;
 
 import io.netty.channel.Channel;
-import io.netty.channel.ChannelPromise;
-import io.netty.channel.DefaultChannelPromise;
-import io.netty.util.concurrent.Future;
-import io.netty.util.concurrent.GenericFutureListener;
 import org.junit.Test;
 
 import static org.mockito.Mockito.*;
@@ -42,7 +38,7 @@
 public class TransportRequestHandlerSuite {
 
   @Test
-  public void handleFetchRequestAndStreamRequest() throws Exception {
+  public void handleStreamRequest() throws Exception {
     RpcHandler rpcHandler = new NoOpRpcHandler();
     OneForOneStreamManager streamManager = (OneForOneStreamManager) (rpcHandler.getStreamManager());
     Channel channel = mock(Channel.class);
@@ -68,18 +64,18 @@ public void handleFetchRequestAndStreamRequest() throws Exception {
     TransportRequestHandler requestHandler = new TransportRequestHandler(channel, reverseClient,
       rpcHandler, 2L);
 
-    RequestMessage request0 = new ChunkFetchRequest(new StreamChunkId(streamId, 0));
+    RequestMessage request0 = new StreamRequest(String.format(""%d_%d"", streamId, 0));
     requestHandler.handle(request0);
     assert responseAndPromisePairs.size() == 1;
-    assert responseAndPromisePairs.get(0).getLeft() instanceof ChunkFetchSuccess;
-    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(0).getLeft())).body() ==
+    assert responseAndPromisePairs.get(0).getLeft() instanceof StreamResponse;
+    assert ((StreamResponse) (responseAndPromisePairs.get(0).getLeft())).body() ==
       managedBuffers.get(0);
 
-    RequestMessage request1 = new ChunkFetchRequest(new StreamChunkId(streamId, 1));
+    RequestMessage request1 = new StreamRequest(String.format(""%d_%d"", streamId, 1));
     requestHandler.handle(request1);
     assert responseAndPromisePairs.size() == 2;
-    assert responseAndPromisePairs.get(1).getLeft() instanceof ChunkFetchSuccess;
-    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(1).getLeft())).body() ==
+    assert responseAndPromisePairs.get(1).getLeft() instanceof StreamResponse;
+    assert ((StreamResponse) (responseAndPromisePairs.get(1).getLeft())).body() ==
       managedBuffers.get(1);
 
     // Finish flushing the response for request0.
@@ -99,41 +95,4 @@ public void handleFetchRequestAndStreamRequest() throws Exception {
     verify(channel, times(1)).close();
     assert responseAndPromisePairs.size() == 3;
   }
-
-  private class ExtendedChannelPromise extends DefaultChannelPromise {
-
-    private List<GenericFutureListener<Future<Void>>> listeners = new ArrayList<>();
-    private boolean success;
-
-    ExtendedChannelPromise(Channel channel) {
-      super(channel);
-      success = false;
-    }
-
-    @Override
-    public ChannelPromise addListener(
-        GenericFutureListener<? extends Future<? super Void>> listener) {
-      @SuppressWarnings(""unchecked"")
-      GenericFutureListener<Future<Void>> gfListener =
-          (GenericFutureListener<Future<Void>>) listener;
-      listeners.add(gfListener);
-      return super.addListener(listener);
-    }
-
-    @Override
-    public boolean isSuccess() {
-      return success;
-    }
-
-    public void finish(boolean success) {
-      this.success = success;
-      listeners.forEach(listener -> {
-        try {
-          listener.operationComplete(this);
-        } catch (Exception e) {
-          // do nothing
-        }
-      });
-    }
-  }
 }
diff --git a/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java b/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java
index a3519fe4a423e..c0aa298a4017c 100644
--- a/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java
+++ b/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java
@@ -18,8 +18,11 @@
 package org.apache.spark.network.crypto;
 
 import java.util.Arrays;
+import java.util.Map;
+import java.security.InvalidKeyException;
 import static java.nio.charset.StandardCharsets.UTF_8;
 
+import com.google.common.collect.ImmutableMap;
 import org.junit.BeforeClass;
 import org.junit.Test;
 import static org.junit.Assert.*;
@@ -104,4 +107,18 @@ public void testBadChallenge() throws Exception {
       challenge.cipher, challenge.keyLength, challenge.nonce, badChallenge));
   }
 
+  @Test(expected = InvalidKeyException.class)
+  public void testBadKeySize() throws Exception {
+    Map<String, String> mconf = ImmutableMap.of(""spark.network.crypto.keyLength"", ""42"");
+    TransportConf conf = new TransportConf(""rpc"", new MapConfigProvider(mconf));
+
+    try (AuthEngine engine = new AuthEngine(""appId"", ""secret"", conf)) {
+      engine.challenge();
+      fail(""Should have failed to create challenge message."");
+
+      // Call close explicitly to make sure it's idempotent.
+      engine.close();
+    }
+  }
+
 }
diff --git a/common/network-shuffle/pom.xml b/common/network-shuffle/pom.xml
index 05335df61a664..a6d99813a8501 100644
--- a/common/network-shuffle/pom.xml
+++ b/common/network-shuffle/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-network-shuffle_2.11</artifactId>
+  <artifactId>spark-network-shuffle_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project Shuffle Streaming Service</name>
   <url>http://spark.apache.org/</url>
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFile.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFile.java
new file mode 100644
index 0000000000000..633622b35175b
--- /dev/null
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFile.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.network.shuffle;
+
+import java.io.IOException;
+
+/**
+ * A handle on the file used when fetching remote data to disk.  Used to ensure the lifecycle of
+ * writing the data, reading it back, and then cleaning it up is followed.  Specific implementations
+ * may also handle encryption.  The data can be read only via DownloadFileWritableChannel,
+ * which ensures data is not read until after the writer is closed.
+ */
+public interface DownloadFile {
+  /**
+   * Delete the file.
+   *
+   * @return  <code>true</code> if and only if the file or directory is
+   *          successfully deleted; <code>false</code> otherwise
+   */
+  boolean delete();
+
+  /**
+   * A channel for writing data to the file.  This special channel allows access to the data for
+   * reading, after the channel is closed, via {@link DownloadFileWritableChannel#closeAndRead()}.
+   */
+  DownloadFileWritableChannel openForWriting() throws IOException;
+
+  /**
+   * The path of the file, intended only for debug purposes.
+   */
+  String path();
+}
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/TempFileManager.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileManager.java
similarity index 75%
rename from common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/TempFileManager.java
rename to common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileManager.java
index 552364d274f19..c335a17ae1fe0 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/TempFileManager.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileManager.java
@@ -17,20 +17,20 @@
 
 package org.apache.spark.network.shuffle;
 
-import java.io.File;
+import org.apache.spark.network.util.TransportConf;
 
 /**
- * A manager to create temp block files to reduce the memory usage and also clean temp
- * files when they won't be used any more.
+ * A manager to create temp block files used when fetching remote data to reduce the memory usage.
+ * It will clean files when they won't be used any more.
  */
-public interface TempFileManager {
+public interface DownloadFileManager {
 
   /** Create a temp block file. */
-  File createTempFile();
+  DownloadFile createTempFile(TransportConf transportConf);
 
   /**
    * Register a temp file to clean up when it won't be used any more. Return whether the
    * file is registered successfully. If `false`, the caller should clean up the file by itself.
    */
-  boolean registerTempFileToClean(File file);
+  boolean registerTempFileToClean(DownloadFile file);
 }
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileWritableChannel.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileWritableChannel.java
new file mode 100644
index 0000000000000..dbbbac43eb741
--- /dev/null
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileWritableChannel.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.network.shuffle;
+
+import org.apache.spark.network.buffer.ManagedBuffer;
+
+import java.nio.channels.WritableByteChannel;
+
+/**
+ * A channel for writing data which is fetched to disk, which allows access to the written data only
+ * after the writer has been closed.  Used with DownloadFile and DownloadFileManager.
+ */
+public interface DownloadFileWritableChannel extends WritableByteChannel {
+  ManagedBuffer closeAndRead();
+}
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java
index 7ed0b6e93a7a8..e49e27ab5aa79 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java
@@ -76,7 +76,7 @@ protected void checkInit() {
   @Override
   public void init(String appId) {
     this.appId = appId;
-    TransportContext context = new TransportContext(conf, new NoOpRpcHandler(), true);
+    TransportContext context = new TransportContext(conf, new NoOpRpcHandler(), true, true);
     List<TransportClientBootstrap> bootstraps = Lists.newArrayList();
     if (authEnabled) {
       bootstraps.add(new AuthClientBootstrap(conf, appId, secretKeyHolder));
@@ -91,7 +91,7 @@ public void fetchBlocks(
       String execId,
       String[] blockIds,
       BlockFetchingListener listener,
-      TempFileManager tempFileManager) {
+      DownloadFileManager downloadFileManager) {
     checkInit();
     logger.debug(""External shuffle fetch from {}:{} (executor id {})"", host, port, execId);
     try {
@@ -99,7 +99,7 @@ public void fetchBlocks(
           (blockIds1, listener1) -> {
             TransportClient client = clientFactory.createClient(host, port);
             new OneForOneBlockFetcher(client, appId, execId,
-              blockIds1, listener1, conf, tempFileManager).start();
+              blockIds1, listener1, conf, downloadFileManager).start();
           };
 
       int maxRetries = conf.maxIORetries();
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java
index 0bc571874f07c..30587023877c1 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java
@@ -17,18 +17,13 @@
 
 package org.apache.spark.network.shuffle;
 
-import java.io.File;
-import java.io.FileOutputStream;
 import java.io.IOException;
 import java.nio.ByteBuffer;
-import java.nio.channels.Channels;
-import java.nio.channels.WritableByteChannel;
 import java.util.Arrays;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.spark.network.buffer.FileSegmentManagedBuffer;
 import org.apache.spark.network.buffer.ManagedBuffer;
 import org.apache.spark.network.client.ChunkReceivedCallback;
 import org.apache.spark.network.client.RpcResponseCallback;
@@ -58,7 +53,7 @@
   private final BlockFetchingListener listener;
   private final ChunkReceivedCallback chunkCallback;
   private final TransportConf transportConf;
-  private final TempFileManager tempFileManager;
+  private final DownloadFileManager downloadFileManager;
 
   private StreamHandle streamHandle = null;
 
@@ -79,14 +74,14 @@ public OneForOneBlockFetcher(
       String[] blockIds,
       BlockFetchingListener listener,
       TransportConf transportConf,
-      TempFileManager tempFileManager) {
+      DownloadFileManager downloadFileManager) {
     this.client = client;
     this.openMessage = new OpenBlocks(appId, execId, blockIds);
     this.blockIds = blockIds;
     this.listener = listener;
     this.chunkCallback = new ChunkCallback();
     this.transportConf = transportConf;
-    this.tempFileManager = tempFileManager;
+    this.downloadFileManager = downloadFileManager;
   }
 
   /** Callback invoked on receipt of each chunk. We equate a single chunk to a single block. */
@@ -125,7 +120,7 @@ public void onSuccess(ByteBuffer response) {
           // Immediately request all chunks -- we expect that the total size of the request is
           // reasonable due to higher level chunking in [[ShuffleBlockFetcherIterator]].
           for (int i = 0; i < streamHandle.numChunks; i++) {
-            if (tempFileManager != null) {
+            if (downloadFileManager != null) {
               client.stream(OneForOneStreamManager.genStreamChunkId(streamHandle.streamId, i),
                 new DownloadCallback(i));
             } else {
@@ -159,13 +154,13 @@ private void failRemainingBlocks(String[] failedBlockIds, Throwable e) {
 
   private class DownloadCallback implements StreamCallback {
 
-    private WritableByteChannel channel = null;
-    private File targetFile = null;
+    private DownloadFileWritableChannel channel = null;
+    private DownloadFile targetFile = null;
     private int chunkIndex;
 
     DownloadCallback(int chunkIndex) throws IOException {
-      this.targetFile = tempFileManager.createTempFile();
-      this.channel = Channels.newChannel(new FileOutputStream(targetFile));
+      this.targetFile = downloadFileManager.createTempFile(transportConf);
+      this.channel = targetFile.openForWriting();
       this.chunkIndex = chunkIndex;
     }
 
@@ -178,11 +173,8 @@ public void onData(String streamId, ByteBuffer buf) throws IOException {
 
     @Override
     public void onComplete(String streamId) throws IOException {
-      channel.close();
-      ManagedBuffer buffer = new FileSegmentManagedBuffer(transportConf, targetFile, 0,
-        targetFile.length());
-      listener.onBlockFetchSuccess(blockIds[chunkIndex], buffer);
-      if (!tempFileManager.registerTempFileToClean(targetFile)) {
+      listener.onBlockFetchSuccess(blockIds[chunkIndex], channel.closeAndRead());
+      if (!downloadFileManager.registerTempFileToClean(targetFile)) {
         targetFile.delete();
       }
     }
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java
index f309dda8afca6..6bf3da94030d4 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java
@@ -101,7 +101,7 @@ void createAndStart(String[] blockIds, BlockFetchingListener listener)
 
   public RetryingBlockFetcher(
       TransportConf conf,
-      BlockFetchStarter fetchStarter,
+      RetryingBlockFetcher.BlockFetchStarter fetchStarter,
       String[] blockIds,
       BlockFetchingListener listener) {
     this.fetchStarter = fetchStarter;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java
index 18b04fedcac5b..62b99c40f61f9 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java
@@ -43,7 +43,7 @@ public void init(String appId) { }
    * @param execId the executor id.
    * @param blockIds block ids to fetch.
    * @param listener the listener to receive block fetching status.
-   * @param tempFileManager TempFileManager to create and clean temp files.
+   * @param downloadFileManager DownloadFileManager to create and clean temp files.
    *                        If it's not <code>null</code>, the remote blocks will be streamed
    *                        into temp shuffle files to reduce the memory usage, otherwise,
    *                        they will be kept in memory.
@@ -54,7 +54,7 @@ public abstract void fetchBlocks(
       String execId,
       String[] blockIds,
       BlockFetchingListener listener,
-      TempFileManager tempFileManager);
+      DownloadFileManager downloadFileManager);
 
   /**
    * Get the shuffle MetricsSet from ShuffleClient, this will be used in MetricsSystem to
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java
index 386738ece51a6..371149bef3974 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java
@@ -37,14 +37,8 @@ public ShuffleIndexInformation(File indexFile) throws IOException {
     size = (int)indexFile.length();
     ByteBuffer buffer = ByteBuffer.allocate(size);
     offsets = buffer.asLongBuffer();
-    DataInputStream dis = null;
-    try {
-      dis = new DataInputStream(Files.newInputStream(indexFile.toPath()));
+    try (DataInputStream dis = new DataInputStream(Files.newInputStream(indexFile.toPath()))) {
       dis.readFully(buffer.array());
-    } finally {
-      if (dis != null) {
-        dis.close();
-      }
     }
   }
 
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/SimpleDownloadFile.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/SimpleDownloadFile.java
new file mode 100644
index 0000000000000..670612fd6f66a
--- /dev/null
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/SimpleDownloadFile.java
@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.network.shuffle;
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.Channels;
+import java.nio.channels.WritableByteChannel;
+
+import org.apache.spark.network.buffer.FileSegmentManagedBuffer;
+import org.apache.spark.network.buffer.ManagedBuffer;
+import org.apache.spark.network.util.TransportConf;
+
+/**
+ * A DownloadFile that does not take any encryption settings into account for reading and
+ * writing data.
+ *
+ * This does *not* mean the data in the file is un-encrypted -- it could be that the data is
+ * already encrypted when its written, and subsequent layer is responsible for decrypting.
+ */
+public class SimpleDownloadFile implements DownloadFile {
+
+  private final File file;
+  private final TransportConf transportConf;
+
+  public SimpleDownloadFile(File file, TransportConf transportConf) {
+    this.file = file;
+    this.transportConf = transportConf;
+  }
+
+  @Override
+  public boolean delete() {
+    return file.delete();
+  }
+
+  @Override
+  public DownloadFileWritableChannel openForWriting() throws IOException {
+    return new SimpleDownloadWritableChannel();
+  }
+
+  @Override
+  public String path() {
+    return file.getAbsolutePath();
+  }
+
+  private class SimpleDownloadWritableChannel implements DownloadFileWritableChannel {
+
+    private final WritableByteChannel channel;
+
+    SimpleDownloadWritableChannel() throws FileNotFoundException {
+      channel = Channels.newChannel(new FileOutputStream(file));
+    }
+
+    @Override
+    public ManagedBuffer closeAndRead() {
+      return new FileSegmentManagedBuffer(transportConf, file, 0, file.length());
+    }
+
+    @Override
+    public int write(ByteBuffer src) throws IOException {
+      return channel.write(src);
+    }
+
+    @Override
+    public boolean isOpen() {
+      return channel.isOpen();
+    }
+
+    @Override
+    public void close() throws IOException {
+      channel.close();
+    }
+  }
+}
diff --git a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java
index d2072a54fa415..459629c5f05fe 100644
--- a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java
+++ b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java
@@ -98,19 +98,19 @@ public void testSortShuffleBlocks() throws IOException {
     resolver.registerExecutor(""app0"", ""exec0"",
       dataContext.createExecutorInfo(SORT_MANAGER));
 
-    InputStream block0Stream =
-      resolver.getBlockData(""app0"", ""exec0"", 0, 0, 0).createInputStream();
-    String block0 = CharStreams.toString(
-        new InputStreamReader(block0Stream, StandardCharsets.UTF_8));
-    block0Stream.close();
-    assertEquals(sortBlock0, block0);
-
-    InputStream block1Stream =
-      resolver.getBlockData(""app0"", ""exec0"", 0, 0, 1).createInputStream();
-    String block1 = CharStreams.toString(
-        new InputStreamReader(block1Stream, StandardCharsets.UTF_8));
-    block1Stream.close();
-    assertEquals(sortBlock1, block1);
+    try (InputStream block0Stream = resolver.getBlockData(
+        ""app0"", ""exec0"", 0, 0, 0).createInputStream()) {
+      String block0 =
+        CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8));
+      assertEquals(sortBlock0, block0);
+    }
+
+    try (InputStream block1Stream = resolver.getBlockData(
+        ""app0"", ""exec0"", 0, 0, 1).createInputStream()) {
+      String block1 =
+        CharStreams.toString(new InputStreamReader(block1Stream, StandardCharsets.UTF_8));
+      assertEquals(sortBlock1, block1);
+    }
   }
 
   @Test
@@ -149,7 +149,7 @@ public void testNormalizeAndInternPathname() {
 
   private void assertPathsMatch(String p1, String p2, String p3, String expectedPathname) {
     String normPathname =
-        ExternalShuffleBlockResolver.createNormalizedInternedPathname(p1, p2, p3);
+      ExternalShuffleBlockResolver.createNormalizedInternedPathname(p1, p2, p3);
     assertEquals(expectedPathname, normPathname);
     File file = new File(normPathname);
     String returnedPath = file.getPath();
diff --git a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java
index a6a1b8d0ac3f1..526b96b364473 100644
--- a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java
+++ b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java
@@ -133,37 +133,37 @@ private FetchResult fetchBlocks(
 
     final Semaphore requestsRemaining = new Semaphore(0);
 
-    ExternalShuffleClient client = new ExternalShuffleClient(clientConf, null, false, 5000);
-    client.init(APP_ID);
-    client.fetchBlocks(TestUtils.getLocalHost(), port, execId, blockIds,
-      new BlockFetchingListener() {
-        @Override
-        public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {
-          synchronized (this) {
-            if (!res.successBlocks.contains(blockId) && !res.failedBlocks.contains(blockId)) {
-              data.retain();
-              res.successBlocks.add(blockId);
-              res.buffers.add(data);
-              requestsRemaining.release();
+    try (ExternalShuffleClient client = new ExternalShuffleClient(clientConf, null, false, 5000)) {
+      client.init(APP_ID);
+      client.fetchBlocks(TestUtils.getLocalHost(), port, execId, blockIds,
+        new BlockFetchingListener() {
+          @Override
+          public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {
+            synchronized (this) {
+              if (!res.successBlocks.contains(blockId) && !res.failedBlocks.contains(blockId)) {
+                data.retain();
+                res.successBlocks.add(blockId);
+                res.buffers.add(data);
+                requestsRemaining.release();
+              }
             }
           }
-        }
-
-        @Override
-        public void onBlockFetchFailure(String blockId, Throwable exception) {
-          synchronized (this) {
-            if (!res.successBlocks.contains(blockId) && !res.failedBlocks.contains(blockId)) {
-              res.failedBlocks.add(blockId);
-              requestsRemaining.release();
+
+          @Override
+          public void onBlockFetchFailure(String blockId, Throwable exception) {
+            synchronized (this) {
+              if (!res.successBlocks.contains(blockId) && !res.failedBlocks.contains(blockId)) {
+                res.failedBlocks.add(blockId);
+                requestsRemaining.release();
+              }
             }
           }
-        }
-      }, null);
+        }, null);
 
-    if (!requestsRemaining.tryAcquire(blockIds.length, 5, TimeUnit.SECONDS)) {
-      fail(""Timeout getting response from the server"");
+      if (!requestsRemaining.tryAcquire(blockIds.length, 5, TimeUnit.SECONDS)) {
+        fail(""Timeout getting response from the server"");
+      }
     }
-    client.close();
     return res;
   }
 
diff --git a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java
index 16bad9f1b319d..82caf392b821b 100644
--- a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java
+++ b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java
@@ -96,14 +96,16 @@ private void validate(String appId, String secretKey, boolean encrypt)
         ImmutableMap.of(""spark.authenticate.enableSaslEncryption"", ""true"")));
     }
 
-    ExternalShuffleClient client =
-      new ExternalShuffleClient(testConf, new TestSecretKeyHolder(appId, secretKey), true, 5000);
-    client.init(appId);
-    // Registration either succeeds or throws an exception.
-    client.registerWithShuffleServer(TestUtils.getLocalHost(), server.getPort(), ""exec0"",
-      new ExecutorShuffleInfo(new String[0], 0,
-        ""org.apache.spark.shuffle.sort.SortShuffleManager""));
-    client.close();
+    try (ExternalShuffleClient client =
+        new ExternalShuffleClient(
+          testConf, new TestSecretKeyHolder(appId, secretKey), true, 5000)) {
+      client.init(appId);
+      // Registration either succeeds or throws an exception.
+      client.registerWithShuffleServer(TestUtils.getLocalHost(), server.getPort(), ""exec0"",
+        new ExecutorShuffleInfo(
+          new String[0], 0, ""org.apache.spark.shuffle.sort.SortShuffleManager"")
+      );
+    }
   }
 
   /** Provides a secret key holder which always returns the given secret key, for a single appId. */
diff --git a/common/network-yarn/pom.xml b/common/network-yarn/pom.xml
index 564e6583c909e..55cdc3140aa08 100644
--- a/common/network-yarn/pom.xml
+++ b/common/network-yarn/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-network-yarn_2.11</artifactId>
+  <artifactId>spark-network-yarn_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project YARN Shuffle Service</name>
   <url>http://spark.apache.org/</url>
diff --git a/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java
index d8b2ed6b5dc7b..72ae1a1295236 100644
--- a/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java
+++ b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java
@@ -35,6 +35,8 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.metrics2.impl.MetricsSystemImpl;
+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
 import org.apache.hadoop.yarn.api.records.ContainerId;
 import org.apache.hadoop.yarn.server.api.*;
 import org.apache.spark.network.util.LevelDBProvider;
@@ -168,6 +170,15 @@ protected void serviceInit(Configuration conf) throws Exception {
       TransportConf transportConf = new TransportConf(""shuffle"", new HadoopConfigProvider(conf));
       blockHandler = new ExternalShuffleBlockHandler(transportConf, registeredExecutorFile);
 
+      // register metrics on the block handler into the Node Manager's metrics system.
+      YarnShuffleServiceMetrics serviceMetrics =
+        new YarnShuffleServiceMetrics(blockHandler.getAllMetrics());
+
+      MetricsSystemImpl metricsSystem = (MetricsSystemImpl) DefaultMetricsSystem.instance();
+      metricsSystem.register(
+        ""sparkShuffleService"", ""Metrics on the Spark Shuffle Service"", serviceMetrics);
+      logger.info(""Registered metrics with Hadoop's DefaultMetricsSystem"");
+
       // If authentication is enabled, set up the shuffle server to use a
       // special RPC handler that filters out unauthenticated fetch requests
       List<TransportServerBootstrap> bootstraps = Lists.newArrayList();
diff --git a/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleServiceMetrics.java b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleServiceMetrics.java
new file mode 100644
index 0000000000000..3e4d479b862b3
--- /dev/null
+++ b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleServiceMetrics.java
@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.network.yarn;
+
+import java.util.Map;
+
+import com.codahale.metrics.*;
+import org.apache.hadoop.metrics2.MetricsCollector;
+import org.apache.hadoop.metrics2.MetricsInfo;
+import org.apache.hadoop.metrics2.MetricsRecordBuilder;
+import org.apache.hadoop.metrics2.MetricsSource;
+
+/**
+ * Forward {@link org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.ShuffleMetrics}
+ * to hadoop metrics system.
+ * NodeManager by default exposes JMX endpoint where can be collected.
+ */
+class YarnShuffleServiceMetrics implements MetricsSource {
+
+  private final MetricSet metricSet;
+
+  YarnShuffleServiceMetrics(MetricSet metricSet) {
+    this.metricSet = metricSet;
+  }
+
+  /**
+   * Get metrics from the source
+   *
+   * @param collector to contain the resulting metrics snapshot
+   * @param all       if true, return all metrics even if unchanged.
+   */
+  @Override
+  public void getMetrics(MetricsCollector collector, boolean all) {
+    MetricsRecordBuilder metricsRecordBuilder = collector.addRecord(""sparkShuffleService"");
+
+    for (Map.Entry<String, Metric> entry : metricSet.getMetrics().entrySet()) {
+      collectMetric(metricsRecordBuilder, entry.getKey(), entry.getValue());
+    }
+  }
+
+  /**
+   * The metric types used in
+   * {@link org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.ShuffleMetrics}.
+   * Visible for testing.
+   */
+  public static void collectMetric(
+    MetricsRecordBuilder metricsRecordBuilder, String name, Metric metric) {
+
+    if (metric instanceof Timer) {
+      Timer t = (Timer) metric;
+      metricsRecordBuilder
+        .addCounter(new ShuffleServiceMetricsInfo(name + ""_count"", ""Count of timer "" + name),
+          t.getCount())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + ""_rate15"", ""15 minute rate of timer "" + name),
+          t.getFifteenMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + ""_rate5"", ""5 minute rate of timer "" + name),
+          t.getFiveMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + ""_rate1"", ""1 minute rate of timer "" + name),
+          t.getOneMinuteRate())
+        .addGauge(new ShuffleServiceMetricsInfo(name + ""_rateMean"", ""Mean rate of timer "" + name),
+          t.getMeanRate());
+    } else if (metric instanceof Meter) {
+      Meter m = (Meter) metric;
+      metricsRecordBuilder
+        .addCounter(new ShuffleServiceMetricsInfo(name + ""_count"", ""Count of meter "" + name),
+          m.getCount())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + ""_rate15"", ""15 minute rate of meter "" + name),
+          m.getFifteenMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + ""_rate5"", ""5 minute rate of meter "" + name),
+          m.getFiveMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + ""_rate1"", ""1 minute rate of meter "" + name),
+          m.getOneMinuteRate())
+        .addGauge(new ShuffleServiceMetricsInfo(name + ""_rateMean"", ""Mean rate of meter "" + name),
+          m.getMeanRate());
+    } else if (metric instanceof Gauge) {
+      final Object gaugeValue = ((Gauge) metric).getValue();
+      if (gaugeValue instanceof Integer) {
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Integer) gaugeValue);
+      } else if (gaugeValue instanceof Long) {
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Long) gaugeValue);
+      } else if (gaugeValue instanceof Float) {
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Float) gaugeValue);
+      } else if (gaugeValue instanceof Double) {
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Double) gaugeValue);
+      } else {
+        throw new IllegalStateException(
+                ""Not supported class type of metric["" + name + ""] for value "" + gaugeValue);
+      }
+    }
+  }
+
+  private static MetricsInfo getShuffleServiceMetricsInfo(String name) {
+    return new ShuffleServiceMetricsInfo(name, ""Value of gauge "" + name);
+  }
+
+  private static class ShuffleServiceMetricsInfo implements MetricsInfo {
+
+    private final String name;
+    private final String description;
+
+    ShuffleServiceMetricsInfo(String name, String description) {
+      this.name = name;
+      this.description = description;
+    }
+
+    @Override
+    public String name() {
+      return name;
+    }
+
+    @Override
+    public String description() {
+      return description;
+    }
+  }
+}
diff --git a/common/sketch/pom.xml b/common/sketch/pom.xml
index 2f04abe8c7e88..3c3c0d2d96a1c 100644
--- a/common/sketch/pom.xml
+++ b/common/sketch/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-sketch_2.11</artifactId>
+  <artifactId>spark-sketch_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project Sketch</name>
   <url>http://spark.apache.org/</url>
diff --git a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java
index f7c22dddb8cc0..06a248c9a27c2 100644
--- a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java
+++ b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java
@@ -191,10 +191,9 @@ public static CountMinSketch readFrom(InputStream in) throws IOException {
    * Reads in a {@link CountMinSketch} from a byte array.
    */
   public static CountMinSketch readFrom(byte[] bytes) throws IOException {
-    InputStream in = new ByteArrayInputStream(bytes);
-    CountMinSketch cms = readFrom(in);
-    in.close();
-    return cms;
+    try (InputStream in = new ByteArrayInputStream(bytes)) {
+      return readFrom(in);
+    }
   }
 
   /**
diff --git a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java
index fd1906d2e5ae9..b78c1677a1213 100644
--- a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java
+++ b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java
@@ -322,10 +322,10 @@ public void writeTo(OutputStream out) throws IOException {
 
   @Override
   public byte[] toByteArray() throws IOException {
-    ByteArrayOutputStream out = new ByteArrayOutputStream();
-    writeTo(out);
-    out.close();
-    return out.toByteArray();
+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {
+      writeTo(out);
+      return out.toByteArray();
+    }
   }
 
   public static CountMinSketchImpl readFrom(InputStream in) throws IOException {
diff --git a/common/tags/pom.xml b/common/tags/pom.xml
index ba127408e1c59..883b73a69c9de 100644
--- a/common/tags/pom.xml
+++ b/common/tags/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-tags_2.11</artifactId>
+  <artifactId>spark-tags_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project Tags</name>
   <url>http://spark.apache.org/</url>
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java b/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java
index 0ecef6db0e039..890f2faca28b0 100644
--- a/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java
+++ b/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java
@@ -29,6 +29,7 @@
  * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
  * comes first.
  */
+@Documented
 @Retention(RetentionPolicy.RUNTIME)
 @Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Evolving.java b/common/tags/src/main/java/org/apache/spark/annotation/Evolving.java
new file mode 100644
index 0000000000000..87e8948f204ff
--- /dev/null
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Evolving.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.annotation;
+
+import java.lang.annotation.*;
+
+/**
+ * APIs that are meant to evolve towards becoming stable APIs, but are not stable APIs yet.
+ * Evolving interfaces can change from one feature release to another release (i.e. 2.1 to 2.2).
+ */
+@Documented
+@Retention(RetentionPolicy.RUNTIME)
+@Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
+  ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})
+public @interface Evolving {}
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java b/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java
index ff8120291455f..96875920cd9c3 100644
--- a/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java
@@ -30,6 +30,7 @@
  * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
  * comes first.
  */
+@Documented
 @Retention(RetentionPolicy.RUNTIME)
 @Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java b/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java
deleted file mode 100644
index 323098f69c6e1..0000000000000
--- a/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.annotation;
-
-import java.lang.annotation.Documented;
-
-/**
- * Annotation to inform users of how much to rely on a particular package,
- * class or method not changing over time.
- */
-public class InterfaceStability {
-
-  /**
-   * Stable APIs that retain source and binary compatibility within a major release.
-   * These interfaces can change from one major release to another major release
-   * (e.g. from 1.0 to 2.0).
-   */
-  @Documented
-  public @interface Stable {};
-
-  /**
-   * APIs that are meant to evolve towards becoming stable APIs, but are not stable APIs yet.
-   * Evolving interfaces can change from one feature release to another release (i.e. 2.1 to 2.2).
-   */
-  @Documented
-  public @interface Evolving {};
-
-  /**
-   * Unstable APIs, with no guarantee on stability.
-   * Classes that are unannotated are considered Unstable.
-   */
-  @Documented
-  public @interface Unstable {};
-}
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Private.java b/common/tags/src/main/java/org/apache/spark/annotation/Private.java
index 9082fcf0c84bc..a460d608ae16b 100644
--- a/common/tags/src/main/java/org/apache/spark/annotation/Private.java
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Private.java
@@ -17,10 +17,7 @@
 
 package org.apache.spark.annotation;
 
-import java.lang.annotation.ElementType;
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
-import java.lang.annotation.Target;
+import java.lang.annotation.*;
 
 /**
  * A class that is considered private to the internals of Spark -- there is a high-likelihood
@@ -35,6 +32,7 @@
  * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
  * comes first.
  */
+@Documented
 @Retention(RetentionPolicy.RUNTIME)
 @Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})
diff --git a/external/flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink/SparkSinkThreadFactory.scala b/common/tags/src/main/java/org/apache/spark/annotation/Stable.java
similarity index 61%
rename from external/flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink/SparkSinkThreadFactory.scala
rename to common/tags/src/main/java/org/apache/spark/annotation/Stable.java
index 845fc8debda75..b198bfbe91e10 100644
--- a/external/flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink/SparkSinkThreadFactory.scala
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Stable.java
@@ -14,22 +14,18 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.spark.streaming.flume.sink
 
-import java.util.concurrent.ThreadFactory
-import java.util.concurrent.atomic.AtomicLong
+package org.apache.spark.annotation;
+
+import java.lang.annotation.*;
 
 /**
- * Thread factory that generates daemon threads with a specified name format.
+ * Stable APIs that retain source and binary compatibility within a major release.
+ * These interfaces can change from one major release to another major release
+ * (e.g. from 1.0 to 2.0).
  */
-private[sink] class SparkSinkThreadFactory(nameFormat: String) extends ThreadFactory {
-
-  private val threadId = new AtomicLong()
-
-  override def newThread(r: Runnable): Thread = {
-    val t = new Thread(r, nameFormat.format(threadId.incrementAndGet()))
-    t.setDaemon(true)
-    t
-  }
-
-}
+@Documented
+@Retention(RetentionPolicy.RUNTIME)
+@Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
+  ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})
+public @interface Stable {}
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Unstable.java b/common/tags/src/main/java/org/apache/spark/annotation/Unstable.java
new file mode 100644
index 0000000000000..88ee72125b23f
--- /dev/null
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Unstable.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.annotation;
+
+import java.lang.annotation.*;
+
+/**
+ * Unstable APIs, with no guarantee on stability.
+ * Classes that are unannotated are considered Unstable.
+ */
+@Documented
+@Retention(RetentionPolicy.RUNTIME)
+@Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
+  ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})
+public @interface Unstable {}
diff --git a/common/unsafe/pom.xml b/common/unsafe/pom.xml
index 1527854730394..93a4f67fd23f2 100644
--- a/common/unsafe/pom.xml
+++ b/common/unsafe/pom.xml
@@ -21,12 +21,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-unsafe_2.11</artifactId>
+  <artifactId>spark-unsafe_2.12</artifactId>
   <packaging>jar</packaging>
   <name>Spark Project Unsafe</name>
   <url>http://spark.apache.org/</url>
@@ -89,6 +89,11 @@
       <artifactId>commons-lang3</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.commons</groupId>
+      <artifactId>commons-text</artifactId>
+      <scope>test</scope>
+    </dependency>
   </dependencies>
   <build>
     <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>
diff --git a/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java b/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java
index 62b75ae8aa01d..73577437ac506 100644
--- a/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java
+++ b/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java
@@ -17,8 +17,7 @@
 
 package org.apache.spark.sql.catalyst.expressions;
 
-import org.apache.spark.unsafe.memory.MemoryBlock;
-import org.apache.spark.unsafe.types.UTF8String;
+import org.apache.spark.unsafe.Platform;
 
 /**
  * Simulates Hive's hashing function from Hive v1.2.1
@@ -39,21 +38,12 @@ public static int hashLong(long input) {
     return (int) ((input >>> 32) ^ input);
   }
 
-  public static int hashUnsafeBytesBlock(MemoryBlock mb) {
-    long lengthInBytes = mb.size();
+  public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes) {
     assert (lengthInBytes >= 0): ""lengthInBytes cannot be negative"";
     int result = 0;
-    for (long i = 0; i < lengthInBytes; i++) {
-      result = (result * 31) + (int) mb.getByte(i);
+    for (int i = 0; i < lengthInBytes; i++) {
+      result = (result * 31) + (int) Platform.getByte(base, offset + i);
     }
     return result;
   }
-
-  public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes) {
-    return hashUnsafeBytesBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes));
-  }
-
-  public static int hashUTF8String(UTF8String str) {
-    return hashUnsafeBytesBlock(str.getMemoryBlock());
-  }
 }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java
index 54dcadf3a7754..4563efcfcf474 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java
@@ -19,10 +19,10 @@
 
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Field;
+import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
 import java.nio.ByteBuffer;
 
-import sun.misc.Cleaner;
 import sun.misc.Unsafe;
 
 public final class Platform {
@@ -67,6 +67,60 @@
     unaligned = _unaligned;
   }
 
+  // Access fields and constructors once and store them, for performance:
+
+  private static final Constructor<?> DBB_CONSTRUCTOR;
+  private static final Field DBB_CLEANER_FIELD;
+  static {
+    try {
+      Class<?> cls = Class.forName(""java.nio.DirectByteBuffer"");
+      Constructor<?> constructor = cls.getDeclaredConstructor(Long.TYPE, Integer.TYPE);
+      constructor.setAccessible(true);
+      Field cleanerField = cls.getDeclaredField(""cleaner"");
+      cleanerField.setAccessible(true);
+      DBB_CONSTRUCTOR = constructor;
+      DBB_CLEANER_FIELD = cleanerField;
+    } catch (ClassNotFoundException | NoSuchMethodException | NoSuchFieldException e) {
+      throw new IllegalStateException(e);
+    }
+  }
+
+  private static final Method CLEANER_CREATE_METHOD;
+  static {
+    // The implementation of Cleaner changed from JDK 8 to 9
+    // Split java.version on non-digit chars:
+    int majorVersion = Integer.parseInt(System.getProperty(""java.version"").split(""\\D+"")[0]);
+    String cleanerClassName;
+    if (majorVersion < 9) {
+      cleanerClassName = ""sun.misc.Cleaner"";
+    } else {
+      cleanerClassName = ""jdk.internal.ref.Cleaner"";
+    }
+    try {
+      Class<?> cleanerClass = Class.forName(cleanerClassName);
+      Method createMethod = cleanerClass.getMethod(""create"", Object.class, Runnable.class);
+      // Accessing jdk.internal.ref.Cleaner should actually fail by default in JDK 9+,
+      // unfortunately, unless the user has allowed access with something like
+      // --add-opens java.base/java.lang=ALL-UNNAMED  If not, we can't really use the Cleaner
+      // hack below. It doesn't break, just means the user might run into the default JVM limit
+      // on off-heap memory and increase it or set the flag above. This tests whether it's
+      // available:
+      try {
+        createMethod.invoke(null, null, null);
+      } catch (IllegalAccessException e) {
+        // Don't throw an exception, but can't log here?
+        createMethod = null;
+      } catch (InvocationTargetException ite) {
+        // shouldn't happen; report it
+        throw new IllegalStateException(ite);
+      }
+      CLEANER_CREATE_METHOD = createMethod;
+    } catch (ClassNotFoundException | NoSuchMethodException e) {
+      throw new IllegalStateException(e);
+    }
+
+  }
+
   /**
    * @return true when running JVM is having sun's Unsafe package available in it and underlying
    *         system having unaligned-access capability.
@@ -120,6 +174,11 @@ public static float getFloat(Object object, long offset) {
   }
 
   public static void putFloat(Object object, long offset, float value) {
+    if (Float.isNaN(value)) {
+      value = Float.NaN;
+    } else if (value == -0.0f) {
+      value = 0.0f;
+    }
     _UNSAFE.putFloat(object, offset, value);
   }
 
@@ -128,6 +187,11 @@ public static double getDouble(Object object, long offset) {
   }
 
   public static void putDouble(Object object, long offset, double value) {
+    if (Double.isNaN(value)) {
+      value = Double.NaN;
+    } else if (value == -0.0d) {
+      value = 0.0d;
+    }
     _UNSAFE.putDouble(object, offset, value);
   }
 
@@ -159,18 +223,18 @@ public static long reallocateMemory(long address, long oldSize, long newSize) {
    * MaxDirectMemorySize limit (the default limit is too low and we do not want to require users
    * to increase it).
    */
-  @SuppressWarnings(""unchecked"")
   public static ByteBuffer allocateDirectBuffer(int size) {
     try {
-      Class<?> cls = Class.forName(""java.nio.DirectByteBuffer"");
-      Constructor<?> constructor = cls.getDeclaredConstructor(Long.TYPE, Integer.TYPE);
-      constructor.setAccessible(true);
-      Field cleanerField = cls.getDeclaredField(""cleaner"");
-      cleanerField.setAccessible(true);
       long memory = allocateMemory(size);
-      ByteBuffer buffer = (ByteBuffer) constructor.newInstance(memory, size);
-      Cleaner cleaner = Cleaner.create(buffer, () -> freeMemory(memory));
-      cleanerField.set(buffer, cleaner);
+      ByteBuffer buffer = (ByteBuffer) DBB_CONSTRUCTOR.newInstance(memory, size);
+      if (CLEANER_CREATE_METHOD != null) {
+        try {
+          DBB_CLEANER_FIELD.set(buffer,
+              CLEANER_CREATE_METHOD.invoke(null, buffer, (Runnable) () -> freeMemory(memory)));
+        } catch (IllegalAccessException | InvocationTargetException e) {
+          throw new IllegalStateException(e);
+        }
+      }
       return buffer;
     } catch (Exception e) {
       throwException(e);
@@ -187,7 +251,7 @@ public static void setMemory(long address, byte value, long size) {
   }
 
   public static void copyMemory(
-      Object src, long srcOffset, Object dst, long dstOffset, long length) {
+    Object src, long srcOffset, Object dst, long dstOffset, long length) {
     // Check if dstOffset is before or after srcOffset to determine if we should copy
     // forward or backwards. This is necessary in case src and dst overlap.
     if (dstOffset < srcOffset) {
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java
index be62e40412f83..546e8780a6606 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java
@@ -39,7 +39,9 @@ public static int getSize(Object object, long offset) {
       case 8:
         return (int)Platform.getLong(object, offset);
       default:
+        // checkstyle.off: RegexpSinglelineJava
         throw new AssertionError(""Illegal UAO_SIZE"");
+        // checkstyle.on: RegexpSinglelineJava
     }
   }
 
@@ -52,7 +54,9 @@ public static void putSize(Object object, long offset, int value) {
         Platform.putLong(object, offset, value);
         break;
       default:
+        // checkstyle.off: RegexpSinglelineJava
         throw new AssertionError(""Illegal UAO_SIZE"");
+        // checkstyle.on: RegexpSinglelineJava
     }
   }
 }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java
index ef0f78d95d1ee..cec8c30887e2f 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java
@@ -18,7 +18,6 @@
 package org.apache.spark.unsafe.array;
 
 import org.apache.spark.unsafe.Platform;
-import org.apache.spark.unsafe.memory.MemoryBlock;
 
 public class ByteArrayMethods {
 
@@ -53,25 +52,15 @@ public static long roundNumberOfBytesToNearestWord(long numBytes) {
   public static int MAX_ROUNDED_ARRAY_LENGTH = Integer.MAX_VALUE - 15;
 
   private static final boolean unaligned = Platform.unaligned();
-  /**
-   * MemoryBlock equality check for MemoryBlocks.
-   * @return true if the arrays are equal, false otherwise
-   */
-  public static boolean arrayEqualsBlock(
-      MemoryBlock leftBase, long leftOffset, MemoryBlock rightBase, long rightOffset, long length) {
-    return arrayEquals(leftBase.getBaseObject(), leftBase.getBaseOffset() + leftOffset,
-      rightBase.getBaseObject(), rightBase.getBaseOffset() + rightOffset, length);
-  }
-
   /**
    * Optimized byte array equality check for byte arrays.
    * @return true if the arrays are equal, false otherwise
    */
   public static boolean arrayEquals(
-      Object leftBase, long leftOffset, Object rightBase, long rightOffset, long length) {
+      Object leftBase, long leftOffset, Object rightBase, long rightOffset, final long length) {
     int i = 0;
 
-    // check if starts align and we can get both offsets to be aligned
+    // check if stars align and we can get both offsets to be aligned
     if ((leftOffset % 8) == (rightOffset % 8)) {
       while ((leftOffset + i) % 8 != 0 && i < length) {
         if (Platform.getByte(leftBase, leftOffset + i) !=
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java
index b74d2de0691d5..2cd39bd60c2ac 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java
@@ -17,6 +17,7 @@
 
 package org.apache.spark.unsafe.array;
 
+import org.apache.spark.unsafe.Platform;
 import org.apache.spark.unsafe.memory.MemoryBlock;
 
 /**
@@ -32,12 +33,16 @@
   private static final long WIDTH = 8;
 
   private final MemoryBlock memory;
+  private final Object baseObj;
+  private final long baseOffset;
 
   private final long length;
 
   public LongArray(MemoryBlock memory) {
     assert memory.size() < (long) Integer.MAX_VALUE * 8: ""Array size >= Integer.MAX_VALUE elements"";
     this.memory = memory;
+    this.baseObj = memory.getBaseObject();
+    this.baseOffset = memory.getBaseOffset();
     this.length = memory.size() / WIDTH;
   }
 
@@ -46,11 +51,11 @@ public MemoryBlock memoryBlock() {
   }
 
   public Object getBaseObject() {
-    return memory.getBaseObject();
+    return baseObj;
   }
 
   public long getBaseOffset() {
-    return memory.getBaseOffset();
+    return baseOffset;
   }
 
   /**
@@ -64,8 +69,8 @@ public long size() {
    * Fill this all with 0L.
    */
   public void zeroOut() {
-    for (long off = 0; off < length * WIDTH; off += WIDTH) {
-      memory.putLong(off, 0);
+    for (long off = baseOffset; off < baseOffset + length * WIDTH; off += WIDTH) {
+      Platform.putLong(baseObj, off, 0);
     }
   }
 
@@ -75,7 +80,7 @@ public void zeroOut() {
   public void set(int index, long value) {
     assert index >= 0 : ""index ("" + index + "") should >= 0"";
     assert index < length : ""index ("" + index + "") should < length ("" + length + "")"";
-    memory.putLong(index * WIDTH, value);
+    Platform.putLong(baseObj, baseOffset + index * WIDTH, value);
   }
 
   /**
@@ -84,6 +89,6 @@ public void set(int index, long value) {
   public long get(int index) {
     assert index >= 0 : ""index ("" + index + "") should >= 0"";
     assert index < length : ""index ("" + index + "") should < length ("" + length + "")"";
-    return memory.getLong(index * WIDTH);
+    return Platform.getLong(baseObj, baseOffset + index * WIDTH);
   }
 }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java
index aff6e93d647fe..d239de6083ad0 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java
@@ -17,10 +17,7 @@
 
 package org.apache.spark.unsafe.hash;
 
-import com.google.common.primitives.Ints;
-
-import org.apache.spark.unsafe.memory.MemoryBlock;
-import org.apache.spark.unsafe.types.UTF8String;
+import org.apache.spark.unsafe.Platform;
 
 /**
  * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.
@@ -52,70 +49,49 @@ public static int hashInt(int input, int seed) {
   }
 
   public int hashUnsafeWords(Object base, long offset, int lengthInBytes) {
-    return hashUnsafeWordsBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
+    return hashUnsafeWords(base, offset, lengthInBytes, seed);
   }
 
-  public static int hashUnsafeWordsBlock(MemoryBlock base, int seed) {
+  public static int hashUnsafeWords(Object base, long offset, int lengthInBytes, int seed) {
     // This is based on Guava's `Murmur32_Hasher.processRemaining(ByteBuffer)` method.
-    int lengthInBytes = Ints.checkedCast(base.size());
     assert (lengthInBytes % 8 == 0): ""lengthInBytes must be a multiple of 8 (word-aligned)"";
-    int h1 = hashBytesByIntBlock(base, seed);
+    int h1 = hashBytesByInt(base, offset, lengthInBytes, seed);
     return fmix(h1, lengthInBytes);
   }
 
-  public static int hashUnsafeWords(Object base, long offset, int lengthInBytes, int seed) {
-    // This is based on Guava's `Murmur32_Hasher.processRemaining(ByteBuffer)` method.
-    return hashUnsafeWordsBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
-  }
-
-  public static int hashUnsafeBytesBlock(MemoryBlock base, int seed) {
+  public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes, int seed) {
     // This is not compatible with original and another implementations.
     // But remain it for backward compatibility for the components existing before 2.3.
-    int lengthInBytes = Ints.checkedCast(base.size());
     assert (lengthInBytes >= 0): ""lengthInBytes cannot be negative"";
     int lengthAligned = lengthInBytes - lengthInBytes % 4;
-    int h1 = hashBytesByIntBlock(base.subBlock(0, lengthAligned), seed);
+    int h1 = hashBytesByInt(base, offset, lengthAligned, seed);
     for (int i = lengthAligned; i < lengthInBytes; i++) {
-      int halfWord = base.getByte(i);
+      int halfWord = Platform.getByte(base, offset + i);
       int k1 = mixK1(halfWord);
       h1 = mixH1(h1, k1);
     }
     return fmix(h1, lengthInBytes);
   }
 
-  public static int hashUTF8String(UTF8String str, int seed) {
-    return hashUnsafeBytesBlock(str.getMemoryBlock(), seed);
-  }
-
-  public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes, int seed) {
-    return hashUnsafeBytesBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
-  }
-
   public static int hashUnsafeBytes2(Object base, long offset, int lengthInBytes, int seed) {
-    return hashUnsafeBytes2Block(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
-  }
-
-  public static int hashUnsafeBytes2Block(MemoryBlock base, int seed) {
-    // This is compatible with original and other implementations.
+    // This is compatible with original and another implementations.
     // Use this method for new components after Spark 2.3.
-    int lengthInBytes = Ints.checkedCast(base.size());
-    assert (lengthInBytes >= 0) : ""lengthInBytes cannot be negative"";
+    assert (lengthInBytes >= 0): ""lengthInBytes cannot be negative"";
     int lengthAligned = lengthInBytes - lengthInBytes % 4;
-    int h1 = hashBytesByIntBlock(base.subBlock(0, lengthAligned), seed);
+    int h1 = hashBytesByInt(base, offset, lengthAligned, seed);
     int k1 = 0;
     for (int i = lengthAligned, shift = 0; i < lengthInBytes; i++, shift += 8) {
-      k1 ^= (base.getByte(i) & 0xFF) << shift;
+      k1 ^= (Platform.getByte(base, offset + i) & 0xFF) << shift;
     }
     h1 ^= mixK1(k1);
     return fmix(h1, lengthInBytes);
   }
 
-  private static int hashBytesByIntBlock(MemoryBlock base, int seed) {
-    long lengthInBytes = base.size();
+  private static int hashBytesByInt(Object base, long offset, int lengthInBytes, int seed) {
     assert (lengthInBytes % 4 == 0);
     int h1 = seed;
-    for (long i = 0; i < lengthInBytes; i += 4) {
-      int halfWord = base.getInt(i);
+    for (int i = 0; i < lengthInBytes; i += 4) {
+      int halfWord = Platform.getInt(base, offset + i);
       int k1 = mixK1(halfWord);
       h1 = mixH1(h1, k1);
     }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/ByteArrayMemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/ByteArrayMemoryBlock.java
deleted file mode 100644
index 9f238632bc87a..0000000000000
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/ByteArrayMemoryBlock.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.unsafe.memory;
-
-import com.google.common.primitives.Ints;
-
-import org.apache.spark.unsafe.Platform;
-
-/**
- * A consecutive block of memory with a byte array on Java heap.
- */
-public final class ByteArrayMemoryBlock extends MemoryBlock {
-
-  private final byte[] array;
-
-  public ByteArrayMemoryBlock(byte[] obj, long offset, long size) {
-    super(obj, offset, size);
-    this.array = obj;
-    assert(offset + size <= Platform.BYTE_ARRAY_OFFSET + obj.length) :
-      ""The sum of size "" + size + "" and offset "" + offset + "" should not be larger than "" +
-        ""the size of the given memory space "" + (obj.length + Platform.BYTE_ARRAY_OFFSET);
-  }
-
-  public ByteArrayMemoryBlock(long length) {
-    this(new byte[Ints.checkedCast(length)], Platform.BYTE_ARRAY_OFFSET, length);
-  }
-
-  @Override
-  public MemoryBlock subBlock(long offset, long size) {
-    checkSubBlockRange(offset, size);
-    if (offset == 0 && size == this.size()) return this;
-    return new ByteArrayMemoryBlock(array, this.offset + offset, size);
-  }
-
-  public byte[] getByteArray() { return array; }
-
-  /**
-   * Creates a memory block pointing to the memory used by the byte array.
-   */
-  public static ByteArrayMemoryBlock fromArray(final byte[] array) {
-    return new ByteArrayMemoryBlock(array, Platform.BYTE_ARRAY_OFFSET, array.length);
-  }
-
-  @Override
-  public int getInt(long offset) {
-    return Platform.getInt(array, this.offset + offset);
-  }
-
-  @Override
-  public void putInt(long offset, int value) {
-    Platform.putInt(array, this.offset + offset, value);
-  }
-
-  @Override
-  public boolean getBoolean(long offset) {
-    return Platform.getBoolean(array, this.offset + offset);
-  }
-
-  @Override
-  public void putBoolean(long offset, boolean value) {
-    Platform.putBoolean(array, this.offset + offset, value);
-  }
-
-  @Override
-  public byte getByte(long offset) {
-    return array[(int)(this.offset + offset - Platform.BYTE_ARRAY_OFFSET)];
-  }
-
-  @Override
-  public void putByte(long offset, byte value) {
-    array[(int)(this.offset + offset - Platform.BYTE_ARRAY_OFFSET)] = value;
-  }
-
-  @Override
-  public short getShort(long offset) {
-    return Platform.getShort(array, this.offset + offset);
-  }
-
-  @Override
-  public void putShort(long offset, short value) {
-    Platform.putShort(array, this.offset + offset, value);
-  }
-
-  @Override
-  public long getLong(long offset) {
-    return Platform.getLong(array, this.offset + offset);
-  }
-
-  @Override
-  public void putLong(long offset, long value) {
-    Platform.putLong(array, this.offset + offset, value);
-  }
-
-  @Override
-  public float getFloat(long offset) {
-    return Platform.getFloat(array, this.offset + offset);
-  }
-
-  @Override
-  public void putFloat(long offset, float value) {
-    Platform.putFloat(array, this.offset + offset, value);
-  }
-
-  @Override
-  public double getDouble(long offset) {
-    return Platform.getDouble(array, this.offset + offset);
-  }
-
-  @Override
-  public void putDouble(long offset, double value) {
-    Platform.putDouble(array, this.offset + offset, value);
-  }
-}
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java
index 36caf80888cda..2733760dd19ef 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java
@@ -23,6 +23,8 @@
 import java.util.LinkedList;
 import java.util.Map;
 
+import org.apache.spark.unsafe.Platform;
+
 /**
  * A simple {@link MemoryAllocator} that can allocate up to 16GB using a JVM long primitive array.
  */
@@ -56,7 +58,7 @@ public MemoryBlock allocate(long size) throws OutOfMemoryError {
             final long[] array = arrayReference.get();
             if (array != null) {
               assert (array.length * 8L >= size);
-              MemoryBlock memory = OnHeapMemoryBlock.fromArray(array, size);
+              MemoryBlock memory = new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, size);
               if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) {
                 memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_CLEAN_VALUE);
               }
@@ -68,7 +70,7 @@ public MemoryBlock allocate(long size) throws OutOfMemoryError {
       }
     }
     long[] array = new long[numWords];
-    MemoryBlock memory = OnHeapMemoryBlock.fromArray(array, size);
+    MemoryBlock memory = new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, size);
     if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) {
       memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_CLEAN_VALUE);
     }
@@ -77,13 +79,12 @@ public MemoryBlock allocate(long size) throws OutOfMemoryError {
 
   @Override
   public void free(MemoryBlock memory) {
-    assert(memory instanceof OnHeapMemoryBlock);
-    assert (memory.getBaseObject() != null) :
+    assert (memory.obj != null) :
       ""baseObject was null; are you trying to use the on-heap allocator to free off-heap memory?"";
-    assert (memory.getPageNumber() != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :
+    assert (memory.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :
       ""page has already been freed"";
-    assert ((memory.getPageNumber() == MemoryBlock.NO_PAGE_NUMBER)
-            || (memory.getPageNumber() == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :
+    assert ((memory.pageNumber == MemoryBlock.NO_PAGE_NUMBER)
+            || (memory.pageNumber == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :
       ""TMM-allocated pages must first be freed via TMM.freePage(), not directly in allocator "" +
         ""free()"";
 
@@ -93,12 +94,12 @@ public void free(MemoryBlock memory) {
     }
 
     // Mark the page as freed (so we can detect double-frees).
-    memory.setPageNumber(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER);
+    memory.pageNumber = MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER;
 
     // As an additional layer of defense against use-after-free bugs, we mutate the
     // MemoryBlock to null out its reference to the long[] array.
-    long[] array = ((OnHeapMemoryBlock)memory).getLongArray();
-    memory.resetObjAndOffset();
+    long[] array = (long[]) memory.obj;
+    memory.setObjAndOffset(null, 0);
 
     long alignedSize = ((size + 7) / 8) * 8;
     if (shouldPool(alignedSize)) {
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java
index 38315fb97b46a..7b588681d9790 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java
@@ -38,7 +38,7 @@
 
   void free(MemoryBlock memory);
 
-  UnsafeMemoryAllocator UNSAFE = new UnsafeMemoryAllocator();
+  MemoryAllocator UNSAFE = new UnsafeMemoryAllocator();
 
-  HeapMemoryAllocator HEAP = new HeapMemoryAllocator();
+  MemoryAllocator HEAP = new HeapMemoryAllocator();
 }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java
index ca7213bbf92da..c333857358d30 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java
@@ -22,10 +22,10 @@
 import org.apache.spark.unsafe.Platform;
 
 /**
- * A representation of a consecutive memory block in Spark. It defines the common interfaces
- * for memory accessing and mutating.
+ * A consecutive block of memory, starting at a {@link MemoryLocation} with a fixed size.
  */
-public abstract class MemoryBlock {
+public class MemoryBlock extends MemoryLocation {
+
   /** Special `pageNumber` value for pages which were not allocated by TaskMemoryManagers */
   public static final int NO_PAGE_NUMBER = -1;
 
@@ -45,163 +45,38 @@
    */
   public static final int FREED_IN_ALLOCATOR_PAGE_NUMBER = -3;
 
-  @Nullable
-  protected Object obj;
-
-  protected long offset;
-
-  protected long length;
+  private final long length;
 
   /**
    * Optional page number; used when this MemoryBlock represents a page allocated by a
-   * TaskMemoryManager. This field can be updated using setPageNumber method so that
-   * this can be modified by the TaskMemoryManager, which lives in a different package.
+   * TaskMemoryManager. This field is public so that it can be modified by the TaskMemoryManager,
+   * which lives in a different package.
    */
-  private int pageNumber = NO_PAGE_NUMBER;
+  public int pageNumber = NO_PAGE_NUMBER;
 
-  protected MemoryBlock(@Nullable Object obj, long offset, long length) {
-    if (offset < 0 || length < 0) {
-      throw new IllegalArgumentException(
-        ""Length "" + length + "" and offset "" + offset + ""must be non-negative"");
-    }
-    this.obj = obj;
-    this.offset = offset;
+  public MemoryBlock(@Nullable Object obj, long offset, long length) {
+    super(obj, offset);
     this.length = length;
   }
 
-  protected MemoryBlock() {
-    this(null, 0, 0);
-  }
-
-  public final Object getBaseObject() {
-    return obj;
-  }
-
-  public final long getBaseOffset() {
-    return offset;
-  }
-
-  public void resetObjAndOffset() {
-    this.obj = null;
-    this.offset = 0;
-  }
-
   /**
    * Returns the size of the memory block.
    */
-  public final long size() {
+  public long size() {
     return length;
   }
 
-  public final void setPageNumber(int pageNum) {
-    pageNumber = pageNum;
-  }
-
-  public final int getPageNumber() {
-    return pageNumber;
-  }
-
-  /**
-   * Fills the memory block with the specified byte value.
-   */
-  public final void fill(byte value) {
-    Platform.setMemory(obj, offset, length, value);
-  }
-
-  /**
-   * Instantiate MemoryBlock for given object type with new offset
-   */
-  public static final MemoryBlock allocateFromObject(Object obj, long offset, long length) {
-    MemoryBlock mb = null;
-    if (obj instanceof byte[]) {
-      byte[] array = (byte[])obj;
-      mb = new ByteArrayMemoryBlock(array, offset, length);
-    } else if (obj instanceof long[]) {
-      long[] array = (long[])obj;
-      mb = new OnHeapMemoryBlock(array, offset, length);
-    } else if (obj == null) {
-      // we assume that to pass null pointer means off-heap
-      mb = new OffHeapMemoryBlock(offset, length);
-    } else {
-      throw new UnsupportedOperationException(
-        ""Instantiate MemoryBlock for type "" + obj.getClass() + "" is not supported now"");
-    }
-    return mb;
-  }
-
   /**
-   * Just instantiate the sub-block with the same type of MemoryBlock with the new size and relative
-   * offset from the original offset. The data is not copied.
-   * If parameters are invalid, an exception is thrown.
+   * Creates a memory block pointing to the memory used by the long array.
    */
-  public abstract MemoryBlock subBlock(long offset, long size);
-
-  protected void checkSubBlockRange(long offset, long size) {
-    if (offset < 0 || size < 0) {
-      throw new ArrayIndexOutOfBoundsException(
-        ""Size "" + size + "" and offset "" + offset + "" must be non-negative"");
-    }
-    if (offset + size > length) {
-      throw new ArrayIndexOutOfBoundsException(""The sum of size "" + size + "" and offset "" +
-        offset + "" should not be larger than the length "" + length + "" in the MemoryBlock"");
-    }
+  public static MemoryBlock fromLongArray(final long[] array) {
+    return new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, array.length * 8L);
   }
 
   /**
-   * getXXX/putXXX does not ensure guarantee behavior if the offset is invalid. e.g  cause illegal
-   * memory access, throw an exception, or etc.
-   * getXXX/putXXX uses an index based on this.offset that includes the size of metadata such as
-   * JVM object header. The offset is 0-based and is expected as an logical offset in the memory
-   * block.
+   * Fills the memory block with the specified byte value.
    */
-  public abstract int getInt(long offset);
-
-  public abstract void putInt(long offset, int value);
-
-  public abstract boolean getBoolean(long offset);
-
-  public abstract void putBoolean(long offset, boolean value);
-
-  public abstract byte getByte(long offset);
-
-  public abstract void putByte(long offset, byte value);
-
-  public abstract short getShort(long offset);
-
-  public abstract void putShort(long offset, short value);
-
-  public abstract long getLong(long offset);
-
-  public abstract void putLong(long offset, long value);
-
-  public abstract float getFloat(long offset);
-
-  public abstract void putFloat(long offset, float value);
-
-  public abstract double getDouble(long offset);
-
-  public abstract void putDouble(long offset, double value);
-
-  public static final void copyMemory(
-      MemoryBlock src, long srcOffset, MemoryBlock dst, long dstOffset, long length) {
-    assert(srcOffset + length <= src.length && dstOffset + length <= dst.length);
-    Platform.copyMemory(src.getBaseObject(), src.getBaseOffset() + srcOffset,
-      dst.getBaseObject(), dst.getBaseOffset() + dstOffset, length);
-  }
-
-  public static final void copyMemory(MemoryBlock src, MemoryBlock dst, long length) {
-    assert(length <= src.length && length <= dst.length);
-    Platform.copyMemory(src.getBaseObject(), src.getBaseOffset(),
-      dst.getBaseObject(), dst.getBaseOffset(), length);
-  }
-
-  public final void copyFrom(Object src, long srcOffset, long dstOffset, long length) {
-    assert(length <= this.length - srcOffset);
-    Platform.copyMemory(src, srcOffset, obj, offset + dstOffset, length);
-  }
-
-  public final void writeTo(long srcOffset, Object dst, long dstOffset, long length) {
-    assert(length <= this.length - srcOffset);
-    Platform.copyMemory(obj, offset + srcOffset, dst, dstOffset, length);
+  public void fill(byte value) {
+    Platform.setMemory(obj, offset, length, value);
   }
 }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java
new file mode 100644
index 0000000000000..74ebc87dc978c
--- /dev/null
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.unsafe.memory;
+
+import javax.annotation.Nullable;
+
+/**
+ * A memory location. Tracked either by a memory address (with off-heap allocation),
+ * or by an offset from a JVM object (in-heap allocation).
+ */
+public class MemoryLocation {
+
+  @Nullable
+  Object obj;
+
+  long offset;
+
+  public MemoryLocation(@Nullable Object obj, long offset) {
+    this.obj = obj;
+    this.offset = offset;
+  }
+
+  public MemoryLocation() {
+    this(null, 0);
+  }
+
+  public void setObjAndOffset(Object newObj, long newOffset) {
+    this.obj = newObj;
+    this.offset = newOffset;
+  }
+
+  public final Object getBaseObject() {
+    return obj;
+  }
+
+  public final long getBaseOffset() {
+    return offset;
+  }
+}
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OffHeapMemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OffHeapMemoryBlock.java
deleted file mode 100644
index 3431b08980eb8..0000000000000
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OffHeapMemoryBlock.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.unsafe.memory;
-
-import org.apache.spark.unsafe.Platform;
-
-public class OffHeapMemoryBlock extends MemoryBlock {
-  public static final OffHeapMemoryBlock NULL = new OffHeapMemoryBlock(0, 0);
-
-  public OffHeapMemoryBlock(long address, long size) {
-    super(null, address, size);
-  }
-
-  @Override
-  public MemoryBlock subBlock(long offset, long size) {
-    checkSubBlockRange(offset, size);
-    if (offset == 0 && size == this.size()) return this;
-    return new OffHeapMemoryBlock(this.offset + offset, size);
-  }
-
-  @Override
-  public final int getInt(long offset) {
-    return Platform.getInt(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putInt(long offset, int value) {
-    Platform.putInt(null, this.offset + offset, value);
-  }
-
-  @Override
-  public final boolean getBoolean(long offset) {
-    return Platform.getBoolean(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putBoolean(long offset, boolean value) {
-    Platform.putBoolean(null, this.offset + offset, value);
-  }
-
-  @Override
-  public final byte getByte(long offset) {
-    return Platform.getByte(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putByte(long offset, byte value) {
-    Platform.putByte(null, this.offset + offset, value);
-  }
-
-  @Override
-  public final short getShort(long offset) {
-    return Platform.getShort(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putShort(long offset, short value) {
-    Platform.putShort(null, this.offset + offset, value);
-  }
-
-  @Override
-  public final long getLong(long offset) {
-    return Platform.getLong(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putLong(long offset, long value) {
-    Platform.putLong(null, this.offset + offset, value);
-  }
-
-  @Override
-  public final float getFloat(long offset) {
-    return Platform.getFloat(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putFloat(long offset, float value) {
-    Platform.putFloat(null, this.offset + offset, value);
-  }
-
-  @Override
-  public final double getDouble(long offset) {
-    return Platform.getDouble(null, this.offset + offset);
-  }
-
-  @Override
-  public final void putDouble(long offset, double value) {
-    Platform.putDouble(null, this.offset + offset, value);
-  }
-}
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OnHeapMemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OnHeapMemoryBlock.java
deleted file mode 100644
index ee42bc27c9c5f..0000000000000
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OnHeapMemoryBlock.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.unsafe.memory;
-
-import com.google.common.primitives.Ints;
-
-import org.apache.spark.unsafe.Platform;
-
-/**
- * A consecutive block of memory with a long array on Java heap.
- */
-public final class OnHeapMemoryBlock extends MemoryBlock {
-
-  private final long[] array;
-
-  public OnHeapMemoryBlock(long[] obj, long offset, long size) {
-    super(obj, offset, size);
-    this.array = obj;
-    assert(offset + size <= obj.length * 8L + Platform.LONG_ARRAY_OFFSET) :
-      ""The sum of size "" + size + "" and offset "" + offset + "" should not be larger than "" +
-        ""the size of the given memory space "" + (obj.length * 8L + Platform.LONG_ARRAY_OFFSET);
-  }
-
-  public OnHeapMemoryBlock(long size) {
-    this(new long[Ints.checkedCast((size + 7) / 8)], Platform.LONG_ARRAY_OFFSET, size);
-  }
-
-  @Override
-  public MemoryBlock subBlock(long offset, long size) {
-    checkSubBlockRange(offset, size);
-    if (offset == 0 && size == this.size()) return this;
-    return new OnHeapMemoryBlock(array, this.offset + offset, size);
-  }
-
-  public long[] getLongArray() { return array; }
-
-  /**
-   * Creates a memory block pointing to the memory used by the long array.
-   */
-  public static OnHeapMemoryBlock fromArray(final long[] array) {
-    return new OnHeapMemoryBlock(array, Platform.LONG_ARRAY_OFFSET, array.length * 8L);
-  }
-
-  public static OnHeapMemoryBlock fromArray(final long[] array, long size) {
-    return new OnHeapMemoryBlock(array, Platform.LONG_ARRAY_OFFSET, size);
-  }
-
-  @Override
-  public int getInt(long offset) {
-    return Platform.getInt(array, this.offset + offset);
-  }
-
-  @Override
-  public void putInt(long offset, int value) {
-    Platform.putInt(array, this.offset + offset, value);
-  }
-
-  @Override
-  public boolean getBoolean(long offset) {
-    return Platform.getBoolean(array, this.offset + offset);
-  }
-
-  @Override
-  public void putBoolean(long offset, boolean value) {
-    Platform.putBoolean(array, this.offset + offset, value);
-  }
-
-  @Override
-  public byte getByte(long offset) {
-    return Platform.getByte(array, this.offset + offset);
-  }
-
-  @Override
-  public void putByte(long offset, byte value) {
-    Platform.putByte(array, this.offset + offset, value);
-  }
-
-  @Override
-  public short getShort(long offset) {
-    return Platform.getShort(array, this.offset + offset);
-  }
-
-  @Override
-  public void putShort(long offset, short value) {
-    Platform.putShort(array, this.offset + offset, value);
-  }
-
-  @Override
-  public long getLong(long offset) {
-    return Platform.getLong(array, this.offset + offset);
-  }
-
-  @Override
-  public void putLong(long offset, long value) {
-    Platform.putLong(array, this.offset + offset, value);
-  }
-
-  @Override
-  public float getFloat(long offset) {
-    return Platform.getFloat(array, this.offset + offset);
-  }
-
-  @Override
-  public void putFloat(long offset, float value) {
-    Platform.putFloat(array, this.offset + offset, value);
-  }
-
-  @Override
-  public double getDouble(long offset) {
-    return Platform.getDouble(array, this.offset + offset);
-  }
-
-  @Override
-  public void putDouble(long offset, double value) {
-    Platform.putDouble(array, this.offset + offset, value);
-  }
-}
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java
index 5310bdf2779a9..4368fb615ba1e 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java
@@ -25,9 +25,9 @@
 public class UnsafeMemoryAllocator implements MemoryAllocator {
 
   @Override
-  public OffHeapMemoryBlock allocate(long size) throws OutOfMemoryError {
+  public MemoryBlock allocate(long size) throws OutOfMemoryError {
     long address = Platform.allocateMemory(size);
-    OffHeapMemoryBlock memory = new OffHeapMemoryBlock(address, size);
+    MemoryBlock memory = new MemoryBlock(null, address, size);
     if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) {
       memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_CLEAN_VALUE);
     }
@@ -36,25 +36,22 @@ public OffHeapMemoryBlock allocate(long size) throws OutOfMemoryError {
 
   @Override
   public void free(MemoryBlock memory) {
-    assert(memory instanceof OffHeapMemoryBlock) :
-      ""UnsafeMemoryAllocator can only free OffHeapMemoryBlock."";
-    if (memory == OffHeapMemoryBlock.NULL) return;
-    assert (memory.getPageNumber() != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :
+    assert (memory.obj == null) :
+      ""baseObject not null; are you trying to use the off-heap allocator to free on-heap memory?"";
+    assert (memory.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :
       ""page has already been freed"";
-    assert ((memory.getPageNumber() == MemoryBlock.NO_PAGE_NUMBER)
-            || (memory.getPageNumber() == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :
+    assert ((memory.pageNumber == MemoryBlock.NO_PAGE_NUMBER)
+            || (memory.pageNumber == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :
       ""TMM-allocated pages must be freed via TMM.freePage(), not directly in allocator free()"";
 
     if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) {
       memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_FREED_VALUE);
     }
-
     Platform.freeMemory(memory.offset);
-
     // As an additional layer of defense against use-after-free bugs, we mutate the
     // MemoryBlock to reset its pointer.
-    memory.resetObjAndOffset();
+    memory.offset = 0;
     // Mark the page as freed (so we can detect double-frees).
-    memory.setPageNumber(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER);
+    memory.pageNumber = MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER;
   }
 }
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java
index e91fc4391425c..3a3bfc4a94bb3 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java
@@ -34,8 +34,6 @@
 import org.apache.spark.unsafe.Platform;
 import org.apache.spark.unsafe.array.ByteArrayMethods;
 import org.apache.spark.unsafe.hash.Murmur3_x86_32;
-import org.apache.spark.unsafe.memory.ByteArrayMemoryBlock;
-import org.apache.spark.unsafe.memory.MemoryBlock;
 
 import static org.apache.spark.unsafe.Platform.*;
 
@@ -53,13 +51,12 @@
 
   // These are only updated by readExternal() or read()
   @Nonnull
-  private MemoryBlock base;
-  // While numBytes has the same value as base.size(), to keep as int avoids cast from long to int
+  private Object base;
+  private long offset;
   private int numBytes;
 
-  public MemoryBlock getMemoryBlock() { return base; }
-  public Object getBaseObject() { return base.getBaseObject(); }
-  public long getBaseOffset() { return base.getBaseOffset(); }
+  public Object getBaseObject() { return base; }
+  public long getBaseOffset() { return offset; }
 
   /**
    * A char in UTF-8 encoding can take 1-4 bytes depending on the first byte which
@@ -112,8 +109,7 @@
    */
   public static UTF8String fromBytes(byte[] bytes) {
     if (bytes != null) {
-      return new UTF8String(
-        new ByteArrayMemoryBlock(bytes, BYTE_ARRAY_OFFSET, bytes.length));
+      return new UTF8String(bytes, BYTE_ARRAY_OFFSET, bytes.length);
     } else {
       return null;
     }
@@ -126,13 +122,19 @@ public static UTF8String fromBytes(byte[] bytes) {
    */
   public static UTF8String fromBytes(byte[] bytes, int offset, int numBytes) {
     if (bytes != null) {
-      return new UTF8String(
-        new ByteArrayMemoryBlock(bytes, BYTE_ARRAY_OFFSET + offset, numBytes));
+      return new UTF8String(bytes, BYTE_ARRAY_OFFSET + offset, numBytes);
     } else {
       return null;
     }
   }
 
+  /**
+   * Creates an UTF8String from given address (base and offset) and length.
+   */
+  public static UTF8String fromAddress(Object base, long offset, int numBytes) {
+    return new UTF8String(base, offset, numBytes);
+  }
+
   /**
    * Creates an UTF8String from String.
    */
@@ -149,13 +151,16 @@ public static UTF8String blankString(int length) {
     return fromBytes(spaces);
   }
 
-  public UTF8String(MemoryBlock base) {
+  protected UTF8String(Object base, long offset, int numBytes) {
     this.base = base;
-    this.numBytes = Ints.checkedCast(base.size());
+    this.offset = offset;
+    this.numBytes = numBytes;
   }
 
   // for serialization
-  public UTF8String() {}
+  public UTF8String() {
+    this(null, 0, 0);
+  }
 
   /**
    * Writes the content of this string into a memory address, identified by an object and an offset.
@@ -163,7 +168,7 @@ public UTF8String() {}
    * bytes in this string.
    */
   public void writeToMemory(Object target, long targetOffset) {
-    base.writeTo(0, target, targetOffset, numBytes);
+    Platform.copyMemory(base, offset, target, targetOffset, numBytes);
   }
 
   public void writeTo(ByteBuffer buffer) {
@@ -183,9 +188,8 @@ public void writeTo(ByteBuffer buffer) {
    */
   @Nonnull
   public ByteBuffer getByteBuffer() {
-    long offset = base.getBaseOffset();
-    if (base instanceof ByteArrayMemoryBlock && offset >= BYTE_ARRAY_OFFSET) {
-      final byte[] bytes = ((ByteArrayMemoryBlock) base).getByteArray();
+    if (base instanceof byte[] && offset >= BYTE_ARRAY_OFFSET) {
+      final byte[] bytes = (byte[]) base;
 
       // the offset includes an object header... this is only needed for unsafe copies
       final long arrayOffset = offset - BYTE_ARRAY_OFFSET;
@@ -252,12 +256,12 @@ public long getPrefix() {
     long mask = 0;
     if (IS_LITTLE_ENDIAN) {
       if (numBytes >= 8) {
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
       } else if (numBytes > 4) {
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
         mask = (1L << (8 - numBytes) * 8) - 1;
       } else if (numBytes > 0) {
-        p = (long) base.getInt(0);
+        p = (long) Platform.getInt(base, offset);
         mask = (1L << (8 - numBytes) * 8) - 1;
       } else {
         p = 0;
@@ -266,12 +270,12 @@ public long getPrefix() {
     } else {
       // byteOrder == ByteOrder.BIG_ENDIAN
       if (numBytes >= 8) {
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
       } else if (numBytes > 4) {
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
         mask = (1L << (8 - numBytes) * 8) - 1;
       } else if (numBytes > 0) {
-        p = ((long) base.getInt(0)) << 32;
+        p = ((long) Platform.getInt(base, offset)) << 32;
         mask = (1L << (8 - numBytes) * 8) - 1;
       } else {
         p = 0;
@@ -286,13 +290,12 @@ public long getPrefix() {
    */
   public byte[] getBytes() {
     // avoid copy if `base` is `byte[]`
-    long offset = base.getBaseOffset();
-    if (offset == BYTE_ARRAY_OFFSET && base instanceof ByteArrayMemoryBlock
-      && (((ByteArrayMemoryBlock) base).getByteArray()).length == numBytes) {
-      return ((ByteArrayMemoryBlock) base).getByteArray();
+    if (offset == BYTE_ARRAY_OFFSET && base instanceof byte[]
+      && ((byte[]) base).length == numBytes) {
+      return (byte[]) base;
     } else {
       byte[] bytes = new byte[numBytes];
-      base.writeTo(0, bytes, BYTE_ARRAY_OFFSET, numBytes);
+      copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, numBytes);
       return bytes;
     }
   }
@@ -322,7 +325,7 @@ public UTF8String substring(final int start, final int until) {
 
     if (i > j) {
       byte[] bytes = new byte[i - j];
-      base.writeTo(j, bytes, BYTE_ARRAY_OFFSET, i - j);
+      copyMemory(base, offset + j, bytes, BYTE_ARRAY_OFFSET, i - j);
       return fromBytes(bytes);
     } else {
       return EMPTY_UTF8;
@@ -363,14 +366,14 @@ public boolean contains(final UTF8String substring) {
    * Returns the byte at position `i`.
    */
   private byte getByte(int i) {
-    return base.getByte(i);
+    return Platform.getByte(base, offset + i);
   }
 
   private boolean matchAt(final UTF8String s, int pos) {
     if (s.numBytes + pos > numBytes || pos < 0) {
       return false;
     }
-    return ByteArrayMethods.arrayEqualsBlock(base, pos, s.base, 0, s.numBytes);
+    return ByteArrayMethods.arrayEquals(base, offset + pos, s.base, s.offset, s.numBytes);
   }
 
   public boolean startsWith(final UTF8String prefix) {
@@ -497,7 +500,8 @@ public int findInSet(UTF8String match) {
     for (int i = 0; i < numBytes; i++) {
       if (getByte(i) == (byte) ',') {
         if (i - (lastComma + 1) == match.numBytes &&
-          ByteArrayMethods.arrayEqualsBlock(base, lastComma + 1, match.base, 0, match.numBytes)) {
+          ByteArrayMethods.arrayEquals(base, offset + (lastComma + 1), match.base, match.offset,
+            match.numBytes)) {
           return n;
         }
         lastComma = i;
@@ -505,7 +509,8 @@ public int findInSet(UTF8String match) {
       }
     }
     if (numBytes - (lastComma + 1) == match.numBytes &&
-      ByteArrayMethods.arrayEqualsBlock(base, lastComma + 1, match.base, 0, match.numBytes)) {
+      ByteArrayMethods.arrayEquals(base, offset + (lastComma + 1), match.base, match.offset,
+        match.numBytes)) {
       return n;
     }
     return 0;
@@ -520,7 +525,7 @@ public int findInSet(UTF8String match) {
   private UTF8String copyUTF8String(int start, int end) {
     int len = end - start + 1;
     byte[] newBytes = new byte[len];
-    base.writeTo(start, newBytes, BYTE_ARRAY_OFFSET, len);
+    copyMemory(base, offset + start, newBytes, BYTE_ARRAY_OFFSET, len);
     return UTF8String.fromBytes(newBytes);
   }
 
@@ -667,7 +672,8 @@ public UTF8String reverse() {
     int i = 0; // position in byte
     while (i < numBytes) {
       int len = numBytesForFirstByte(getByte(i));
-      base.writeTo(i, result, BYTE_ARRAY_OFFSET + result.length - i - len, len);
+      copyMemory(this.base, this.offset + i, result,
+        BYTE_ARRAY_OFFSET + result.length - i - len, len);
 
       i += len;
     }
@@ -681,7 +687,7 @@ public UTF8String repeat(int times) {
     }
 
     byte[] newBytes = new byte[numBytes * times];
-    base.writeTo(0, newBytes, BYTE_ARRAY_OFFSET, numBytes);
+    copyMemory(this.base, this.offset, newBytes, BYTE_ARRAY_OFFSET, numBytes);
 
     int copied = 1;
     while (copied < times) {
@@ -718,7 +724,7 @@ public int indexOf(UTF8String v, int start) {
       if (i + v.numBytes > numBytes) {
         return -1;
       }
-      if (ByteArrayMethods.arrayEqualsBlock(base, i, v.base, 0, v.numBytes)) {
+      if (ByteArrayMethods.arrayEquals(base, offset + i, v.base, v.offset, v.numBytes)) {
         return c;
       }
       i += numBytesForFirstByte(getByte(i));
@@ -734,7 +740,7 @@ public int indexOf(UTF8String v, int start) {
   private int find(UTF8String str, int start) {
     assert (str.numBytes > 0);
     while (start <= numBytes - str.numBytes) {
-      if (ByteArrayMethods.arrayEqualsBlock(base, start, str.base, 0, str.numBytes)) {
+      if (ByteArrayMethods.arrayEquals(base, offset + start, str.base, str.offset, str.numBytes)) {
         return start;
       }
       start += 1;
@@ -748,7 +754,7 @@ private int find(UTF8String str, int start) {
   private int rfind(UTF8String str, int start) {
     assert (str.numBytes > 0);
     while (start >= 0) {
-      if (ByteArrayMethods.arrayEqualsBlock(base, start, str.base, 0, str.numBytes)) {
+      if (ByteArrayMethods.arrayEquals(base, offset + start, str.base, str.offset, str.numBytes)) {
         return start;
       }
       start -= 1;
@@ -781,7 +787,7 @@ public UTF8String subStringIndex(UTF8String delim, int count) {
         return EMPTY_UTF8;
       }
       byte[] bytes = new byte[idx];
-      base.writeTo(0, bytes, BYTE_ARRAY_OFFSET, idx);
+      copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, idx);
       return fromBytes(bytes);
 
     } else {
@@ -801,7 +807,7 @@ public UTF8String subStringIndex(UTF8String delim, int count) {
       }
       int size = numBytes - delim.numBytes - idx;
       byte[] bytes = new byte[size];
-      base.writeTo(idx + delim.numBytes, bytes, BYTE_ARRAY_OFFSET, size);
+      copyMemory(base, offset + idx + delim.numBytes, bytes, BYTE_ARRAY_OFFSET, size);
       return fromBytes(bytes);
     }
   }
@@ -824,15 +830,15 @@ public UTF8String rpad(int len, UTF8String pad) {
       UTF8String remain = pad.substring(0, spaces - padChars * count);
 
       byte[] data = new byte[this.numBytes + pad.numBytes * count + remain.numBytes];
-      base.writeTo(0, data, BYTE_ARRAY_OFFSET, this.numBytes);
+      copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET, this.numBytes);
       int offset = this.numBytes;
       int idx = 0;
       while (idx < count) {
-        pad.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
+        copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
         ++ idx;
         offset += pad.numBytes;
       }
-      remain.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);
+      copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);
 
       return UTF8String.fromBytes(data);
     }
@@ -860,13 +866,13 @@ public UTF8String lpad(int len, UTF8String pad) {
       int offset = 0;
       int idx = 0;
       while (idx < count) {
-        pad.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
+        copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
         ++ idx;
         offset += pad.numBytes;
       }
-      remain.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);
+      copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);
       offset += remain.numBytes;
-      base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, numBytes());
+      copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET + offset, numBytes());
 
       return UTF8String.fromBytes(data);
     }
@@ -891,8 +897,8 @@ public static UTF8String concat(UTF8String... inputs) {
     int offset = 0;
     for (int i = 0; i < inputs.length; i++) {
       int len = inputs[i].numBytes;
-      inputs[i].base.writeTo(
-        0,
+      copyMemory(
+        inputs[i].base, inputs[i].offset,
         result, BYTE_ARRAY_OFFSET + offset,
         len);
       offset += len;
@@ -931,8 +937,8 @@ public static UTF8String concatWs(UTF8String separator, UTF8String... inputs) {
     for (int i = 0, j = 0; i < inputs.length; i++) {
       if (inputs[i] != null) {
         int len = inputs[i].numBytes;
-        inputs[i].base.writeTo(
-          0,
+        copyMemory(
+          inputs[i].base, inputs[i].offset,
           result, BYTE_ARRAY_OFFSET + offset,
           len);
         offset += len;
@@ -940,8 +946,8 @@ public static UTF8String concatWs(UTF8String separator, UTF8String... inputs) {
         j++;
         // Add separator if this is not the last input.
         if (j < numInputs) {
-          separator.base.writeTo(
-            0,
+          copyMemory(
+            separator.base, separator.offset,
             result, BYTE_ARRAY_OFFSET + offset,
             separator.numBytes);
           offset += separator.numBytes;
@@ -952,6 +958,12 @@ public static UTF8String concatWs(UTF8String separator, UTF8String... inputs) {
   }
 
   public UTF8String[] split(UTF8String pattern, int limit) {
+    // Java String's split method supports ""ignore empty string"" behavior when the limit is 0
+    // whereas other languages do not. To avoid this java specific behavior, we fall back to
+    // -1 when the limit is 0.
+    if (limit == 0) {
+      limit = -1;
+    }
     String[] splits = toString().split(pattern.toString(), limit);
     UTF8String[] res = new UTF8String[splits.length];
     for (int i = 0; i < res.length; i++) {
@@ -1215,7 +1227,7 @@ public UTF8String clone() {
 
   public UTF8String copy() {
     byte[] bytes = new byte[numBytes];
-    base.writeTo(0, bytes, BYTE_ARRAY_OFFSET, numBytes);
+    copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, numBytes);
     return fromBytes(bytes);
   }
 
@@ -1223,10 +1235,11 @@ public UTF8String copy() {
   public int compareTo(@Nonnull final UTF8String other) {
     int len = Math.min(numBytes, other.numBytes);
     int wordMax = (len / 8) * 8;
-    MemoryBlock rbase = other.base;
+    long roffset = other.offset;
+    Object rbase = other.base;
     for (int i = 0; i < wordMax; i += 8) {
-      long left = base.getLong(i);
-      long right = rbase.getLong(i);
+      long left = getLong(base, offset + i);
+      long right = getLong(rbase, roffset + i);
       if (left != right) {
         if (IS_LITTLE_ENDIAN) {
           return Long.compareUnsigned(Long.reverseBytes(left), Long.reverseBytes(right));
@@ -1237,7 +1250,7 @@ public int compareTo(@Nonnull final UTF8String other) {
     }
     for (int i = wordMax; i < len; i++) {
       // In UTF-8, the byte should be unsigned, so we should compare them as unsigned int.
-      int res = (getByte(i) & 0xFF) - (rbase.getByte(i) & 0xFF);
+      int res = (getByte(i) & 0xFF) - (Platform.getByte(rbase, roffset + i) & 0xFF);
       if (res != 0) {
         return res;
       }
@@ -1256,7 +1269,7 @@ public boolean equals(final Object other) {
       if (numBytes != o.numBytes) {
         return false;
       }
-      return ByteArrayMethods.arrayEqualsBlock(base, 0, o.base, 0, numBytes);
+      return ByteArrayMethods.arrayEquals(base, offset, o.base, o.offset, numBytes);
     } else {
       return false;
     }
@@ -1312,8 +1325,8 @@ public int levenshteinDistance(UTF8String other) {
               num_bytes_j != numBytesForFirstByte(s.getByte(i_bytes))) {
           cost = 1;
         } else {
-          cost = (ByteArrayMethods.arrayEqualsBlock(t.base, j_bytes, s.base,
-            i_bytes, num_bytes_j)) ? 0 : 1;
+          cost = (ByteArrayMethods.arrayEquals(t.base, t.offset + j_bytes, s.base,
+              s.offset + i_bytes, num_bytes_j)) ? 0 : 1;
         }
         d[i + 1] = Math.min(Math.min(d[i] + 1, p[i + 1] + 1), p[i] + cost);
       }
@@ -1328,7 +1341,7 @@ public int levenshteinDistance(UTF8String other) {
 
   @Override
   public int hashCode() {
-    return Murmur3_x86_32.hashUnsafeBytesBlock(base,42);
+    return Murmur3_x86_32.hashUnsafeBytes(base, offset, numBytes, 42);
   }
 
   /**
@@ -1391,10 +1404,10 @@ public void writeExternal(ObjectOutput out) throws IOException {
   }
 
   public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
+    offset = BYTE_ARRAY_OFFSET;
     numBytes = in.readInt();
-    byte[] bytes = new byte[numBytes];
-    in.readFully(bytes);
-    base = ByteArrayMemoryBlock.fromArray(bytes);
+    base = new byte[numBytes];
+    in.readFully((byte[]) base);
   }
 
   @Override
@@ -1406,10 +1419,10 @@ public void write(Kryo kryo, Output out) {
 
   @Override
   public void read(Kryo kryo, Input in) {
-    numBytes = in.readInt();
-    byte[] bytes = new byte[numBytes];
-    in.read(bytes);
-    base = ByteArrayMemoryBlock.fromArray(bytes);
+    this.offset = BYTE_ARRAY_OFFSET;
+    this.numBytes = in.readInt();
+    this.base = new byte[numBytes];
+    in.read((byte[]) base);
   }
 
 }
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java
index 583a148b3845d..2474081dad5c9 100644
--- a/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java
@@ -81,7 +81,7 @@ public void freeingOnHeapMemoryBlockResetsBaseObjectAndOffset() {
     MemoryAllocator.HEAP.free(block);
     Assert.assertNull(block.getBaseObject());
     Assert.assertEquals(0, block.getBaseOffset());
-    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.getPageNumber());
+    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.pageNumber);
   }
 
   @Test
@@ -92,7 +92,7 @@ public void freeingOffHeapMemoryBlockResetsOffset() {
     MemoryAllocator.UNSAFE.free(block);
     Assert.assertNull(block.getBaseObject());
     Assert.assertEquals(0, block.getBaseOffset());
-    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.getPageNumber());
+    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.pageNumber);
   }
 
   @Test(expected = AssertionError.class)
@@ -157,4 +157,22 @@ public void heapMemoryReuse() {
     Assert.assertEquals(onheap4.size(), 1024 * 1024 + 7);
     Assert.assertEquals(obj3, onheap4.getBaseObject());
   }
+
+  @Test
+  // SPARK-26021
+  public void writeMinusZeroIsReplacedWithZero() {
+    byte[] doubleBytes = new byte[Double.BYTES];
+    byte[] floatBytes = new byte[Float.BYTES];
+    Platform.putDouble(doubleBytes, Platform.BYTE_ARRAY_OFFSET, -0.0d);
+    Platform.putFloat(floatBytes, Platform.BYTE_ARRAY_OFFSET, -0.0f);
+
+    byte[] doubleBytes2 = new byte[Double.BYTES];
+    byte[] floatBytes2 = new byte[Float.BYTES];
+    Platform.putDouble(doubleBytes, Platform.BYTE_ARRAY_OFFSET, 0.0d);
+    Platform.putFloat(floatBytes, Platform.BYTE_ARRAY_OFFSET, 0.0f);
+
+    // Make sure the bytes we write from 0.0 and -0.0 are same.
+    Assert.assertArrayEquals(doubleBytes, doubleBytes2);
+    Assert.assertArrayEquals(floatBytes, floatBytes2);
+  }
 }
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java
index 8c2e98c2bfc54..fb8e53b3348f3 100644
--- a/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java
@@ -20,13 +20,14 @@
 import org.junit.Assert;
 import org.junit.Test;
 
-import org.apache.spark.unsafe.memory.OnHeapMemoryBlock;
+import org.apache.spark.unsafe.memory.MemoryBlock;
 
 public class LongArraySuite {
 
   @Test
   public void basicTest() {
-    LongArray arr = new LongArray(new OnHeapMemoryBlock(16));
+    long[] bytes = new long[2];
+    LongArray arr = new LongArray(MemoryBlock.fromLongArray(bytes));
     arr.set(0, 1L);
     arr.set(1, 2L);
     arr.set(1, 3L);
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java
index d9898771720ae..6348a73bf3895 100644
--- a/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java
@@ -70,24 +70,6 @@ public void testKnownBytesInputs() {
       Murmur3_x86_32.hashUnsafeBytes2(tes, Platform.BYTE_ARRAY_OFFSET, tes.length, 0));
   }
 
-  @Test
-  public void testKnownWordsInputs() {
-    byte[] bytes = new byte[16];
-    long offset = Platform.BYTE_ARRAY_OFFSET;
-    for (int i = 0; i < 16; i++) {
-      bytes[i] = 0;
-    }
-    Assert.assertEquals(-300363099, Murmur3_x86_32.hashUnsafeWords(bytes, offset, 16, 42));
-    for (int i = 0; i < 16; i++) {
-      bytes[i] = -1;
-    }
-    Assert.assertEquals(-1210324667, Murmur3_x86_32.hashUnsafeWords(bytes, offset, 16, 42));
-    for (int i = 0; i < 16; i++) {
-      bytes[i] = (byte)i;
-    }
-    Assert.assertEquals(-634919701, Murmur3_x86_32.hashUnsafeWords(bytes, offset, 16, 42));
-  }
-
   @Test
   public void randomizedStressTest() {
     int size = 65536;
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/memory/MemoryBlockSuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/memory/MemoryBlockSuite.java
deleted file mode 100644
index ef5ff8ee70ec0..0000000000000
--- a/common/unsafe/src/test/java/org/apache/spark/unsafe/memory/MemoryBlockSuite.java
+++ /dev/null
@@ -1,179 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.unsafe.memory;
-
-import org.apache.spark.unsafe.Platform;
-import org.junit.Assert;
-import org.junit.Test;
-
-import java.nio.ByteOrder;
-
-import static org.hamcrest.core.StringContains.containsString;
-
-public class MemoryBlockSuite {
-  private static final boolean bigEndianPlatform =
-    ByteOrder.nativeOrder().equals(ByteOrder.BIG_ENDIAN);
-
-  private void check(MemoryBlock memory, Object obj, long offset, int length) {
-    memory.setPageNumber(1);
-    memory.fill((byte)-1);
-    memory.putBoolean(0, true);
-    memory.putByte(1, (byte)127);
-    memory.putShort(2, (short)257);
-    memory.putInt(4, 0x20000002);
-    memory.putLong(8, 0x1234567089ABCDEFL);
-    memory.putFloat(16, 1.0F);
-    memory.putLong(20, 0x1234567089ABCDEFL);
-    memory.putDouble(28, 2.0);
-    MemoryBlock.copyMemory(memory, 0L, memory, 36, 4);
-    int[] a = new int[2];
-    a[0] = 0x12345678;
-    a[1] = 0x13579BDF;
-    memory.copyFrom(a, Platform.INT_ARRAY_OFFSET, 40, 8);
-    byte[] b = new byte[8];
-    memory.writeTo(40, b, Platform.BYTE_ARRAY_OFFSET, 8);
-
-    Assert.assertEquals(obj, memory.getBaseObject());
-    Assert.assertEquals(offset, memory.getBaseOffset());
-    Assert.assertEquals(length, memory.size());
-    Assert.assertEquals(1, memory.getPageNumber());
-    Assert.assertEquals(true, memory.getBoolean(0));
-    Assert.assertEquals((byte)127, memory.getByte(1 ));
-    Assert.assertEquals((short)257, memory.getShort(2));
-    Assert.assertEquals(0x20000002, memory.getInt(4));
-    Assert.assertEquals(0x1234567089ABCDEFL, memory.getLong(8));
-    Assert.assertEquals(1.0F, memory.getFloat(16), 0);
-    Assert.assertEquals(0x1234567089ABCDEFL, memory.getLong(20));
-    Assert.assertEquals(2.0, memory.getDouble(28), 0);
-    Assert.assertEquals(true, memory.getBoolean(36));
-    Assert.assertEquals((byte)127, memory.getByte(37 ));
-    Assert.assertEquals((short)257, memory.getShort(38));
-    Assert.assertEquals(a[0], memory.getInt(40));
-    Assert.assertEquals(a[1], memory.getInt(44));
-    if (bigEndianPlatform) {
-      Assert.assertEquals(a[0],
-        ((int)b[0] & 0xff) << 24 | ((int)b[1] & 0xff) << 16 |
-        ((int)b[2] & 0xff) << 8 | ((int)b[3] & 0xff));
-      Assert.assertEquals(a[1],
-        ((int)b[4] & 0xff) << 24 | ((int)b[5] & 0xff) << 16 |
-        ((int)b[6] & 0xff) << 8 | ((int)b[7] & 0xff));
-    } else {
-      Assert.assertEquals(a[0],
-        ((int)b[3] & 0xff) << 24 | ((int)b[2] & 0xff) << 16 |
-        ((int)b[1] & 0xff) << 8 | ((int)b[0] & 0xff));
-      Assert.assertEquals(a[1],
-        ((int)b[7] & 0xff) << 24 | ((int)b[6] & 0xff) << 16 |
-        ((int)b[5] & 0xff) << 8 | ((int)b[4] & 0xff));
-    }
-    for (int i = 48; i < memory.size(); i++) {
-      Assert.assertEquals((byte) -1, memory.getByte(i));
-    }
-
-    assert(memory.subBlock(0, memory.size()) == memory);
-
-    try {
-      memory.subBlock(-8, 8);
-      Assert.fail();
-    } catch (Exception expected) {
-      Assert.assertThat(expected.getMessage(), containsString(""non-negative""));
-    }
-
-    try {
-      memory.subBlock(0, -8);
-      Assert.fail();
-    } catch (Exception expected) {
-      Assert.assertThat(expected.getMessage(), containsString(""non-negative""));
-    }
-
-    try {
-      memory.subBlock(0, length + 8);
-      Assert.fail();
-    } catch (Exception expected) {
-      Assert.assertThat(expected.getMessage(), containsString(""should not be larger than""));
-    }
-
-    try {
-      memory.subBlock(8, length - 4);
-      Assert.fail();
-    } catch (Exception expected) {
-      Assert.assertThat(expected.getMessage(), containsString(""should not be larger than""));
-    }
-
-    try {
-      memory.subBlock(length + 8, 4);
-      Assert.fail();
-    } catch (Exception expected) {
-      Assert.assertThat(expected.getMessage(), containsString(""should not be larger than""));
-    }
-
-    memory.setPageNumber(MemoryBlock.NO_PAGE_NUMBER);
-  }
-
-  @Test
-  public void testByteArrayMemoryBlock() {
-    byte[] obj = new byte[56];
-    long offset = Platform.BYTE_ARRAY_OFFSET;
-    int length = obj.length;
-
-    MemoryBlock memory = new ByteArrayMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-
-    memory = ByteArrayMemoryBlock.fromArray(obj);
-    check(memory, obj, offset, length);
-
-    obj = new byte[112];
-    memory = new ByteArrayMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-  }
-
-  @Test
-  public void testOnHeapMemoryBlock() {
-    long[] obj = new long[7];
-    long offset = Platform.LONG_ARRAY_OFFSET;
-    int length = obj.length * 8;
-
-    MemoryBlock memory = new OnHeapMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-
-    memory = OnHeapMemoryBlock.fromArray(obj);
-    check(memory, obj, offset, length);
-
-    obj = new long[14];
-    memory = new OnHeapMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-  }
-
-  @Test
-  public void testOffHeapArrayMemoryBlock() {
-    MemoryAllocator memoryAllocator = new UnsafeMemoryAllocator();
-    MemoryBlock memory = memoryAllocator.allocate(56);
-    Object obj = memory.getBaseObject();
-    long offset = memory.getBaseOffset();
-    int length = 56;
-
-    check(memory, obj, offset, length);
-    memoryAllocator.free(memory);
-
-    long address = Platform.allocateMemory(112);
-    memory = new OffHeapMemoryBlock(address, length);
-    obj = memory.getBaseObject();
-    offset = memory.getBaseOffset();
-    check(memory, obj, offset, length);
-    Platform.freeMemory(address);
-  }
-}
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java
index 42dda30480702..cf9cc6b1800a9 100644
--- a/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java
@@ -25,8 +25,7 @@
 import java.util.*;
 
 import com.google.common.collect.ImmutableMap;
-import org.apache.spark.unsafe.memory.ByteArrayMemoryBlock;
-import org.apache.spark.unsafe.memory.OnHeapMemoryBlock;
+import org.apache.spark.unsafe.Platform;
 import org.junit.Test;
 
 import static org.junit.Assert.*;
@@ -394,12 +393,14 @@ public void substringSQL() {
 
   @Test
   public void split() {
-    assertTrue(Arrays.equals(fromString(""ab,def,ghi"").split(fromString("",""), -1),
-      new UTF8String[]{fromString(""ab""), fromString(""def""), fromString(""ghi"")}));
-    assertTrue(Arrays.equals(fromString(""ab,def,ghi"").split(fromString("",""), 2),
-      new UTF8String[]{fromString(""ab""), fromString(""def,ghi"")}));
-    assertTrue(Arrays.equals(fromString(""ab,def,ghi"").split(fromString("",""), 2),
-      new UTF8String[]{fromString(""ab""), fromString(""def,ghi"")}));
+    UTF8String[] negativeAndZeroLimitCase =
+      new UTF8String[]{fromString(""ab""), fromString(""def""), fromString(""ghi""), fromString("""")};
+    assertTrue(Arrays.equals(fromString(""ab,def,ghi,"").split(fromString("",""), 0),
+      negativeAndZeroLimitCase));
+    assertTrue(Arrays.equals(fromString(""ab,def,ghi,"").split(fromString("",""), -1),
+      negativeAndZeroLimitCase));
+    assertTrue(Arrays.equals(fromString(""ab,def,ghi,"").split(fromString("",""), 2),
+      new UTF8String[]{fromString(""ab""), fromString(""def,ghi,"")}));
   }
 
   @Test
@@ -513,6 +514,21 @@ public void soundex() {
     assertEquals(fromString(""世界千世"").soundex(), fromString(""世界千世""));
   }
 
+  @Test
+  public void writeToOutputStreamUnderflow() throws IOException {
+    // offset underflow is apparently supported?
+    final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
+    final byte[] test = ""01234567"".getBytes(StandardCharsets.UTF_8);
+
+    for (int i = 1; i <= Platform.BYTE_ARRAY_OFFSET; ++i) {
+      UTF8String.fromAddress(test, Platform.BYTE_ARRAY_OFFSET - i, test.length + i)
+          .writeTo(outputStream);
+      final ByteBuffer buffer = ByteBuffer.wrap(outputStream.toByteArray(), i, test.length);
+      assertEquals(""01234567"", StandardCharsets.UTF_8.decode(buffer).toString());
+      outputStream.reset();
+    }
+  }
+
   @Test
   public void writeToOutputStreamSlice() throws IOException {
     final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
@@ -520,7 +536,7 @@ public void writeToOutputStreamSlice() throws IOException {
 
     for (int i = 0; i < test.length; ++i) {
       for (int j = 0; j < test.length - i; ++j) {
-        new UTF8String(ByteArrayMemoryBlock.fromArray(test).subBlock(i, j))
+        UTF8String.fromAddress(test, Platform.BYTE_ARRAY_OFFSET + i, j)
             .writeTo(outputStream);
 
         assertArrayEquals(Arrays.copyOfRange(test, i, i + j), outputStream.toByteArray());
@@ -551,7 +567,7 @@ public void writeToOutputStreamOverflow() throws IOException {
 
     for (final long offset : offsets) {
       try {
-        new UTF8String(ByteArrayMemoryBlock.fromArray(test).subBlock(offset, test.length))
+        fromAddress(test, BYTE_ARRAY_OFFSET + offset, test.length)
             .writeTo(outputStream);
 
         throw new IllegalStateException(Long.toString(offset));
@@ -578,25 +594,26 @@ public void writeToOutputStream() throws IOException {
   }
 
   @Test
-  public void writeToOutputStreamLongArray() throws IOException {
+  public void writeToOutputStreamIntArray() throws IOException {
     // verify that writes work on objects that are not byte arrays
-    final ByteBuffer buffer = StandardCharsets.UTF_8.encode(""3千大千世界"");
+    final ByteBuffer buffer = StandardCharsets.UTF_8.encode(""大千世界"");
     buffer.position(0);
     buffer.order(ByteOrder.nativeOrder());
 
     final int length = buffer.limit();
-    assertEquals(16, length);
+    assertEquals(12, length);
 
-    final int longs = length / 8;
-    final long[] array = new long[longs];
+    final int ints = length / 4;
+    final int[] array = new int[ints];
 
-    for (int i = 0; i < longs; ++i) {
-      array[i] = buffer.getLong();
+    for (int i = 0; i < ints; ++i) {
+      array[i] = buffer.getInt();
     }
 
     final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
-    new UTF8String(OnHeapMemoryBlock.fromArray(array)).writeTo(outputStream);
-    assertEquals(""3千大千世界"", outputStream.toString(""UTF-8""));
+    fromAddress(array, Platform.INT_ARRAY_OFFSET, length)
+        .writeTo(outputStream);
+    assertEquals(""大千世界"", outputStream.toString(""UTF-8""));
   }
 
   @Test
diff --git a/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala b/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala
index 7d3331f44f015..fdb81a06d41c9 100644
--- a/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala
+++ b/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala
@@ -17,7 +17,7 @@
 
 package org.apache.spark.unsafe.types
 
-import org.apache.commons.lang3.StringUtils
+import org.apache.commons.text.similarity.LevenshteinDistance
 import org.scalacheck.{Arbitrary, Gen}
 import org.scalatest.prop.GeneratorDrivenPropertyChecks
 // scalastyle:off
@@ -63,6 +63,7 @@ class UTF8StringPropertyCheckSuite extends FunSuite with GeneratorDrivenProperty
     }
   }
 
+  // scalastyle:off caselocale
   test(""toUpperCase"") {
     forAll { (s: String) =>
       assert(toUTF8(s).toUpperCase === toUTF8(s.toUpperCase))
@@ -74,6 +75,7 @@ class UTF8StringPropertyCheckSuite extends FunSuite with GeneratorDrivenProperty
       assert(toUTF8(s).toLowerCase === toUTF8(s.toLowerCase))
     }
   }
+  // scalastyle:on caselocale
 
   test(""compare"") {
     forAll { (s1: String, s2: String) =>
@@ -230,7 +232,7 @@ class UTF8StringPropertyCheckSuite extends FunSuite with GeneratorDrivenProperty
   test(""levenshteinDistance"") {
     forAll { (one: String, another: String) =>
       assert(toUTF8(one).levenshteinDistance(toUTF8(another)) ===
-        StringUtils.getLevenshteinDistance(one, another))
+        LevenshteinDistance.getDefaultInstance.apply(one, another))
     }
   }
 
diff --git a/core/benchmarks/KryoBenchmark-results.txt b/core/benchmarks/KryoBenchmark-results.txt
new file mode 100644
index 0000000000000..91e22f3afc14f
--- /dev/null
+++ b/core/benchmarks/KryoBenchmark-results.txt
@@ -0,0 +1,29 @@
+================================================================================================
+Benchmark Kryo Unsafe vs safe Serialization
+================================================================================================
+
+Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11 on Mac OS X 10.13.6
+Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz
+
+Benchmark Kryo Unsafe vs safe Serialization: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
+------------------------------------------------------------------------------------------------
+basicTypes: Int with unsafe:true               138 /  149          7.2         138.0       1.0X
+basicTypes: Long with unsafe:true              168 /  173          6.0         167.7       0.8X
+basicTypes: Float with unsafe:true             153 /  174          6.5         153.1       0.9X
+basicTypes: Double with unsafe:true            161 /  185          6.2         161.1       0.9X
+Array: Int with unsafe:true                      2 /    3        409.7           2.4      56.5X
+Array: Long with unsafe:true                     4 /    5        232.5           4.3      32.1X
+Array: Float with unsafe:true                    3 /    4        367.3           2.7      50.7X
+Array: Double with unsafe:true                   4 /    5        228.5           4.4      31.5X
+Map of string->Double  with unsafe:true         38 /   45         26.5          37.8       3.7X
+basicTypes: Int with unsafe:false              176 /  187          5.7         175.9       0.8X
+basicTypes: Long with unsafe:false             191 /  203          5.2         191.2       0.7X
+basicTypes: Float with unsafe:false            166 /  176          6.0         166.2       0.8X
+basicTypes: Double with unsafe:false           174 /  190          5.7         174.3       0.8X
+Array: Int with unsafe:false                    19 /   26         52.9          18.9       7.3X
+Array: Long with unsafe:false                   27 /   31         37.7          26.5       5.2X
+Array: Float with unsafe:false                   8 /   10        124.3           8.0      17.2X
+Array: Double with unsafe:false                 12 /   13         83.6          12.0      11.5X
+Map of string->Double  with unsafe:false        38 /   42         26.1          38.3       3.6X
+
+
diff --git a/core/benchmarks/KryoSerializerBenchmark-results.txt b/core/benchmarks/KryoSerializerBenchmark-results.txt
new file mode 100644
index 0000000000000..c3ce336d93241
--- /dev/null
+++ b/core/benchmarks/KryoSerializerBenchmark-results.txt
@@ -0,0 +1,12 @@
+================================================================================================
+Benchmark KryoPool vs ""pool of 1""
+================================================================================================
+
+Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11 on Mac OS X 10.14
+Intel(R) Core(TM) i7-4770HQ CPU @ 2.20GHz
+Benchmark KryoPool vs ""pool of 1"":       Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
+------------------------------------------------------------------------------------------------
+KryoPool:true                                 2682 / 3425          0.0     5364627.9       1.0X
+KryoPool:false                                8176 / 9292          0.0    16351252.2       0.3X
+
+
diff --git a/core/pom.xml b/core/pom.xml
index 5fa3a86de6b01..49b1a54e32598 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -20,12 +20,12 @@
   <modelVersion>4.0.0</modelVersion>
   <parent>
     <groupId>org.apache.spark</groupId>
-    <artifactId>spark-parent_2.11</artifactId>
-    <version>2.4.0-SNAPSHOT</version>
+    <artifactId>spark-parent_2.12</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
 
-  <artifactId>spark-core_2.11</artifactId>
+  <artifactId>spark-core_2.12</artifactId>
   <properties>
     <sbt.project.name>core</sbt.project.name>
   </properties>
@@ -56,7 +56,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.xbean</groupId>
-      <artifactId>xbean-asm6-shaded</artifactId>
+      <artifactId>xbean-asm7-shaded</artifactId>
     </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
@@ -350,7 +350,7 @@
     <dependency>
       <groupId>net.sf.py4j</groupId>
       <artifactId>py4j</artifactId>
-      <version>0.10.7</version>
+      <version>0.10.8.1</version>
     </dependency>
     <dependency>
       <groupId>org.apache.spark</groupId>
@@ -408,6 +408,19 @@
       <scope>provided</scope>
     </dependency>
 
+    <!--
+     The following kafka dependency used to obtain delegation token.
+     In order to prevent spark-core from depending on kafka, these deps have been placed in the
+     ""provided"" scope, rather than the ""compile"" scope, and NoClassDefFoundError exceptions are
+     handled when the user explicitly use neither spark-streaming-kafka nor spark-sql-kafka modules.
+    -->
+    <dependency>
+      <groupId>org.apache.kafka</groupId>
+      <artifactId>kafka-clients</artifactId>
+      <version>${kafka.version}</version>
+      <scope>provided</scope>
+    </dependency>
+
   </dependencies>
   <build>
     <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>
diff --git a/core/src/main/java/org/apache/spark/ExecutorPlugin.java b/core/src/main/java/org/apache/spark/ExecutorPlugin.java
new file mode 100644
index 0000000000000..f86520c81df33
--- /dev/null
+++ b/core/src/main/java/org/apache/spark/ExecutorPlugin.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark;
+
+import org.apache.spark.annotation.DeveloperApi;
+
+/**
+ * A plugin which can be automatically instantiated within each Spark executor.  Users can specify
+ * plugins which should be created with the ""spark.executor.plugins"" configuration.  An instance
+ * of each plugin will be created for every executor, including those created by dynamic allocation,
+ * before the executor starts running any tasks.
+ *
+ * The specific api exposed to the end users still considered to be very unstable.  We will
+ * hopefully be able to keep compatibility by providing default implementations for any methods
+ * added, but make no guarantees this will always be possible across all Spark releases.
+ *
+ * Spark does nothing to verify the plugin is doing legitimate things, or to manage the resources
+ * it uses.  A plugin acquires the same privileges as the user running the task.  A bad plugin
+ * could also interfere with task execution and make the executor fail in unexpected ways.
+ */
+@DeveloperApi
+public interface ExecutorPlugin {
+
+  /**
+   * Initialize the executor plugin.
+   *
+   * <p>Each executor will, during its initialization, invoke this method on each
+   * plugin provided in the spark.executor.plugins configuration.</p>
+   *
+   * <p>Plugins should create threads in their implementation of this method for
+   * any polling, blocking, or intensive computation.</p>
+   */
+  default void init() {}
+
+  /**
+   * Clean up and terminate this plugin.
+   *
+   * <p>This function is called during the executor shutdown phase. The executor
+   * will wait for the plugin to terminate before continuing its own shutdown.</p>
+   */
+  default void shutdown() {}
+}
diff --git a/core/src/main/java/org/apache/spark/SparkFirehoseListener.java b/core/src/main/java/org/apache/spark/SparkFirehoseListener.java
index 94c5c11b61a50..731f6fc767dfd 100644
--- a/core/src/main/java/org/apache/spark/SparkFirehoseListener.java
+++ b/core/src/main/java/org/apache/spark/SparkFirehoseListener.java
@@ -103,6 +103,12 @@ public final void onExecutorMetricsUpdate(
     onEvent(executorMetricsUpdate);
   }
 
+  @Override
+  public final void onStageExecutorMetrics(
+      SparkListenerStageExecutorMetrics executorMetrics) {
+    onEvent(executorMetrics);
+  }
+
   @Override
   public final void onExecutorAdded(SparkListenerExecutorAdded executorAdded) {
     onEvent(executorAdded);
diff --git a/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java b/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
deleted file mode 100644
index 0dd8fafbf2c82..0000000000000
--- a/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.api.java;
-
-import java.util.ArrayList;
-import java.util.List;
-
-// See
-// http://scala-programming-language.1934581.n4.nabble.com/Workaround-for-implementing-java-varargs-in-2-7-2-final-tp1944767p1944772.html
-abstract class JavaSparkContextVarargsWorkaround {
-
-  @SafeVarargs
-  public final <T> JavaRDD<T> union(JavaRDD<T>... rdds) {
-    if (rdds.length == 0) {
-      throw new IllegalArgumentException(""Union called on empty list"");
-    }
-    List<JavaRDD<T>> rest = new ArrayList<>(rdds.length - 1);
-    for (int i = 1; i < rdds.length; i++) {
-      rest.add(rdds[i]);
-    }
-    return union(rdds[0], rest);
-  }
-
-  public JavaDoubleRDD union(JavaDoubleRDD... rdds) {
-    if (rdds.length == 0) {
-      throw new IllegalArgumentException(""Union called on empty list"");
-    }
-    List<JavaDoubleRDD> rest = new ArrayList<>(rdds.length - 1);
-    for (int i = 1; i < rdds.length; i++) {
-      rest.add(rdds[i]);
-    }
-    return union(rdds[0], rest);
-  }
-
-  @SafeVarargs
-  public final <K, V> JavaPairRDD<K, V> union(JavaPairRDD<K, V>... rdds) {
-    if (rdds.length == 0) {
-      throw new IllegalArgumentException(""Union called on empty list"");
-    }
-    List<JavaPairRDD<K, V>> rest = new ArrayList<>(rdds.length - 1);
-    for (int i = 1; i < rdds.length; i++) {
-      rest.add(rdds[i]);
-    }
-    return union(rdds[0], rest);
-  }
-
-  // These methods take separate ""first"" and ""rest"" elements to avoid having the same type erasure
-  public abstract <T> JavaRDD<T> union(JavaRDD<T> first, List<JavaRDD<T>> rest);
-  public abstract JavaDoubleRDD union(JavaDoubleRDD first, List<JavaDoubleRDD> rest);
-  public abstract <K, V> JavaPairRDD<K, V> union(JavaPairRDD<K, V> first, List<JavaPairRDD<K, V>>
-    rest);
-}
diff --git a/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java b/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java
index f6d1288cb263d..92bf0ecc1b5cb 100644
--- a/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java
+++ b/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java
@@ -27,7 +27,7 @@
  * to read a file to avoid extra copy of data between Java and
  * native memory which happens when using {@link java.io.BufferedInputStream}.
  * Unfortunately, this is not something already available in JDK,
- * {@link sun.nio.ch.ChannelInputStream} supports reading a file using nio,
+ * {@code sun.nio.ch.ChannelInputStream} supports reading a file using nio,
  * but does not support buffering.
  */
 public final class NioBufferedFileInputStream extends InputStream {
@@ -130,6 +130,7 @@ public synchronized void close() throws IOException {
     StorageUtils.dispose(byteBuffer);
   }
 
+  @SuppressWarnings(""deprecation"")
   @Override
   protected void finalize() throws IOException {
     close();
diff --git a/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java b/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java
index 0cced9e222952..2e18715b600e0 100644
--- a/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java
+++ b/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java
@@ -135,62 +135,58 @@ private void readAsync() throws IOException {
     } finally {
       stateChangeLock.unlock();
     }
-    executorService.execute(new Runnable() {
-
-      @Override
-      public void run() {
-        stateChangeLock.lock();
-        try {
-          if (isClosed) {
-            readInProgress = false;
-            return;
-          }
-          // Flip this so that the close method will not close the underlying input stream when we
-          // are reading.
-          isReading = true;
-        } finally {
-          stateChangeLock.unlock();
+    executorService.execute(() -> {
+      stateChangeLock.lock();
+      try {
+        if (isClosed) {
+          readInProgress = false;
+          return;
         }
+        // Flip this so that the close method will not close the underlying input stream when we
+        // are reading.
+        isReading = true;
+      } finally {
+        stateChangeLock.unlock();
+      }
 
-        // Please note that it is safe to release the lock and read into the read ahead buffer
-        // because either of following two conditions will hold - 1. The active buffer has
-        // data available to read so the reader will not read from the read ahead buffer.
-        // 2. This is the first time read is called or the active buffer is exhausted,
-        // in that case the reader waits for this async read to complete.
-        // So there is no race condition in both the situations.
-        int read = 0;
-        int off = 0, len = arr.length;
-        Throwable exception = null;
-        try {
-          // try to fill the read ahead buffer.
-          // if a reader is waiting, possibly return early.
-          do {
-            read = underlyingInputStream.read(arr, off, len);
-            if (read <= 0) break;
-            off += read;
-            len -= read;
-          } while (len > 0 && !isWaiting.get());
-        } catch (Throwable ex) {
-          exception = ex;
-          if (ex instanceof Error) {
-            // `readException` may not be reported to the user. Rethrow Error to make sure at least
-            // The user can see Error in UncaughtExceptionHandler.
-            throw (Error) ex;
-          }
-        } finally {
-          stateChangeLock.lock();
-          readAheadBuffer.limit(off);
-          if (read < 0 || (exception instanceof EOFException)) {
-            endOfStream = true;
-          } else if (exception != null) {
-            readAborted = true;
-            readException = exception;
-          }
-          readInProgress = false;
-          signalAsyncReadComplete();
-          stateChangeLock.unlock();
-          closeUnderlyingInputStreamIfNecessary();
+      // Please note that it is safe to release the lock and read into the read ahead buffer
+      // because either of following two conditions will hold - 1. The active buffer has
+      // data available to read so the reader will not read from the read ahead buffer.
+      // 2. This is the first time read is called or the active buffer is exhausted,
+      // in that case the reader waits for this async read to complete.
+      // So there is no race condition in both the situations.
+      int read = 0;
+      int off = 0, len = arr.length;
+      Throwable exception = null;
+      try {
+        // try to fill the read ahead buffer.
+        // if a reader is waiting, possibly return early.
+        do {
+          read = underlyingInputStream.read(arr, off, len);
+          if (read <= 0) break;
+          off += read;
+          len -= read;
+        } while (len > 0 && !isWaiting.get());
+      } catch (Throwable ex) {
+        exception = ex;
+        if (ex instanceof Error) {
+          // `readException` may not be reported to the user. Rethrow Error to make sure at least
+          // The user can see Error in UncaughtExceptionHandler.
+          throw (Error) ex;
         }
+      } finally {
+        stateChangeLock.lock();
+        readAheadBuffer.limit(off);
+        if (read < 0 || (exception instanceof EOFException)) {
+          endOfStream = true;
+        } else if (exception != null) {
+          readAborted = true;
+          readException = exception;
+        }
+        readInProgress = false;
+        signalAsyncReadComplete();
+        stateChangeLock.unlock();
+        closeUnderlyingInputStreamIfNecessary();
       }
     });
   }
diff --git a/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java b/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java
index 115e1fbb79a2e..4bfd2d358f36f 100644
--- a/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java
+++ b/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java
@@ -83,10 +83,10 @@ public void spill() throws IOException {
   public abstract long spill(long size, MemoryConsumer trigger) throws IOException;
 
   /**
-   * Allocates a LongArray of `size`. Note that this method may throw `OutOfMemoryError` if Spark
-   * doesn't have enough memory for this allocation, or throw `TooLargePageException` if this
-   * `LongArray` is too large to fit in a single page. The caller side should take care of these
-   * two exceptions, or make sure the `size` is small enough that won't trigger exceptions.
+   * Allocates a LongArray of `size`. Note that this method may throw `SparkOutOfMemoryError`
+   * if Spark doesn't have enough memory for this allocation, or throw `TooLargePageException`
+   * if this `LongArray` is too large to fit in a single page. The caller side should take care of
+   * these two exceptions, or make sure the `size` is small enough that won't trigger exceptions.
    *
    * @throws SparkOutOfMemoryError
    * @throws TooLargePageException
@@ -111,7 +111,7 @@ public void freeArray(LongArray array) {
   /**
    * Allocate a memory block with at least `required` bytes.
    *
-   * @throws OutOfMemoryError
+   * @throws SparkOutOfMemoryError
    */
   protected MemoryBlock allocatePage(long required) {
     MemoryBlock page = taskMemoryManager.allocatePage(Math.max(pageSize, required), this);
@@ -154,7 +154,9 @@ private void throwOom(final MemoryBlock page, final long required) {
       taskMemoryManager.freePage(page, this);
     }
     taskMemoryManager.showMemoryUsage();
+    // checkstyle.off: RegexpSinglelineJava
     throw new SparkOutOfMemoryError(""Unable to acquire "" + required + "" bytes of memory, got "" +
       got);
+    // checkstyle.on: RegexpSinglelineJava
   }
 }
diff --git a/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java b/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java
index 8651a639c07f7..28b646ba3c951 100644
--- a/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java
+++ b/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java
@@ -194,8 +194,10 @@ public long acquireExecutionMemory(long required, MemoryConsumer consumer) {
             throw new RuntimeException(e.getMessage());
           } catch (IOException e) {
             logger.error(""error while calling spill() on "" + c, e);
+            // checkstyle.off: RegexpSinglelineJava
             throw new SparkOutOfMemoryError(""error while calling spill() on "" + c + "" : ""
               + e.getMessage());
+            // checkstyle.on: RegexpSinglelineJava
           }
         }
       }
@@ -215,8 +217,10 @@ public long acquireExecutionMemory(long required, MemoryConsumer consumer) {
           throw new RuntimeException(e.getMessage());
         } catch (IOException e) {
           logger.error(""error while calling spill() on "" + consumer, e);
+          // checkstyle.off: RegexpSinglelineJava
           throw new SparkOutOfMemoryError(""error while calling spill() on "" + consumer + "" : ""
             + e.getMessage());
+          // checkstyle.on: RegexpSinglelineJava
         }
       }
 
@@ -311,7 +315,7 @@ public MemoryBlock allocatePage(long size, MemoryConsumer consumer) {
       // this could trigger spilling to free some pages.
       return allocatePage(size, consumer);
     }
-    page.setPageNumber(pageNumber);
+    page.pageNumber = pageNumber;
     pageTable[pageNumber] = page;
     if (logger.isTraceEnabled()) {
       logger.trace(""Allocate page number {} ({} bytes)"", pageNumber, acquired);
@@ -323,25 +327,25 @@ public MemoryBlock allocatePage(long size, MemoryConsumer consumer) {
    * Free a block of memory allocated via {@link TaskMemoryManager#allocatePage}.
    */
   public void freePage(MemoryBlock page, MemoryConsumer consumer) {
-    assert (page.getPageNumber() != MemoryBlock.NO_PAGE_NUMBER) :
+    assert (page.pageNumber != MemoryBlock.NO_PAGE_NUMBER) :
       ""Called freePage() on memory that wasn't allocated with allocatePage()"";
-    assert (page.getPageNumber() != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :
+    assert (page.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :
       ""Called freePage() on a memory block that has already been freed"";
-    assert (page.getPageNumber() != MemoryBlock.FREED_IN_TMM_PAGE_NUMBER) :
+    assert (page.pageNumber != MemoryBlock.FREED_IN_TMM_PAGE_NUMBER) :
             ""Called freePage() on a memory block that has already been freed"";
-    assert(allocatedPages.get(page.getPageNumber()));
-    pageTable[page.getPageNumber()] = null;
+    assert(allocatedPages.get(page.pageNumber));
+    pageTable[page.pageNumber] = null;
     synchronized (this) {
-      allocatedPages.clear(page.getPageNumber());
+      allocatedPages.clear(page.pageNumber);
     }
     if (logger.isTraceEnabled()) {
-      logger.trace(""Freed page number {} ({} bytes)"", page.getPageNumber(), page.size());
+      logger.trace(""Freed page number {} ({} bytes)"", page.pageNumber, page.size());
     }
     long pageSize = page.size();
     // Clear the page number before passing the block to the MemoryAllocator's free().
     // Doing this allows the MemoryAllocator to detect when a TaskMemoryManager-managed
     // page has been inappropriately directly freed without calling TMM.freePage().
-    page.setPageNumber(MemoryBlock.FREED_IN_TMM_PAGE_NUMBER);
+    page.pageNumber = MemoryBlock.FREED_IN_TMM_PAGE_NUMBER;
     memoryManager.tungstenMemoryAllocator().free(page);
     releaseExecutionMemory(pageSize, consumer);
   }
@@ -363,7 +367,7 @@ public long encodePageNumberAndOffset(MemoryBlock page, long offsetInPage) {
       // relative to the page's base offset; this relative offset will fit in 51 bits.
       offsetInPage -= page.getBaseOffset();
     }
-    return encodePageNumberAndOffset(page.getPageNumber(), offsetInPage);
+    return encodePageNumberAndOffset(page.pageNumber, offsetInPage);
   }
 
   @VisibleForTesting
@@ -434,7 +438,7 @@ public long cleanUpAllAllocatedMemory() {
       for (MemoryBlock page : pageTable) {
         if (page != null) {
           logger.debug(""unreleased page: "" + page + "" in task "" + taskAttemptId);
-          page.setPageNumber(MemoryBlock.FREED_IN_TMM_PAGE_NUMBER);
+          page.pageNumber = MemoryBlock.FREED_IN_TMM_PAGE_NUMBER;
           memoryManager.tungstenMemoryAllocator().free(page);
         }
       }
diff --git a/core/src/main/java/org/apache/spark/package-info.java b/core/src/main/java/org/apache/spark/package-info.java
index 4426c7afcebdd..a029931f9e4c0 100644
--- a/core/src/main/java/org/apache/spark/package-info.java
+++ b/core/src/main/java/org/apache/spark/package-info.java
@@ -16,8 +16,8 @@
  */
 
 /**
- * Core Spark classes in Scala. A few classes here, such as {@link org.apache.spark.Accumulator}
- * and {@link org.apache.spark.storage.StorageLevel}, are also used in Java, but the
+ * Core Spark classes in Scala. A few classes here, such as
+ * {@link org.apache.spark.storage.StorageLevel}, are also used in Java, but the
  * {@link org.apache.spark.api.java} package contains the main Java API.
  */
 package org.apache.spark;
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java b/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java
index e3bd5496cf5ba..fda33cd8293d5 100644
--- a/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java
@@ -37,12 +37,11 @@
 import org.apache.spark.Partitioner;
 import org.apache.spark.ShuffleDependency;
 import org.apache.spark.SparkConf;
-import org.apache.spark.TaskContext;
-import org.apache.spark.executor.ShuffleWriteMetrics;
 import org.apache.spark.scheduler.MapStatus;
 import org.apache.spark.scheduler.MapStatus$;
 import org.apache.spark.serializer.Serializer;
 import org.apache.spark.serializer.SerializerInstance;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;
 import org.apache.spark.shuffle.IndexShuffleBlockResolver;
 import org.apache.spark.shuffle.ShuffleWriter;
 import org.apache.spark.storage.*;
@@ -79,7 +78,7 @@
   private final int numPartitions;
   private final BlockManager blockManager;
   private final Partitioner partitioner;
-  private final ShuffleWriteMetrics writeMetrics;
+  private final ShuffleWriteMetricsReporter writeMetrics;
   private final int shuffleId;
   private final int mapId;
   private final Serializer serializer;
@@ -103,8 +102,8 @@
       IndexShuffleBlockResolver shuffleBlockResolver,
       BypassMergeSortShuffleHandle<K, V> handle,
       int mapId,
-      TaskContext taskContext,
-      SparkConf conf) {
+      SparkConf conf,
+      ShuffleWriteMetricsReporter writeMetrics) {
     // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided
     this.fileBufferSize = (int) conf.getSizeAsKb(""spark.shuffle.file.buffer"", ""32k"") * 1024;
     this.transferToEnabled = conf.getBoolean(""spark.file.transferTo"", true);
@@ -114,7 +113,7 @@
     this.shuffleId = dep.shuffleId();
     this.partitioner = dep.partitioner();
     this.numPartitions = partitioner.numPartitions();
-    this.writeMetrics = taskContext.taskMetrics().shuffleWriteMetrics();
+    this.writeMetrics = writeMetrics;
     this.serializer = dep.serializer();
     this.shuffleBlockResolver = shuffleBlockResolver;
   }
@@ -125,7 +124,7 @@ public void write(Iterator<Product2<K, V>> records) throws IOException {
     if (!records.hasNext()) {
       partitionLengths = new long[numPartitions];
       shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, null);
-      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths, 0);
+      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
       return;
     }
     final SerializerInstance serInstance = serializer.newInstance();
@@ -152,9 +151,9 @@ public void write(Iterator<Product2<K, V>> records) throws IOException {
     }
 
     for (int i = 0; i < numPartitions; i++) {
-      final DiskBlockObjectWriter writer = partitionWriters[i];
-      partitionWriterSegments[i] = writer.commitAndGet();
-      writer.close();
+      try (DiskBlockObjectWriter writer = partitionWriters[i]) {
+        partitionWriterSegments[i] = writer.commitAndGet();
+      }
     }
 
     File output = shuffleBlockResolver.getDataFile(shuffleId, mapId);
@@ -167,8 +166,7 @@ public void write(Iterator<Product2<K, V>> records) throws IOException {
         logger.error(""Error while deleting temp file {}"", tmp.getAbsolutePath());
       }
     }
-    mapStatus = MapStatus$.MODULE$.apply(
-      blockManager.shuffleServerId(), partitionLengths, writeMetrics.recordsWritten());
+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
   }
 
   @VisibleForTesting
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java
index c7d2db4217d96..6ee9d5f0eec3b 100644
--- a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java
@@ -38,6 +38,7 @@
 import org.apache.spark.memory.TooLargePageException;
 import org.apache.spark.serializer.DummySerializerInstance;
 import org.apache.spark.serializer.SerializerInstance;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;
 import org.apache.spark.storage.BlockManager;
 import org.apache.spark.storage.DiskBlockObjectWriter;
 import org.apache.spark.storage.FileSegment;
@@ -75,7 +76,7 @@
   private final TaskMemoryManager taskMemoryManager;
   private final BlockManager blockManager;
   private final TaskContext taskContext;
-  private final ShuffleWriteMetrics writeMetrics;
+  private final ShuffleWriteMetricsReporter writeMetrics;
 
   /**
    * Force this sorter to spill when there are this many elements in memory.
@@ -113,7 +114,7 @@
       int initialSize,
       int numPartitions,
       SparkConf conf,
-      ShuffleWriteMetrics writeMetrics) {
+      ShuffleWriteMetricsReporter writeMetrics) {
     super(memoryManager,
       (int) Math.min(PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES, memoryManager.pageSizeBytes()),
       memoryManager.getTungstenMemoryMode());
@@ -144,7 +145,7 @@
    */
   private void writeSortedFile(boolean isLastFile) {
 
-    final ShuffleWriteMetrics writeMetricsToUse;
+    final ShuffleWriteMetricsReporter writeMetricsToUse;
 
     if (isLastFile) {
       // We're writing the final non-spill file, so we _do_ want to count this as shuffle bytes.
@@ -181,42 +182,43 @@ private void writeSortedFile(boolean isLastFile) {
     // around this, we pass a dummy no-op serializer.
     final SerializerInstance ser = DummySerializerInstance.INSTANCE;
 
-    final DiskBlockObjectWriter writer =
-      blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);
-
     int currentPartition = -1;
-    final int uaoSize = UnsafeAlignedOffset.getUaoSize();
-    while (sortedRecords.hasNext()) {
-      sortedRecords.loadNext();
-      final int partition = sortedRecords.packedRecordPointer.getPartitionId();
-      assert (partition >= currentPartition);
-      if (partition != currentPartition) {
-        // Switch to the new partition
-        if (currentPartition != -1) {
-          final FileSegment fileSegment = writer.commitAndGet();
-          spillInfo.partitionLengths[currentPartition] = fileSegment.length();
+    final FileSegment committedSegment;
+    try (DiskBlockObjectWriter writer =
+        blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse)) {
+
+      final int uaoSize = UnsafeAlignedOffset.getUaoSize();
+      while (sortedRecords.hasNext()) {
+        sortedRecords.loadNext();
+        final int partition = sortedRecords.packedRecordPointer.getPartitionId();
+        assert (partition >= currentPartition);
+        if (partition != currentPartition) {
+          // Switch to the new partition
+          if (currentPartition != -1) {
+            final FileSegment fileSegment = writer.commitAndGet();
+            spillInfo.partitionLengths[currentPartition] = fileSegment.length();
+          }
+          currentPartition = partition;
         }
-        currentPartition = partition;
-      }
 
-      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();
-      final Object recordPage = taskMemoryManager.getPage(recordPointer);
-      final long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);
-      int dataRemaining = UnsafeAlignedOffset.getSize(recordPage, recordOffsetInPage);
-      long recordReadPosition = recordOffsetInPage + uaoSize; // skip over record length
-      while (dataRemaining > 0) {
-        final int toTransfer = Math.min(diskWriteBufferSize, dataRemaining);
-        Platform.copyMemory(
-          recordPage, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);
-        writer.write(writeBuffer, 0, toTransfer);
-        recordReadPosition += toTransfer;
-        dataRemaining -= toTransfer;
+        final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();
+        final Object recordPage = taskMemoryManager.getPage(recordPointer);
+        final long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);
+        int dataRemaining = UnsafeAlignedOffset.getSize(recordPage, recordOffsetInPage);
+        long recordReadPosition = recordOffsetInPage + uaoSize; // skip over record length
+        while (dataRemaining > 0) {
+          final int toTransfer = Math.min(diskWriteBufferSize, dataRemaining);
+          Platform.copyMemory(
+            recordPage, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);
+          writer.write(writeBuffer, 0, toTransfer);
+          recordReadPosition += toTransfer;
+          dataRemaining -= toTransfer;
+        }
+        writer.recordWritten();
       }
-      writer.recordWritten();
-    }
 
-    final FileSegment committedSegment = writer.commitAndGet();
-    writer.close();
+      committedSegment = writer.commitAndGet();
+    }
     // If `writeSortedFile()` was called from `closeAndGetSpills()` and no records were inserted,
     // then the file might be empty. Note that it might be better to avoid calling
     // writeSortedFile() in that case.
@@ -240,9 +242,14 @@ private void writeSortedFile(boolean isLastFile) {
       //
       // Note that we intentionally ignore the value of `writeMetricsToUse.shuffleWriteTime()`.
       // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.
-      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.
-      writeMetrics.incRecordsWritten(writeMetricsToUse.recordsWritten());
-      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.bytesWritten());
+      // SPARK-3577 tracks the spill time separately.
+
+      // This is guaranteed to be a ShuffleWriteMetrics based on the if check in the beginning
+      // of this method.
+      writeMetrics.incRecordsWritten(
+        ((ShuffleWriteMetrics)writeMetricsToUse).recordsWritten());
+      taskContext.taskMetrics().incDiskBytesSpilled(
+        ((ShuffleWriteMetrics)writeMetricsToUse).bytesWritten());
     }
   }
 
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java
index 4b48599ad311e..0d069125dc60e 100644
--- a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java
@@ -20,6 +20,7 @@
 import java.util.Comparator;
 
 import org.apache.spark.memory.MemoryConsumer;
+import org.apache.spark.unsafe.Platform;
 import org.apache.spark.unsafe.array.LongArray;
 import org.apache.spark.unsafe.memory.MemoryBlock;
 import org.apache.spark.util.collection.Sorter;
@@ -112,7 +113,13 @@ public void reset() {
 
   public void expandPointerArray(LongArray newArray) {
     assert(newArray.size() > array.size());
-    MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);
+    Platform.copyMemory(
+      array.getBaseObject(),
+      array.getBaseOffset(),
+      newArray.getBaseObject(),
+      newArray.getBaseOffset(),
+      pos * 8L
+    );
     consumer.freeArray(array);
     array = newArray;
     usableCapacity = getUsableCapacity();
@@ -181,7 +188,10 @@ public ShuffleSorterIterator getSortedIterator() {
         PackedRecordPointer.PARTITION_ID_START_BYTE_INDEX,
         PackedRecordPointer.PARTITION_ID_END_BYTE_INDEX, false, false);
     } else {
-      MemoryBlock unused = array.memoryBlock().subBlock(pos * 8L, (array.size() - pos) * 8L);
+      MemoryBlock unused = new MemoryBlock(
+        array.getBaseObject(),
+        array.getBaseOffset() + pos * 8L,
+        (array.size() - pos) * 8L);
       LongArray buffer = new LongArray(unused);
       Sorter<PackedRecordPointer, LongArray> sorter =
         new Sorter<>(new ShuffleSortDataFormat(buffer));
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java
index 254449e95443e..717bdd79d47ef 100644
--- a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java
@@ -17,8 +17,8 @@
 
 package org.apache.spark.shuffle.sort;
 
+import org.apache.spark.unsafe.Platform;
 import org.apache.spark.unsafe.array.LongArray;
-import org.apache.spark.unsafe.memory.MemoryBlock;
 import org.apache.spark.util.collection.SortDataFormat;
 
 final class ShuffleSortDataFormat extends SortDataFormat<PackedRecordPointer, LongArray> {
@@ -60,8 +60,13 @@ public void copyElement(LongArray src, int srcPos, LongArray dst, int dstPos) {
 
   @Override
   public void copyRange(LongArray src, int srcPos, LongArray dst, int dstPos, int length) {
-    MemoryBlock.copyMemory(src.memoryBlock(), srcPos * 8L,
-      dst.memoryBlock(),dstPos * 8L,length * 8L);
+    Platform.copyMemory(
+      src.getBaseObject(),
+      src.getBaseOffset() + srcPos * 8L,
+      dst.getBaseObject(),
+      dst.getBaseOffset() + dstPos * 8L,
+      length * 8L
+    );
   }
 
   @Override
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java b/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java
index 069e6d5f224d7..4b0c74341551e 100644
--- a/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java
@@ -37,7 +37,6 @@
 
 import org.apache.spark.*;
 import org.apache.spark.annotation.Private;
-import org.apache.spark.executor.ShuffleWriteMetrics;
 import org.apache.spark.io.CompressionCodec;
 import org.apache.spark.io.CompressionCodec$;
 import org.apache.spark.io.NioBufferedFileInputStream;
@@ -47,6 +46,7 @@
 import org.apache.spark.network.util.LimitedInputStream;
 import org.apache.spark.scheduler.MapStatus;
 import org.apache.spark.scheduler.MapStatus$;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;
 import org.apache.spark.serializer.SerializationStream;
 import org.apache.spark.serializer.SerializerInstance;
 import org.apache.spark.shuffle.IndexShuffleBlockResolver;
@@ -73,7 +73,7 @@
   private final TaskMemoryManager memoryManager;
   private final SerializerInstance serializer;
   private final Partitioner partitioner;
-  private final ShuffleWriteMetrics writeMetrics;
+  private final ShuffleWriteMetricsReporter writeMetrics;
   private final int shuffleId;
   private final int mapId;
   private final TaskContext taskContext;
@@ -122,7 +122,8 @@ public UnsafeShuffleWriter(
       SerializedShuffleHandle<K, V> handle,
       int mapId,
       TaskContext taskContext,
-      SparkConf sparkConf) throws IOException {
+      SparkConf sparkConf,
+      ShuffleWriteMetricsReporter writeMetrics) throws IOException {
     final int numPartitions = handle.dependency().partitioner().numPartitions();
     if (numPartitions > SortShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE()) {
       throw new IllegalArgumentException(
@@ -138,7 +139,7 @@ public UnsafeShuffleWriter(
     this.shuffleId = dep.shuffleId();
     this.serializer = dep.serializer().newInstance();
     this.partitioner = dep.partitioner();
-    this.writeMetrics = taskContext.taskMetrics().shuffleWriteMetrics();
+    this.writeMetrics = writeMetrics;
     this.taskContext = taskContext;
     this.sparkConf = sparkConf;
     this.transferToEnabled = sparkConf.getBoolean(""spark.file.transferTo"", true);
@@ -248,8 +249,7 @@ void closeAndWriteOutput() throws IOException {
         logger.error(""Error while deleting temp file {}"", tmp.getAbsolutePath());
       }
     }
-    mapStatus = MapStatus$.MODULE$.apply(
-      blockManager.shuffleServerId(), partitionLengths, writeMetrics.recordsWritten());
+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
   }
 
   @VisibleForTesting
diff --git a/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java b/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java
index 5d0555a8c28e1..fcba3b73445c9 100644
--- a/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java
+++ b/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java
@@ -21,7 +21,7 @@
 import java.io.OutputStream;
 
 import org.apache.spark.annotation.Private;
-import org.apache.spark.executor.ShuffleWriteMetrics;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;
 
 /**
  * Intercepts write calls and tracks total time spent writing in order to update shuffle write
@@ -30,10 +30,11 @@
 @Private
 public final class TimeTrackingOutputStream extends OutputStream {
 
-  private final ShuffleWriteMetrics writeMetrics;
+  private final ShuffleWriteMetricsReporter writeMetrics;
   private final OutputStream outputStream;
 
-  public TimeTrackingOutputStream(ShuffleWriteMetrics writeMetrics, OutputStream outputStream) {
+  public TimeTrackingOutputStream(
+      ShuffleWriteMetricsReporter writeMetrics, OutputStream outputStream) {
     this.writeMetrics = writeMetrics;
     this.outputStream = outputStream;
   }
diff --git a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
index 9b6cbab38cbcc..a4e88598f7607 100644
--- a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
+++ b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
@@ -31,6 +31,7 @@
 import org.apache.spark.SparkEnv;
 import org.apache.spark.executor.ShuffleWriteMetrics;
 import org.apache.spark.memory.MemoryConsumer;
+import org.apache.spark.memory.SparkOutOfMemoryError;
 import org.apache.spark.memory.TaskMemoryManager;
 import org.apache.spark.serializer.SerializerManager;
 import org.apache.spark.storage.BlockManager;
@@ -741,7 +742,7 @@ public boolean append(Object kbase, long koff, int klen, Object vbase, long voff
         if (numKeys >= growthThreshold && longArray.size() < MAX_CAPACITY) {
           try {
             growAndRehash();
-          } catch (OutOfMemoryError oom) {
+          } catch (SparkOutOfMemoryError oom) {
             canGrowArray = false;
           }
         }
@@ -757,7 +758,7 @@ public boolean append(Object kbase, long koff, int klen, Object vbase, long voff
   private boolean acquireNewPage(long required) {
     try {
       currentPage = allocatePage(required);
-    } catch (OutOfMemoryError e) {
+    } catch (SparkOutOfMemoryError e) {
       return false;
     }
     dataPages.add(currentPage);
diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java
index 399251b80e649..5056652a2420b 100644
--- a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java
@@ -544,7 +544,7 @@ public long spill() throws IOException {
           // is accessing the current record. We free this page in that caller's next loadNext()
           // call.
           for (MemoryBlock page : allocatedPages) {
-            if (!loaded || page.getPageNumber() !=
+            if (!loaded || page.pageNumber !=
                     ((UnsafeInMemorySorter.SortedIterator)upstream).getCurrentPageNumber()) {
               released += page.size();
               freePage(page);
diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java
index 717823ebbd320..1a9453a8b3e80 100644
--- a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java
@@ -26,6 +26,7 @@
 import org.apache.spark.memory.MemoryConsumer;
 import org.apache.spark.memory.SparkOutOfMemoryError;
 import org.apache.spark.memory.TaskMemoryManager;
+import org.apache.spark.unsafe.Platform;
 import org.apache.spark.unsafe.UnsafeAlignedOffset;
 import org.apache.spark.unsafe.array.LongArray;
 import org.apache.spark.unsafe.memory.MemoryBlock;
@@ -213,9 +214,16 @@ public boolean hasSpaceForAnotherRecord() {
 
   public void expandPointerArray(LongArray newArray) {
     if (newArray.size() < array.size()) {
+      // checkstyle.off: RegexpSinglelineJava
       throw new SparkOutOfMemoryError(""Not enough memory to grow pointer array"");
+      // checkstyle.on: RegexpSinglelineJava
     }
-    MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);
+    Platform.copyMemory(
+      array.getBaseObject(),
+      array.getBaseOffset(),
+      newArray.getBaseObject(),
+      newArray.getBaseOffset(),
+      pos * 8L);
     consumer.freeArray(array);
     array = newArray;
     usableCapacity = getUsableCapacity();
@@ -342,7 +350,10 @@ public UnsafeSorterIterator getSortedIterator() {
           array, nullBoundaryPos, (pos - nullBoundaryPos) / 2L, 0, 7,
           radixSortSupport.sortDescending(), radixSortSupport.sortSigned());
       } else {
-        MemoryBlock unused = array.memoryBlock().subBlock(pos * 8L, (array.size() - pos) * 8L);
+        MemoryBlock unused = new MemoryBlock(
+          array.getBaseObject(),
+          array.getBaseOffset() + pos * 8L,
+          (array.size() - pos) * 8L);
         LongArray buffer = new LongArray(unused);
         Sorter<RecordPointerAndKeyPrefix, LongArray> sorter =
           new Sorter<>(new UnsafeSortDataFormat(buffer));
diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java
index 9399024f01783..c1d71a23b1dbe 100644
--- a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java
@@ -42,7 +42,10 @@
 
   private final SparkConf conf = new SparkConf();
 
-  /** The buffer size to use when writing the sorted records to an on-disk file */
+  /**
+   * The buffer size to use when writing the sorted records to an on-disk file, and
+   * this space used by prefix + len + recordLength must be greater than 4 + 8 bytes.
+   */
   private final int diskWriteBufferSize =
     (int) (long) conf.get(package$.MODULE$.SHUFFLE_DISK_WRITE_BUFFER_SIZE());
 
diff --git a/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html b/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html
index 5c91304e49fd7..f2c17aef097a4 100644
--- a/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html
+++ b/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html
@@ -16,10 +16,10 @@
 -->
 
 <script id=""executors-summary-template"" type=""text/html"">
-  <h4 style=""clear: left; display: inline-block;"">Summary</h4>
+  <h4 class=""title-table"">Summary</h4>
   <div class=""container-fluid"">
     <div class=""container-fluid"">
-      <table id=""summary-execs-table"" class=""table table-striped compact"">
+      <table id=""summary-execs-table"" class=""table table-striped compact cell-border"">
         <thead>
         <th></th>
         <th>RDD Blocks</th>
@@ -64,10 +64,10 @@ <h4 style=""clear: left; display: inline-block;"">Summary</h4>
       </table>
     </div>
   </div>
-  <h4 style=""clear: left; display: inline-block;"">Executors</h4>
+  <h4 class=""title-table"">Executors</h4>
   <div class=""container-fluid"">
     <div class=""container-fluid"">
-      <table id=""active-executors-table"" class=""table table-striped compact"">
+      <table id=""active-executors-table"" class=""table table-striped compact cell-border"">
         <thead>
         <tr>
           <th>
diff --git a/core/src/main/resources/org/apache/spark/ui/static/executorspage.js b/core/src/main/resources/org/apache/spark/ui/static/executorspage.js
index 6717af3ac4daf..a48c02ae279ba 100644
--- a/core/src/main/resources/org/apache/spark/ui/static/executorspage.js
+++ b/core/src/main/resources/org/apache/spark/ui/static/executorspage.js
@@ -59,78 +59,6 @@ $(document).ajaxStart(function () {
     $.blockUI({message: '<h3>Loading Executors Page...</h3>'});
 });
 
-function createTemplateURI(appId) {
-    var words = document.baseURI.split('/');
-    var ind = words.indexOf(""proxy"");
-    if (ind > 0) {
-        var baseURI = words.slice(0, ind + 1).join('/') + '/' + appId + '/static/executorspage-template.html';
-        return baseURI;
-    }
-    ind = words.indexOf(""history"");
-    if(ind > 0) {
-        var baseURI = words.slice(0, ind).join('/') + '/static/executorspage-template.html';
-        return baseURI;
-    }
-    return location.origin + ""/static/executorspage-template.html"";
-}
-
-function getStandAloneppId(cb) {
-    var words = document.baseURI.split('/');
-    var ind = words.indexOf(""proxy"");
-    if (ind > 0) {
-        var appId = words[ind + 1];
-        cb(appId);
-        return;
-    }
-    ind = words.indexOf(""history"");
-    if (ind > 0) {
-        var appId = words[ind + 1];
-        cb(appId);
-        return;
-    }
-    //Looks like Web UI is running in standalone mode
-    //Let's get application-id using REST End Point
-    $.getJSON(location.origin + ""/api/v1/applications"", function(response, status, jqXHR) {
-        if (response && response.length > 0) {
-            var appId = response[0].id
-            cb(appId);
-            return;
-        }
-    });
-}
-
-function createRESTEndPoint(appId) {
-    var words = document.baseURI.split('/');
-    var ind = words.indexOf(""proxy"");
-    if (ind > 0) {
-        var appId = words[ind + 1];
-        var newBaseURI = words.slice(0, ind + 2).join('/');
-        return newBaseURI + ""/api/v1/applications/"" + appId + ""/allexecutors""
-    }
-    ind = words.indexOf(""history"");
-    if (ind > 0) {
-        var appId = words[ind + 1];
-        var attemptId = words[ind + 2];
-        var newBaseURI = words.slice(0, ind).join('/');
-        if (isNaN(attemptId)) {
-            return newBaseURI + ""/api/v1/applications/"" + appId + ""/allexecutors"";
-        } else {
-            return newBaseURI + ""/api/v1/applications/"" + appId + ""/"" + attemptId + ""/allexecutors"";
-        }
-    }
-    return location.origin + ""/api/v1/applications/"" + appId + ""/allexecutors"";
-}
-
-function formatLogsCells(execLogs, type) {
-    if (type !== 'display') return Object.keys(execLogs);
-    if (!execLogs) return;
-    var result = '';
-    $.each(execLogs, function (logName, logUrl) {
-        result += '<div><a href=' + logUrl + '>' + logName + '</a></div>'
-    });
-    return result;
-}
-
 function logsExist(execs) {
     return execs.some(function(exec) {
         return !($.isEmptyObject(exec[""executorLogs""]));
@@ -178,17 +106,13 @@ function totalDurationColor(totalGCTime, totalDuration) {
 }
 
 $(document).ready(function () {
-    $.extend($.fn.dataTable.defaults, {
-        stateSave: true,
-        lengthMenu: [[20, 40, 60, 100, -1], [20, 40, 60, 100, ""All""]],
-        pageLength: 20
-    });
+    setDataTableDefaults();
 
     executorsSummary = $(""#active-executors"");
 
-    getStandAloneppId(function (appId) {
+    getStandAloneAppId(function (appId) {
 
-        var endPoint = createRESTEndPoint(appId);
+        var endPoint = createRESTEndPointForExecutorsPage(appId);
         $.getJSON(endPoint, function (response, status, jqXHR) {
             var summary = [];
             var allExecCnt = 0;
@@ -408,7 +332,7 @@ $(document).ready(function () {
             };
 
             var data = {executors: response, ""execSummary"": [activeSummary, deadSummary, totalSummary]};
-            $.get(createTemplateURI(appId), function (template) {
+            $.get(createTemplateURI(appId, ""executorspage""), function (template) {
 
                 executorsSummary.append(Mustache.render($(template).filter(""#executors-summary-template"").html(), data));
                 var selector = ""#active-executors-table"";
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc.png
new file mode 100644
index 0000000000000..e1ba61a8055fc
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc.png differ
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc_disabled.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc_disabled.png
new file mode 100644
index 0000000000000..fb11dfe24a6c5
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc_disabled.png differ
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_both.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_both.png
new file mode 100644
index 0000000000000..af5bc7c5a10b9
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_both.png differ
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc.png
new file mode 100644
index 0000000000000..0e156deb5f61d
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc.png differ
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc_disabled.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc_disabled.png
new file mode 100644
index 0000000000000..c9fdd8a1502fd
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc_disabled.png differ
diff --git a/core/src/main/resources/org/apache/spark/ui/static/stagepage.js b/core/src/main/resources/org/apache/spark/ui/static/stagepage.js
new file mode 100644
index 0000000000000..4c83ec7e95ab1
--- /dev/null
+++ b/core/src/main/resources/org/apache/spark/ui/static/stagepage.js
@@ -0,0 +1,958 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+var shouldBlockUI = true;
+
+$(document).ajaxStop(function () {
+    if (shouldBlockUI) {
+        $.unblockUI();
+        shouldBlockUI = false;
+    }
+});
+
+$(document).ajaxStart(function () {
+    if (shouldBlockUI) {
+        $.blockUI({message: '<h3>Loading Stage Page...</h3>'});
+    }
+});
+
+$.extend( $.fn.dataTable.ext.type.order, {
+    ""duration-pre"": ConvertDurationString,
+
+    ""duration-asc"": function ( a, b ) {
+        a = ConvertDurationString( a );
+        b = ConvertDurationString( b );
+        return ((a < b) ? -1 : ((a > b) ? 1 : 0));
+    },
+
+    ""duration-desc"": function ( a, b ) {
+        a = ConvertDurationString( a );
+        b = ConvertDurationString( b );
+        return ((a < b) ? 1 : ((a > b) ? -1 : 0));
+    }
+} );
+
+// This function will only parse the URL under certain format
+// e.g. (history) https://domain:50509/history/application_1536254569791_3806251/1/stages/stage/?id=4&attempt=1
+// e.g. (proxy) https://domain:50505/proxy/application_1502220952225_59143/stages/stage?id=4&attempt=1
+function stageEndPoint(appId) {
+    var queryString = document.baseURI.split('?');
+    var words = document.baseURI.split('/');
+    var indexOfProxy = words.indexOf(""proxy"");
+    var stageId = queryString[1].split(""&"").filter(word => word.includes(""id=""))[0].split(""="")[1];
+    if (indexOfProxy > 0) {
+        var appId = words[indexOfProxy + 1];
+        var newBaseURI = words.slice(0, words.indexOf(""proxy"") + 2).join('/');
+        return newBaseURI + ""/api/v1/applications/"" + appId + ""/stages/"" + stageId;
+    }
+    var indexOfHistory = words.indexOf(""history"");
+    if (indexOfHistory > 0) {
+        var appId = words[indexOfHistory + 1];
+        var appAttemptId = words[indexOfHistory + 2];
+        var newBaseURI = words.slice(0, words.indexOf(""history"")).join('/');
+        if (isNaN(appAttemptId) || appAttemptId == ""0"") {
+            return newBaseURI + ""/api/v1/applications/"" + appId + ""/stages/"" + stageId;
+        } else {
+            return newBaseURI + ""/api/v1/applications/"" + appId + ""/"" + appAttemptId + ""/stages/"" + stageId;
+        }
+    }
+    return location.origin + ""/api/v1/applications/"" + appId + ""/stages/"" + stageId;
+}
+
+function getColumnNameForTaskMetricSummary(columnKey) {
+    switch(columnKey) {
+        case ""executorRunTime"":
+            return ""Duration"";
+
+        case ""jvmGcTime"":
+            return ""GC Time"";
+
+        case ""gettingResultTime"":
+            return ""Getting Result Time"";
+
+        case ""inputMetrics"":
+            return ""Input Size / Records"";
+
+        case ""outputMetrics"":
+            return ""Output Size / Records"";
+
+        case ""peakExecutionMemory"":
+            return ""Peak Execution Memory"";
+
+        case ""resultSerializationTime"":
+            return ""Result Serialization Time"";
+
+        case ""schedulerDelay"":
+            return ""Scheduler Delay"";
+
+        case ""diskBytesSpilled"":
+            return ""Shuffle spill (disk)"";
+
+        case ""memoryBytesSpilled"":
+            return ""Shuffle spill (memory)"";
+
+        case ""shuffleReadMetrics"":
+            return ""Shuffle Read Size / Records"";
+
+        case ""shuffleWriteMetrics"":
+            return ""Shuffle Write Size / Records"";
+
+        case ""executorDeserializeTime"":
+            return ""Task Deserialization Time"";
+
+        case ""shuffleReadBlockedTime"":
+            return ""Shuffle Read Blocked Time"";
+
+        case ""shuffleRemoteReads"":
+            return ""Shuffle Remote Reads"";
+
+        default:
+            return ""NA"";
+    }
+}
+
+function displayRowsForSummaryMetricsTable(row, type, columnIndex) {
+    switch(row.columnKey) {
+        case 'inputMetrics':
+            var str = formatBytes(row.data.bytesRead[columnIndex], type) + "" / "" +
+              row.data.recordsRead[columnIndex];
+            return str;
+            break;
+
+        case 'outputMetrics':
+            var str = formatBytes(row.data.bytesWritten[columnIndex], type) + "" / "" +
+              row.data.recordsWritten[columnIndex];
+            return str;
+            break;
+
+        case 'shuffleReadMetrics':
+            var str = formatBytes(row.data.readBytes[columnIndex], type) + "" / "" +
+              row.data.readRecords[columnIndex];
+            return str;
+            break;
+
+        case 'shuffleReadBlockedTime':
+            var str = formatDuration(row.data.fetchWaitTime[columnIndex]);
+            return str;
+            break;
+
+        case 'shuffleRemoteReads':
+            var str = formatBytes(row.data.remoteBytesRead[columnIndex], type);
+            return str;
+            break;
+
+        case 'shuffleWriteMetrics':
+            var str = formatBytes(row.data.writeBytes[columnIndex], type) + "" / "" +
+              row.data.writeRecords[columnIndex];
+            return str;
+            break;
+
+        default:
+            return (row.columnKey == 'peakExecutionMemory' || row.columnKey == 'memoryBytesSpilled'
+                    || row.columnKey == 'diskBytesSpilled') ? formatBytes(
+                    row.data[columnIndex], type) : (formatDuration(row.data[columnIndex]));
+
+    }
+}
+
+function createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTable) {
+    var taskMetricsTable = ""#summary-metrics-table"";
+    if ($.fn.dataTable.isDataTable(taskMetricsTable)) {
+        taskSummaryMetricsDataTable.clear().draw();
+        taskSummaryMetricsDataTable.rows.add(taskSummaryMetricsTable).draw();
+    } else {
+        var taskConf = {
+            ""data"": taskSummaryMetricsTable,
+            ""columns"": [
+                {data : 'metric'},
+                // Min
+                {
+                    data: function (row, type) {
+                        return displayRowsForSummaryMetricsTable(row, type, 0);
+                    }
+                },
+                // 25th percentile
+                {
+                    data: function (row, type) {
+                        return displayRowsForSummaryMetricsTable(row, type, 1);
+                    }
+                },
+                // Median
+                {
+                    data: function (row, type) {
+                        return displayRowsForSummaryMetricsTable(row, type, 2);
+                    }
+                },
+                // 75th percentile
+                {
+                    data: function (row, type) {
+                        return displayRowsForSummaryMetricsTable(row, type, 3);
+                    }
+                },
+                // Max
+                {
+                    data: function (row, type) {
+                        return displayRowsForSummaryMetricsTable(row, type, 4);
+                    }
+                }
+            ],
+            ""columnDefs"": [
+                { ""type"": ""duration"", ""targets"": 1 },
+                { ""type"": ""duration"", ""targets"": 2 },
+                { ""type"": ""duration"", ""targets"": 3 },
+                { ""type"": ""duration"", ""targets"": 4 },
+                { ""type"": ""duration"", ""targets"": 5 }
+            ],
+            ""paging"": false,
+            ""searching"": false,
+            ""order"": [[0, ""asc""]],
+            ""bSort"": false,
+            ""bAutoWidth"": false
+        };
+        taskSummaryMetricsDataTable = $(taskMetricsTable).DataTable(taskConf);
+    }
+    taskSummaryMetricsTableCurrentStateArray = taskSummaryMetricsTable.slice();
+}
+
+function createRowMetadataForColumn(colKey, data, checkboxId) {
+  var row = {
+      ""metric"": getColumnNameForTaskMetricSummary(colKey),
+      ""data"": data,
+      ""checkboxId"": checkboxId,
+      ""columnKey"": colKey
+  };
+  return row;
+}
+
+function reselectCheckboxesBasedOnTaskTableState() {
+    var allChecked = true;
+    var taskSummaryMetricsTableCurrentFilteredArray = taskSummaryMetricsTableCurrentStateArray.slice();
+    if (typeof taskTableSelector !== 'undefined' && taskSummaryMetricsTableCurrentStateArray.length > 0) {
+        for (var k = 0; k < optionalColumns.length; k++) {
+            if (taskTableSelector.column(optionalColumns[k]).visible()) {
+                $(""#box-""+optionalColumns[k]).prop('checked', true);
+                taskSummaryMetricsTableCurrentStateArray.push(taskSummaryMetricsTableArray.filter(row => (row.checkboxId).toString() == optionalColumns[k])[0]);
+                taskSummaryMetricsTableCurrentFilteredArray = taskSummaryMetricsTableCurrentStateArray.slice();
+            } else {
+                allChecked = false;
+            }
+        }
+        if (allChecked) {
+            $(""#box-0"").prop('checked', true);
+        }
+        createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableCurrentFilteredArray);
+    }
+}
+
+function getStageAttemptId() {
+  var words = document.baseURI.split('?');
+  var attemptIdStr = words[1].split('&')[1];
+  var digitsRegex = /[0-9]+/;
+  // We are using regex here to extract the stage attempt id as there might be certain url's with format
+  // like /proxy/application_1539986433979_27115/stages/stage/?id=0&attempt=0#tasksTitle
+  var stgAttemptId = words[1].split(""&"").filter(
+      word => word.includes(""attempt=""))[0].split(""="")[1].match(digitsRegex);
+  return stgAttemptId;
+}
+
+var taskSummaryMetricsTableArray = [];
+var taskSummaryMetricsTableCurrentStateArray = [];
+var taskSummaryMetricsDataTable;
+var optionalColumns = [11, 12, 13, 14, 15, 16, 17];
+var taskTableSelector;
+
+$(document).ready(function () {
+    setDataTableDefaults();
+
+    $(""#showAdditionalMetrics"").append(
+        ""<div><a id='additionalMetrics'>"" +
+        ""<span class='expand-input-rate-arrow arrow-closed' id='arrowtoggle1'></span>"" +
+        "" Show Additional Metrics"" +
+        ""</a></div>"" +
+        ""<div class='container-fluid container-fluid-div' id='toggle-metrics' hidden>"" +
+        ""<div><input type='checkbox' class='toggle-vis' id='box-0' data-column='0'> Select All</div>"" +
+        ""<div id='scheduler_delay' class='scheduler-delay-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-11' data-column='11'> Scheduler Delay</div>"" +
+        ""<div id='task_deserialization_time' class='task-deserialization-time-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-12' data-column='12'> Task Deserialization Time</div>"" +
+        ""<div id='shuffle_read_blocked_time' class='shuffle-read-blocked-time-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-13' data-column='13'> Shuffle Read Blocked Time</div>"" +
+        ""<div id='shuffle_remote_reads' class='shuffle-remote-reads-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-14' data-column='14'> Shuffle Remote Reads</div>"" +
+        ""<div id='result_serialization_time' class='result-serialization-time-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-15' data-column='15'> Result Serialization Time</div>"" +
+        ""<div id='getting_result_time' class='getting-result-time-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-16' data-column='16'> Getting Result Time</div>"" +
+        ""<div id='peak_execution_memory' class='peak-execution-memory-checkbox-div'><input type='checkbox' class='toggle-vis' id='box-17' data-column='17'> Peak Execution Memory</div>"" +
+        ""</div>"");
+
+    $('#scheduler_delay').attr(""data-toggle"", ""tooltip"")
+        .attr(""data-placement"", ""right"")
+        .attr(""title"", ""Scheduler delay includes time to ship the task from the scheduler to the executor, and time to send "" +
+            ""the task result from the executor to the scheduler. If scheduler delay is large, consider decreasing the size of tasks or decreasing the size of task results."");
+    $('#task_deserialization_time').attr(""data-toggle"", ""tooltip"")
+        .attr(""data-placement"", ""right"")
+        .attr(""title"", ""Time spent deserializing the task closure on the executor, including the time to read the broadcasted task."");
+    $('#shuffle_read_blocked_time').attr(""data-toggle"", ""tooltip"")
+        .attr(""data-placement"", ""right"")
+        .attr(""title"", ""Time that the task spent blocked waiting for shuffle data to be read from remote machines."");
+    $('#shuffle_remote_reads').attr(""data-toggle"", ""tooltip"")
+        .attr(""data-placement"", ""right"")
+        .attr(""title"", ""Total shuffle bytes read from remote executors. This is a subset of the shuffle read bytes; the remaining shuffle data is read locally. "");
+    $('#result_serialization_time').attr(""data-toggle"", ""tooltip"")
+            .attr(""data-placement"", ""right"")
+            .attr(""title"", ""Time spent serializing the task result on the executor before sending it back to the driver."");
+    $('#getting_result_time').attr(""data-toggle"", ""tooltip"")
+            .attr(""data-placement"", ""right"")
+            .attr(""title"", ""Time that the driver spends fetching task results from workers. If this is large, consider decreasing the amount of data returned from each task."");
+    $('#peak_execution_memory').attr(""data-toggle"", ""tooltip"")
+            .attr(""data-placement"", ""right"")
+            .attr(""title"", ""Execution memory refers to the memory used by internal data structures created during "" +
+                ""shuffles, aggregations and joins when Tungsten is enabled. The value of this accumulator "" +
+                ""should be approximately the sum of the peak sizes across all such data structures created "" +
+                ""in this task. For SQL jobs, this only tracks all unsafe operators, broadcast joins, and "" +
+                ""external sort."");
+    $('[data-toggle=""tooltip""]').tooltip();
+    tasksSummary = $(""#parent-container"");
+    getStandAloneAppId(function (appId) {
+
+        var endPoint = stageEndPoint(appId);
+        var stageAttemptId = getStageAttemptId();
+        $.getJSON(endPoint + ""/"" + stageAttemptId, function(response, status, jqXHR) {
+
+            var responseBody = response;
+            var dataToShow = {};
+            dataToShow.showInputData = responseBody.inputBytes > 0;
+            dataToShow.showOutputData = responseBody.outputBytes > 0;
+            dataToShow.showShuffleReadData = responseBody.shuffleReadBytes > 0;
+            dataToShow.showShuffleWriteData = responseBody.shuffleWriteBytes > 0;
+            dataToShow.showBytesSpilledData =
+                (responseBody.diskBytesSpilled > 0 || responseBody.memoryBytesSpilled > 0);
+
+            if (!dataToShow.showShuffleReadData) {
+                $('#shuffle_read_blocked_time').remove();
+                $('#shuffle_remote_reads').remove();
+                optionalColumns.splice(2, 2);
+            }
+
+            // prepare data for executor summary table
+            stageExecutorSummaryInfoKeys = Object.keys(responseBody.executorSummary);
+            $.getJSON(createRESTEndPointForExecutorsPage(appId),
+              function(executorSummaryResponse, status, jqXHR) {
+                var executorDetailsMap = {};
+                executorSummaryResponse.forEach(function (executorDetail) {
+                    executorDetailsMap[executorDetail.id] = executorDetail;
+                });
+
+                var executorSummaryTable = [];
+                stageExecutorSummaryInfoKeys.forEach(function (columnKeyIndex) {
+                    var executorSummary = responseBody.executorSummary[columnKeyIndex];
+                    var executorDetail = executorDetailsMap[columnKeyIndex.toString()];
+                    executorSummary.id = columnKeyIndex;
+                    executorSummary.executorLogs = {};
+                    executorSummary.hostPort = ""CANNOT FIND ADDRESS"";
+
+                    if (executorDetail) {
+                        if (executorDetail[""executorLogs""]) {
+                            responseBody.executorSummary[columnKeyIndex].executorLogs =
+                                executorDetail[""executorLogs""];
+                            }
+                        if (executorDetail[""hostPort""]) {
+                            responseBody.executorSummary[columnKeyIndex].hostPort =
+                                executorDetail[""hostPort""];
+                        }
+                    }
+                    executorSummaryTable.push(responseBody.executorSummary[columnKeyIndex]);
+                });
+                // building task aggregated metrics by executor table
+                var executorSummaryConf = {
+                    ""data"": executorSummaryTable,
+                    ""columns"": [
+                        {data : ""id""},
+                        {data : ""executorLogs"", render: formatLogsCells},
+                        {data : ""hostPort""},
+                        {
+                            data : function (row, type) {
+                                return type === 'display' ? formatDuration(row.taskTime) : row.taskTime;
+                            }
+                        },
+                        {
+                            data : function (row, type) {
+                                var totaltasks = row.succeededTasks + row.failedTasks + row.killedTasks;
+                                return type === 'display' ? totaltasks : totaltasks.toString();
+                            }
+                        },
+                        {data : ""failedTasks""},
+                        {data : ""killedTasks""},
+                        {data : ""succeededTasks""},
+                        {data : ""isBlacklistedForStage""},
+                        {
+                            data : function (row, type) {
+                                return row.inputRecords != 0 ? formatBytes(row.inputBytes, type) + "" / "" + row.inputRecords : """";
+                            }
+                        },
+                        {
+                            data : function (row, type) {
+                                return row.outputRecords != 0 ? formatBytes(row.outputBytes, type) + "" / "" + row.outputRecords : """";
+                            }
+                        },
+                        {
+                            data : function (row, type) {
+                                return row.shuffleReadRecords != 0 ? formatBytes(row.shuffleRead, type) + "" / "" + row.shuffleReadRecords : """";
+                            }
+                        },
+                        {
+                            data : function (row, type) {
+                                return row.shuffleWriteRecords != 0 ? formatBytes(row.shuffleWrite, type) + "" / "" + row.shuffleWriteRecords : """";
+                            }
+                        },
+                        {
+                            data : function (row, type) {
+                                return typeof row.memoryBytesSpilled != 'undefined' ? formatBytes(row.memoryBytesSpilled, type) : """";
+                            }
+                        },
+                        {
+                            data : function (row, type) {
+                                return typeof row.diskBytesSpilled != 'undefined' ? formatBytes(row.diskBytesSpilled, type) : """";
+                            }
+                        }
+                    ],
+                    ""order"": [[0, ""asc""]],
+                    ""bAutoWidth"": false
+                }
+                var executorSummaryTableSelector =
+                    $(""#summary-executor-table"").DataTable(executorSummaryConf);
+                $('#parent-container [data-toggle=""tooltip""]').tooltip();
+
+                executorSummaryTableSelector.column(9).visible(dataToShow.showInputData);
+                if (dataToShow.showInputData) {
+                    $('#executor-summary-input').attr(""data-toggle"", ""tooltip"")
+                        .attr(""data-placement"", ""top"")
+                        .attr(""title"", ""Bytes and records read from Hadoop or from Spark storage."");
+                    $('#executor-summary-input').tooltip(true);
+                }
+                executorSummaryTableSelector.column(10).visible(dataToShow.showOutputData);
+                if (dataToShow.showOutputData) {
+                    $('#executor-summary-output').attr(""data-toggle"", ""tooltip"")
+                        .attr(""data-placement"", ""top"")
+                        .attr(""title"", ""Bytes and records written to Hadoop."");
+                    $('#executor-summary-output').tooltip(true);
+                }
+                executorSummaryTableSelector.column(11).visible(dataToShow.showShuffleReadData);
+                if (dataToShow.showShuffleReadData) {
+                    $('#executor-summary-shuffle-read').attr(""data-toggle"", ""tooltip"")
+                        .attr(""data-placement"", ""top"")
+                        .attr(""title"", ""Total shuffle bytes and records read (includes both data read locally and data read from remote executors)."");
+                    $('#executor-summary-shuffle-read').tooltip(true);
+                }
+                executorSummaryTableSelector.column(12).visible(dataToShow.showShuffleWriteData);
+                if (dataToShow.showShuffleWriteData) {
+                    $('#executor-summary-shuffle-write').attr(""data-toggle"", ""tooltip"")
+                        .attr(""data-placement"", ""top"")
+                        .attr(""title"", ""Bytes and records written to disk in order to be read by a shuffle in a future stage."");
+                    $('#executor-summary-shuffle-write').tooltip(true);
+                }
+                executorSummaryTableSelector.column(13).visible(dataToShow.showBytesSpilledData);
+                executorSummaryTableSelector.column(14).visible(dataToShow.showBytesSpilledData);
+            });
+
+            // prepare data for accumulatorUpdates
+            var accumulatorTable = responseBody.accumulatorUpdates.filter(accumUpdate =>
+                !(accumUpdate.name).toString().includes(""internal.""));
+
+            // rendering the UI page
+            var data = {""executors"": response};
+            $.get(createTemplateURI(appId, ""stagespage""), function(template) {
+                tasksSummary.append(Mustache.render($(template).filter(""#stages-summary-template"").html(), data));
+
+                $(""#additionalMetrics"").click(function(){
+                    $(""#arrowtoggle1"").toggleClass(""arrow-open arrow-closed"");
+                    $(""#toggle-metrics"").toggle();
+                    if (window.localStorage) {
+                        window.localStorage.setItem(""arrowtoggle1class"", $(""#arrowtoggle1"").attr('class'));
+                    }
+                });
+
+                $(""#aggregatedMetrics"").click(function(){
+                    $(""#arrowtoggle2"").toggleClass(""arrow-open arrow-closed"");
+                    $(""#toggle-aggregatedMetrics"").toggle();
+                    if (window.localStorage) {
+                        window.localStorage.setItem(""arrowtoggle2class"", $(""#arrowtoggle2"").attr('class'));
+                    }
+                });
+
+                var quantiles = ""0,0.25,0.5,0.75,1.0"";
+                $.getJSON(endPoint + ""/"" + stageAttemptId + ""/taskSummary?quantiles="" + quantiles,
+                  function(taskMetricsResponse, status, jqXHR) {
+                    var taskMetricKeys = Object.keys(taskMetricsResponse);
+                    taskMetricKeys.forEach(function (columnKey) {
+                        switch(columnKey) {
+                            case ""shuffleReadMetrics"":
+                                var row1 = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 3);
+                                var row2 = createRowMetadataForColumn(
+                                    ""shuffleReadBlockedTime"", taskMetricsResponse[columnKey], 13);
+                                var row3 = createRowMetadataForColumn(
+                                    ""shuffleRemoteReads"", taskMetricsResponse[columnKey], 14);
+                                if (dataToShow.showShuffleReadData) {
+                                    taskSummaryMetricsTableArray.push(row1);
+                                    taskSummaryMetricsTableArray.push(row2);
+                                    taskSummaryMetricsTableArray.push(row3);
+                                }
+                                break;
+
+                            case ""schedulerDelay"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 11);
+                                taskSummaryMetricsTableArray.push(row);
+                                break;
+
+                            case ""executorDeserializeTime"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 12);
+                                taskSummaryMetricsTableArray.push(row);
+                                break;
+
+                            case ""resultSerializationTime"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 15);
+                                taskSummaryMetricsTableArray.push(row);
+                                break;
+
+                            case ""gettingResultTime"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 16);
+                                taskSummaryMetricsTableArray.push(row);
+                                break;
+
+                            case ""peakExecutionMemory"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 17);
+                                taskSummaryMetricsTableArray.push(row);
+                                break;
+
+                            case ""inputMetrics"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 1);
+                                if (dataToShow.showInputData) {
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
+                                break;
+
+                            case ""outputMetrics"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 2);
+                                if (dataToShow.showOutputData) {
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
+                                break;
+
+                            case ""shuffleWriteMetrics"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 4);
+                                if (dataToShow.showShuffleWriteData) {
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
+                                break;
+
+                            case ""diskBytesSpilled"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 5);
+                                if (dataToShow.showBytesSpilledData) {
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
+                                break;
+
+                            case ""memoryBytesSpilled"":
+                                var row = createRowMetadataForColumn(
+                                    columnKey, taskMetricsResponse[columnKey], 6);
+                                if (dataToShow.showBytesSpilledData) {
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
+                                break;
+
+                            default:
+                                if (getColumnNameForTaskMetricSummary(columnKey) != ""NA"") {
+                                    var row = createRowMetadataForColumn(
+                                        columnKey, taskMetricsResponse[columnKey], 0);
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
+                                break;
+                        }
+                    });
+                    var taskSummaryMetricsTableFilteredArray =
+                        taskSummaryMetricsTableArray.filter(row => row.checkboxId < 11);
+                    taskSummaryMetricsTableCurrentStateArray = taskSummaryMetricsTableFilteredArray.slice();
+                    reselectCheckboxesBasedOnTaskTableState();
+                });
+
+                // building accumulator update table
+                var accumulatorConf = {
+                    ""data"": accumulatorTable,
+                    ""columns"": [
+                        {data : ""id""},
+                        {data : ""name""},
+                        {data : ""value""}
+                    ],
+                    ""paging"": false,
+                    ""searching"": false,
+                    ""order"": [[0, ""asc""]],
+                    ""bAutoWidth"": false
+                }
+                $(""#accumulator-table"").DataTable(accumulatorConf);
+
+                // building tasks table that uses server side functionality
+                var totalTasksToShow = responseBody.numCompleteTasks + responseBody.numActiveTasks;
+                var taskTable = ""#active-tasks-table"";
+                var taskConf = {
+                    ""serverSide"": true,
+                    ""paging"": true,
+                    ""info"": true,
+                    ""processing"": true,
+                    ""lengthMenu"": [[20, 40, 60, 100, totalTasksToShow], [20, 40, 60, 100, ""All""]],
+                    ""orderMulti"": false,
+                    ""bAutoWidth"": false,
+                    ""ajax"": {
+                        ""url"": endPoint + ""/"" + stageAttemptId + ""/taskTable"",
+                        ""data"": function (data) {
+                            var columnIndexToSort = 0;
+                            var columnNameToSort = ""Index"";
+                            if (data.order[0].column && data.order[0].column != """") {
+                                columnIndexToSort = parseInt(data.order[0].column);
+                                columnNameToSort = data.columns[columnIndexToSort].name;
+                            }
+                            delete data.columns;
+                            data.numTasks = totalTasksToShow;
+                            data.columnIndexToSort = columnIndexToSort;
+                            data.columnNameToSort = columnNameToSort;
+                        },
+                        ""dataSrc"": function (jsons) {
+                            var jsonStr = JSON.stringify(jsons);
+                            var tasksToShow = JSON.parse(jsonStr);
+                            return tasksToShow.aaData;
+                        },
+                        ""error"": function (jqXHR, textStatus, errorThrown) {
+                            alert(""Unable to connect to the server. Looks like the Spark "" +
+                              ""application must have ended. Please Switch to the history UI."");
+                            $(""#active-tasks-table_processing"").css(""display"",""none"");
+                        }
+                    },
+                    ""columns"": [
+                        {data: function (row, type) {
+                            return type !== 'display' ? (isNaN(row.index) ? 0 : row.index ) : row.index;
+                            },
+                            name: ""Index""
+                        },
+                        {data : ""taskId"", name: ""ID""},
+                        {data : ""attempt"", name: ""Attempt""},
+                        {data : ""status"", name: ""Status""},
+                        {data : ""taskLocality"", name: ""Locality Level""},
+                        {data : ""executorId"", name: ""Executor ID""},
+                        {data : ""host"", name: ""Host""},
+                        {data : ""executorLogs"", name: ""Logs"", render: formatLogsCells},
+                        {data : ""launchTime"", name: ""Launch Time"", render: formatDate},
+                        {
+                            data : function (row, type) {
+                                if (row.duration) {
+                                    return type === 'display' ? formatDuration(row.duration) : row.duration;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Duration""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.jvmGcTime) {
+                                    return type === 'display' ? formatDuration(row.taskMetrics.jvmGcTime) : row.taskMetrics.jvmGcTime;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""GC Time""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.schedulerDelay) {
+                                    return type === 'display' ? formatDuration(row.schedulerDelay) : row.schedulerDelay;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Scheduler Delay""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.executorDeserializeTime) {
+                                    return type === 'display' ? formatDuration(row.taskMetrics.executorDeserializeTime) : row.taskMetrics.executorDeserializeTime;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Task Deserialization Time""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.shuffleReadMetrics) {
+                                    return type === 'display' ? formatDuration(row.taskMetrics.shuffleReadMetrics.fetchWaitTime) : row.taskMetrics.shuffleReadMetrics.fetchWaitTime;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Shuffle Read Blocked Time""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.shuffleReadMetrics) {
+                                    return type === 'display' ? formatBytes(row.taskMetrics.shuffleReadMetrics.remoteBytesRead, type) : row.taskMetrics.shuffleReadMetrics.remoteBytesRead;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Shuffle Remote Reads""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.resultSerializationTime) {
+                                    return type === 'display' ? formatDuration(row.taskMetrics.resultSerializationTime) : row.taskMetrics.resultSerializationTime;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Result Serialization Time""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.gettingResultTime) {
+                                    return type === 'display' ? formatDuration(row.gettingResultTime) : row.gettingResultTime;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Getting Result Time""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.peakExecutionMemory) {
+                                    return type === 'display' ? formatBytes(row.taskMetrics.peakExecutionMemory, type) : row.taskMetrics.peakExecutionMemory;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Peak Execution Memory""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (accumulatorTable.length > 0 && row.accumulatorUpdates.length > 0) {
+                                    var accIndex = row.accumulatorUpdates.length - 1;
+                                    return row.accumulatorUpdates[accIndex].name + ' : ' + row.accumulatorUpdates[accIndex].update;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Accumulators""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.inputMetrics && row.taskMetrics.inputMetrics.bytesRead > 0) {
+                                    if (type === 'display') {
+                                        return formatBytes(row.taskMetrics.inputMetrics.bytesRead, type) + "" / "" + row.taskMetrics.inputMetrics.recordsRead;
+                                    } else {
+                                        return row.taskMetrics.inputMetrics.bytesRead + "" / "" + row.taskMetrics.inputMetrics.recordsRead;
+                                    }
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Input Size / Records""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.outputMetrics && row.taskMetrics.outputMetrics.bytesWritten > 0) {
+                                    if (type === 'display') {
+                                        return formatBytes(row.taskMetrics.outputMetrics.bytesWritten, type) + "" / "" + row.taskMetrics.outputMetrics.recordsWritten;
+                                    } else {
+                                        return row.taskMetrics.outputMetrics.bytesWritten + "" / "" + row.taskMetrics.outputMetrics.recordsWritten;
+                                    }
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Output Size / Records""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.shuffleWriteMetrics && row.taskMetrics.shuffleWriteMetrics.writeTime > 0) {
+                                    return type === 'display' ? formatDuration(parseInt(row.taskMetrics.shuffleWriteMetrics.writeTime) / 1000000) : row.taskMetrics.shuffleWriteMetrics.writeTime;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Write Time""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.shuffleWriteMetrics && row.taskMetrics.shuffleWriteMetrics.bytesWritten > 0) {
+                                    if (type === 'display') {
+                                        return formatBytes(row.taskMetrics.shuffleWriteMetrics.bytesWritten, type) + "" / "" + row.taskMetrics.shuffleWriteMetrics.recordsWritten;
+                                    } else {
+                                        return row.taskMetrics.shuffleWriteMetrics.bytesWritten + "" / "" + row.taskMetrics.shuffleWriteMetrics.recordsWritten;
+                                    }
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Shuffle Write Size / Records""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.shuffleReadMetrics && row.taskMetrics.shuffleReadMetrics.localBytesRead > 0) {
+                                    var totalBytesRead = parseInt(row.taskMetrics.shuffleReadMetrics.localBytesRead) + parseInt(row.taskMetrics.shuffleReadMetrics.remoteBytesRead);
+                                    if (type === 'display') {
+                                        return formatBytes(totalBytesRead, type) + "" / "" + row.taskMetrics.shuffleReadMetrics.recordsRead;
+                                    } else {
+                                        return totalBytesRead + "" / "" + row.taskMetrics.shuffleReadMetrics.recordsRead;
+                                    }
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Shuffle Read Size / Records""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.memoryBytesSpilled && row.taskMetrics.memoryBytesSpilled > 0) {
+                                    return type === 'display' ? formatBytes(row.taskMetrics.memoryBytesSpilled, type) : row.taskMetrics.memoryBytesSpilled;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Shuffle Spill (Memory)""
+                        },
+                        {
+                            data : function (row, type) {
+                                if (row.taskMetrics && row.taskMetrics.diskBytesSpilled && row.taskMetrics.diskBytesSpilled > 0) {
+                                    return type === 'display' ? formatBytes(row.taskMetrics.diskBytesSpilled, type) : row.taskMetrics.diskBytesSpilled;
+                                } else {
+                                    return """";
+                                }
+                            },
+                            name: ""Shuffle Spill (Disk)""
+                        },
+                        {
+                            data : function (row, type) {
+                                var msg = row.errorMessage;
+                                if (typeof msg === 'undefined') {
+                                    return """";
+                                } else {
+                                    var formHead = msg.substring(0, msg.indexOf(""at""));
+                                    var form = ""<span onclick=\""this.parentNode.querySelector('.stacktrace-details').classList.toggle('collapsed')\"" class=\""expand-details\"">+details</span>"";
+                                    var formMsg = ""<div class=\""stacktrace-details collapsed\""><pre>"" + row.errorMessage + ""</pre></div>"";
+                                    return formHead + form + formMsg;
+                                }
+                            },
+                            name: ""Errors""
+                        }
+                    ],
+                    ""columnDefs"": [
+                        { ""visible"": false, ""targets"": 11 },
+                        { ""visible"": false, ""targets"": 12 },
+                        { ""visible"": false, ""targets"": 13 },
+                        { ""visible"": false, ""targets"": 14 },
+                        { ""visible"": false, ""targets"": 15 },
+                        { ""visible"": false, ""targets"": 16 },
+                        { ""visible"": false, ""targets"": 17 },
+                        { ""visible"": false, ""targets"": 18 }
+                    ],
+                };
+                taskTableSelector = $(taskTable).DataTable(taskConf);
+                $('#active-tasks-table_filter input').unbind();
+                var searchEvent;
+                $('#active-tasks-table_filter input').bind('keyup', function(e) {
+                  if (typeof searchEvent !== 'undefined') {
+                    window.clearTimeout(searchEvent);
+                  }
+                  var value = this.value;
+                  searchEvent = window.setTimeout(function(){
+                    taskTableSelector.search( value ).draw();}, 500);
+                });
+                reselectCheckboxesBasedOnTaskTableState();
+
+                // hide or show columns dynamically event
+                $('input.toggle-vis').on('click', function(e){
+                    // Get the column
+                    var para = $(this).attr('data-column');
+                    if (para == ""0"") {
+                        var column = taskTableSelector.column(optionalColumns);
+                        if ($(this).is("":checked"")) {
+                            $("".toggle-vis"").prop('checked', true);
+                            column.visible(true);
+                            createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableArray);
+                        } else {
+                            $("".toggle-vis"").prop('checked', false);
+                            column.visible(false);
+                            var taskSummaryMetricsTableFilteredArray =
+                                taskSummaryMetricsTableArray.filter(row => row.checkboxId < 11);
+                            createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableFilteredArray);
+                        }
+                    } else {
+                        var column = taskTableSelector.column(para);
+                        // Toggle the visibility
+                        column.visible(!column.visible());
+                        var taskSummaryMetricsTableFilteredArray = [];
+                        if ($(this).is("":checked"")) {
+                            taskSummaryMetricsTableCurrentStateArray.push(taskSummaryMetricsTableArray.filter(row => (row.checkboxId).toString() == para)[0]);
+                            taskSummaryMetricsTableFilteredArray = taskSummaryMetricsTableCurrentStateArray.slice();
+                        } else {
+                            taskSummaryMetricsTableFilteredArray =
+                                taskSummaryMetricsTableCurrentStateArray.filter(row => (row.checkboxId).toString() != para);
+                        }
+                        createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableFilteredArray);
+                    }
+                });
+
+                // title number and toggle list
+                $(""#summaryMetricsTitle"").html(""Summary Metrics for "" + ""<a href='#tasksTitle'>"" + responseBody.numCompleteTasks + "" Completed Tasks"" + ""</a>"");
+                $(""#tasksTitle"").html(""Task ("" + totalTasksToShow + "")"");
+
+                // hide or show the accumulate update table
+                if (accumulatorTable.length == 0) {
+                    $(""#accumulator-update-table"").hide();
+                } else {
+                    taskTableSelector.column(18).visible(true);
+                    $(""#accumulator-update-table"").show();
+                }
+                // Showing relevant stage data depending on stage type for task table and executor
+                // summary table
+                taskTableSelector.column(19).visible(dataToShow.showInputData);
+                taskTableSelector.column(20).visible(dataToShow.showOutputData);
+                taskTableSelector.column(21).visible(dataToShow.showShuffleWriteData);
+                taskTableSelector.column(22).visible(dataToShow.showShuffleWriteData);
+                taskTableSelector.column(23).visible(dataToShow.showShuffleReadData);
+                taskTableSelector.column(24).visible(dataToShow.showBytesSpilledData);
+                taskTableSelector.column(25).visible(dataToShow.showBytesSpilledData);
+
+                if (window.localStorage) {
+                    if (window.localStorage.getItem(""arrowtoggle1class"") !== null &&
+                        window.localStorage.getItem(""arrowtoggle1class"").includes(""arrow-open"")) {
+                        $(""#arrowtoggle1"").toggleClass(""arrow-open arrow-closed"");
+                        $(""#toggle-metrics"").toggle();
+                    }
+                    if (window.localStorage.getItem(""arrowtoggle2class"") !== null &&
+                        window.localStorage.getItem(""arrowtoggle2class"").includes(""arrow-open"")) {
+                        $(""#arrowtoggle2"").toggleClass(""arrow-open arrow-closed"");
+                        $(""#toggle-aggregatedMetrics"").toggle();
+                    }
+                }
+            });
+        });
+    });
+});
diff --git a/core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html b/core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html
new file mode 100644
index 0000000000000..6f950c61b2d63
--- /dev/null
+++ b/core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html
@@ -0,0 +1,124 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to You under the Apache License, Version 2.0
+(the ""License""); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+
+<script id=""stages-summary-template"" type=""text/html"">
+    <h4 id=""summaryMetricsTitle"" class=""title-table""></h4>
+    <div class=""container-fluid"">
+        <div class=""container-fluid"">
+            <table id=""summary-metrics-table"" class=""table table-striped compact table-dataTable cell-border"">
+                <thead>
+                <th>Metric</th>
+                <th>Min</th>
+                <th>25th percentile</th>
+                <th>Median</th>
+                <th>75th percentile</th>
+                <th>Max</th>
+                </thead>
+                <tbody>
+                </tbody>
+            </table>
+        </div>
+    </div>
+    <a id=""aggregatedMetrics"">
+        <span class=""expand-input-rate-arrow arrow-closed"" id=""arrowtoggle2""></span>
+        <h4 class=""title-table"">Aggregated Metrics by Executor</h4>
+    </a>
+    <br>
+    <div class=""container-fluid"" id=""toggle-aggregatedMetrics"" hidden>
+        <div class=""container-fluid"">
+            <table id=""summary-executor-table"" class=""table table-striped compact table-dataTable cell-border"">
+                <thead>
+                <th>Executor ID</th>
+                <th>Logs</th>
+                <th>Address</th>
+                <th>Task Time</th>
+                <th>Total Tasks</th>
+                <th>Failed Tasks</th>
+                <th>Killed Tasks</th>
+                <th>Succeeded Tasks</th>
+                <th>
+          <span data-toggle=""tooltip"" data-placement=""top""
+                title=""Shows if this executor has been blacklisted by the scheduler due to task failures."">
+            Blacklisted</span>
+                </th>
+                <th><span id=""executor-summary-input"">Input Size / Records</span></th>
+                <th><span id=""executor-summary-output"">Output Size / Records</span></th>
+                <th><span id=""executor-summary-shuffle-read"">Shuffle Read Size / Records</span></th>
+                <th><span id=""executor-summary-shuffle-write"">Shuffle Write Size / Records</span></th>
+                <th>Shuffle Spill (Memory) </th>
+                <th>Shuffle Spill (Disk) </th>
+                </thead>
+                <tbody>
+                </tbody>
+            </table>
+        </div>
+    </div>
+    <div class=""container-fluid"" id=""accumulator-update-table"">
+        <h4 class=""title-table"">Accumulators</h4>
+        <div class=""container-fluid"">
+            <table id=""accumulator-table"" class=""table table-striped compact table-dataTable cell-border"">
+                <thead>
+                <th>ID</th>
+                <th>Name</th>
+                <th>Value</th>
+                </thead>
+                <tbody>
+                </tbody>
+            </table>
+        </div>
+    </div>
+    <h4 id=""tasksTitle"" class=""title-table""></h4>
+    <div class=""container-fluid"">
+        <div class=""container-fluid"">
+            <table id=""active-tasks-table"" class=""table table-striped compact table-dataTable cell-border"">
+                <thead>
+                <tr>
+                    <th>Index</th>
+                    <th>Task ID</th>
+                    <th>Attempt</th>
+                    <th>Status</th>
+                    <th>Locality level</th>
+                    <th>Executor ID</th>
+                    <th>Host</th>
+                    <th>Logs</th>
+                    <th>Launch Time</th>
+                    <th>Duration</th>
+                    <th>GC Time</th>
+                    <th>Scheduler Delay</th>
+                    <th>Task Deserialization Time</th>
+                    <th>Shuffle Read Blocked Time</th>
+                    <th>Shuffle Remote Reads</th>
+                    <th>Result Serialization Time</th>
+                    <th>Getting Result Time</th>
+                    <th>Peak Execution Memory</th>
+                    <th>Accumulators</th>
+                    <th>Input Size / Records</th>
+                    <th>Output Size / Records</th>
+                    <th>Write Time</th>
+                    <th>Shuffle Write Size / Records</th>
+                    <th>Shuffle Read Size / Records</th>
+                    <th>Shuffle Spill (Memory)</th>
+                    <th>Shuffle Spill (Disk)</th>
+                    <th>Errors</th>
+                </tr>
+                </thead>
+                <tbody>
+                </tbody>
+            </table>
+        </div>
+    </div>
+</script>
diff --git a/core/src/main/resources/org/apache/spark/ui/static/utils.js b/core/src/main/resources/org/apache/spark/ui/static/utils.js
index 4f63f6413d6de..deeafad4eb5f5 100644
--- a/core/src/main/resources/org/apache/spark/ui/static/utils.js
+++ b/core/src/main/resources/org/apache/spark/ui/static/utils.js
@@ -18,7 +18,7 @@
 // this function works exactly the same as UIUtils.formatDuration
 function formatDuration(milliseconds) {
     if (milliseconds < 100) {
-        return milliseconds + "" ms"";
+        return parseInt(milliseconds).toFixed(1) + "" ms"";
     }
     var seconds = milliseconds * 1.0 / 1000;
     if (seconds < 1) {
@@ -74,3 +74,114 @@ function getTimeZone() {
     return new Date().toString().match(/\((.*)\)/)[1];
   }
 }
+
+function formatLogsCells(execLogs, type) {
+  if (type !== 'display') return Object.keys(execLogs);
+  if (!execLogs) return;
+  var result = '';
+  $.each(execLogs, function (logName, logUrl) {
+    result += '<div><a href=' + logUrl + '>' + logName + '</a></div>'
+  });
+  return result;
+}
+
+function getStandAloneAppId(cb) {
+  var words = document.baseURI.split('/');
+  var ind = words.indexOf(""proxy"");
+  if (ind > 0) {
+    var appId = words[ind + 1];
+    cb(appId);
+    return;
+  }
+  ind = words.indexOf(""history"");
+  if (ind > 0) {
+    var appId = words[ind + 1];
+    cb(appId);
+    return;
+  }
+  // Looks like Web UI is running in standalone mode
+  // Let's get application-id using REST End Point
+  $.getJSON(location.origin + ""/api/v1/applications"", function(response, status, jqXHR) {
+    if (response && response.length > 0) {
+      var appId = response[0].id;
+      cb(appId);
+      return;
+    }
+  });
+}
+
+// This function is a helper function for sorting in datatable.
+// When the data is in duration (e.g. 12ms 2s 2min 2h )
+// It will convert the string into integer for correct ordering
+function ConvertDurationString(data) {
+  data = data.toString();
+  var units = data.replace(/[\d\.]/g, '' )
+                  .replace(' ', '')
+                  .toLowerCase();
+  var multiplier = 1;
+
+  switch(units) {
+    case 's':
+      multiplier = 1000;
+      break;
+    case 'min':
+      multiplier = 600000;
+      break;
+    case 'h':
+      multiplier = 3600000;
+      break;
+    default:
+      break;
+  }
+  return parseFloat(data) * multiplier;
+}
+
+function createTemplateURI(appId, templateName) {
+  var words = document.baseURI.split('/');
+  var ind = words.indexOf(""proxy"");
+  if (ind > 0) {
+    var baseURI = words.slice(0, ind + 1).join('/') + '/' + appId + '/static/' + templateName + '-template.html';
+    return baseURI;
+  }
+  ind = words.indexOf(""history"");
+  if(ind > 0) {
+    var baseURI = words.slice(0, ind).join('/') + '/static/' + templateName + '-template.html';
+    return baseURI;
+  }
+  return location.origin + ""/static/"" + templateName + ""-template.html"";
+}
+
+function setDataTableDefaults() {
+  $.extend($.fn.dataTable.defaults, {
+    stateSave: true,
+    lengthMenu: [[20, 40, 60, 100, -1], [20, 40, 60, 100, ""All""]],
+    pageLength: 20
+  });
+}
+
+function formatDate(date) {
+  if (date <= 0) return ""-"";
+  else return date.split(""."")[0].replace(""T"", "" "");
+}
+
+function createRESTEndPointForExecutorsPage(appId) {
+    var words = document.baseURI.split('/');
+    var ind = words.indexOf(""proxy"");
+    if (ind > 0) {
+        var appId = words[ind + 1];
+        var newBaseURI = words.slice(0, ind + 2).join('/');
+        return newBaseURI + ""/api/v1/applications/"" + appId + ""/allexecutors""
+    }
+    ind = words.indexOf(""history"");
+    if (ind > 0) {
+        var appId = words[ind + 1];
+        var attemptId = words[ind + 2];
+        var newBaseURI = words.slice(0, ind).join('/');
+        if (isNaN(attemptId)) {
+            return newBaseURI + ""/api/v1/applications/"" + appId + ""/allexecutors"";
+        } else {
+            return newBaseURI + ""/api/v1/applications/"" + appId + ""/"" + attemptId + ""/allexecutors"";
+        }
+    }
+    return location.origin + ""/api/v1/applications/"" + appId + ""/allexecutors"";
+}
diff --git a/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/package.scala b/core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css
similarity index 79%
rename from external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/package.scala
rename to core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css
index 47c5187f8751f..f6b4abed21e0d 100644
--- a/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/package.scala
+++ b/core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css
@@ -15,9 +15,6 @@
  * limitations under the License.
  */
 
-package org.apache.spark.streaming
+table.dataTable thead .sorting_asc { background: url('images/sort_asc.png') no-repeat bottom right; }
 
-/**
- * Kafka receiver for spark streaming,
- */
-package object kafka
+table.dataTable thead .sorting_desc { background: url('images/sort_desc.png') no-repeat bottom right; }
\ No newline at end of file
diff --git a/core/src/main/resources/org/apache/spark/ui/static/webui.css b/core/src/main/resources/org/apache/spark/ui/static/webui.css
index 935d9b1aec615..fe5bb25687af1 100644
--- a/core/src/main/resources/org/apache/spark/ui/static/webui.css
+++ b/core/src/main/resources/org/apache/spark/ui/static/webui.css
@@ -88,6 +88,10 @@ a.kill-link {
   float: right;
 }
 
+a.name-link {
+  word-wrap: break-word;
+}
+
 span.expand-details {
   font-size: 10pt;
   cursor: pointer;
@@ -251,4 +255,110 @@ a.expandbutton {
 
 .table-cell-width-limited td {
   max-width: 600px;
+}
+
+.paginate_button.active > a {
+    color: #999999;
+    text-decoration: underline;
+}
+
+.title-table {
+  clear: left;
+  display: inline-block;
+}
+
+.table-dataTable {
+  width: 100%;
+}
+
+.container-fluid-div {
+  width: 200px;
+}
+
+.scheduler-delay-checkbox-div {
+  width: 120px;
+}
+
+.task-deserialization-time-checkbox-div {
+  width: 175px;
+}
+
+.shuffle-read-blocked-time-checkbox-div {
+  width: 187px;
+}
+
+.shuffle-remote-reads-checkbox-div {
+  width: 157px;
+}
+
+.result-serialization-time-checkbox-div {
+  width: 171px;
+}
+
+.getting-result-time-checkbox-div {
+  width: 141px;
+}
+
+.peak-execution-memory-checkbox-div {
+  width: 170px;
+}
+
+#active-tasks-table th {
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
+
+#active-tasks-table th:first-child {
+  border-left: 1px solid #dddddd;
+}
+
+#accumulator-table th {
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
+
+#accumulator-table th:first-child {
+  border-left: 1px solid #dddddd;
+}
+
+#summary-executor-table th {
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
+
+#summary-executor-table th:first-child {
+  border-left: 1px solid #dddddd;
+}
+
+#summary-metrics-table th {
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
+
+#summary-metrics-table th:first-child {
+  border-left: 1px solid #dddddd;
+}
+
+#summary-execs-table th {
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
+
+#summary-execs-table th:first-child {
+  border-left: 1px solid #dddddd;
+}
+
+#active-executors-table th {
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
+
+#active-executors-table th:first-child {
+  border-left: 1px solid #dddddd;
 }
\ No newline at end of file
diff --git a/core/src/main/resources/org/apache/spark/ui/static/webui.js b/core/src/main/resources/org/apache/spark/ui/static/webui.js
index f01c567ba58ad..b1254e08fa504 100644
--- a/core/src/main/resources/org/apache/spark/ui/static/webui.js
+++ b/core/src/main/resources/org/apache/spark/ui/static/webui.js
@@ -83,4 +83,7 @@ $(function() {
   collapseTablePageLoad('collapse-aggregated-rdds','aggregated-rdds');
   collapseTablePageLoad('collapse-aggregated-activeBatches','aggregated-activeBatches');
   collapseTablePageLoad('collapse-aggregated-completedBatches','aggregated-completedBatches');
+  collapseTablePageLoad('collapse-aggregated-runningExecutions','aggregated-runningExecutions');
+  collapseTablePageLoad('collapse-aggregated-completedExecutions','aggregated-completedExecutions');
+  collapseTablePageLoad('collapse-aggregated-failedExecutions','aggregated-failedExecutions');
 });
\ No newline at end of file
diff --git a/core/src/main/scala/org/apache/spark/Accumulable.scala b/core/src/main/scala/org/apache/spark/Accumulable.scala
deleted file mode 100644
index 3092074232d18..0000000000000
--- a/core/src/main/scala/org/apache/spark/Accumulable.scala
+++ /dev/null
@@ -1,226 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark
-
-import java.io.Serializable
-
-import scala.collection.generic.Growable
-import scala.reflect.ClassTag
-
-import org.apache.spark.scheduler.AccumulableInfo
-import org.apache.spark.serializer.JavaSerializer
-import org.apache.spark.util.{AccumulatorContext, AccumulatorMetadata, LegacyAccumulatorWrapper}
-
-
-/**
- * A data type that can be accumulated, i.e. has a commutative and associative ""add"" operation,
- * but where the result type, `R`, may be different from the element type being added, `T`.
- *
- * You must define how to add data, and how to merge two of these together.  For some data types,
- * such as a counter, these might be the same operation. In that case, you can use the simpler
- * [[org.apache.spark.Accumulator]]. They won't always be the same, though -- e.g., imagine you are
- * accumulating a set. You will add items to the set, and you will union two sets together.
- *
- * Operations are not thread-safe.
- *
- * @param id ID of this accumulator; for internal use only.
- * @param initialValue initial value of accumulator
- * @param param helper object defining how to add elements of type `R` and `T`
- * @param name human-readable name for use in Spark's web UI
- * @param countFailedValues whether to accumulate values from failed tasks. This is set to true
- *                          for system and time metrics like serialization time or bytes spilled,
- *                          and false for things with absolute values like number of input rows.
- *                          This should be used for internal metrics only.
- * @tparam R the full accumulated data (result type)
- * @tparam T partial data that can be added in
- */
-@deprecated(""use AccumulatorV2"", ""2.0.0"")
-class Accumulable[R, T] private (
-    val id: Long,
-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile
-    @transient private val initialValue: R,
-    param: AccumulableParam[R, T],
-    val name: Option[String],
-    private[spark] val countFailedValues: Boolean)
-  extends Serializable {
-
-  private[spark] def this(
-      initialValue: R,
-      param: AccumulableParam[R, T],
-      name: Option[String],
-      countFailedValues: Boolean) = {
-    this(AccumulatorContext.newId(), initialValue, param, name, countFailedValues)
-  }
-
-  private[spark] def this(initialValue: R, param: AccumulableParam[R, T], name: Option[String]) = {
-    this(initialValue, param, name, false /* countFailedValues */)
-  }
-
-  def this(initialValue: R, param: AccumulableParam[R, T]) = this(initialValue, param, None)
-
-  val zero = param.zero(initialValue)
-  private[spark] val newAcc = new LegacyAccumulatorWrapper(initialValue, param)
-  newAcc.metadata = AccumulatorMetadata(id, name, countFailedValues)
-  // Register the new accumulator in ctor, to follow the previous behaviour.
-  AccumulatorContext.register(newAcc)
-
-  /**
-   * Add more data to this accumulator / accumulable
-   * @param term the data to add
-   */
-  def += (term: T) { newAcc.add(term) }
-
-  /**
-   * Add more data to this accumulator / accumulable
-   * @param term the data to add
-   */
-  def add(term: T) { newAcc.add(term) }
-
-  /**
-   * Merge two accumulable objects together
-   *
-   * Normally, a user will not want to use this version, but will instead call `+=`.
-   * @param term the other `R` that will get merged with this
-   */
-  def ++= (term: R) { newAcc._value = param.addInPlace(newAcc._value, term) }
-
-  /**
-   * Merge two accumulable objects together
-   *
-   * Normally, a user will not want to use this version, but will instead call `add`.
-   * @param term the other `R` that will get merged with this
-   */
-  def merge(term: R) { newAcc._value = param.addInPlace(newAcc._value, term) }
-
-  /**
-   * Access the accumulator's current value; only allowed on driver.
-   */
-  def value: R = {
-    if (newAcc.isAtDriverSide) {
-      newAcc.value
-    } else {
-      throw new UnsupportedOperationException(""Can't read accumulator value in task"")
-    }
-  }
-
-  /**
-   * Get the current value of this accumulator from within a task.
-   *
-   * This is NOT the global value of the accumulator.  To get the global value after a
-   * completed operation on the dataset, call `value`.
-   *
-   * The typical use of this method is to directly mutate the local value, eg., to add
-   * an element to a Set.
-   */
-  def localValue: R = newAcc.value
-
-  /**
-   * Set the accumulator's value; only allowed on driver.
-   */
-  def value_= (newValue: R) {
-    if (newAcc.isAtDriverSide) {
-      newAcc._value = newValue
-    } else {
-      throw new UnsupportedOperationException(""Can't assign accumulator value in task"")
-    }
-  }
-
-  /**
-   * Set the accumulator's value. For internal use only.
-   */
-  def setValue(newValue: R): Unit = { newAcc._value = newValue }
-
-  /**
-   * Set the accumulator's value. For internal use only.
-   */
-  private[spark] def setValueAny(newValue: Any): Unit = { setValue(newValue.asInstanceOf[R]) }
-
-  /**
-   * Create an [[AccumulableInfo]] representation of this [[Accumulable]] with the provided values.
-   */
-  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {
-    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))
-    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)
-  }
-
-  override def toString: String = if (newAcc._value == null) ""null"" else newAcc._value.toString
-}
-
-
-/**
- * Helper object defining how to accumulate values of a particular type. An implicit
- * AccumulableParam needs to be available when you create [[Accumulable]]s of a specific type.
- *
- * @tparam R the full accumulated data (result type)
- * @tparam T partial data that can be added in
- */
-@deprecated(""use AccumulatorV2"", ""2.0.0"")
-trait AccumulableParam[R, T] extends Serializable {
-  /**
-   * Add additional data to the accumulator value. Is allowed to modify and return `r`
-   * for efficiency (to avoid allocating objects).
-   *
-   * @param r the current value of the accumulator
-   * @param t the data to be added to the accumulator
-   * @return the new value of the accumulator
-   */
-  def addAccumulator(r: R, t: T): R
-
-  /**
-   * Merge two accumulated values together. Is allowed to modify and return the first value
-   * for efficiency (to avoid allocating objects).
-   *
-   * @param r1 one set of accumulated data
-   * @param r2 another set of accumulated data
-   * @return both data sets merged together
-   */
-  def addInPlace(r1: R, r2: R): R
-
-  /**
-   * Return the ""zero"" (identity) value for an accumulator type, given its initial value. For
-   * example, if R was a vector of N dimensions, this would return a vector of N zeroes.
-   */
-  def zero(initialValue: R): R
-}
-
-
-@deprecated(""use AccumulatorV2"", ""2.0.0"")
-private[spark] class
-GrowableAccumulableParam[R : ClassTag, T]
-  (implicit rg: R => Growable[T] with TraversableOnce[T] with Serializable)
-  extends AccumulableParam[R, T] {
-
-  def addAccumulator(growable: R, elem: T): R = {
-    growable += elem
-    growable
-  }
-
-  def addInPlace(t1: R, t2: R): R = {
-    t1 ++= t2
-    t1
-  }
-
-  def zero(initialValue: R): R = {
-    // We need to clone initialValue, but it's hard to specify that R should also be Cloneable.
-    // Instead we'll serialize it to a buffer and load it back.
-    val ser = new JavaSerializer(new SparkConf(false)).newInstance()
-    val copy = ser.deserialize[R](ser.serialize(initialValue))
-    copy.clear()   // In case it contained stuff
-    copy
-  }
-}
diff --git a/core/src/main/scala/org/apache/spark/Accumulator.scala b/core/src/main/scala/org/apache/spark/Accumulator.scala
deleted file mode 100644
index 9d5fbefc824ad..0000000000000
--- a/core/src/main/scala/org/apache/spark/Accumulator.scala
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark
-
-/**
- * A simpler value of [[Accumulable]] where the result type being accumulated is the same
- * as the types of elements being merged, i.e. variables that are only ""added"" to through an
- * associative and commutative operation and can therefore be efficiently supported in parallel.
- * They can be used to implement counters (as in MapReduce) or sums. Spark natively supports
- * accumulators of numeric value types, and programmers can add support for new types.
- *
- * An accumulator is created from an initial value `v` by calling `SparkContext.accumulator`.
- * Tasks running on the cluster can then add to it using the `+=` operator.
- * However, they cannot read its value. Only the driver program can read the accumulator's value,
- * using its [[#value]] method.
- *
- * The interpreter session below shows an accumulator being used to add up the elements of an array:
- *
- * {{{
- * scala> val accum = sc.accumulator(0)
- * accum: org.apache.spark.Accumulator[Int] = 0
- *
- * scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
- * ...
- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s
- *
- * scala> accum.value
- * res2: Int = 10
- * }}}
- *
- * @param initialValue initial value of accumulator
- * @param param helper object defining how to add elements of type `T`
- * @param name human-readable name associated with this accumulator
- * @param countFailedValues whether to accumulate values from failed tasks
- * @tparam T result type
-*/
-@deprecated(""use AccumulatorV2"", ""2.0.0"")
-class Accumulator[T] private[spark] (
-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile
-    @transient private val initialValue: T,
-    param: AccumulatorParam[T],
-    name: Option[String] = None,
-    countFailedValues: Boolean = false)
-  extends Accumulable[T, T](initialValue, param, name, countFailedValues)
-
-
-/**
- * A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add
- * in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be
- * available when you create Accumulators of a specific type.
- *
- * @tparam T type of value to accumulate
- */
-@deprecated(""use AccumulatorV2"", ""2.0.0"")
-trait AccumulatorParam[T] extends AccumulableParam[T, T] {
-  def addAccumulator(t1: T, t2: T): T = {
-    addInPlace(t1, t2)
-  }
-}
-
-
-@deprecated(""use AccumulatorV2"", ""2.0.0"")
-object AccumulatorParam {
-
-  // The following implicit objects were in SparkContext before 1.2 and users had to
-  // `import SparkContext._` to enable them. Now we move them here to make the compiler find
-  // them automatically. However, as there are duplicate codes in SparkContext for backward
-  // compatibility, please update them accordingly if you modify the following implicit objects.
-
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] {
-    def addInPlace(t1: Double, t2: Double): Double = t1 + t2
-    def zero(initialValue: Double): Double = 0.0
-  }
-
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  implicit object IntAccumulatorParam extends AccumulatorParam[Int] {
-    def addInPlace(t1: Int, t2: Int): Int = t1 + t2
-    def zero(initialValue: Int): Int = 0
-  }
-
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  implicit object LongAccumulatorParam extends AccumulatorParam[Long] {
-    def addInPlace(t1: Long, t2: Long): Long = t1 + t2
-    def zero(initialValue: Long): Long = 0L
-  }
-
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  implicit object FloatAccumulatorParam extends AccumulatorParam[Float] {
-    def addInPlace(t1: Float, t2: Float): Float = t1 + t2
-    def zero(initialValue: Float): Float = 0f
-  }
-
-  // Note: when merging values, this param just adopts the newer value. This is used only
-  // internally for things that shouldn't really be accumulated across tasks, like input
-  // read method, which should be the same across all tasks in the same stage.
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  private[spark] object StringAccumulatorParam extends AccumulatorParam[String] {
-    def addInPlace(t1: String, t2: String): String = t2
-    def zero(initialValue: String): String = """"
-  }
-}
diff --git a/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala b/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala
index 90a5c4130f799..6a497afac444d 100644
--- a/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala
+++ b/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala
@@ -41,14 +41,14 @@ import org.apache.spark.util._
 class BarrierTaskContext private[spark] (
     taskContext: TaskContext) extends TaskContext with Logging {
 
+  import BarrierTaskContext._
+
   // Find the driver side RPCEndpointRef of the coordinator that handles all the barrier() calls.
   private val barrierCoordinator: RpcEndpointRef = {
     val env = SparkEnv.get
     RpcUtils.makeDriverRef(""barrierSync"", env.conf, env.rpcEnv)
   }
 
-  private val timer = new Timer(""Barrier task timer for barrier() calls."")
-
   // Local barrierEpoch that identify a barrier() call from current task, it shall be identical
   // with the driver side epoch.
   private var barrierEpoch = 0
@@ -158,8 +158,6 @@ class BarrierTaskContext private[spark] (
 
   override def isInterrupted(): Boolean = taskContext.isInterrupted()
 
-  override def isRunningLocally(): Boolean = taskContext.isRunningLocally()
-
   override def addTaskCompletionListener(listener: TaskCompletionListener): this.type = {
     taskContext.addTaskCompletionListener(listener)
     this
@@ -234,4 +232,7 @@ object BarrierTaskContext {
   @Experimental
   @Since(""2.4.0"")
   def get(): BarrierTaskContext = TaskContext.get().asInstanceOf[BarrierTaskContext]
+
+  private val timer = new Timer(""Barrier task timer for barrier() calls."")
+
 }
diff --git a/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala b/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala
index bcbc8df0d5865..ab0ae55ed357d 100644
--- a/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala
+++ b/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala
@@ -22,6 +22,7 @@ import java.util.concurrent.{ScheduledFuture, TimeUnit}
 import scala.collection.mutable
 import scala.concurrent.Future
 
+import org.apache.spark.executor.ExecutorMetrics
 import org.apache.spark.internal.Logging
 import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}
 import org.apache.spark.scheduler._
@@ -37,7 +38,8 @@ import org.apache.spark.util._
 private[spark] case class Heartbeat(
     executorId: String,
     accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], // taskId -> accumulator updates
-    blockManagerId: BlockManagerId)
+    blockManagerId: BlockManagerId,
+    executorUpdates: ExecutorMetrics) // executor level updates
 
 /**
  * An event that SparkContext uses to notify HeartbeatReceiver that SparkContext.taskScheduler is
@@ -119,14 +121,14 @@ private[spark] class HeartbeatReceiver(sc: SparkContext, clock: Clock)
       context.reply(true)
 
     // Messages received from executors
-    case heartbeat @ Heartbeat(executorId, accumUpdates, blockManagerId) =>
+    case heartbeat @ Heartbeat(executorId, accumUpdates, blockManagerId, executorMetrics) =>
       if (scheduler != null) {
         if (executorLastSeen.contains(executorId)) {
           executorLastSeen(executorId) = clock.getTimeMillis()
           eventLoopThread.submit(new Runnable {
             override def run(): Unit = Utils.tryLogNonFatalError {
               val unknownExecutor = !scheduler.executorHeartbeatReceived(
-                executorId, accumUpdates, blockManagerId)
+                executorId, accumUpdates, blockManagerId, executorMetrics)
               val response = HeartbeatResponse(reregisterBlockManager = unknownExecutor)
               context.reply(response)
             }
diff --git a/core/src/main/scala/org/apache/spark/Heartbeater.scala b/core/src/main/scala/org/apache/spark/Heartbeater.scala
new file mode 100644
index 0000000000000..84091eef04306
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/Heartbeater.scala
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark
+
+import java.util.concurrent.TimeUnit
+
+import org.apache.spark.executor.ExecutorMetrics
+import org.apache.spark.internal.Logging
+import org.apache.spark.memory.MemoryManager
+import org.apache.spark.metrics.ExecutorMetricType
+import org.apache.spark.util.{ThreadUtils, Utils}
+
+/**
+ * Creates a heartbeat thread which will call the specified reportHeartbeat function at
+ * intervals of intervalMs.
+ *
+ * @param memoryManager the memory manager for execution and storage memory.
+ * @param reportHeartbeat the heartbeat reporting function to call.
+ * @param name the thread name for the heartbeater.
+ * @param intervalMs the interval between heartbeats.
+ */
+private[spark] class Heartbeater(
+    memoryManager: MemoryManager,
+    reportHeartbeat: () => Unit,
+    name: String,
+    intervalMs: Long) extends Logging {
+  // Executor for the heartbeat task
+  private val heartbeater = ThreadUtils.newDaemonSingleThreadScheduledExecutor(name)
+
+  /** Schedules a task to report a heartbeat. */
+  def start(): Unit = {
+    // Wait a random interval so the heartbeats don't end up in sync
+    val initialDelay = intervalMs + (math.random * intervalMs).asInstanceOf[Int]
+
+    val heartbeatTask = new Runnable() {
+      override def run(): Unit = Utils.logUncaughtExceptions(reportHeartbeat())
+    }
+    heartbeater.scheduleAtFixedRate(heartbeatTask, initialDelay, intervalMs, TimeUnit.MILLISECONDS)
+  }
+
+  /** Stops the heartbeat thread. */
+  def stop(): Unit = {
+    heartbeater.shutdown()
+    heartbeater.awaitTermination(10, TimeUnit.SECONDS)
+  }
+
+  /**
+   * Get the current executor level metrics. These are returned as an array, with the index
+   * determined by ExecutorMetricType.values
+   */
+  def getCurrentMetrics(): ExecutorMetrics = {
+    val metrics = ExecutorMetricType.values.map(_.getMetricValue(memoryManager)).toArray
+    new ExecutorMetrics(metrics)
+  }
+}
+
diff --git a/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala b/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala
index ff85e11409e35..f8a6f1d0d8cbb 100644
--- a/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala
+++ b/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala
@@ -23,9 +23,5 @@ package org.apache.spark
  * @param shuffleId ID of the shuffle
  * @param bytesByPartitionId approximate number of output bytes for each map output partition
  *   (may be inexact due to use of compressed map statuses)
- * @param recordsByPartitionId number of output records for each map output partition
  */
-private[spark] class MapOutputStatistics(
-    val shuffleId: Int,
-    val bytesByPartitionId: Array[Long],
-    val recordsByPartitionId: Array[Long])
+private[spark] class MapOutputStatistics(val shuffleId: Int, val bytesByPartitionId: Array[Long])
diff --git a/core/src/main/scala/org/apache/spark/MapOutputTracker.scala b/core/src/main/scala/org/apache/spark/MapOutputTracker.scala
index 41575ce4e6e3d..1c4fa4bc6541f 100644
--- a/core/src/main/scala/org/apache/spark/MapOutputTracker.scala
+++ b/core/src/main/scala/org/apache/spark/MapOutputTracker.scala
@@ -522,19 +522,16 @@ private[spark] class MapOutputTrackerMaster(
   def getStatistics(dep: ShuffleDependency[_, _, _]): MapOutputStatistics = {
     shuffleStatuses(dep.shuffleId).withMapStatuses { statuses =>
       val totalSizes = new Array[Long](dep.partitioner.numPartitions)
-      val recordsByMapTask = new Array[Long](statuses.length)
-
       val parallelAggThreshold = conf.get(
         SHUFFLE_MAP_OUTPUT_PARALLEL_AGGREGATION_THRESHOLD)
       val parallelism = math.min(
         Runtime.getRuntime.availableProcessors(),
         statuses.length.toLong * totalSizes.length / parallelAggThreshold + 1).toInt
       if (parallelism <= 1) {
-        statuses.zipWithIndex.foreach { case (s, index) =>
+        for (s <- statuses) {
           for (i <- 0 until totalSizes.length) {
             totalSizes(i) += s.getSizeForBlock(i)
           }
-          recordsByMapTask(index) = s.numberOfOutput
         }
       } else {
         val threadPool = ThreadUtils.newDaemonFixedThreadPool(parallelism, ""map-output-aggregate"")
@@ -551,11 +548,8 @@ private[spark] class MapOutputTrackerMaster(
         } finally {
           threadPool.shutdown()
         }
-        statuses.zipWithIndex.foreach { case (s, index) =>
-          recordsByMapTask(index) = s.numberOfOutput
-        }
       }
-      new MapOutputStatistics(dep.shuffleId, totalSizes, recordsByMapTask)
+      new MapOutputStatistics(dep.shuffleId, totalSizes)
     }
   }
 
diff --git a/core/src/main/scala/org/apache/spark/SparkConf.scala b/core/src/main/scala/org/apache/spark/SparkConf.scala
index 6c4c5c94cfa28..21c5cbc04d813 100644
--- a/core/src/main/scala/org/apache/spark/SparkConf.scala
+++ b/core/src/main/scala/org/apache/spark/SparkConf.scala
@@ -25,9 +25,9 @@ import scala.collection.mutable.LinkedHashSet
 
 import org.apache.avro.{Schema, SchemaNormalization}
 
-import org.apache.spark.deploy.history.config._
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
+import org.apache.spark.internal.config.History._
 import org.apache.spark.serializer.KryoSerializer
 import org.apache.spark.util.Utils
 
@@ -609,13 +609,14 @@ class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Seria
     require(!encryptionEnabled || get(NETWORK_AUTH_ENABLED),
       s""${NETWORK_AUTH_ENABLED.key} must be enabled when enabling encryption."")
 
-    val executorTimeoutThreshold = getTimeAsSeconds(""spark.network.timeout"", ""120s"")
-    val executorHeartbeatInterval = getTimeAsSeconds(""spark.executor.heartbeatInterval"", ""10s"")
+    val executorTimeoutThresholdMs =
+      getTimeAsSeconds(""spark.network.timeout"", ""120s"") * 1000
+    val executorHeartbeatIntervalMs = get(EXECUTOR_HEARTBEAT_INTERVAL)
     // If spark.executor.heartbeatInterval bigger than spark.network.timeout,
     // it will almost always cause ExecutorLostFailure. See SPARK-22754.
-    require(executorTimeoutThreshold > executorHeartbeatInterval, ""The value of "" +
-      s""spark.network.timeout=${executorTimeoutThreshold}s must be no less than the value of "" +
-      s""spark.executor.heartbeatInterval=${executorHeartbeatInterval}s."")
+    require(executorTimeoutThresholdMs > executorHeartbeatIntervalMs, ""The value of "" +
+      s""spark.network.timeout=${executorTimeoutThresholdMs}ms must be no less than the value of "" +
+      s""spark.executor.heartbeatInterval=${executorHeartbeatIntervalMs}ms."")
   }
 
   /**
@@ -726,7 +727,13 @@ private[spark] object SparkConf extends Logging {
     DRIVER_MEMORY_OVERHEAD.key -> Seq(
       AlternateConfig(""spark.yarn.driver.memoryOverhead"", ""2.3"")),
     EXECUTOR_MEMORY_OVERHEAD.key -> Seq(
-      AlternateConfig(""spark.yarn.executor.memoryOverhead"", ""2.3""))
+      AlternateConfig(""spark.yarn.executor.memoryOverhead"", ""2.3"")),
+    KEYTAB.key -> Seq(
+      AlternateConfig(""spark.yarn.keytab"", ""3.0"")),
+    PRINCIPAL.key -> Seq(
+      AlternateConfig(""spark.yarn.principal"", ""3.0"")),
+    KERBEROS_RELOGIN_PERIOD.key -> Seq(
+      AlternateConfig(""spark.yarn.kerberos.relogin.period"", ""3.0""))
   )
 
   /**
diff --git a/core/src/main/scala/org/apache/spark/SparkContext.scala b/core/src/main/scala/org/apache/spark/SparkContext.scala
index e5b1e0ecd1586..845a3d5f6d6f9 100644
--- a/core/src/main/scala/org/apache/spark/SparkContext.scala
+++ b/core/src/main/scala/org/apache/spark/SparkContext.scala
@@ -25,7 +25,6 @@ import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger, AtomicReferenc
 
 import scala.collection.JavaConverters._
 import scala.collection.Map
-import scala.collection.generic.Growable
 import scala.collection.mutable.HashMap
 import scala.language.implicitConversions
 import scala.reflect.{classTag, ClassTag}
@@ -51,14 +50,15 @@ import org.apache.spark.partial.{ApproximateEvaluator, PartialResult}
 import org.apache.spark.rdd._
 import org.apache.spark.rpc.RpcEndpointRef
 import org.apache.spark.scheduler._
-import org.apache.spark.scheduler.cluster.{CoarseGrainedSchedulerBackend, StandaloneSchedulerBackend}
+import org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend
 import org.apache.spark.scheduler.local.LocalSchedulerBackend
-import org.apache.spark.status.AppStatusStore
+import org.apache.spark.status.{AppStatusSource, AppStatusStore}
 import org.apache.spark.status.api.v1.ThreadStackTrace
 import org.apache.spark.storage._
 import org.apache.spark.storage.BlockManagerMessages.TriggerThreadDump
 import org.apache.spark.ui.{ConsoleProgressBar, SparkUI}
 import org.apache.spark.util._
+import org.apache.spark.util.logging.DriverLogger
 
 /**
  * Main entry point for Spark functionality. A SparkContext represents the connection to a Spark
@@ -206,6 +206,7 @@ class SparkContext(config: SparkConf) extends Logging {
   private var _applicationId: String = _
   private var _applicationAttemptId: Option[String] = None
   private var _eventLogger: Option[EventLoggingListener] = None
+  private var _driverLogger: Option[DriverLogger] = None
   private var _executorAllocationManager: Option[ExecutorAllocationManager] = None
   private var _cleaner: Option[ContextCleaner] = None
   private var _listenerBusStarted: Boolean = false
@@ -213,6 +214,7 @@ class SparkContext(config: SparkConf) extends Logging {
   private var _files: Seq[String] = _
   private var _shutdownHookRef: AnyRef = _
   private var _statusStore: AppStatusStore = _
+  private var _heartbeater: Heartbeater = _
 
   /* ------------------------------------------------------------------------------------- *
    | Accessors and public fields. These provide access to the internal state of the        |
@@ -371,6 +373,8 @@ class SparkContext(config: SparkConf) extends Logging {
       throw new SparkException(""An application name must be set in your configuration"")
     }
 
+    _driverLogger = DriverLogger(_conf)
+
     // log out spark.app.name in the Spark driver logs
     logInfo(s""Submitted application: $appName"")
 
@@ -417,7 +421,8 @@ class SparkContext(config: SparkConf) extends Logging {
 
     // Initialize the app status store and listener before SparkEnv is created so that it gets
     // all events.
-    _statusStore = AppStatusStore.createLiveStore(conf)
+    val appStatusSource = AppStatusSource.createSource(conf)
+    _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource)
     listenerBus.addToStatusQueue(_statusStore.listener.get)
 
     // Create the Spark execution environment (cache, map output tracker, etc)
@@ -496,6 +501,13 @@ class SparkContext(config: SparkConf) extends Logging {
     _dagScheduler = new DAGScheduler(this)
     _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
 
+    // create and start the heartbeater for collecting memory metrics
+    _heartbeater = new Heartbeater(env.memoryManager,
+      () => SparkContext.this.reportHeartBeat(),
+      ""driver-heartbeater"",
+      conf.get(EXECUTOR_HEARTBEAT_INTERVAL))
+    _heartbeater.start()
+
     // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's
     // constructor
     _taskScheduler.start()
@@ -563,7 +575,7 @@ class SparkContext(config: SparkConf) extends Logging {
     _executorAllocationManager.foreach { e =>
       _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
     }
-
+    appStatusSource.foreach(_env.metricsSystem.registerSource(_))
     // Make sure the context is stopped if the user forgets about it. This avoids leaving
     // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM
     // is killed, though.
@@ -1330,76 +1342,6 @@ class SparkContext(config: SparkConf) extends Logging {
 
   // Methods for creating shared variables
 
-  /**
-   * Create an [[org.apache.spark.Accumulator]] variable of a given type, which tasks can ""add""
-   * values to using the `+=` method. Only the driver can access the accumulator's `value`.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]): Accumulator[T] = {
-    val acc = new Accumulator(initialValue, param)
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] variable of a given type, with a name for display
-   * in the Spark UI. Tasks can ""add"" values to the accumulator using the `+=` method. Only the
-   * driver can access the accumulator's `value`.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulator[T](initialValue: T, name: String)(implicit param: AccumulatorParam[T])
-    : Accumulator[T] = {
-    val acc = new Accumulator(initialValue, param, Option(name))
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }
-
-  /**
-   * Create an [[org.apache.spark.Accumulable]] shared variable, to which tasks can add values
-   * with `+=`. Only the driver can access the accumulable's `value`.
-   * @tparam R accumulator result type
-   * @tparam T type that can be added to the accumulator
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulable[R, T](initialValue: R)(implicit param: AccumulableParam[R, T])
-    : Accumulable[R, T] = {
-    val acc = new Accumulable(initialValue, param)
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }
-
-  /**
-   * Create an [[org.apache.spark.Accumulable]] shared variable, with a name for display in the
-   * Spark UI. Tasks can add values to the accumulable using the `+=` operator. Only the driver can
-   * access the accumulable's `value`.
-   * @tparam R accumulator result type
-   * @tparam T type that can be added to the accumulator
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulable[R, T](initialValue: R, name: String)(implicit param: AccumulableParam[R, T])
-    : Accumulable[R, T] = {
-    val acc = new Accumulable(initialValue, param, Option(name))
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }
-
-  /**
-   * Create an accumulator from a ""mutable collection"" type.
-   *
-   * Growable and TraversableOnce are the standard APIs that guarantee += and ++=, implemented by
-   * standard mutable collections. So you can use this with mutable Map, Set, etc.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulableCollection[R <% Growable[T] with TraversableOnce[T] with Serializable: ClassTag, T]
-      (initialValue: R): Accumulable[R, T] = {
-    // TODO the context bound (<%) above should be replaced with simple type bound and implicit
-    // conversion but is a breaking change. This should be fixed in Spark 3.x.
-    val param = new GrowableAccumulableParam[R, T]
-    val acc = new Accumulable(initialValue, param)
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }
-
   /**
    * Register the given accumulator.
    *
@@ -1930,6 +1872,9 @@ class SparkContext(config: SparkConf) extends Logging {
     Utils.tryLogNonFatalError {
       postApplicationEnd()
     }
+    Utils.tryLogNonFatalError {
+      _driverLogger.foreach(_.stop())
+    }
     Utils.tryLogNonFatalError {
       _ui.foreach(_.stop())
     }
@@ -1959,6 +1904,12 @@ class SparkContext(config: SparkConf) extends Logging {
     Utils.tryLogNonFatalError {
       _eventLogger.foreach(_.stop())
     }
+    if (_heartbeater != null) {
+      Utils.tryLogNonFatalError {
+        _heartbeater.stop()
+      }
+      _heartbeater = null
+    }
     if (env != null && _heartbeatReceiver != null) {
       Utils.tryLogNonFatalError {
         env.rpcEnv.stop(_heartbeatReceiver)
@@ -2409,6 +2360,7 @@ class SparkContext(config: SparkConf) extends Logging {
     // the cluster manager to get an application ID (in case the cluster manager provides one).
     listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),
       startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))
+    _driverLogger.foreach(_.startSync(_hadoopConfiguration))
   }
 
   /** Post the application end event */
@@ -2429,6 +2381,14 @@ class SparkContext(config: SparkConf) extends Logging {
     }
   }
 
+  /** Reports heartbeat metrics for the driver. */
+  private def reportHeartBeat(): Unit = {
+    val driverUpdates = _heartbeater.getCurrentMetrics()
+    val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0)
+    listenerBus.post(SparkListenerExecutorMetricsUpdate(""driver"", accumUpdates,
+      Some(driverUpdates)))
+  }
+
   // In order to prevent multiple SparkContexts from being active at the same time, mark this
   // context as having finished construction.
   // NOTE: this must be placed at the end of the SparkContext constructor.
diff --git a/core/src/main/scala/org/apache/spark/SparkEnv.scala b/core/src/main/scala/org/apache/spark/SparkEnv.scala
index 72123f2232532..66038eeaea54f 100644
--- a/core/src/main/scala/org/apache/spark/SparkEnv.scala
+++ b/core/src/main/scala/org/apache/spark/SparkEnv.scala
@@ -261,7 +261,7 @@ object SparkEnv extends Logging {
       // SparkConf, then one taking no arguments
       try {
         cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE)
-          .newInstance(conf, new java.lang.Boolean(isDriver))
+          .newInstance(conf, java.lang.Boolean.valueOf(isDriver))
           .asInstanceOf[T]
       } catch {
         case _: NoSuchMethodException =>
diff --git a/core/src/main/scala/org/apache/spark/TaskContext.scala b/core/src/main/scala/org/apache/spark/TaskContext.scala
index 2b939dabb1105..959f246f3f9f6 100644
--- a/core/src/main/scala/org/apache/spark/TaskContext.scala
+++ b/core/src/main/scala/org/apache/spark/TaskContext.scala
@@ -96,13 +96,6 @@ abstract class TaskContext extends Serializable {
    */
   def isInterrupted(): Boolean
 
-  /**
-   * Returns true if the task is running locally in the driver program.
-   * @return false
-   */
-  @deprecated(""Local execution was removed, so this always returns false"", ""2.0.0"")
-  def isRunningLocally(): Boolean
-
   /**
    * Adds a (Java friendly) listener to be executed on task completion.
    * This will be called in all situations - success, failure, or cancellation. Adding a listener
diff --git a/core/src/main/scala/org/apache/spark/TaskContextImpl.scala b/core/src/main/scala/org/apache/spark/TaskContextImpl.scala
index 89730424e5acf..76296c5d0abd3 100644
--- a/core/src/main/scala/org/apache/spark/TaskContextImpl.scala
+++ b/core/src/main/scala/org/apache/spark/TaskContextImpl.scala
@@ -157,8 +157,6 @@ private[spark] class TaskContextImpl(
   @GuardedBy(""this"")
   override def isCompleted(): Boolean = synchronized(completed)
 
-  override def isRunningLocally(): Boolean = false
-
   override def isInterrupted(): Boolean = reasonIfKilled.isDefined
 
   override def getLocalProperty(key: String): String = localProperties.getProperty(key)
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala b/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala
index 9544475ff0428..50ed8d9bd3f68 100644
--- a/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala
@@ -19,7 +19,7 @@ package org.apache.spark.api.java
 
 import java.{lang => jl}
 import java.lang.{Iterable => JIterable}
-import java.util.{Comparator, List => JList}
+import java.util.{Comparator, Iterator => JIterator, List => JList}
 
 import scala.collection.JavaConverters._
 import scala.language.implicitConversions
@@ -34,7 +34,8 @@ import org.apache.spark.{HashPartitioner, Partitioner}
 import org.apache.spark.Partitioner._
 import org.apache.spark.api.java.JavaSparkContext.fakeClassTag
 import org.apache.spark.api.java.JavaUtils.mapAsSerializableJavaMap
-import org.apache.spark.api.java.function.{Function => JFunction, Function2 => JFunction2, PairFunction}
+import org.apache.spark.api.java.function.{FlatMapFunction, Function => JFunction,
+  Function2 => JFunction2, PairFunction}
 import org.apache.spark.partial.{BoundedDouble, PartialResult}
 import org.apache.spark.rdd.{OrderedRDDFunctions, RDD}
 import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
@@ -674,8 +675,8 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    * Pass each value in the key-value pair RDD through a flatMap function without changing the
    * keys; this also retains the original RDD's partitioning.
    */
-  def flatMapValues[U](f: JFunction[V, java.lang.Iterable[U]]): JavaPairRDD[K, U] = {
-    def fn: (V) => Iterable[U] = (x: V) => f.call(x).asScala
+  def flatMapValues[U](f: FlatMapFunction[V, U]): JavaPairRDD[K, U] = {
+    def fn: (V) => Iterator[U] = (x: V) => f.call(x).asScala
     implicit val ctag: ClassTag[U] = fakeClassTag
     fromRDD(rdd.flatMapValues(fn))
   }
@@ -951,7 +952,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
@@ -968,7 +969,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
@@ -984,7 +985,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala b/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala
index 91ae1002abd21..5ba821935ac69 100644
--- a/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala
@@ -685,7 +685,7 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala b/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala
index 09c83849e26b2..03f259d73e975 100644
--- a/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala
@@ -21,6 +21,7 @@ import java.io.Closeable
 import java.util
 import java.util.{Map => JMap}
 
+import scala.annotation.varargs
 import scala.collection.JavaConverters._
 import scala.language.implicitConversions
 import scala.reflect.ClassTag
@@ -30,11 +31,10 @@ import org.apache.hadoop.mapred.{InputFormat, JobConf}
 import org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat}
 
 import org.apache.spark._
-import org.apache.spark.AccumulatorParam._
 import org.apache.spark.api.java.JavaSparkContext.fakeClassTag
 import org.apache.spark.broadcast.Broadcast
 import org.apache.spark.input.PortableDataStream
-import org.apache.spark.rdd.{EmptyRDD, HadoopRDD, NewHadoopRDD, RDD}
+import org.apache.spark.rdd.{EmptyRDD, HadoopRDD, NewHadoopRDD}
 
 /**
  * A Java-friendly version of [[org.apache.spark.SparkContext]] that returns
@@ -43,8 +43,7 @@ import org.apache.spark.rdd.{EmptyRDD, HadoopRDD, NewHadoopRDD, RDD}
  * Only one SparkContext may be active per JVM.  You must `stop()` the active SparkContext before
  * creating a new one.  This limitation may eventually be removed; see SPARK-2243 for more details.
  */
-class JavaSparkContext(val sc: SparkContext)
-  extends JavaSparkContextVarargsWorkaround with Closeable {
+class JavaSparkContext(val sc: SparkContext) extends Closeable {
 
   /**
    * Create a JavaSparkContext that loads settings from system properties (for instance, when
@@ -507,141 +506,31 @@ class JavaSparkContext(val sc: SparkContext)
     new JavaNewHadoopRDD(rdd.asInstanceOf[NewHadoopRDD[K, V]])
   }
 
-  /** Build the union of two or more RDDs. */
-  override def union[T](first: JavaRDD[T], rest: java.util.List[JavaRDD[T]]): JavaRDD[T] = {
-    val rdds: Seq[RDD[T]] = (Seq(first) ++ rest.asScala).map(_.rdd)
-    implicit val ctag: ClassTag[T] = first.classTag
-    sc.union(rdds)
+  /** Build the union of JavaRDDs. */
+  @varargs
+  def union[T](rdds: JavaRDD[T]*): JavaRDD[T] = {
+    require(rdds.nonEmpty, ""Union called on no RDDs"")
+    implicit val ctag: ClassTag[T] = rdds.head.classTag
+    sc.union(rdds.map(_.rdd))
   }
 
-  /** Build the union of two or more RDDs. */
-  override def union[K, V](first: JavaPairRDD[K, V], rest: java.util.List[JavaPairRDD[K, V]])
-      : JavaPairRDD[K, V] = {
-    val rdds: Seq[RDD[(K, V)]] = (Seq(first) ++ rest.asScala).map(_.rdd)
-    implicit val ctag: ClassTag[(K, V)] = first.classTag
-    implicit val ctagK: ClassTag[K] = first.kClassTag
-    implicit val ctagV: ClassTag[V] = first.vClassTag
-    new JavaPairRDD(sc.union(rdds))
+  /** Build the union of JavaPairRDDs. */
+  @varargs
+  def union[K, V](rdds: JavaPairRDD[K, V]*): JavaPairRDD[K, V] = {
+    require(rdds.nonEmpty, ""Union called on no RDDs"")
+    implicit val ctag: ClassTag[(K, V)] = rdds.head.classTag
+    implicit val ctagK: ClassTag[K] = rdds.head.kClassTag
+    implicit val ctagV: ClassTag[V] = rdds.head.vClassTag
+    new JavaPairRDD(sc.union(rdds.map(_.rdd)))
   }
 
-  /** Build the union of two or more RDDs. */
-  override def union(first: JavaDoubleRDD, rest: java.util.List[JavaDoubleRDD]): JavaDoubleRDD = {
-    val rdds: Seq[RDD[Double]] = (Seq(first) ++ rest.asScala).map(_.srdd)
-    new JavaDoubleRDD(sc.union(rdds))
+  /** Build the union of JavaDoubleRDDs. */
+  @varargs
+  def union(rdds: JavaDoubleRDD*): JavaDoubleRDD = {
+    require(rdds.nonEmpty, ""Union called on no RDDs"")
+    new JavaDoubleRDD(sc.union(rdds.map(_.srdd)))
   }
 
-  /**
-   * Create an [[org.apache.spark.Accumulator]] integer variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   */
-  @deprecated(""use sc().longAccumulator()"", ""2.0.0"")
-  def intAccumulator(initialValue: Int): Accumulator[java.lang.Integer] =
-    sc.accumulator(initialValue)(IntAccumulatorParam).asInstanceOf[Accumulator[java.lang.Integer]]
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] integer variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   *
-   * This version supports naming the accumulator for display in Spark's web UI.
-   */
-  @deprecated(""use sc().longAccumulator(String)"", ""2.0.0"")
-  def intAccumulator(initialValue: Int, name: String): Accumulator[java.lang.Integer] =
-    sc.accumulator(initialValue, name)(IntAccumulatorParam)
-      .asInstanceOf[Accumulator[java.lang.Integer]]
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] double variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   */
-  @deprecated(""use sc().doubleAccumulator()"", ""2.0.0"")
-  def doubleAccumulator(initialValue: Double): Accumulator[java.lang.Double] =
-    sc.accumulator(initialValue)(DoubleAccumulatorParam).asInstanceOf[Accumulator[java.lang.Double]]
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] double variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   *
-   * This version supports naming the accumulator for display in Spark's web UI.
-   */
-  @deprecated(""use sc().doubleAccumulator(String)"", ""2.0.0"")
-  def doubleAccumulator(initialValue: Double, name: String): Accumulator[java.lang.Double] =
-    sc.accumulator(initialValue, name)(DoubleAccumulatorParam)
-      .asInstanceOf[Accumulator[java.lang.Double]]
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] integer variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   */
-  @deprecated(""use sc().longAccumulator()"", ""2.0.0"")
-  def accumulator(initialValue: Int): Accumulator[java.lang.Integer] = intAccumulator(initialValue)
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] integer variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   *
-   * This version supports naming the accumulator for display in Spark's web UI.
-   */
-  @deprecated(""use sc().longAccumulator(String)"", ""2.0.0"")
-  def accumulator(initialValue: Int, name: String): Accumulator[java.lang.Integer] =
-    intAccumulator(initialValue, name)
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] double variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   */
-  @deprecated(""use sc().doubleAccumulator()"", ""2.0.0"")
-  def accumulator(initialValue: Double): Accumulator[java.lang.Double] =
-    doubleAccumulator(initialValue)
-
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] double variable, which tasks can ""add"" values
-   * to using the `add` method. Only the master can access the accumulator's `value`.
-   *
-   * This version supports naming the accumulator for display in Spark's web UI.
-   */
-  @deprecated(""use sc().doubleAccumulator(String)"", ""2.0.0"")
-  def accumulator(initialValue: Double, name: String): Accumulator[java.lang.Double] =
-    doubleAccumulator(initialValue, name)
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] variable of a given type, which tasks can ""add""
-   * values to using the `add` method. Only the master can access the accumulator's `value`.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulator[T](initialValue: T, accumulatorParam: AccumulatorParam[T]): Accumulator[T] =
-    sc.accumulator(initialValue)(accumulatorParam)
-
-  /**
-   * Create an [[org.apache.spark.Accumulator]] variable of a given type, which tasks can ""add""
-   * values to using the `add` method. Only the master can access the accumulator's `value`.
-   *
-   * This version supports naming the accumulator for display in Spark's web UI.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulator[T](initialValue: T, name: String, accumulatorParam: AccumulatorParam[T])
-      : Accumulator[T] =
-    sc.accumulator(initialValue, name)(accumulatorParam)
-
-  /**
-   * Create an [[org.apache.spark.Accumulable]] shared variable of the given type, to which tasks
-   * can ""add"" values with `add`. Only the master can access the accumulable's `value`.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulable[T, R](initialValue: T, param: AccumulableParam[T, R]): Accumulable[T, R] =
-    sc.accumulable(initialValue)(param)
-
-  /**
-   * Create an [[org.apache.spark.Accumulable]] shared variable of the given type, to which tasks
-   * can ""add"" values with `add`. Only the master can access the accumulable's `value`.
-   *
-   * This version supports naming the accumulator for display in Spark's web UI.
-   */
-  @deprecated(""use AccumulatorV2"", ""2.0.0"")
-  def accumulable[T, R](initialValue: T, name: String, param: AccumulableParam[T, R])
-      : Accumulable[T, R] =
-    sc.accumulable(initialValue, name)(param)
-
   /**
    * Broadcast a read-only variable to the cluster, returning a
    * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions.
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala b/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala
index 6259bead3ea88..2ab8add63efae 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala
@@ -43,7 +43,8 @@ private[python] object Converter extends Logging {
                   defaultConverter: Converter[Any, Any]): Converter[Any, Any] = {
     converterClass.map { cc =>
       Try {
-        val c = Utils.classForName(cc).newInstance().asInstanceOf[Converter[Any, Any]]
+        val c = Utils.classForName(cc).getConstructor().
+          newInstance().asInstanceOf[Converter[Any, Any]]
         logInfo(s""Loaded converter: $cc"")
         c
       } match {
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala b/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala
index e639a842754bd..5ed5070558af7 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala
@@ -24,8 +24,10 @@ import java.util.{ArrayList => JArrayList, List => JList, Map => JMap}
 
 import scala.collection.JavaConverters._
 import scala.collection.mutable
+import scala.concurrent.Promise
+import scala.concurrent.duration.Duration
 import scala.language.existentials
-import scala.util.control.NonFatal
+import scala.util.Try
 
 import org.apache.hadoop.conf.Configuration
 import org.apache.hadoop.io.compress.CompressionCodec
@@ -37,6 +39,7 @@ import org.apache.spark.api.java.{JavaPairRDD, JavaRDD, JavaSparkContext}
 import org.apache.spark.broadcast.Broadcast
 import org.apache.spark.input.PortableDataStream
 import org.apache.spark.internal.Logging
+import org.apache.spark.network.util.JavaUtils
 import org.apache.spark.rdd.RDD
 import org.apache.spark.security.SocketAuthHelper
 import org.apache.spark.util._
@@ -169,27 +172,34 @@ private[spark] object PythonRDD extends Logging {
 
   def readRDDFromFile(sc: JavaSparkContext, filename: String, parallelism: Int):
   JavaRDD[Array[Byte]] = {
-    val file = new DataInputStream(new FileInputStream(filename))
+    readRDDFromInputStream(sc.sc, new FileInputStream(filename), parallelism)
+  }
+
+  def readRDDFromInputStream(
+      sc: SparkContext,
+      in: InputStream,
+      parallelism: Int): JavaRDD[Array[Byte]] = {
+    val din = new DataInputStream(in)
     try {
       val objs = new mutable.ArrayBuffer[Array[Byte]]
       try {
         while (true) {
-          val length = file.readInt()
+          val length = din.readInt()
           val obj = new Array[Byte](length)
-          file.readFully(obj)
+          din.readFully(obj)
           objs += obj
         }
       } catch {
         case eof: EOFException => // No-op
       }
-      JavaRDD.fromRDD(sc.sc.parallelize(objs, parallelism))
+      JavaRDD.fromRDD(sc.parallelize(objs, parallelism))
     } finally {
-      file.close()
+      din.close()
     }
   }
 
-  def readBroadcastFromFile(sc: JavaSparkContext, path: String): Broadcast[PythonBroadcast] = {
-    sc.broadcast(new PythonBroadcast(path))
+  def setupBroadcast(path: String): PythonBroadcast = {
+    new PythonBroadcast(path)
   }
 
   def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream) {
@@ -419,34 +429,15 @@ private[spark] object PythonRDD extends Logging {
    */
   private[spark] def serveToStream(
       threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = {
-    val serverSocket = new ServerSocket(0, 1, InetAddress.getByName(""localhost""))
-    // Close the socket if no connection in 15 seconds
-    serverSocket.setSoTimeout(15000)
-
-    new Thread(threadName) {
-      setDaemon(true)
-      override def run() {
-        try {
-          val sock = serverSocket.accept()
-          authHelper.authClient(sock)
-
-          val out = new BufferedOutputStream(sock.getOutputStream)
-          Utils.tryWithSafeFinally {
-            writeFunc(out)
-          } {
-            out.close()
-            sock.close()
-          }
-        } catch {
-          case NonFatal(e) =>
-            logError(s""Error while sending iterator"", e)
-        } finally {
-          serverSocket.close()
-        }
+    val (port, secret) = PythonServer.setupOneConnectionServer(authHelper, threadName) { s =>
+      val out = new BufferedOutputStream(s.getOutputStream())
+      Utils.tryWithSafeFinally {
+        writeFunc(out)
+      } {
+        out.close()
       }
-    }.start()
-
-    Array(serverSocket.getLocalPort, authHelper.secret)
+    }
+    Array(port, secret)
   }
 
   private def getMergedConf(confAsMap: java.util.HashMap[String, String],
@@ -664,13 +655,12 @@ private[spark] class PythonAccumulatorV2(
   }
 }
 
-/**
- * A Wrapper for Python Broadcast, which is written into disk by Python. It also will
- * write the data into disk after deserialization, then Python can read it from disks.
- */
 // scalastyle:off no.finalize
 private[spark] class PythonBroadcast(@transient var path: String) extends Serializable
-  with Logging {
+    with Logging {
+
+  private var encryptionServer: PythonServer[Unit] = null
+  private var decryptionServer: PythonServer[Unit] = null
 
   /**
    * Read data from disks, then copy it to `out`
@@ -713,5 +703,255 @@ private[spark] class PythonBroadcast(@transient var path: String) extends Serial
     }
     super.finalize()
   }
+
+  def setupEncryptionServer(): Array[Any] = {
+    encryptionServer = new PythonServer[Unit](""broadcast-encrypt-server"") {
+      override def handleConnection(sock: Socket): Unit = {
+        val env = SparkEnv.get
+        val in = sock.getInputStream()
+        val abspath = new File(path).getAbsolutePath
+        val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath))
+        DechunkedInputStream.dechunkAndCopyToOutput(in, out)
+      }
+    }
+    Array(encryptionServer.port, encryptionServer.secret)
+  }
+
+  def setupDecryptionServer(): Array[Any] = {
+    decryptionServer = new PythonServer[Unit](""broadcast-decrypt-server-for-driver"") {
+      override def handleConnection(sock: Socket): Unit = {
+        val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream()))
+        Utils.tryWithSafeFinally {
+          val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path))
+          Utils.tryWithSafeFinally {
+            Utils.copyStream(in, out, false)
+          } {
+            in.close()
+          }
+          out.flush()
+        } {
+          JavaUtils.closeQuietly(out)
+        }
+      }
+    }
+    Array(decryptionServer.port, decryptionServer.secret)
+  }
+
+  def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult()
+
+  def waitTillDataReceived(): Unit = encryptionServer.getResult()
 }
 // scalastyle:on no.finalize
+
+/**
+ * The inverse of pyspark's ChunkedStream for sending data of unknown size.
+ *
+ * We might be serializing a really large object from python -- we don't want
+ * python to buffer the whole thing in memory, nor can it write to a file,
+ * so we don't know the length in advance.  So python writes it in chunks, each chunk
+ * preceeded by a length, till we get a ""length"" of -1 which serves as EOF.
+ *
+ * Tested from python tests.
+ */
+private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging {
+  private val din = new DataInputStream(wrapped)
+  private var remainingInChunk = din.readInt()
+
+  override def read(): Int = {
+    val into = new Array[Byte](1)
+    val n = read(into, 0, 1)
+    if (n == -1) {
+      -1
+    } else {
+      // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted
+      // as an EOF
+      val b = into(0)
+      if (b < 0) {
+        256 + b
+      } else {
+        b
+      }
+    }
+  }
+
+  override def read(dest: Array[Byte], off: Int, len: Int): Int = {
+    if (remainingInChunk == -1) {
+      return -1
+    }
+    var destSpace = len
+    var destPos = off
+    while (destSpace > 0 && remainingInChunk != -1) {
+      val toCopy = math.min(remainingInChunk, destSpace)
+      val read = din.read(dest, destPos, toCopy)
+      destPos += read
+      destSpace -= read
+      remainingInChunk -= read
+      if (remainingInChunk == 0) {
+        remainingInChunk = din.readInt()
+      }
+    }
+    assert(destSpace == 0 || remainingInChunk == -1)
+    return destPos - off
+  }
+
+  override def close(): Unit = wrapped.close()
+}
+
+private[spark] object DechunkedInputStream {
+
+  /**
+   * Dechunks the input, copies to output, and closes both input and the output safely.
+   */
+  def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = {
+    val dechunked = new DechunkedInputStream(chunked)
+    Utils.tryWithSafeFinally {
+      Utils.copyStream(dechunked, out)
+    } {
+      JavaUtils.closeQuietly(out)
+      JavaUtils.closeQuietly(dechunked)
+    }
+  }
+}
+
+/**
+ * Creates a server in the jvm to communicate with python for handling one batch of data, with
+ * authentication and error handling.
+ */
+private[spark] abstract class PythonServer[T](
+    authHelper: SocketAuthHelper,
+    threadName: String) {
+
+  def this(env: SparkEnv, threadName: String) = this(new SocketAuthHelper(env.conf), threadName)
+  def this(threadName: String) = this(SparkEnv.get, threadName)
+
+  val (port, secret) = PythonServer.setupOneConnectionServer(authHelper, threadName) { sock =>
+    promise.complete(Try(handleConnection(sock)))
+  }
+
+  /**
+   * Handle a connection which has already been authenticated.  Any error from this function
+   * will clean up this connection and the entire server, and get propogated to [[getResult]].
+   */
+  def handleConnection(sock: Socket): T
+
+  val promise = Promise[T]()
+
+  /**
+   * Blocks indefinitely for [[handleConnection]] to finish, and returns that result.  If
+   * handleConnection throws an exception, this will throw an exception which includes the original
+   * exception as a cause.
+   */
+  def getResult(): T = {
+    getResult(Duration.Inf)
+  }
+
+  def getResult(wait: Duration): T = {
+    ThreadUtils.awaitResult(promise.future, wait)
+  }
+
+}
+
+private[spark] object PythonServer {
+
+  /**
+   * Create a socket server and run user function on the socket in a background thread.
+   *
+   * The socket server can only accept one connection, or close if no connection
+   * in 15 seconds.
+   *
+   * The thread will terminate after the supplied user function, or if there are any exceptions.
+   *
+   * If you need to get a result of the supplied function, create a subclass of [[PythonServer]]
+   *
+   * @return The port number of a local socket and the secret for authentication.
+   */
+  def setupOneConnectionServer(
+      authHelper: SocketAuthHelper,
+      threadName: String)
+      (func: Socket => Unit): (Int, String) = {
+    val serverSocket = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))
+    // Close the socket if no connection in 15 seconds
+    serverSocket.setSoTimeout(15000)
+
+    new Thread(threadName) {
+      setDaemon(true)
+      override def run(): Unit = {
+        var sock: Socket = null
+        try {
+          sock = serverSocket.accept()
+          authHelper.authClient(sock)
+          func(sock)
+        } finally {
+          JavaUtils.closeQuietly(serverSocket)
+          JavaUtils.closeQuietly(sock)
+        }
+      }
+    }.start()
+    (serverSocket.getLocalPort, authHelper.secret)
+  }
+}
+
+/**
+ * Sends decrypted broadcast data to python worker.  See [[PythonRunner]] for entire protocol.
+ */
+private[spark] class EncryptedPythonBroadcastServer(
+    val env: SparkEnv,
+    val idsAndFiles: Seq[(Long, String)])
+    extends PythonServer[Unit](""broadcast-decrypt-server"") with Logging {
+
+  override def handleConnection(socket: Socket): Unit = {
+    val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream()))
+    var socketIn: InputStream = null
+    // send the broadcast id, then the decrypted data.  We don't need to send the length, the
+    // the python pickle module just needs a stream.
+    Utils.tryWithSafeFinally {
+      (idsAndFiles).foreach { case (id, path) =>
+        out.writeLong(id)
+        val in = env.serializerManager.wrapForEncryption(new FileInputStream(path))
+        Utils.tryWithSafeFinally {
+          Utils.copyStream(in, out, false)
+        } {
+          in.close()
+        }
+      }
+      logTrace(""waiting for python to accept broadcast data over socket"")
+      out.flush()
+      socketIn = socket.getInputStream()
+      socketIn.read()
+      logTrace(""done serving broadcast data"")
+    } {
+      JavaUtils.closeQuietly(socketIn)
+      JavaUtils.closeQuietly(out)
+    }
+  }
+
+  def waitTillBroadcastDataSent(): Unit = {
+    getResult()
+  }
+}
+
+/**
+ * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python
+ * over a socket.  This is used in preference to writing data to a file when encryption is enabled.
+ */
+private[spark] abstract class PythonRDDServer
+    extends PythonServer[JavaRDD[Array[Byte]]](""pyspark-parallelize-server"") {
+
+  def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = {
+    val in = sock.getInputStream()
+    val dechunkedInput: InputStream = new DechunkedInputStream(in)
+    streamToRDD(dechunkedInput)
+  }
+
+  protected def streamToRDD(input: InputStream): RDD[Array[Byte]]
+
+}
+
+private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int)
+    extends PythonRDDServer {
+
+  override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = {
+    PythonRDD.readRDDFromInputStream(sc, input, parallelism)
+  }
+}
+
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
index 4c53bc269a104..f73e95eac8f79 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
@@ -106,15 +106,17 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
       envVars.put(""PYSPARK_EXECUTOR_MEMORY_MB"", memoryMb.get.toString)
     }
     val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)
-    // Whether is the worker released into idle pool
-    val released = new AtomicBoolean(false)
+    // Whether is the worker released into idle pool or closed. When any codes try to release or
+    // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make
+    // sure there is only one winner that is going to release or close the worker.
+    val releasedOrClosed = new AtomicBoolean(false)
 
     // Start a thread to feed the process input from our parent's iterator
     val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context)
 
     context.addTaskCompletionListener[Unit] { _ =>
       writerThread.shutdownOnTaskCompletion()
-      if (!reuseWorker || !released.get) {
+      if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) {
         try {
           worker.close()
         } catch {
@@ -131,7 +133,7 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
     val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))
 
     val stdoutIterator = newReaderIterator(
-      stream, writerThread, startTime, env, worker, released, context)
+      stream, writerThread, startTime, env, worker, releasedOrClosed, context)
     new InterruptibleIterator(context, stdoutIterator)
   }
 
@@ -148,7 +150,7 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
       startTime: Long,
       env: SparkEnv,
       worker: Socket,
-      released: AtomicBoolean,
+      releasedOrClosed: AtomicBoolean,
       context: TaskContext): Iterator[OUT]
 
   /**
@@ -289,19 +291,51 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
         val newBids = broadcastVars.map(_.id).toSet
         // number of different broadcasts
         val toRemove = oldBids.diff(newBids)
-        val cnt = toRemove.size + newBids.diff(oldBids).size
+        val addedBids = newBids.diff(oldBids)
+        val cnt = toRemove.size + addedBids.size
+        val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty
+        dataOut.writeBoolean(needsDecryptionServer)
         dataOut.writeInt(cnt)
-        for (bid <- toRemove) {
-          // remove the broadcast from worker
-          dataOut.writeLong(- bid - 1)  // bid >= 0
-          oldBids.remove(bid)
+        def sendBidsToRemove(): Unit = {
+          for (bid <- toRemove) {
+            // remove the broadcast from worker
+            dataOut.writeLong(-bid - 1) // bid >= 0
+            oldBids.remove(bid)
+          }
         }
-        for (broadcast <- broadcastVars) {
-          if (!oldBids.contains(broadcast.id)) {
+        if (needsDecryptionServer) {
+          // if there is encryption, we setup a server which reads the encrypted files, and sends
+          // the decrypted data to python
+          val idsAndFiles = broadcastVars.flatMap { broadcast =>
+            if (!oldBids.contains(broadcast.id)) {
+              Some((broadcast.id, broadcast.value.path))
+            } else {
+              None
+            }
+          }
+          val server = new EncryptedPythonBroadcastServer(env, idsAndFiles)
+          dataOut.writeInt(server.port)
+          logTrace(s""broadcast decryption server setup on ${server.port}"")
+          PythonRDD.writeUTF(server.secret, dataOut)
+          sendBidsToRemove()
+          idsAndFiles.foreach { case (id, _) =>
             // send new broadcast
-            dataOut.writeLong(broadcast.id)
-            PythonRDD.writeUTF(broadcast.value.path, dataOut)
-            oldBids.add(broadcast.id)
+            dataOut.writeLong(id)
+            oldBids.add(id)
+          }
+          dataOut.flush()
+          logTrace(""waiting for python to read decrypted broadcast data from server"")
+          server.waitTillBroadcastDataSent()
+          logTrace(""done sending decrypted data to python"")
+        } else {
+          sendBidsToRemove()
+          for (broadcast <- broadcastVars) {
+            if (!oldBids.contains(broadcast.id)) {
+              // send new broadcast
+              dataOut.writeLong(broadcast.id)
+              PythonRDD.writeUTF(broadcast.value.path, dataOut)
+              oldBids.add(broadcast.id)
+            }
           }
         }
         dataOut.flush()
@@ -360,7 +394,7 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
       startTime: Long,
       env: SparkEnv,
       worker: Socket,
-      released: AtomicBoolean,
+      releasedOrClosed: AtomicBoolean,
       context: TaskContext)
     extends Iterator[OUT] {
 
@@ -431,9 +465,8 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
       }
       // Check whether the worker is ready to be re-used.
       if (stream.readInt() == SpecialLengths.END_OF_STREAM) {
-        if (reuseWorker) {
+        if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) {
           env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker)
-          released.set(true)
         }
       }
       eos = true
@@ -533,9 +566,9 @@ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions])
       startTime: Long,
       env: SparkEnv,
       worker: Socket,
-      released: AtomicBoolean,
+      releasedOrClosed: AtomicBoolean,
       context: TaskContext): Iterator[Array[Byte]] = {
-    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {
+    new ReaderIterator(stream, writerThread, startTime, env, worker, releasedOrClosed, context) {
 
       protected override def read(): Array[Byte] = {
         if (writerThread.exception.isDefined) {
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala b/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala
index 27a5e19f96a14..b6b0cac910d69 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala
@@ -32,7 +32,8 @@ private[spark] object PythonUtils {
     val pythonPath = new ArrayBuffer[String]
     for (sparkHome <- sys.env.get(""SPARK_HOME"")) {
       pythonPath += Seq(sparkHome, ""python"", ""lib"", ""pyspark.zip"").mkString(File.separator)
-      pythonPath += Seq(sparkHome, ""python"", ""lib"", ""py4j-0.10.7-src.zip"").mkString(File.separator)
+      pythonPath +=
+        Seq(sparkHome, ""python"", ""lib"", ""py4j-0.10.8.1-src.zip"").mkString(File.separator)
     }
     pythonPath ++= SparkContext.jarOfObject(this)
     pythonPath.mkString(File.pathSeparator)
@@ -74,4 +75,8 @@ private[spark] object PythonUtils {
   def toScalaMap[K, V](jm: java.util.Map[K, V]): Map[K, V] = {
     jm.asScala.toMap
   }
+
+  def getEncryptionEnabled(sc: JavaSparkContext): Boolean = {
+    sc.conf.get(org.apache.spark.internal.config.IO_ENCRYPTION_ENABLED)
+  }
 }
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala
index 6afa37aa36fd3..1f2f503a28d49 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala
@@ -21,6 +21,7 @@ import java.io.{DataInputStream, DataOutputStream, EOFException, InputStream, Ou
 import java.net.{InetAddress, ServerSocket, Socket, SocketException}
 import java.nio.charset.StandardCharsets
 import java.util.Arrays
+import javax.annotation.concurrent.GuardedBy
 
 import scala.collection.JavaConverters._
 import scala.collection.mutable
@@ -31,7 +32,7 @@ import org.apache.spark.security.SocketAuthHelper
 import org.apache.spark.util.{RedirectThread, Utils}
 
 private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String, String])
-  extends Logging {
+  extends Logging { self =>
 
   import PythonWorkerFactory._
 
@@ -39,7 +40,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   // pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon
   // currently only works on UNIX-based systems now because it uses signals for child management,
   // so we can also fall back to launching workers, pyspark/worker.py (by default) directly.
-  val useDaemon = {
+  private val useDaemon = {
     val useDaemonEnabled = SparkEnv.get.conf.getBoolean(""spark.python.use.daemon"", true)
 
     // This flag is ignored on Windows as it's unable to fork.
@@ -51,44 +52,52 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   // as expert-only option, and shouldn't be used before knowing what it means exactly.
 
   // This configuration indicates the module to run the daemon to execute its Python workers.
-  val daemonModule = SparkEnv.get.conf.getOption(""spark.python.daemon.module"").map { value =>
-    logInfo(
-      s""Python daemon module in PySpark is set to [$value] in 'spark.python.daemon.module', "" +
-      ""using this to start the daemon up. Note that this configuration only has an effect when "" +
-      ""'spark.python.use.daemon' is enabled and the platform is not Windows."")
-    value
-  }.getOrElse(""pyspark.daemon"")
+  private val daemonModule =
+    SparkEnv.get.conf.getOption(""spark.python.daemon.module"").map { value =>
+      logInfo(
+        s""Python daemon module in PySpark is set to [$value] in 'spark.python.daemon.module', "" +
+        ""using this to start the daemon up. Note that this configuration only has an effect when "" +
+        ""'spark.python.use.daemon' is enabled and the platform is not Windows."")
+      value
+    }.getOrElse(""pyspark.daemon"")
 
   // This configuration indicates the module to run each Python worker.
-  val workerModule = SparkEnv.get.conf.getOption(""spark.python.worker.module"").map { value =>
-    logInfo(
-      s""Python worker module in PySpark is set to [$value] in 'spark.python.worker.module', "" +
-      ""using this to start the worker up. Note that this configuration only has an effect when "" +
-      ""'spark.python.use.daemon' is disabled or the platform is Windows."")
-    value
-  }.getOrElse(""pyspark.worker"")
+  private val workerModule =
+    SparkEnv.get.conf.getOption(""spark.python.worker.module"").map { value =>
+      logInfo(
+        s""Python worker module in PySpark is set to [$value] in 'spark.python.worker.module', "" +
+        ""using this to start the worker up. Note that this configuration only has an effect when "" +
+        ""'spark.python.use.daemon' is disabled or the platform is Windows."")
+      value
+    }.getOrElse(""pyspark.worker"")
 
   private val authHelper = new SocketAuthHelper(SparkEnv.get.conf)
 
-  var daemon: Process = null
+  @GuardedBy(""self"")
+  private var daemon: Process = null
   val daemonHost = InetAddress.getByAddress(Array(127, 0, 0, 1))
-  var daemonPort: Int = 0
-  val daemonWorkers = new mutable.WeakHashMap[Socket, Int]()
-  val idleWorkers = new mutable.Queue[Socket]()
-  var lastActivity = 0L
+  @GuardedBy(""self"")
+  private var daemonPort: Int = 0
+  @GuardedBy(""self"")
+  private val daemonWorkers = new mutable.WeakHashMap[Socket, Int]()
+  @GuardedBy(""self"")
+  private val idleWorkers = new mutable.Queue[Socket]()
+  @GuardedBy(""self"")
+  private var lastActivity = 0L
   new MonitorThread().start()
 
-  var simpleWorkers = new mutable.WeakHashMap[Socket, Process]()
+  @GuardedBy(""self"")
+  private val simpleWorkers = new mutable.WeakHashMap[Socket, Process]()
 
-  val pythonPath = PythonUtils.mergePythonPaths(
+  private val pythonPath = PythonUtils.mergePythonPaths(
     PythonUtils.sparkPythonPath,
     envVars.getOrElse(""PYTHONPATH"", """"),
     sys.env.getOrElse(""PYTHONPATH"", """"))
 
   def create(): Socket = {
     if (useDaemon) {
-      synchronized {
-        if (idleWorkers.size > 0) {
+      self.synchronized {
+        if (idleWorkers.nonEmpty) {
           return idleWorkers.dequeue()
         }
       }
@@ -117,7 +126,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
       socket
     }
 
-    synchronized {
+    self.synchronized {
       // Start the daemon if it hasn't been started
       startDaemon()
 
@@ -163,7 +172,9 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
       try {
         val socket = serverSocket.accept()
         authHelper.authClient(socket)
-        simpleWorkers.put(socket, worker)
+        self.synchronized {
+          simpleWorkers.put(socket, worker)
+        }
         return socket
       } catch {
         case e: Exception =>
@@ -178,7 +189,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }
 
   private def startDaemon() {
-    synchronized {
+    self.synchronized {
       // Is it already running?
       if (daemon != null) {
         return
@@ -278,7 +289,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
 
     override def run() {
       while (true) {
-        synchronized {
+        self.synchronized {
           if (lastActivity + IDLE_WORKER_TIMEOUT_MS < System.currentTimeMillis()) {
             cleanupIdleWorkers()
             lastActivity = System.currentTimeMillis()
@@ -303,7 +314,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }
 
   private def stopDaemon() {
-    synchronized {
+    self.synchronized {
       if (useDaemon) {
         cleanupIdleWorkers()
 
@@ -325,7 +336,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }
 
   def stopWorker(worker: Socket) {
-    synchronized {
+    self.synchronized {
       if (useDaemon) {
         if (daemon != null) {
           daemonWorkers.get(worker).foreach { pid =>
@@ -345,7 +356,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
 
   def releaseWorker(worker: Socket) {
     if (useDaemon) {
-      synchronized {
+      self.synchronized {
         lastActivity = System.currentTimeMillis()
         idleWorkers.enqueue(worker)
       }
diff --git a/core/src/main/scala/org/apache/spark/api/r/RBackend.scala b/core/src/main/scala/org/apache/spark/api/r/RBackend.scala
index 7ce2581555014..50c8fdf5316d6 100644
--- a/core/src/main/scala/org/apache/spark/api/r/RBackend.scala
+++ b/core/src/main/scala/org/apache/spark/api/r/RBackend.scala
@@ -17,7 +17,7 @@
 
 package org.apache.spark.api.r
 
-import java.io.{DataInputStream, DataOutputStream, File, FileOutputStream, IOException}
+import java.io.{DataOutputStream, File, FileOutputStream, IOException}
 import java.net.{InetAddress, InetSocketAddress, ServerSocket, Socket}
 import java.util.concurrent.TimeUnit
 
@@ -32,8 +32,6 @@ import io.netty.handler.timeout.ReadTimeoutHandler
 
 import org.apache.spark.SparkConf
 import org.apache.spark.internal.Logging
-import org.apache.spark.network.util.JavaUtils
-import org.apache.spark.util.Utils
 
 /**
  * Netty-based backend server that is used to communicate between R and Java.
@@ -99,7 +97,7 @@ private[spark] class RBackend {
     if (bootstrap != null && bootstrap.config().group() != null) {
       bootstrap.config().group().shutdownGracefully()
     }
-    if (bootstrap != null && bootstrap.childGroup() != null) {
+    if (bootstrap != null && bootstrap.config().childGroup() != null) {
       bootstrap.config().childGroup().shutdownGracefully()
     }
     bootstrap = null
@@ -147,7 +145,7 @@ private[spark] object RBackend extends Logging {
       new Thread(""wait for socket to close"") {
         setDaemon(true)
         override def run(): Unit = {
-          // any un-catched exception will also shutdown JVM
+          // any uncaught exception will also shutdown JVM
           val buf = new Array[Byte](1024)
           // shutdown JVM if R does not connect back in 10 seconds
           serverSocket.setSoTimeout(10000)
diff --git a/core/src/main/scala/org/apache/spark/api/r/RRDD.scala b/core/src/main/scala/org/apache/spark/api/r/RRDD.scala
index 295355c7bf018..1dc61c7eef33c 100644
--- a/core/src/main/scala/org/apache/spark/api/r/RRDD.scala
+++ b/core/src/main/scala/org/apache/spark/api/r/RRDD.scala
@@ -17,7 +17,9 @@
 
 package org.apache.spark.api.r
 
-import java.io.File
+import java.io.{DataInputStream, File}
+import java.net.Socket
+import java.nio.charset.StandardCharsets.UTF_8
 import java.util.{Map => JMap}
 
 import scala.collection.JavaConverters._
@@ -25,10 +27,11 @@ import scala.reflect.ClassTag
 
 import org.apache.spark._
 import org.apache.spark.api.java.{JavaPairRDD, JavaRDD, JavaSparkContext}
-import org.apache.spark.api.python.PythonRDD
+import org.apache.spark.api.python.{PythonRDD, PythonServer}
 import org.apache.spark.broadcast.Broadcast
 import org.apache.spark.internal.Logging
 import org.apache.spark.rdd.RDD
+import org.apache.spark.security.SocketAuthHelper
 
 private abstract class BaseRRDD[T: ClassTag, U: ClassTag](
     parent: RDD[T],
@@ -163,3 +166,29 @@ private[r] object RRDD {
     PythonRDD.readRDDFromFile(jsc, fileName, parallelism)
   }
 }
+
+/**
+ * Helper for making RDD[Array[Byte]] from some R data, by reading the data from R
+ * over a socket. This is used in preference to writing data to a file when encryption is enabled.
+ */
+private[spark] class RParallelizeServer(sc: JavaSparkContext, parallelism: Int)
+    extends PythonServer[JavaRDD[Array[Byte]]](
+      new RSocketAuthHelper(), ""sparkr-parallelize-server"") {
+
+  override def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = {
+    val in = sock.getInputStream()
+    PythonRDD.readRDDFromInputStream(sc.sc, in, parallelism)
+  }
+}
+
+private[spark] class RSocketAuthHelper extends SocketAuthHelper(SparkEnv.get.conf) {
+  override protected def readUtf8(s: Socket): String = {
+    val din = new DataInputStream(s.getInputStream())
+    val len = din.readInt()
+    val bytes = new Array[Byte](len)
+    din.readFully(bytes)
+    // The R code adds a null terminator to serialized strings, so ignore it here.
+    assert(bytes(bytes.length - 1) == 0) // sanity check.
+    new String(bytes, 0, bytes.length - 1, UTF_8)
+  }
+}
diff --git a/core/src/main/scala/org/apache/spark/api/r/RUtils.scala b/core/src/main/scala/org/apache/spark/api/r/RUtils.scala
index fdd8cf62f0e5f..9bf35af1da925 100644
--- a/core/src/main/scala/org/apache/spark/api/r/RUtils.scala
+++ b/core/src/main/scala/org/apache/spark/api/r/RUtils.scala
@@ -21,6 +21,8 @@ import java.io.File
 import java.util.Arrays
 
 import org.apache.spark.{SparkEnv, SparkException}
+import org.apache.spark.api.java.JavaSparkContext
+import org.apache.spark.api.python.PythonUtils
 
 private[spark] object RUtils {
   // Local path where R binary packages built from R source code contained in the spark
@@ -104,4 +106,6 @@ private[spark] object RUtils {
       case e: Exception => false
     }
   }
+
+  def getEncryptionEnabled(sc: JavaSparkContext): Boolean = PythonUtils.getEncryptionEnabled(sc)
 }
diff --git a/core/src/main/scala/org/apache/spark/api/r/SerDe.scala b/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
index 537ab57f9664d..6e0a3f63988d4 100644
--- a/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
+++ b/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
@@ -74,9 +74,9 @@ private[spark] object SerDe {
       jvmObjectTracker: JVMObjectTracker): Object = {
     dataType match {
       case 'n' => null
-      case 'i' => new java.lang.Integer(readInt(dis))
-      case 'd' => new java.lang.Double(readDouble(dis))
-      case 'b' => new java.lang.Boolean(readBoolean(dis))
+      case 'i' => java.lang.Integer.valueOf(readInt(dis))
+      case 'd' => java.lang.Double.valueOf(readDouble(dis))
+      case 'b' => java.lang.Boolean.valueOf(readBoolean(dis))
       case 'c' => readString(dis)
       case 'e' => readMap(dis, jvmObjectTracker)
       case 'r' => readBytes(dis)
diff --git a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala
index cbd49e070f2eb..26ead57316e18 100644
--- a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala
+++ b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala
@@ -18,6 +18,7 @@
 package org.apache.spark.broadcast
 
 import java.io._
+import java.lang.ref.SoftReference
 import java.nio.ByteBuffer
 import java.util.zip.Adler32
 
@@ -61,9 +62,11 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long)
    * Value of the broadcast object on executors. This is reconstructed by [[readBroadcastBlock]],
    * which builds this value by reading blocks from the driver and/or other executors.
    *
-   * On the driver, if the value is required, it is read lazily from the block manager.
+   * On the driver, if the value is required, it is read lazily from the block manager. We hold
+   * a soft reference so that it can be garbage collected if required, as we can always reconstruct
+   * in the future.
    */
-  @transient private lazy val _value: T = readBroadcastBlock()
+  @transient private var _value: SoftReference[T] = _
 
   /** The compression codec to use, or None if compression is disabled */
   @transient private var compressionCodec: Option[CompressionCodec] = _
@@ -92,8 +95,15 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long)
   /** The checksum for all the blocks. */
   private var checksums: Array[Int] = _
 
-  override protected def getValue() = {
-    _value
+  override protected def getValue() = synchronized {
+    val memoized: T = if (_value == null) null.asInstanceOf[T] else _value.get
+    if (memoized != null) {
+      memoized
+    } else {
+      val newlyRead = readBroadcastBlock()
+      _value = new SoftReference[T](newlyRead)
+      newlyRead
+    }
   }
 
   private def calcChecksum(block: ByteBuffer): Int = {
@@ -205,8 +215,8 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long)
   }
 
   private def readBroadcastBlock(): T = Utils.tryOrIOException {
-    TorrentBroadcast.synchronized {
-      val broadcastCache = SparkEnv.get.broadcastManager.cachedValues
+    val broadcastCache = SparkEnv.get.broadcastManager.cachedValues
+    broadcastCache.synchronized {
 
       Option(broadcastCache.get(broadcastId)).map(_.asInstanceOf[T]).getOrElse {
         setConf(SparkEnv.get.conf)
diff --git a/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala b/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala
index 178bdcfccb603..5a17a6b6e169c 100644
--- a/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala
@@ -61,11 +61,12 @@ private[deploy] object DependencyUtils extends Logging {
       hadoopConf: Configuration,
       secMgr: SecurityManager): String = {
     val targetDir = Utils.createTempDir()
+    val userJarName = userJar.split(File.separatorChar).last
     Option(jars)
       .map {
         resolveGlobPaths(_, hadoopConf)
           .split("","")
-          .filterNot(_.contains(userJar.split(""/"").last))
+          .filterNot(_.contains(userJarName))
           .mkString("","")
       }
       .filterNot(_ == """")
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala b/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala
index 4cc0063d010ef..7bb2a419107d6 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala
@@ -30,24 +30,20 @@ import scala.util.control.NonFatal
 
 import com.google.common.primitives.Longs
 import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.fs.{FileStatus, FileSystem, Path, PathFilter}
+import org.apache.hadoop.fs._
 import org.apache.hadoop.mapred.JobConf
 import org.apache.hadoop.security.{Credentials, UserGroupInformation}
 import org.apache.hadoop.security.token.{Token, TokenIdentifier}
 import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier
 
 import org.apache.spark.{SparkConf, SparkException}
-import org.apache.spark.annotation.DeveloperApi
 import org.apache.spark.internal.Logging
-import org.apache.spark.internal.config._
 import org.apache.spark.util.Utils
 
 /**
- * :: DeveloperApi ::
  * Contains util methods to interact with Hadoop from Spark.
  */
-@DeveloperApi
-class SparkHadoopUtil extends Logging {
+private[spark] class SparkHadoopUtil extends Logging {
   private val sparkConf = new SparkConf(false).loadFromSystemProperties(true)
   val conf: Configuration = newConfiguration(sparkConf)
   UserGroupInformation.setConfiguration(conf)
@@ -387,7 +383,7 @@ class SparkHadoopUtil extends Logging {
 
 }
 
-object SparkHadoopUtil {
+private[spark] object SparkHadoopUtil {
 
   private lazy val instance = new SparkHadoopUtil
 
@@ -412,20 +408,6 @@ object SparkHadoopUtil {
 
   def get: SparkHadoopUtil = instance
 
-  /**
-   * Given an expiration date for the current set of credentials, calculate the time when new
-   * credentials should be created.
-   *
-   * @param expirationDate Drop-dead expiration date
-   * @param conf Spark configuration
-   * @return Timestamp when new credentials should be created.
-   */
-  private[spark] def nextCredentialRenewalTime(expirationDate: Long, conf: SparkConf): Long = {
-    val ct = System.currentTimeMillis
-    val ratio = conf.get(CREDENTIALS_RENEWAL_INTERVAL_RATIO)
-    (ct + (ratio * (expirationDate - ct))).toLong
-  }
-
   /**
    * Returns a Configuration object with Spark configuration applied on top. Unlike
    * the instance method, this will always return a Configuration instance, and not a
@@ -471,4 +453,33 @@ object SparkHadoopUtil {
       hadoopConf.set(key.substring(""spark.hadoop."".length), value)
     }
   }
+
+  // scalastyle:off line.size.limit
+  /**
+   * Create a path that uses replication instead of erasure coding (ec), regardless of the default
+   * configuration in hdfs for the given path.  This can be helpful as hdfs ec doesn't support
+   * hflush(), hsync(), or append()
+   * https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Limitations
+   */
+  // scalastyle:on line.size.limit
+  def createNonECFile(fs: FileSystem, path: Path): FSDataOutputStream = {
+    try {
+      // Use reflection as this uses apis only avialable in hadoop 3
+      val builderMethod = fs.getClass().getMethod(""createFile"", classOf[Path])
+      val builder = builderMethod.invoke(fs, path)
+      val builderCls = builder.getClass()
+      // this may throw a NoSuchMethodException if the path is not on hdfs
+      val replicateMethod = builderCls.getMethod(""replicate"")
+      val buildMethod = builderCls.getMethod(""build"")
+      val b2 = replicateMethod.invoke(builder)
+      buildMethod.invoke(b2).asInstanceOf[FSDataOutputStream]
+    } catch {
+      case  _: NoSuchMethodException =>
+        // No createFile() method, we're using an older hdfs client, which doesn't give us control
+        // over EC vs. replication.  Older hdfs doesn't have EC anyway, so just create a file with
+        // old apis.
+        fs.create(path)
+    }
+  }
+
 }
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
index cf902db8709e7..324f6f8894d34 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
@@ -318,7 +318,7 @@ private[spark] class SparkSubmit extends Logging {
 
       if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
         args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
-        if (args.isPython) {
+        if (args.isPython || isInternal(args.primaryResource)) {
           args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
         }
       }
@@ -335,7 +335,7 @@ private[spark] class SparkSubmit extends Logging {
     val targetDir = Utils.createTempDir()
 
     // assure a keytab is available from any place in a JVM
-    if (clusterManager == YARN || clusterManager == LOCAL || isMesosClient) {
+    if (clusterManager == YARN || clusterManager == LOCAL || isMesosClient || isKubernetesCluster) {
       if (args.principal != null) {
         if (args.keytab != null) {
           require(new File(args.keytab).exists(), s""Keytab file: ${args.keytab} does not exist"")
@@ -520,6 +520,10 @@ private[spark] class SparkSubmit extends Logging {
         confKey = ""spark.driver.extraJavaOptions""),
       OptionAssigner(args.driverExtraLibraryPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
         confKey = ""spark.driver.extraLibraryPath""),
+      OptionAssigner(args.principal, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
+        confKey = PRINCIPAL.key),
+      OptionAssigner(args.keytab, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
+        confKey = KEYTAB.key),
 
       // Propagate attributes for dependency resolution at the driver side
       OptionAssigner(args.packages, STANDALONE | MESOS, CLUSTER, confKey = ""spark.jars.packages""),
@@ -537,8 +541,6 @@ private[spark] class SparkSubmit extends Logging {
       OptionAssigner(args.jars, YARN, ALL_DEPLOY_MODES, confKey = ""spark.yarn.dist.jars""),
       OptionAssigner(args.files, YARN, ALL_DEPLOY_MODES, confKey = ""spark.yarn.dist.files""),
       OptionAssigner(args.archives, YARN, ALL_DEPLOY_MODES, confKey = ""spark.yarn.dist.archives""),
-      OptionAssigner(args.principal, YARN, ALL_DEPLOY_MODES, confKey = ""spark.yarn.principal""),
-      OptionAssigner(args.keytab, YARN, ALL_DEPLOY_MODES, confKey = ""spark.yarn.keytab""),
 
       // Other options
       OptionAssigner(args.executorCores, STANDALONE | YARN | KUBERNETES, ALL_DEPLOY_MODES,
@@ -644,7 +646,8 @@ private[spark] class SparkSubmit extends Logging {
       }
     }
 
-    if (clusterManager == MESOS && UserGroupInformation.isSecurityEnabled) {
+    if ((clusterManager == MESOS || clusterManager == KUBERNETES)
+       && UserGroupInformation.isSecurityEnabled) {
       setRMPrincipal(sparkConf)
     }
 
@@ -760,8 +763,8 @@ private[spark] class SparkSubmit extends Logging {
   }
 
   // [SPARK-20328]. HadoopRDD calls into a Hadoop library that fetches delegation tokens with
-  // renewer set to the YARN ResourceManager.  Since YARN isn't configured in Mesos mode, we
-  // must trick it into thinking we're YARN.
+  // renewer set to the YARN ResourceManager.  Since YARN isn't configured in Mesos or Kubernetes
+  // mode, we must trick it into thinking we're YARN.
   private def setRMPrincipal(sparkConf: SparkConf): Unit = {
     val shortUserName = UserGroupInformation.getCurrentUser.getShortUserName
     val key = s""spark.hadoop.${YarnConfiguration.RM_PRINCIPAL}""
@@ -826,7 +829,7 @@ private[spark] class SparkSubmit extends Logging {
     }
 
     val app: SparkApplication = if (classOf[SparkApplication].isAssignableFrom(mainClass)) {
-      mainClass.newInstance().asInstanceOf[SparkApplication]
+      mainClass.getConstructor().newInstance().asInstanceOf[SparkApplication]
     } else {
       // SPARK-4170
       if (classOf[scala.App].isAssignableFrom(mainClass)) {
@@ -925,8 +928,6 @@ object SparkSubmit extends CommandLineUtils with Logging {
         } catch {
           case e: SparkUserAppException =>
             exitFn(e.exitCode)
-          case e: SparkException =>
-            printErrorAndExit(e.getMessage())
         }
       }
 
@@ -991,9 +992,9 @@ private[spark] object SparkSubmitUtils {
 
   // Exposed for testing.
   // These components are used to make the default exclusion rules for Spark dependencies.
-  // We need to specify each component explicitly, otherwise we miss spark-streaming-kafka-0-8 and
-  // other spark-streaming utility components. Underscore is there to differentiate between
-  // spark-streaming_2.1x and spark-streaming-kafka-0-8-assembly_2.1x
+  // We need to specify each component explicitly, otherwise we miss
+  // spark-streaming utility components. Underscore is there to differentiate between
+  // spark-streaming_2.1x and spark-streaming-kafka-0-10-assembly_2.1x
   val IVY_DEFAULT_EXCLUDES = Seq(""catalyst_"", ""core_"", ""graphx_"", ""kvstore_"", ""launcher_"", ""mllib_"",
     ""mllib-local_"", ""network-common_"", ""network-shuffle_"", ""repl_"", ""sketch_"", ""sql_"", ""streaming_"",
     ""tags_"", ""unsafe_"")
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
index 0998757715457..4cf08a7980f55 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
@@ -199,8 +199,14 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
     numExecutors = Option(numExecutors)
       .getOrElse(sparkProperties.get(""spark.executor.instances"").orNull)
     queue = Option(queue).orElse(sparkProperties.get(""spark.yarn.queue"")).orNull
-    keytab = Option(keytab).orElse(sparkProperties.get(""spark.yarn.keytab"")).orNull
-    principal = Option(principal).orElse(sparkProperties.get(""spark.yarn.principal"")).orNull
+    keytab = Option(keytab)
+      .orElse(sparkProperties.get(""spark.kerberos.keytab""))
+      .orElse(sparkProperties.get(""spark.yarn.keytab""))
+      .orNull
+    principal = Option(principal)
+      .orElse(sparkProperties.get(""spark.kerberos.principal""))
+      .orElse(sparkProperties.get(""spark.yarn.principal""))
+      .orNull
     dynamicAllocationEnabled =
       sparkProperties.get(""spark.dynamicAllocation.enabled"").exists(""true"".equalsIgnoreCase)
 
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala b/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala
index 44d23908146c7..da6e5f03aabb5 100644
--- a/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala
@@ -19,7 +19,6 @@ package org.apache.spark.deploy.history
 
 import java.io.{File, FileNotFoundException, IOException}
 import java.nio.file.Files
-import java.nio.file.attribute.PosixFilePermissions
 import java.util.{Date, ServiceLoader}
 import java.util.concurrent.{ConcurrentHashMap, ExecutorService, Future, TimeUnit}
 import java.util.zip.{ZipEntry, ZipOutputStream}
@@ -35,7 +34,7 @@ import com.fasterxml.jackson.annotation.JsonIgnore
 import com.google.common.io.ByteStreams
 import com.google.common.util.concurrent.MoreExecutors
 import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}
-import org.apache.hadoop.hdfs.DistributedFileSystem
+import org.apache.hadoop.hdfs.{DFSInputStream, DistributedFileSystem}
 import org.apache.hadoop.hdfs.protocol.HdfsConstants
 import org.apache.hadoop.security.AccessControlException
 import org.fusesource.leveldbjni.internal.NativeDB
@@ -43,13 +42,15 @@ import org.fusesource.leveldbjni.internal.NativeDB
 import org.apache.spark.{SecurityManager, SparkConf, SparkException}
 import org.apache.spark.deploy.SparkHadoopUtil
 import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config.DRIVER_LOG_DFS_DIR
+import org.apache.spark.internal.config.History._
+import org.apache.spark.internal.config.Status._
 import org.apache.spark.io.CompressionCodec
 import org.apache.spark.scheduler._
 import org.apache.spark.scheduler.ReplayListenerBus._
 import org.apache.spark.status._
 import org.apache.spark.status.KVUtils._
 import org.apache.spark.status.api.v1.{ApplicationAttemptInfo, ApplicationInfo}
-import org.apache.spark.status.config._
 import org.apache.spark.ui.SparkUI
 import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}
 import org.apache.spark.util.kvstore._
@@ -87,7 +88,6 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
     this(conf, new SystemClock())
   }
 
-  import config._
   import FsHistoryProvider._
 
   // Interval between safemode checks.
@@ -98,7 +98,7 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
   private val UPDATE_INTERVAL_S = conf.getTimeAsSeconds(""spark.history.fs.update.interval"", ""10s"")
 
   // Interval between each cleaner checks for event logs to delete
-  private val CLEAN_INTERVAL_S = conf.getTimeAsSeconds(""spark.history.fs.cleaner.interval"", ""1d"")
+  private val CLEAN_INTERVAL_S = conf.get(CLEANER_INTERVAL_S)
 
   // Number of threads used to replay event logs.
   private val NUM_PROCESSING_THREADS = conf.getInt(SPARK_HISTORY_FS_NUM_REPLAY_THREADS,
@@ -133,9 +133,8 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
 
   // Visible for testing.
   private[history] val listing: KVStore = storePath.map { path =>
-    val perms = PosixFilePermissions.fromString(""rwx------"")
-    val dbPath = Files.createDirectories(new File(path, ""listing.ldb"").toPath(),
-      PosixFilePermissions.asFileAttribute(perms)).toFile()
+    val dbPath = Files.createDirectories(new File(path, ""listing.ldb"").toPath()).toFile()
+    Utils.chmod700(dbPath)
 
     val metadata = new FsHistoryProviderMetadata(CURRENT_LISTING_VERSION,
       AppStatusStore.CURRENT_VERSION, logDir.toString())
@@ -276,11 +275,18 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
       pool.scheduleWithFixedDelay(
         getRunner(() => checkForLogs()), 0, UPDATE_INTERVAL_S, TimeUnit.SECONDS)
 
-      if (conf.getBoolean(""spark.history.fs.cleaner.enabled"", false)) {
+      if (conf.get(CLEANER_ENABLED)) {
         // A task that periodically cleans event logs on disk.
         pool.scheduleWithFixedDelay(
           getRunner(() => cleanLogs()), 0, CLEAN_INTERVAL_S, TimeUnit.SECONDS)
       }
+
+      if (conf.contains(DRIVER_LOG_DFS_DIR) && conf.get(DRIVER_LOG_CLEANER_ENABLED)) {
+        pool.scheduleWithFixedDelay(getRunner(() => cleanDriverLogs()),
+          0,
+          conf.get(DRIVER_LOG_CLEANER_INTERVAL),
+          TimeUnit.SECONDS)
+      }
     } else {
       logDebug(""Background update thread disabled for testing"")
     }
@@ -451,10 +457,32 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
               listing.write(info.copy(lastProcessed = newLastScanTime, fileSize = entry.getLen()))
             }
 
-            if (info.fileSize < entry.getLen()) {
+            if (shouldReloadLog(info, entry)) {
               if (info.appId.isDefined && fastInProgressParsing) {
                 // When fast in-progress parsing is on, we don't need to re-parse when the
                 // size changes, but we do need to invalidate any existing UIs.
+                // Also, we need to update the `lastUpdated time` to display the updated time in
+                // the HistoryUI and to avoid cleaning the inprogress app while running.
+                val appInfo = listing.read(classOf[ApplicationInfoWrapper], info.appId.get)
+
+                val attemptList = appInfo.attempts.map { attempt =>
+                  if (attempt.info.attemptId == info.attemptId) {
+                    new AttemptInfoWrapper(
+                      attempt.info.copy(lastUpdated = new Date(newLastScanTime)),
+                      attempt.logPath,
+                      attempt.fileSize,
+                      attempt.adminAcls,
+                      attempt.viewAcls,
+                      attempt.adminAclsGroups,
+                      attempt.viewAclsGroups)
+                  } else {
+                    attempt
+                  }
+                }
+
+                val updatedAppInfo = new ApplicationInfoWrapper(appInfo.info, attemptList)
+                listing.write(updatedAppInfo)
+
                 invalidateUI(info.appId.get, info.attemptId)
                 false
               } else {
@@ -468,8 +496,8 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
               // If the file is currently not being tracked by the SHS, add an entry for it and try
               // to parse it. This will allow the cleaner code to detect the file as stale later on
               // if it was not possible to parse it.
-              listing.write(LogInfo(entry.getPath().toString(), newLastScanTime, None, None,
-                entry.getLen()))
+              listing.write(LogInfo(entry.getPath().toString(), newLastScanTime, LogType.EventLogs,
+                None, None, entry.getLen()))
               entry.getLen() > 0
           }
         }
@@ -543,6 +571,24 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
     }
   }
 
+  private[history] def shouldReloadLog(info: LogInfo, entry: FileStatus): Boolean = {
+    var result = info.fileSize < entry.getLen
+    if (!result && info.logPath.endsWith(EventLoggingListener.IN_PROGRESS)) {
+      try {
+        result = Utils.tryWithResource(fs.open(entry.getPath)) { in =>
+          in.getWrappedStream match {
+            case dfsIn: DFSInputStream => info.fileSize < dfsIn.getFileLength
+            case _ => false
+          }
+        }
+      } catch {
+        case e: Exception =>
+          logDebug(s""Failed to check the length for the file : ${info.logPath}"", e)
+      }
+    }
+    result
+  }
+
   private def cleanAppData(appId: String, attemptId: Option[String], logPath: String): Unit = {
     try {
       val app = load(appId)
@@ -708,7 +754,7 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
         // listing data is good.
         invalidateUI(app.info.id, app.attempts.head.info.attemptId)
         addListing(app)
-        listing.write(LogInfo(logPath.toString(), scanTime, Some(app.info.id),
+        listing.write(LogInfo(logPath.toString(), scanTime, LogType.EventLogs, Some(app.info.id),
           app.attempts.head.info.attemptId, fileStatus.getLen()))
 
         // For a finished log, remove the corresponding ""in progress"" entry from the listing DB if
@@ -737,7 +783,8 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
         // If the app hasn't written down its app ID to the logs, still record the entry in the
         // listing db, with an empty ID. This will make the log eligible for deletion if the app
         // does not make progress after the configured max log age.
-        listing.write(LogInfo(logPath.toString(), scanTime, None, None, fileStatus.getLen()))
+        listing.write(
+          LogInfo(logPath.toString(), scanTime, LogType.EventLogs, None, None, fileStatus.getLen()))
     }
   }
 
@@ -782,7 +829,7 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
         val logPath = new Path(logDir, attempt.logPath)
         listing.delete(classOf[LogInfo], logPath.toString())
         cleanAppData(app.id, attempt.info.attemptId, logPath.toString())
-        deleteLog(logPath)
+        deleteLog(fs, logPath)
       }
 
       if (remaining.isEmpty) {
@@ -796,11 +843,12 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
       .reverse()
       .first(maxTime)
       .asScala
+      .filter { l => l.logType == null || l.logType == LogType.EventLogs }
       .toList
     stale.foreach { log =>
       if (log.appId.isEmpty) {
         logInfo(s""Deleting invalid / corrupt event log ${log.logPath}"")
-        deleteLog(new Path(log.logPath))
+        deleteLog(fs, new Path(log.logPath))
         listing.delete(classOf[LogInfo], log.logPath)
       }
     }
@@ -808,6 +856,61 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
     clearBlacklist(CLEAN_INTERVAL_S)
   }
 
+  /**
+   * Delete driver logs from the configured spark dfs dir that exceed the configured max age
+   */
+  private[history] def cleanDriverLogs(): Unit = Utils.tryLog {
+    val driverLogDir = conf.get(DRIVER_LOG_DFS_DIR).get
+    val driverLogFs = new Path(driverLogDir).getFileSystem(hadoopConf)
+    val currentTime = clock.getTimeMillis()
+    val maxTime = currentTime - conf.get(MAX_DRIVER_LOG_AGE_S) * 1000
+    val logFiles = driverLogFs.listLocatedStatus(new Path(driverLogDir))
+    while (logFiles.hasNext()) {
+      val f = logFiles.next()
+      // Do not rely on 'modtime' as it is not updated for all filesystems when files are written to
+      val deleteFile =
+        try {
+          val info = listing.read(classOf[LogInfo], f.getPath().toString())
+          // Update the lastprocessedtime of file if it's length or modification time has changed
+          if (info.fileSize < f.getLen() || info.lastProcessed < f.getModificationTime()) {
+            listing.write(
+              info.copy(lastProcessed = currentTime, fileSize = f.getLen()))
+            false
+          } else if (info.lastProcessed > maxTime) {
+            false
+          } else {
+            true
+          }
+        } catch {
+          case e: NoSuchElementException =>
+            // For every new driver log file discovered, create a new entry in listing
+            listing.write(LogInfo(f.getPath().toString(), currentTime, LogType.DriverLogs, None,
+              None, f.getLen()))
+          false
+        }
+      if (deleteFile) {
+        logInfo(s""Deleting expired driver log for: ${f.getPath().getName()}"")
+        listing.delete(classOf[LogInfo], f.getPath().toString())
+        deleteLog(driverLogFs, f.getPath())
+      }
+    }
+
+    // Delete driver log file entries that exceed the configured max age and
+    // may have been deleted on filesystem externally.
+    val stale = listing.view(classOf[LogInfo])
+      .index(""lastProcessed"")
+      .reverse()
+      .first(maxTime)
+      .asScala
+      .filter { l => l.logType != null && l.logType == LogType.DriverLogs }
+      .toList
+    stale.foreach { log =>
+      logInfo(s""Deleting invalid driver log ${log.logPath}"")
+      listing.delete(classOf[LogInfo], log.logPath)
+      deleteLog(driverLogFs, new Path(log.logPath))
+    }
+  }
+
   /**
    * Rebuilds the application state store from its event log.
    */
@@ -964,7 +1067,7 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
       throw new NoSuchElementException(s""Cannot find attempt $attemptId of $appId.""))
   }
 
-  private def deleteLog(log: Path): Unit = {
+  private def deleteLog(fs: FileSystem, log: Path): Unit = {
     if (isBlacklisted(log)) {
       logDebug(s""Skipping deleting $log as we don't have permissions on it."")
     } else {
@@ -1009,6 +1112,10 @@ private[history] case class FsHistoryProviderMetadata(
     uiVersion: Long,
     logDir: String)
 
+private[history] object LogType extends Enumeration {
+  val DriverLogs, EventLogs = Value
+}
+
 /**
  * Tracking info for event logs detected in the configured log directory. Tracks both valid and
  * invalid logs (e.g. unparseable logs, recorded as logs with no app ID) so that the cleaner
@@ -1017,6 +1124,7 @@ private[history] case class FsHistoryProviderMetadata(
 private[history] case class LogInfo(
     @KVIndexParam logPath: String,
     @KVIndexParam(""lastProcessed"") lastProcessed: Long,
+    logType: LogType.Value,
     appId: Option[String],
     attemptId: Option[String],
     fileSize: Long)
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala
index 32667ddf5c7ea..00ca4efa4d266 100644
--- a/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala
@@ -31,8 +31,8 @@ private[history] class HistoryPage(parent: HistoryServer) extends WebUIPage("""")
     val requestedIncomplete =
       Option(UIUtils.stripXSS(request.getParameter(""showIncomplete""))).getOrElse(""false"").toBoolean
 
-    val allAppsSize = parent.getApplicationList()
-      .count(isApplicationCompleted(_) != requestedIncomplete)
+    val displayApplications = parent.getApplicationList()
+      .exists(isApplicationCompleted(_) != requestedIncomplete)
     val eventLogsUnderProcessCount = parent.getEventLogsUnderProcess()
     val lastUpdatedTime = parent.getLastUpdatedTime()
     val providerConfig = parent.getProviderConfig()
@@ -63,9 +63,9 @@ private[history] class HistoryPage(parent: HistoryServer) extends WebUIPage("""")
             }
 
             {
-            if (allAppsSize > 0) {
+            if (displayApplications) {
               <script src={UIUtils.prependBaseUri(
-                  request, ""/static/dataTables.rowsGroup.js"")}></script> ++
+                request, ""/static/dataTables.rowsGroup.js"")}></script> ++
                 <div id=""history-summary"" class=""row-fluid""></div> ++
                 <script src={UIUtils.prependBaseUri(request, ""/static/historypage.js"")}></script> ++
                 <script>setAppLimit({parent.maxApplications})</script>
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala
index 56f3f59504a7d..5856c7057b745 100644
--- a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala
@@ -28,9 +28,9 @@ import org.eclipse.jetty.servlet.{ServletContextHandler, ServletHolder}
 
 import org.apache.spark.{SecurityManager, SparkConf}
 import org.apache.spark.deploy.SparkHadoopUtil
-import org.apache.spark.deploy.history.config.HISTORY_SERVER_UI_PORT
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
+import org.apache.spark.internal.config.History.HISTORY_SERVER_UI_PORT
 import org.apache.spark.status.api.v1.{ApiRootResource, ApplicationInfo, UIRoot}
 import org.apache.spark.ui.{SparkUI, UIUtils, WebUI}
 import org.apache.spark.ui.JettyUtils._
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala
index 080ba12c2f0d1..49f00cb10179e 100644
--- a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala
@@ -34,35 +34,21 @@ private[history] class HistoryServerArguments(conf: SparkConf, args: Array[Strin
 
   @tailrec
   private def parse(args: List[String]): Unit = {
-    if (args.length == 1) {
-      setLogDirectory(args.head)
-    } else {
-      args match {
-        case (""--dir"" | ""-d"") :: value :: tail =>
-          setLogDirectory(value)
-          parse(tail)
+    args match {
+      case (""--help"" | ""-h"") :: tail =>
+        printUsageAndExit(0)
 
-        case (""--help"" | ""-h"") :: tail =>
-          printUsageAndExit(0)
+      case (""--properties-file"") :: value :: tail =>
+        propertiesFile = value
+        parse(tail)
 
-        case (""--properties-file"") :: value :: tail =>
-          propertiesFile = value
-          parse(tail)
+      case Nil =>
 
-        case Nil =>
-
-        case _ =>
-          printUsageAndExit(1)
-      }
+      case _ =>
+        printUsageAndExit(1)
     }
   }
 
-  private def setLogDirectory(value: String): Unit = {
-    logWarning(""Setting log directory through the command line is deprecated as of "" +
-      ""Spark 1.1.0. Please set this through spark.history.fs.logDirectory instead."")
-    conf.set(""spark.history.fs.logDirectory"", value)
-  }
-
    // This mutates the SparkConf, so all accesses to it must be made after this line
    Utils.loadDefaultSparkProperties(conf, propertiesFile)
 
@@ -73,8 +59,6 @@ private[history] class HistoryServerArguments(conf: SparkConf, args: Array[Strin
       |Usage: HistoryServer [options]
       |
       |Options:
-      |  DIR                         Deprecated; set spark.history.fs.logDirectory directly
-      |  --dir DIR (-d DIR)          Deprecated; set spark.history.fs.logDirectory directly
       |  --properties-file FILE      Path to a custom Spark properties file.
       |                              Default is conf/spark-defaults.conf.
       |
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala
index c03a360b91ef8..0a1f33395ad62 100644
--- a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala
@@ -18,8 +18,6 @@
 package org.apache.spark.deploy.history
 
 import java.io.File
-import java.nio.file.Files
-import java.nio.file.attribute.PosixFilePermissions
 import java.util.concurrent.atomic.AtomicLong
 
 import scala.collection.JavaConverters._
@@ -29,6 +27,7 @@ import org.apache.commons.io.FileUtils
 
 import org.apache.spark.SparkConf
 import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config.History._
 import org.apache.spark.status.KVUtils._
 import org.apache.spark.util.{Clock, Utils}
 import org.apache.spark.util.kvstore.KVStore
@@ -52,8 +51,6 @@ private class HistoryServerDiskManager(
     listing: KVStore,
     clock: Clock) extends Logging {
 
-  import config._
-
   private val appStoreDir = new File(path, ""apps"")
   if (!appStoreDir.isDirectory() && !appStoreDir.mkdir()) {
     throw new IllegalArgumentException(s""Failed to create app directory ($appStoreDir)."")
@@ -107,9 +104,8 @@ private class HistoryServerDiskManager(
     val needed = approximateSize(eventLogSize, isCompressed)
     makeRoom(needed)
 
-    val perms = PosixFilePermissions.fromString(""rwx------"")
-    val tmp = Files.createTempDirectory(tmpStoreDir.toPath(), ""appstore"",
-      PosixFilePermissions.asFileAttribute(perms)).toFile()
+    val tmp = Utils.createTempDir(tmpStoreDir.getPath(), ""appstore"")
+    Utils.chmod700(tmp)
 
     updateUsage(needed)
     val current = currentUsage.get()
diff --git a/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala b/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala
index 31a8e3e60c067..afa413fe165df 100644
--- a/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala
@@ -408,6 +408,10 @@ private[spark] class RestSubmissionClient(master: String) extends Logging {
 }
 
 private[spark] object RestSubmissionClient {
+
+  // SPARK_HOME and SPARK_CONF_DIR are filtered out because they are usually wrong
+  // on the remote machine (SPARK-12345) (SPARK-25934)
+  private val BLACKLISTED_SPARK_ENV_VARS = Set(""SPARK_ENV_LOADED"", ""SPARK_HOME"", ""SPARK_CONF_DIR"")
   private val REPORT_DRIVER_STATUS_INTERVAL = 1000
   private val REPORT_DRIVER_STATUS_MAX_TRIES = 10
   val PROTOCOL_VERSION = ""v1""
@@ -417,9 +421,7 @@ private[spark] object RestSubmissionClient {
    */
   private[rest] def filterSystemEnvironment(env: Map[String, String]): Map[String, String] = {
     env.filterKeys { k =>
-      // SPARK_HOME is filtered out because it is usually wrong on the remote machine (SPARK-12345)
-      (k.startsWith(""SPARK_"") && k != ""SPARK_ENV_LOADED"" && k != ""SPARK_HOME"") ||
-        k.startsWith(""MESOS_"")
+      (k.startsWith(""SPARK_"") && !BLACKLISTED_SPARK_ENV_VARS.contains(k)) || k.startsWith(""MESOS_"")
     }
   }
 }
diff --git a/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala b/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala
index 22b65abce611a..afa1a5fbba792 100644
--- a/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala
@@ -138,6 +138,16 @@ private[rest] class StandaloneSubmitRequestServlet(
     val driverExtraClassPath = sparkProperties.get(""spark.driver.extraClassPath"")
     val driverExtraLibraryPath = sparkProperties.get(""spark.driver.extraLibraryPath"")
     val superviseDriver = sparkProperties.get(""spark.driver.supervise"")
+    // The semantics of ""spark.master"" and the masterUrl are different. While the
+    // property ""spark.master"" could contain all registered masters, masterUrl
+    // contains only the active master. To make sure a Spark driver can recover
+    // in a multi-master setup, we use the ""spark.master"" property while submitting
+    // the driver.
+    val masters = sparkProperties.get(""spark.master"")
+    val (_, masterPort) = Utils.extractHostPortFromSparkUrl(masterUrl)
+    val masterRestPort = this.conf.getInt(""spark.master.rest.port"", 6066)
+    val updatedMasters = masters.map(
+      _.replace(s"":$masterRestPort"", s"":$masterPort"")).getOrElse(masterUrl)
     val appArgs = request.appArgs
     // Filter SPARK_LOCAL_(IP|HOSTNAME) environment variables from being set on the remote system.
     val environmentVariables =
@@ -146,7 +156,7 @@ private[rest] class StandaloneSubmitRequestServlet(
     // Construct driver description
     val conf = new SparkConf(false)
       .setAll(sparkProperties)
-      .set(""spark.master"", masterUrl)
+      .set(""spark.master"", updatedMasters)
     val extraClassPath = driverExtraClassPath.toSeq.flatMap(_.split(File.pathSeparator))
     val extraLibraryPath = driverExtraLibraryPath.toSeq.flatMap(_.split(File.pathSeparator))
     val extraJavaOpts = driverExtraJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)
diff --git a/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala b/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala
index ef5a7e35ad562..97b689cdadd5f 100644
--- a/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala
@@ -36,7 +36,7 @@ import org.apache.spark.util.Utils
  *   (2) the Spark version of the client / server
  *   (3) an optional message
  */
-@JsonInclude(Include.NON_NULL)
+@JsonInclude(Include.NON_ABSENT)
 @JsonAutoDetect(getterVisibility = Visibility.ANY, setterVisibility = Visibility.ANY)
 @JsonPropertyOrder(alphabetic = true)
 private[rest] abstract class SubmitRestProtocolMessage {
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala b/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala
index ab8d8d96a9b08..126a6ab801369 100644
--- a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala
@@ -17,76 +17,158 @@
 
 package org.apache.spark.deploy.security
 
+import java.io.File
+import java.security.PrivilegedExceptionAction
+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}
+import java.util.concurrent.atomic.AtomicReference
+
 import org.apache.hadoop.conf.Configuration
 import org.apache.hadoop.fs.FileSystem
-import org.apache.hadoop.security.Credentials
+import org.apache.hadoop.security.{Credentials, UserGroupInformation}
 
 import org.apache.spark.SparkConf
+import org.apache.spark.deploy.SparkHadoopUtil
 import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config._
+import org.apache.spark.rpc.RpcEndpointRef
+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens
+import org.apache.spark.ui.UIUtils
+import org.apache.spark.util.ThreadUtils
 
 /**
- * Manages all the registered HadoopDelegationTokenProviders and offer APIs for other modules to
- * obtain delegation tokens and their renewal time. By default [[HadoopFSDelegationTokenProvider]],
- * [[HiveDelegationTokenProvider]] and [[HBaseDelegationTokenProvider]] will be loaded in if not
- * explicitly disabled.
+ * Manager for delegation tokens in a Spark application.
+ *
+ * This manager has two modes of operation:
+ *
+ * 1.  When configured with a principal and a keytab, it will make sure long-running apps can run
+ * without interruption while accessing secured services. It periodically logs in to the KDC with
+ * user-provided credentials, and contacts all the configured secure services to obtain delegation
+ * tokens to be distributed to the rest of the application.
+ *
+ * Because the Hadoop UGI API does not expose the TTL of the TGT, a configuration controls how often
+ * to check that a relogin is necessary. This is done reasonably often since the check is a no-op
+ * when the relogin is not yet needed. The check period can be overridden in the configuration.
  *
- * Also, each HadoopDelegationTokenProvider is controlled by
- * spark.security.credentials.{service}.enabled, and will not be loaded if this config is set to
- * false. For example, Hive's delegation token provider [[HiveDelegationTokenProvider]] can be
- * enabled/disabled by the configuration spark.security.credentials.hive.enabled.
+ * New delegation tokens are created once 75% of the renewal interval of the original tokens has
+ * elapsed. The new tokens are sent to the Spark driver endpoint once it's registered with the AM.
+ * The driver is tasked with distributing the tokens to other processes that might need them.
  *
- * @param sparkConf Spark configuration
- * @param hadoopConf Hadoop configuration
- * @param fileSystems Delegation tokens will be fetched for these Hadoop filesystems.
+ * 2. When operating without an explicit principal and keytab, token renewal will not be available.
+ * Starting the manager will distribute an initial set of delegation tokens to the provided Spark
+ * driver, but the app will not get new tokens when those expire.
+ *
+ * It can also be used just to create delegation tokens, by calling the `obtainDelegationTokens`
+ * method. This option does not require calling the `start` method, but leaves it up to the
+ * caller to distribute the tokens that were generated.
  */
 private[spark] class HadoopDelegationTokenManager(
-    sparkConf: SparkConf,
-    hadoopConf: Configuration,
-    fileSystems: Configuration => Set[FileSystem])
-  extends Logging {
+    protected val sparkConf: SparkConf,
+    protected val hadoopConf: Configuration) extends Logging {
 
   private val deprecatedProviderEnabledConfigs = List(
     ""spark.yarn.security.tokens.%s.enabled"",
     ""spark.yarn.security.credentials.%s.enabled"")
   private val providerEnabledConfig = ""spark.security.credentials.%s.enabled""
 
-  // Maintain all the registered delegation token providers
-  private val delegationTokenProviders = getDelegationTokenProviders
+  private val principal = sparkConf.get(PRINCIPAL).orNull
+  private val keytab = sparkConf.get(KEYTAB).orNull
+
+  require((principal == null) == (keytab == null),
+    ""Both principal and keytab must be defined, or neither."")
+  require(keytab == null || new File(keytab).isFile(), s""Cannot find keytab at $keytab."")
+
+  private val delegationTokenProviders = loadProviders()
   logDebug(""Using the following builtin delegation token providers: "" +
     s""${delegationTokenProviders.keys.mkString("", "")}."")
 
-  /** Construct a [[HadoopDelegationTokenManager]] for the default Hadoop filesystem */
-  def this(sparkConf: SparkConf, hadoopConf: Configuration) = {
-    this(
-      sparkConf,
-      hadoopConf,
-      hadoopConf => Set(FileSystem.get(hadoopConf).getHomeDirectory.getFileSystem(hadoopConf)))
+  private var renewalExecutor: ScheduledExecutorService = _
+  private val driverRef = new AtomicReference[RpcEndpointRef]()
+
+  /** Set the endpoint used to send tokens to the driver. */
+  def setDriverRef(ref: RpcEndpointRef): Unit = {
+    driverRef.set(ref)
   }
 
-  private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {
-    val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++
-      safeCreateProvider(new HiveDelegationTokenProvider) ++
-      safeCreateProvider(new HBaseDelegationTokenProvider)
+  /** @return Whether delegation token renewal is enabled. */
+  def renewalEnabled: Boolean = principal != null
 
-    // Filter out providers for which spark.security.credentials.{service}.enabled is false.
-    providers
-      .filter { p => isServiceEnabled(p.serviceName) }
-      .map { p => (p.serviceName, p) }
-      .toMap
+  /**
+   * Start the token renewer. Requires a principal and keytab. Upon start, the renewer will:
+   *
+   * - log in the configured principal, and set up a task to keep that user's ticket renewed
+   * - obtain delegation tokens from all available providers
+   * - send the tokens to the driver, if it's already registered
+   * - schedule a periodic task to update the tokens when needed.
+   *
+   * @return The newly logged in user.
+   */
+  def start(): UserGroupInformation = {
+    require(renewalEnabled, ""Token renewal must be enabled to start the renewer."")
+    renewalExecutor =
+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(""Credential Renewal Thread"")
+
+    val originalCreds = UserGroupInformation.getCurrentUser().getCredentials()
+    val ugi = doLogin()
+
+    val tgtRenewalTask = new Runnable() {
+      override def run(): Unit = {
+        ugi.checkTGTAndReloginFromKeytab()
+      }
+    }
+    val tgtRenewalPeriod = sparkConf.get(KERBEROS_RELOGIN_PERIOD)
+    renewalExecutor.scheduleAtFixedRate(tgtRenewalTask, tgtRenewalPeriod, tgtRenewalPeriod,
+      TimeUnit.SECONDS)
+
+    val creds = obtainTokensAndScheduleRenewal(ugi)
+    ugi.addCredentials(creds)
+
+    val driver = driverRef.get()
+    if (driver != null) {
+      val tokens = SparkHadoopUtil.get.serialize(creds)
+      driver.send(UpdateDelegationTokens(tokens))
+    }
+
+    // Transfer the original user's tokens to the new user, since it may contain needed tokens
+    // (such as those user to connect to YARN). Explicitly avoid overwriting tokens that already
+    // exist in the current user's credentials, since those were freshly obtained above
+    // (see SPARK-23361).
+    val existing = ugi.getCredentials()
+    existing.mergeAll(originalCreds)
+    ugi.addCredentials(existing)
+    ugi
   }
 
-  private def safeCreateProvider(
-      createFn: => HadoopDelegationTokenProvider): Option[HadoopDelegationTokenProvider] = {
-    try {
-      Some(createFn)
-    } catch {
-      case t: Throwable =>
-        logDebug(s""Failed to load built in provider."", t)
-        None
+  def stop(): Unit = {
+    if (renewalExecutor != null) {
+      renewalExecutor.shutdown()
     }
   }
 
-  def isServiceEnabled(serviceName: String): Boolean = {
+  /**
+   * Fetch new delegation tokens for configured services, storing them in the given credentials.
+   * Tokens are fetched for the current logged in user.
+   *
+   * @param creds Credentials object where to store the delegation tokens.
+   * @return The time by which the tokens must be renewed.
+   */
+  def obtainDelegationTokens(creds: Credentials): Long = {
+    delegationTokenProviders.values.flatMap { provider =>
+      if (provider.delegationTokensRequired(sparkConf, hadoopConf)) {
+        provider.obtainDelegationTokens(hadoopConf, sparkConf, creds)
+      } else {
+        logDebug(s""Service ${provider.serviceName} does not require a token."" +
+          s"" Check your configuration to see if security is disabled or not."")
+        None
+      }
+    }.foldLeft(Long.MaxValue)(math.min)
+  }
+
+  // Visible for testing.
+  def isProviderLoaded(serviceName: String): Boolean = {
+    delegationTokenProviders.contains(serviceName)
+  }
+
+  protected def isServiceEnabled(serviceName: String): Boolean = {
     val key = providerEnabledConfig.format(serviceName)
 
     deprecatedProviderEnabledConfigs.foreach { pattern =>
@@ -110,32 +192,107 @@ private[spark] class HadoopDelegationTokenManager(
   }
 
   /**
-   * Get delegation token provider for the specified service.
+   * List of file systems for which to obtain delegation tokens. The base implementation
+   * returns just the default file system in the given Hadoop configuration.
    */
-  def getServiceDelegationTokenProvider(service: String): Option[HadoopDelegationTokenProvider] = {
-    delegationTokenProviders.get(service)
+  protected def fileSystemsToAccess(): Set[FileSystem] = {
+    Set(FileSystem.get(hadoopConf))
+  }
+
+  private def scheduleRenewal(delay: Long): Unit = {
+    val _delay = math.max(0, delay)
+    logInfo(s""Scheduling login from keytab in ${UIUtils.formatDuration(delay)}."")
+
+    val renewalTask = new Runnable() {
+      override def run(): Unit = {
+        updateTokensTask()
+      }
+    }
+    renewalExecutor.schedule(renewalTask, _delay, TimeUnit.MILLISECONDS)
   }
 
   /**
-   * Writes delegation tokens to creds.  Delegation tokens are fetched from all registered
-   * providers.
-   *
-   * @param hadoopConf hadoop Configuration
-   * @param creds Credentials that will be updated in place (overwritten)
-   * @return Time after which the fetched delegation tokens should be renewed.
+   * Periodic task to login to the KDC and create new delegation tokens. Re-schedules itself
+   * to fetch the next set of tokens when needed.
    */
-  def obtainDelegationTokens(
-      hadoopConf: Configuration,
-      creds: Credentials): Long = {
-    delegationTokenProviders.values.flatMap { provider =>
-      if (provider.delegationTokensRequired(sparkConf, hadoopConf)) {
-        provider.obtainDelegationTokens(hadoopConf, sparkConf, creds)
+  private def updateTokensTask(): Unit = {
+    try {
+      val freshUGI = doLogin()
+      val creds = obtainTokensAndScheduleRenewal(freshUGI)
+      val tokens = SparkHadoopUtil.get.serialize(creds)
+
+      val driver = driverRef.get()
+      if (driver != null) {
+        logInfo(""Updating delegation tokens."")
+        driver.send(UpdateDelegationTokens(tokens))
       } else {
-        logDebug(s""Service ${provider.serviceName} does not require a token."" +
-          s"" Check your configuration to see if security is disabled or not."")
-        None
+        // This shouldn't really happen, since the driver should register way before tokens expire.
+        logWarning(""Delegation tokens close to expiration but no driver has registered yet."")
+        SparkHadoopUtil.get.addDelegationTokens(tokens, sparkConf)
       }
-    }.foldLeft(Long.MaxValue)(math.min)
+    } catch {
+      case e: Exception =>
+        val delay = TimeUnit.SECONDS.toMillis(sparkConf.get(CREDENTIALS_RENEWAL_RETRY_WAIT))
+        logWarning(s""Failed to update tokens, will try again in ${UIUtils.formatDuration(delay)}!"" +
+          "" If this happens too often tasks will fail."", e)
+        scheduleRenewal(delay)
+    }
   }
-}
 
+  /**
+   * Obtain new delegation tokens from the available providers. Schedules a new task to fetch
+   * new tokens before the new set expires.
+   *
+   * @return Credentials containing the new tokens.
+   */
+  private def obtainTokensAndScheduleRenewal(ugi: UserGroupInformation): Credentials = {
+    ugi.doAs(new PrivilegedExceptionAction[Credentials]() {
+      override def run(): Credentials = {
+        val creds = new Credentials()
+        val nextRenewal = obtainDelegationTokens(creds)
+
+        // Calculate the time when new credentials should be created, based on the configured
+        // ratio.
+        val now = System.currentTimeMillis
+        val ratio = sparkConf.get(CREDENTIALS_RENEWAL_INTERVAL_RATIO)
+        val delay = (ratio * (nextRenewal - now)).toLong
+        scheduleRenewal(delay)
+        creds
+      }
+    })
+  }
+
+  private def doLogin(): UserGroupInformation = {
+    logInfo(s""Attempting to login to KDC using principal: $principal"")
+    val ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)
+    logInfo(""Successfully logged into KDC."")
+    ugi
+  }
+
+  private def loadProviders(): Map[String, HadoopDelegationTokenProvider] = {
+    val providers = Seq(
+      new HadoopFSDelegationTokenProvider(
+        () => HadoopDelegationTokenManager.this.fileSystemsToAccess())) ++
+      safeCreateProvider(new HiveDelegationTokenProvider) ++
+      safeCreateProvider(new HBaseDelegationTokenProvider) ++
+      safeCreateProvider(new KafkaDelegationTokenProvider)
+
+    // Filter out providers for which spark.security.credentials.{service}.enabled is false.
+    providers
+      .filter { p => isServiceEnabled(p.serviceName) }
+      .map { p => (p.serviceName, p) }
+      .toMap
+  }
+
+  private def safeCreateProvider(
+      createFn: => HadoopDelegationTokenProvider): Option[HadoopDelegationTokenProvider] = {
+    try {
+      Some(createFn)
+    } catch {
+      case t: Throwable =>
+        logDebug(s""Failed to load built in provider."", t)
+        None
+    }
+  }
+
+}
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala b/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala
index 21ca669ea98f0..767b5521e8d7b 100644
--- a/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala
@@ -30,7 +30,7 @@ import org.apache.spark.{SparkConf, SparkException}
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
 
-private[deploy] class HadoopFSDelegationTokenProvider(fileSystems: Configuration => Set[FileSystem])
+private[deploy] class HadoopFSDelegationTokenProvider(fileSystems: () => Set[FileSystem])
     extends HadoopDelegationTokenProvider with Logging {
 
   // This tokenRenewalInterval will be set in the first call to obtainDelegationTokens.
@@ -44,8 +44,7 @@ private[deploy] class HadoopFSDelegationTokenProvider(fileSystems: Configuration
       hadoopConf: Configuration,
       sparkConf: SparkConf,
       creds: Credentials): Option[Long] = {
-
-    val fsToGetTokens = fileSystems(hadoopConf)
+    val fsToGetTokens = fileSystems()
     val fetchCreds = fetchDelegationTokens(getTokenRenewer(hadoopConf), fsToGetTokens, creds)
 
     // Get the token renewal interval if it is not set. It will only be called once.
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/KafkaDelegationTokenProvider.scala b/core/src/main/scala/org/apache/spark/deploy/security/KafkaDelegationTokenProvider.scala
new file mode 100644
index 0000000000000..45995be630cc5
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/deploy/security/KafkaDelegationTokenProvider.scala
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy.security
+
+import scala.language.existentials
+import scala.util.control.NonFatal
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.security.Credentials
+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}
+
+import org.apache.spark.SparkConf
+import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config._
+
+private[security] class KafkaDelegationTokenProvider
+  extends HadoopDelegationTokenProvider with Logging {
+
+  override def serviceName: String = ""kafka""
+
+  override def obtainDelegationTokens(
+      hadoopConf: Configuration,
+      sparkConf: SparkConf,
+      creds: Credentials): Option[Long] = {
+    try {
+      logDebug(""Attempting to fetch Kafka security token."")
+      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)
+      creds.addToken(token.getService, token)
+      return Some(nextRenewalDate)
+    } catch {
+      case NonFatal(e) =>
+        logInfo(s""Failed to get token from service $serviceName"", e)
+    }
+    None
+  }
+
+  override def delegationTokensRequired(
+      sparkConf: SparkConf,
+      hadoopConf: Configuration): Boolean = {
+    val protocol = sparkConf.get(Kafka.SECURITY_PROTOCOL)
+    sparkConf.contains(Kafka.BOOTSTRAP_SERVERS) &&
+      (protocol == SASL_SSL.name ||
+        protocol == SSL.name ||
+        protocol == SASL_PLAINTEXT.name)
+  }
+}
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/KafkaTokenUtil.scala b/core/src/main/scala/org/apache/spark/deploy/security/KafkaTokenUtil.scala
new file mode 100644
index 0000000000000..c890cee59ffe0
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/deploy/security/KafkaTokenUtil.scala
@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy.security
+
+import java.{ util => ju }
+import java.text.SimpleDateFormat
+
+import scala.util.control.NonFatal
+
+import org.apache.hadoop.io.Text
+import org.apache.hadoop.security.token.{Token, TokenIdentifier}
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier
+import org.apache.kafka.clients.CommonClientConfigs
+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}
+import org.apache.kafka.common.config.SaslConfigs
+import org.apache.kafka.common.security.JaasContext
+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}
+import org.apache.kafka.common.security.token.delegation.DelegationToken
+
+import org.apache.spark.SparkConf
+import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config._
+
+private[spark] object KafkaTokenUtil extends Logging {
+  val TOKEN_KIND = new Text(""KAFKA_DELEGATION_TOKEN"")
+  val TOKEN_SERVICE = new Text(""kafka.server.delegation.token"")
+
+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {
+    override def getKind: Text = TOKEN_KIND
+  }
+
+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {
+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))
+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()
+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)
+    val token = createResult.delegationToken().get()
+    printToken(token)
+
+    (new Token[KafkaDelegationTokenIdentifier](
+      token.tokenInfo.tokenId.getBytes,
+      token.hmacAsBase64String.getBytes,
+      TOKEN_KIND,
+      TOKEN_SERVICE
+    ), token.tokenInfo.expiryTimestamp)
+  }
+
+  private[security] def createAdminClientProperties(sparkConf: SparkConf): ju.Properties = {
+    val adminClientProperties = new ju.Properties
+
+    val bootstrapServers = sparkConf.get(Kafka.BOOTSTRAP_SERVERS)
+    require(bootstrapServers.nonEmpty, s""Tried to obtain kafka delegation token but bootstrap "" +
+      ""servers not configured."")
+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)
+
+    val protocol = sparkConf.get(Kafka.SECURITY_PROTOCOL)
+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)
+    protocol match {
+      case SASL_SSL.name =>
+        setTrustStoreProperties(sparkConf, adminClientProperties)
+
+      case SSL.name =>
+        setTrustStoreProperties(sparkConf, adminClientProperties)
+        setKeyStoreProperties(sparkConf, adminClientProperties)
+        logWarning(""Obtaining kafka delegation token with SSL protocol. Please "" +
+          ""configure 2-way authentication on the broker side."")
+
+      case SASL_PLAINTEXT.name =>
+        logWarning(""Obtaining kafka delegation token through plain communication channel. Please "" +
+          ""consider the security impact."")
+    }
+
+    // There are multiple possibilities to log in and applied in the following order:
+    // - JVM global security provided -> try to log in with JVM global security configuration
+    //   which can be configured for example with 'java.security.auth.login.config'.
+    //   For this no additional parameter needed.
+    // - Keytab is provided -> try to log in with kerberos module and keytab using kafka's dynamic
+    //   JAAS configuration.
+    // - Keytab not provided -> try to log in with kerberos module and ticket cache using kafka's
+    //   dynamic JAAS configuration.
+    // Kafka client is unable to use subject from JVM which already logged in
+    // to kdc (see KAFKA-7677)
+    if (isGlobalJaasConfigurationProvided) {
+      logDebug(""JVM global security configuration detected, using it for login."")
+    } else {
+      adminClientProperties.put(SaslConfigs.SASL_MECHANISM, SaslConfigs.GSSAPI_MECHANISM)
+      if (sparkConf.contains(KEYTAB)) {
+        logDebug(""Keytab detected, using it for login."")
+        val jaasParams = getKeytabJaasParams(sparkConf)
+        adminClientProperties.put(SaslConfigs.SASL_JAAS_CONFIG, jaasParams)
+      } else {
+        logDebug(""Using ticket cache for login."")
+        val jaasParams = getTicketCacheJaasParams(sparkConf)
+        adminClientProperties.put(SaslConfigs.SASL_JAAS_CONFIG, jaasParams)
+      }
+    }
+
+    adminClientProperties
+  }
+
+  def isGlobalJaasConfigurationProvided: Boolean = {
+    try {
+      JaasContext.loadClientContext(ju.Collections.emptyMap[String, Object]())
+      true
+    } catch {
+      case NonFatal(_) => false
+    }
+  }
+
+  private def setTrustStoreProperties(sparkConf: SparkConf, properties: ju.Properties): Unit = {
+    sparkConf.get(Kafka.TRUSTSTORE_LOCATION).foreach { truststoreLocation =>
+      properties.put(""ssl.truststore.location"", truststoreLocation)
+    }
+    sparkConf.get(Kafka.TRUSTSTORE_PASSWORD).foreach { truststorePassword =>
+      properties.put(""ssl.truststore.password"", truststorePassword)
+    }
+  }
+
+  private def setKeyStoreProperties(sparkConf: SparkConf, properties: ju.Properties): Unit = {
+    sparkConf.get(Kafka.KEYSTORE_LOCATION).foreach { keystoreLocation =>
+      properties.put(""ssl.keystore.location"", keystoreLocation)
+    }
+    sparkConf.get(Kafka.KEYSTORE_PASSWORD).foreach { keystorePassword =>
+      properties.put(""ssl.keystore.password"", keystorePassword)
+    }
+    sparkConf.get(Kafka.KEY_PASSWORD).foreach { keyPassword =>
+      properties.put(""ssl.key.password"", keyPassword)
+    }
+  }
+
+  private[security] def getKeytabJaasParams(sparkConf: SparkConf): String = {
+    val serviceName = sparkConf.get(Kafka.KERBEROS_SERVICE_NAME)
+    require(serviceName.nonEmpty, ""Kerberos service name must be defined"")
+
+    val params =
+      s""""""
+      |${getKrb5LoginModuleName} required
+      | useKeyTab=true
+      | serviceName=""${serviceName.get}""
+      | keyTab=""${sparkConf.get(KEYTAB).get}""
+      | principal=""${sparkConf.get(PRINCIPAL).get}"";
+      """""".stripMargin.replace(""\n"", """")
+    logDebug(s""Krb keytab JAAS params: $params"")
+    params
+  }
+
+  def getTicketCacheJaasParams(sparkConf: SparkConf): String = {
+    val serviceName = sparkConf.get(Kafka.KERBEROS_SERVICE_NAME)
+    require(serviceName.nonEmpty, ""Kerberos service name must be defined"")
+
+    val params =
+      s""""""
+      |${getKrb5LoginModuleName} required
+      | useTicketCache=true
+      | serviceName=""${serviceName.get}"";
+      """""".stripMargin.replace(""\n"", """")
+    logDebug(s""Krb ticket cache JAAS params: $params"")
+    params
+  }
+
+  /**
+   * Krb5LoginModule package vary in different JVMs.
+   * Please see Hadoop UserGroupInformation for further details.
+   */
+  private def getKrb5LoginModuleName(): String = {
+    if (System.getProperty(""java.vendor"").contains(""IBM"")) {
+      ""com.ibm.security.auth.module.Krb5LoginModule""
+    } else {
+      ""com.sun.security.auth.module.Krb5LoginModule""
+    }
+  }
+
+  private def printToken(token: DelegationToken): Unit = {
+    if (log.isDebugEnabled) {
+      val dateFormat = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm"")
+      logDebug(""%-15s %-30s %-15s %-25s %-15s %-15s %-15s"".format(
+        ""TOKENID"", ""HMAC"", ""OWNER"", ""RENEWERS"", ""ISSUEDATE"", ""EXPIRYDATE"", ""MAXDATE""))
+      val tokenInfo = token.tokenInfo
+      logDebug(""%-15s [hidden] %-15s %-25s %-15s %-15s %-15s"".format(
+        tokenInfo.tokenId,
+        tokenInfo.owner,
+        tokenInfo.renewersAsString,
+        dateFormat.format(tokenInfo.issueTimestamp),
+        dateFormat.format(tokenInfo.expiryTimestamp),
+        dateFormat.format(tokenInfo.maxTimestamp)))
+    }
+  }
+}
diff --git a/core/src/main/scala/org/apache/spark/executor/Executor.scala b/core/src/main/scala/org/apache/spark/executor/Executor.scala
index 86b19578037df..a30a501e5d4a1 100644
--- a/core/src/main/scala/org/apache/spark/executor/Executor.scala
+++ b/core/src/main/scala/org/apache/spark/executor/Executor.scala
@@ -28,6 +28,7 @@ import javax.annotation.concurrent.GuardedBy
 
 import scala.collection.JavaConverters._
 import scala.collection.mutable.{ArrayBuffer, HashMap, Map}
+import scala.concurrent.duration._
 import scala.util.control.NonFatal
 
 import com.google.common.util.concurrent.ThreadFactoryBuilder
@@ -38,7 +39,7 @@ import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
 import org.apache.spark.memory.{SparkOutOfMemoryError, TaskMemoryManager}
 import org.apache.spark.rpc.RpcTimeout
-import org.apache.spark.scheduler.{DirectTaskResult, IndirectTaskResult, Task, TaskDescription}
+import org.apache.spark.scheduler._
 import org.apache.spark.shuffle.FetchFailedException
 import org.apache.spark.storage.{StorageLevel, TaskResultBlockId}
 import org.apache.spark.util._
@@ -120,7 +121,7 @@ private[spark] class Executor(
   }
 
   // Whether to load classes in user jars before those in Spark jars
-  private val userClassPathFirst = conf.getBoolean(""spark.executor.userClassPathFirst"", false)
+  private val userClassPathFirst = conf.get(EXECUTOR_USER_CLASS_PATH_FIRST)
 
   // Whether to monitor killed / interrupted tasks
   private val taskReaperEnabled = conf.getBoolean(""spark.task.reaper.enabled"", false)
@@ -136,6 +137,29 @@ private[spark] class Executor(
   // for fetching remote cached RDD blocks, so need to make sure it uses the right classloader too.
   env.serializerManager.setDefaultClassLoader(replClassLoader)
 
+  private val executorPlugins: Seq[ExecutorPlugin] = {
+    val pluginNames = conf.get(EXECUTOR_PLUGINS)
+    if (pluginNames.nonEmpty) {
+      logDebug(s""Initializing the following plugins: ${pluginNames.mkString("", "")}"")
+
+      // Plugins need to load using a class loader that includes the executor's user classpath
+      val pluginList: Seq[ExecutorPlugin] =
+        Utils.withContextClassLoader(replClassLoader) {
+          val plugins = Utils.loadExtensions(classOf[ExecutorPlugin], pluginNames, conf)
+          plugins.foreach { plugin =>
+            plugin.init()
+            logDebug(s""Successfully loaded plugin "" + plugin.getClass().getCanonicalName())
+          }
+          plugins
+        }
+
+      logDebug(""Finished initializing plugins"")
+      pluginList
+    } else {
+      Nil
+    }
+  }
+
   // Max size of direct result. If task result is bigger than this, we use the block manager
   // to send the result back.
   private val maxDirectResultSize = Math.min(
@@ -147,19 +171,34 @@ private[spark] class Executor(
   // Maintains the list of running tasks.
   private val runningTasks = new ConcurrentHashMap[Long, TaskRunner]
 
-  // Executor for the heartbeat task.
-  private val heartbeater = ThreadUtils.newDaemonSingleThreadScheduledExecutor(""driver-heartbeater"")
-
-  // must be initialized before running startDriverHeartbeat()
-  private val heartbeatReceiverRef =
-    RpcUtils.makeDriverRef(HeartbeatReceiver.ENDPOINT_NAME, conf, env.rpcEnv)
-
   /**
    * When an executor is unable to send heartbeats to the driver more than `HEARTBEAT_MAX_FAILURES`
    * times, it should kill itself. The default value is 60. It means we will retry to send
    * heartbeats about 10 minutes because the heartbeat interval is 10s.
    */
-  private val HEARTBEAT_MAX_FAILURES = conf.getInt(""spark.executor.heartbeat.maxFailures"", 60)
+  private val HEARTBEAT_MAX_FAILURES = conf.get(EXECUTOR_HEARTBEAT_MAX_FAILURES)
+
+  /**
+   * Whether to drop empty accumulators from heartbeats sent to the driver. Including the empty
+   * accumulators (that satisfy isZero) can make the size of the heartbeat message very large.
+   */
+  private val HEARTBEAT_DROP_ZEROES = conf.get(EXECUTOR_HEARTBEAT_DROP_ZERO_ACCUMULATOR_UPDATES)
+
+  /**
+   * Interval to send heartbeats, in milliseconds
+   */
+  private val HEARTBEAT_INTERVAL_MS = conf.get(EXECUTOR_HEARTBEAT_INTERVAL)
+
+  // Executor for the heartbeat task.
+  private val heartbeater = new Heartbeater(
+    env.memoryManager,
+    () => Executor.this.reportHeartBeat(),
+    ""executor-heartbeater"",
+    HEARTBEAT_INTERVAL_MS)
+
+  // must be initialized before running startDriverHeartbeat()
+  private val heartbeatReceiverRef =
+    RpcUtils.makeDriverRef(HeartbeatReceiver.ENDPOINT_NAME, conf, env.rpcEnv)
 
   /**
    * Count the failure times of heartbeat. It should only be accessed in the heartbeat thread. Each
@@ -167,7 +206,7 @@ private[spark] class Executor(
    */
   private var heartbeatFailures = 0
 
-  startDriverHeartbeater()
+  heartbeater.start()
 
   private[executor] def numRunningTasks: Int = runningTasks.size()
 
@@ -216,9 +255,25 @@ private[spark] class Executor(
 
   def stop(): Unit = {
     env.metricsSystem.report()
-    heartbeater.shutdown()
-    heartbeater.awaitTermination(10, TimeUnit.SECONDS)
+    try {
+      heartbeater.stop()
+    } catch {
+      case NonFatal(e) =>
+        logWarning(""Unable to stop heartbeater"", e)
+     }
     threadPool.shutdown()
+
+    // Notify plugins that executor is shutting down so they can terminate cleanly
+    Utils.withContextClassLoader(replClassLoader) {
+      executorPlugins.foreach { plugin =>
+        try {
+          plugin.shutdown()
+        } catch {
+          case e: Exception =>
+            logWarning(""Plugin "" + plugin.getClass().getCanonicalName() + "" shutdown failed"", e)
+        }
+      }
+    }
     if (!isLocal) {
       env.stop()
     }
@@ -464,7 +519,7 @@ private[spark] class Executor(
         executorSource.METRIC_OUTPUT_BYTES_WRITTEN
           .inc(task.metrics.outputMetrics.bytesWritten)
         executorSource.METRIC_OUTPUT_RECORDS_WRITTEN
-          .inc(task.metrics.inputMetrics.recordsRead)
+          .inc(task.metrics.outputMetrics.recordsWritten)
         executorSource.METRIC_RESULT_SIZE.inc(task.metrics.resultSize)
         executorSource.METRIC_DISK_BYTES_SPILLED.inc(task.metrics.diskBytesSpilled)
         executorSource.METRIC_MEMORY_BYTES_SPILLED.inc(task.metrics.memoryBytesSpilled)
@@ -787,18 +842,28 @@ private[spark] class Executor(
     val accumUpdates = new ArrayBuffer[(Long, Seq[AccumulatorV2[_, _]])]()
     val curGCTime = computeTotalGcTime()
 
+    // get executor level memory metrics
+    val executorUpdates = heartbeater.getCurrentMetrics()
+
     for (taskRunner <- runningTasks.values().asScala) {
       if (taskRunner.task != null) {
         taskRunner.task.metrics.mergeShuffleReadMetrics()
         taskRunner.task.metrics.setJvmGCTime(curGCTime - taskRunner.startGCTime)
-        accumUpdates += ((taskRunner.taskId, taskRunner.task.metrics.accumulators()))
+        val accumulatorsToReport =
+          if (HEARTBEAT_DROP_ZEROES) {
+            taskRunner.task.metrics.accumulators().filterNot(_.isZero)
+          } else {
+            taskRunner.task.metrics.accumulators()
+          }
+        accumUpdates += ((taskRunner.taskId, accumulatorsToReport))
       }
     }
 
-    val message = Heartbeat(executorId, accumUpdates.toArray, env.blockManager.blockManagerId)
+    val message = Heartbeat(executorId, accumUpdates.toArray, env.blockManager.blockManagerId,
+      executorUpdates)
     try {
       val response = heartbeatReceiverRef.askSync[HeartbeatResponse](
-          message, RpcTimeout(conf, ""spark.executor.heartbeatInterval"", ""10s""))
+        message, new RpcTimeout(HEARTBEAT_INTERVAL_MS.millis, EXECUTOR_HEARTBEAT_INTERVAL.key))
       if (response.reregisterBlockManager) {
         logInfo(""Told to re-register on heartbeat"")
         env.blockManager.reregister()
@@ -815,21 +880,6 @@ private[spark] class Executor(
         }
     }
   }
-
-  /**
-   * Schedules a task to report heartbeat and partial metrics for active tasks to driver.
-   */
-  private def startDriverHeartbeater(): Unit = {
-    val intervalMs = conf.getTimeAsMs(""spark.executor.heartbeatInterval"", ""10s"")
-
-    // Wait a random interval so the heartbeats don't end up in sync
-    val initialDelay = intervalMs + (math.random * intervalMs).asInstanceOf[Int]
-
-    val heartbeatTask = new Runnable() {
-      override def run(): Unit = Utils.logUncaughtExceptions(reportHeartBeat())
-    }
-    heartbeater.scheduleAtFixedRate(heartbeatTask, initialDelay, intervalMs, TimeUnit.MILLISECONDS)
-  }
 }
 
 private[spark] object Executor {
diff --git a/core/src/main/scala/org/apache/spark/executor/ExecutorMetrics.scala b/core/src/main/scala/org/apache/spark/executor/ExecutorMetrics.scala
new file mode 100644
index 0000000000000..1befd27de1cba
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/executor/ExecutorMetrics.scala
@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.executor
+
+import org.apache.spark.annotation.DeveloperApi
+import org.apache.spark.metrics.ExecutorMetricType
+
+/**
+ * :: DeveloperApi ::
+ * Metrics tracked for executors and the driver.
+ *
+ * Executor-level metrics are sent from each executor to the driver as part of the Heartbeat.
+ */
+@DeveloperApi
+class ExecutorMetrics private[spark] extends Serializable {
+
+  // Metrics are indexed by ExecutorMetricType.values
+  private val metrics = new Array[Long](ExecutorMetricType.values.length)
+
+  // the first element is initialized to -1, indicating that the values for the array
+  // haven't been set yet.
+  metrics(0) = -1
+
+  /** Returns the value for the specified metricType. */
+  def getMetricValue(metricType: ExecutorMetricType): Long = {
+    metrics(ExecutorMetricType.metricIdxMap(metricType))
+  }
+
+  /** Returns true if the values for the metrics have been set, false otherwise. */
+  def isSet(): Boolean = metrics(0) > -1
+
+  private[spark] def this(metrics: Array[Long]) {
+    this()
+    Array.copy(metrics, 0, this.metrics, 0, Math.min(metrics.size, this.metrics.size))
+  }
+
+  /**
+   * Constructor: create the ExecutorMetrics with the values specified.
+   *
+   * @param executorMetrics map of executor metric name to value
+   */
+  private[spark] def this(executorMetrics: Map[String, Long]) {
+    this()
+    (0 until ExecutorMetricType.values.length).foreach { idx =>
+      metrics(idx) = executorMetrics.getOrElse(ExecutorMetricType.values(idx).name, 0L)
+    }
+  }
+
+  /**
+   * Compare the specified executor metrics values with the current executor metric values,
+   * and update the value for any metrics where the new value for the metric is larger.
+   *
+   * @param executorMetrics the executor metrics to compare
+   * @return if there is a new peak value for any metric
+   */
+  private[spark] def compareAndUpdatePeakValues(executorMetrics: ExecutorMetrics): Boolean = {
+    var updated = false
+
+    (0 until ExecutorMetricType.values.length).foreach { idx =>
+       if (executorMetrics.metrics(idx) > metrics(idx)) {
+        updated = true
+        metrics(idx) = executorMetrics.metrics(idx)
+      }
+    }
+    updated
+  }
+}
diff --git a/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala b/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala
index 4be395c8358b2..12c4b8f67f71c 100644
--- a/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala
+++ b/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala
@@ -18,6 +18,7 @@
 package org.apache.spark.executor
 
 import org.apache.spark.annotation.DeveloperApi
+import org.apache.spark.shuffle.ShuffleReadMetricsReporter
 import org.apache.spark.util.LongAccumulator
 
 
@@ -123,12 +124,13 @@ class ShuffleReadMetrics private[spark] () extends Serializable {
   }
 }
 
+
 /**
  * A temporary shuffle read metrics holder that is used to collect shuffle read metrics for each
  * shuffle dependency, and all temporary metrics will be merged into the [[ShuffleReadMetrics]] at
  * last.
  */
-private[spark] class TempShuffleReadMetrics {
+private[spark] class TempShuffleReadMetrics extends ShuffleReadMetricsReporter {
   private[this] var _remoteBlocksFetched = 0L
   private[this] var _localBlocksFetched = 0L
   private[this] var _remoteBytesRead = 0L
@@ -137,13 +139,13 @@ private[spark] class TempShuffleReadMetrics {
   private[this] var _fetchWaitTime = 0L
   private[this] var _recordsRead = 0L
 
-  def incRemoteBlocksFetched(v: Long): Unit = _remoteBlocksFetched += v
-  def incLocalBlocksFetched(v: Long): Unit = _localBlocksFetched += v
-  def incRemoteBytesRead(v: Long): Unit = _remoteBytesRead += v
-  def incRemoteBytesReadToDisk(v: Long): Unit = _remoteBytesReadToDisk += v
-  def incLocalBytesRead(v: Long): Unit = _localBytesRead += v
-  def incFetchWaitTime(v: Long): Unit = _fetchWaitTime += v
-  def incRecordsRead(v: Long): Unit = _recordsRead += v
+  override def incRemoteBlocksFetched(v: Long): Unit = _remoteBlocksFetched += v
+  override def incLocalBlocksFetched(v: Long): Unit = _localBlocksFetched += v
+  override def incRemoteBytesRead(v: Long): Unit = _remoteBytesRead += v
+  override def incRemoteBytesReadToDisk(v: Long): Unit = _remoteBytesReadToDisk += v
+  override def incLocalBytesRead(v: Long): Unit = _localBytesRead += v
+  override def incFetchWaitTime(v: Long): Unit = _fetchWaitTime += v
+  override def incRecordsRead(v: Long): Unit = _recordsRead += v
 
   def remoteBlocksFetched: Long = _remoteBlocksFetched
   def localBlocksFetched: Long = _localBlocksFetched
diff --git a/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala b/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala
index ada2e1bc08593..d0b0e7da079c9 100644
--- a/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala
+++ b/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala
@@ -18,6 +18,7 @@
 package org.apache.spark.executor
 
 import org.apache.spark.annotation.DeveloperApi
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter
 import org.apache.spark.util.LongAccumulator
 
 
@@ -27,7 +28,7 @@ import org.apache.spark.util.LongAccumulator
  * Operations are not thread-safe.
  */
 @DeveloperApi
-class ShuffleWriteMetrics private[spark] () extends Serializable {
+class ShuffleWriteMetrics private[spark] () extends ShuffleWriteMetricsReporter with Serializable {
   private[executor] val _bytesWritten = new LongAccumulator
   private[executor] val _recordsWritten = new LongAccumulator
   private[executor] val _writeTime = new LongAccumulator
@@ -47,23 +48,13 @@ class ShuffleWriteMetrics private[spark] () extends Serializable {
    */
   def writeTime: Long = _writeTime.sum
 
-  private[spark] def incBytesWritten(v: Long): Unit = _bytesWritten.add(v)
-  private[spark] def incRecordsWritten(v: Long): Unit = _recordsWritten.add(v)
-  private[spark] def incWriteTime(v: Long): Unit = _writeTime.add(v)
-  private[spark] def decBytesWritten(v: Long): Unit = {
+  private[spark] override def incBytesWritten(v: Long): Unit = _bytesWritten.add(v)
+  private[spark] override def incRecordsWritten(v: Long): Unit = _recordsWritten.add(v)
+  private[spark] override def incWriteTime(v: Long): Unit = _writeTime.add(v)
+  private[spark] override def decBytesWritten(v: Long): Unit = {
     _bytesWritten.setValue(bytesWritten - v)
   }
-  private[spark] def decRecordsWritten(v: Long): Unit = {
+  private[spark] override def decRecordsWritten(v: Long): Unit = {
     _recordsWritten.setValue(recordsWritten - v)
   }
-
-  // Legacy methods for backward compatibility.
-  // TODO: remove these once we make this class private.
-  @deprecated(""use bytesWritten instead"", ""2.0.0"")
-  def shuffleBytesWritten: Long = bytesWritten
-  @deprecated(""use writeTime instead"", ""2.0.0"")
-  def shuffleWriteTime: Long = writeTime
-  @deprecated(""use recordsWritten instead"", ""2.0.0"")
-  def shuffleRecordsWritten: Long = recordsWritten
-
 }
diff --git a/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala b/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala
index ab020aaf6fa4f..5b33c110154d6 100644
--- a/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala
+++ b/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala
@@ -52,6 +52,18 @@ private[spark] abstract class StreamFileInputFormat[T]
     val totalBytes = files.filterNot(_.isDirectory).map(_.getLen + openCostInBytes).sum
     val bytesPerCore = totalBytes / defaultParallelism
     val maxSplitSize = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))
+
+    // For small files we need to ensure the min split size per node & rack <= maxSplitSize
+    val jobConfig = context.getConfiguration
+    val minSplitSizePerNode = jobConfig.getLong(CombineFileInputFormat.SPLIT_MINSIZE_PERNODE, 0L)
+    val minSplitSizePerRack = jobConfig.getLong(CombineFileInputFormat.SPLIT_MINSIZE_PERRACK, 0L)
+
+    if (maxSplitSize < minSplitSizePerNode) {
+      super.setMinSplitSizeNode(maxSplitSize)
+    }
+    if (maxSplitSize < minSplitSizePerRack) {
+      super.setMinSplitSizeRack(maxSplitSize)
+    }
     super.setMaxSplitSize(maxSplitSize)
   }
 
diff --git a/core/src/main/scala/org/apache/spark/internal/Logging.scala b/core/src/main/scala/org/apache/spark/internal/Logging.scala
index c0d709ad25f29..00db9af846ab9 100644
--- a/core/src/main/scala/org/apache/spark/internal/Logging.scala
+++ b/core/src/main/scala/org/apache/spark/internal/Logging.scala
@@ -17,7 +17,11 @@
 
 package org.apache.spark.internal
 
-import org.apache.log4j.{Level, LogManager, PropertyConfigurator}
+import java.util.concurrent.ConcurrentHashMap
+
+import scala.collection.JavaConverters._
+
+import org.apache.log4j._
 import org.slf4j.{Logger, LoggerFactory}
 import org.slf4j.impl.StaticLoggerBinder
 
@@ -143,13 +147,25 @@ trait Logging {
         // overriding the root logger's config if they're different.
         val replLogger = LogManager.getLogger(logName)
         val replLevel = Option(replLogger.getLevel()).getOrElse(Level.WARN)
+        // Update the consoleAppender threshold to replLevel
         if (replLevel != rootLogger.getEffectiveLevel()) {
           if (!silent) {
             System.err.printf(""Setting default log level to \""%s\"".\n"", replLevel)
             System.err.println(""To adjust logging level use sc.setLogLevel(newLevel). "" +
               ""For SparkR, use setLogLevel(newLevel)."")
           }
-          rootLogger.setLevel(replLevel)
+          rootLogger.getAllAppenders().asScala.foreach {
+            case ca: ConsoleAppender =>
+              Option(ca.getThreshold()) match {
+                case Some(t) =>
+                  Logging.consoleAppenderToThreshold.put(ca, t)
+                  if (!t.isGreaterOrEqual(replLevel)) {
+                    ca.setThreshold(replLevel)
+                  }
+                case None => ca.setThreshold(replLevel)
+              }
+            case _ => // no-op
+          }
         }
       }
       // scalastyle:on println
@@ -166,6 +182,7 @@ private[spark] object Logging {
   @volatile private var initialized = false
   @volatile private var defaultRootLevel: Level = null
   @volatile private var defaultSparkLog4jConfig = false
+  private val consoleAppenderToThreshold = new ConcurrentHashMap[ConsoleAppender, Priority]()
 
   val initLock = new Object()
   try {
@@ -192,7 +209,13 @@ private[spark] object Logging {
         defaultSparkLog4jConfig = false
         LogManager.resetConfiguration()
       } else {
-        LogManager.getRootLogger().setLevel(defaultRootLevel)
+        val rootLogger = LogManager.getRootLogger()
+        rootLogger.setLevel(defaultRootLevel)
+        rootLogger.getAllAppenders().asScala.foreach {
+          case ca: ConsoleAppender =>
+            ca.setThreshold(consoleAppenderToThreshold.get(ca))
+          case _ => // no-op
+        }
       }
     }
     this.initialized = false
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/config.scala b/core/src/main/scala/org/apache/spark/internal/config/History.scala
similarity index 77%
rename from core/src/main/scala/org/apache/spark/deploy/history/config.scala
rename to core/src/main/scala/org/apache/spark/internal/config/History.scala
index 25ba9edb9e014..b7d8061d26d21 100644
--- a/core/src/main/scala/org/apache/spark/deploy/history/config.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/History.scala
@@ -15,14 +15,13 @@
  * limitations under the License.
  */
 
-package org.apache.spark.deploy.history
+package org.apache.spark.internal.config
 
 import java.util.concurrent.TimeUnit
 
-import org.apache.spark.internal.config.ConfigBuilder
 import org.apache.spark.network.util.ByteUnit
 
-private[spark] object config {
+private[spark] object History {
 
   val DEFAULT_LOG_DIR = ""file:/tmp/spark-events""
 
@@ -30,6 +29,14 @@ private[spark] object config {
     .stringConf
     .createWithDefault(DEFAULT_LOG_DIR)
 
+  val CLEANER_ENABLED = ConfigBuilder(""spark.history.fs.cleaner.enabled"")
+    .booleanConf
+    .createWithDefault(false)
+
+  val CLEANER_INTERVAL_S = ConfigBuilder(""spark.history.fs.cleaner.interval"")
+    .timeConf(TimeUnit.SECONDS)
+    .createWithDefaultString(""1d"")
+
   val MAX_LOG_AGE_S = ConfigBuilder(""spark.history.fs.cleaner.maxAge"")
     .timeConf(TimeUnit.SECONDS)
     .createWithDefaultString(""7d"")
@@ -64,4 +71,12 @@ private[spark] object config {
       .bytesConf(ByteUnit.BYTE)
       .createWithDefaultString(""1m"")
 
+  val DRIVER_LOG_CLEANER_ENABLED = ConfigBuilder(""spark.history.fs.driverlog.cleaner.enabled"")
+    .fallbackConf(CLEANER_ENABLED)
+
+  val DRIVER_LOG_CLEANER_INTERVAL = ConfigBuilder(""spark.history.fs.driverlog.cleaner.interval"")
+    .fallbackConf(CLEANER_INTERVAL_S)
+
+  val MAX_DRIVER_LOG_AGE_S = ConfigBuilder(""spark.history.fs.driverlog.cleaner.maxAge"")
+    .fallbackConf(MAX_LOG_AGE_S)
 }
diff --git a/core/src/main/scala/org/apache/spark/internal/config/Kafka.scala b/core/src/main/scala/org/apache/spark/internal/config/Kafka.scala
new file mode 100644
index 0000000000000..85d74c27142ad
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/internal/config/Kafka.scala
@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.internal.config
+
+private[spark] object Kafka {
+
+  val BOOTSTRAP_SERVERS =
+    ConfigBuilder(""spark.kafka.bootstrap.servers"")
+      .doc(""A list of coma separated host/port pairs to use for establishing the initial "" +
+        ""connection to the Kafka cluster. For further details please see kafka documentation. "" +
+        ""Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+
+  val SECURITY_PROTOCOL =
+    ConfigBuilder(""spark.kafka.security.protocol"")
+      .doc(""Protocol used to communicate with brokers. For further details please see kafka "" +
+        ""documentation. Only used to obtain delegation token."")
+      .stringConf
+      .createWithDefault(""SASL_SSL"")
+
+  val KERBEROS_SERVICE_NAME =
+    ConfigBuilder(""spark.kafka.sasl.kerberos.service.name"")
+      .doc(""The Kerberos principal name that Kafka runs as. This can be defined either in "" +
+        ""Kafka's JAAS config or in Kafka's config. For further details please see kafka "" +
+        ""documentation. Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+
+  val TRUSTSTORE_LOCATION =
+    ConfigBuilder(""spark.kafka.ssl.truststore.location"")
+      .doc(""The location of the trust store file. For further details please see kafka "" +
+        ""documentation. Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+
+  val TRUSTSTORE_PASSWORD =
+    ConfigBuilder(""spark.kafka.ssl.truststore.password"")
+      .doc(""The store password for the trust store file. This is optional for client and only "" +
+        ""needed if ssl.truststore.location is configured. For further details please see kafka "" +
+        ""documentation. Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+
+  val KEYSTORE_LOCATION =
+    ConfigBuilder(""spark.kafka.ssl.keystore.location"")
+      .doc(""The location of the key store file. This is optional for client and can be used for "" +
+        ""two-way authentication for client. For further details please see kafka documentation. "" +
+        ""Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+
+  val KEYSTORE_PASSWORD =
+    ConfigBuilder(""spark.kafka.ssl.keystore.password"")
+      .doc(""The store password for the key store file. This is optional for client and only "" +
+        ""needed if ssl.keystore.location is configured. For further details please see kafka "" +
+        ""documentation. Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+
+  val KEY_PASSWORD =
+    ConfigBuilder(""spark.kafka.ssl.key.password"")
+      .doc(""The password of the private key in the key store file. This is optional for client. "" +
+        ""For further details please see kafka documentation. Only used to obtain delegation token."")
+      .stringConf
+      .createOptional
+}
diff --git a/core/src/main/scala/org/apache/spark/status/config.scala b/core/src/main/scala/org/apache/spark/internal/config/Status.scala
similarity index 83%
rename from core/src/main/scala/org/apache/spark/status/config.scala
rename to core/src/main/scala/org/apache/spark/internal/config/Status.scala
index 67801b8f046f4..c56157227f8fc 100644
--- a/core/src/main/scala/org/apache/spark/status/config.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/Status.scala
@@ -15,13 +15,11 @@
  * limitations under the License.
  */
 
-package org.apache.spark.status
+package org.apache.spark.internal.config
 
 import java.util.concurrent.TimeUnit
 
-import org.apache.spark.internal.config._
-
-private[spark] object config {
+private[spark] object Status {
 
   val ASYNC_TRACKING_ENABLED = ConfigBuilder(""spark.appStateStore.asyncTracking.enable"")
     .booleanConf
@@ -51,4 +49,10 @@ private[spark] object config {
     .intConf
     .createWithDefault(Int.MaxValue)
 
+  val APP_STATUS_METRICS_ENABLED =
+    ConfigBuilder(""spark.app.status.metrics.enabled"")
+      .doc(""Whether Dropwizard/Codahale metrics "" +
+        ""will be reported for the status of the running spark app."")
+      .booleanConf
+      .createWithDefault(false)
 }
diff --git a/core/src/main/scala/org/apache/spark/internal/config/package.scala b/core/src/main/scala/org/apache/spark/internal/config/package.scala
index 319e664a19677..646b3881a79b0 100644
--- a/core/src/main/scala/org/apache/spark/internal/config/package.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/package.scala
@@ -21,6 +21,7 @@ import java.util.concurrent.TimeUnit
 
 import org.apache.spark.launcher.SparkLauncher
 import org.apache.spark.network.util.ByteUnit
+import org.apache.spark.unsafe.array.ByteArrayMethods
 import org.apache.spark.util.Utils
 
 package object config {
@@ -48,6 +49,19 @@ package object config {
     .bytesConf(ByteUnit.MiB)
     .createOptional
 
+  private[spark] val DRIVER_LOG_DFS_DIR =
+    ConfigBuilder(""spark.driver.log.dfsDir"").stringConf.createOptional
+
+  private[spark] val DRIVER_LOG_LAYOUT =
+    ConfigBuilder(""spark.driver.log.layout"")
+      .stringConf
+      .createOptional
+
+  private[spark] val DRIVER_LOG_PERSISTTODFS =
+    ConfigBuilder(""spark.driver.log.persistToDfs.enabled"")
+      .booleanConf
+      .createWithDefault(false)
+
   private[spark] val EVENT_LOG_COMPRESS =
     ConfigBuilder(""spark.eventLog.compress"")
       .booleanConf
@@ -58,6 +72,11 @@ package object config {
       .booleanConf
       .createWithDefault(false)
 
+  private[spark] val EVENT_LOG_ALLOW_EC =
+    ConfigBuilder(""spark.eventLog.allowErasureCoding"")
+      .booleanConf
+      .createWithDefault(false)
+
   private[spark] val EVENT_LOG_TESTING =
     ConfigBuilder(""spark.eventLog.testing"")
       .internal()
@@ -69,15 +88,34 @@ package object config {
     .bytesConf(ByteUnit.KiB)
     .createWithDefaultString(""100k"")
 
+  private[spark] val EVENT_LOG_STAGE_EXECUTOR_METRICS =
+    ConfigBuilder(""spark.eventLog.logStageExecutorMetrics.enabled"")
+      .booleanConf
+      .createWithDefault(false)
+
   private[spark] val EVENT_LOG_OVERWRITE =
     ConfigBuilder(""spark.eventLog.overwrite"").booleanConf.createWithDefault(false)
 
-  private[spark] val EVENT_LOG_CALLSITE_FORM =
-    ConfigBuilder(""spark.eventLog.callsite"").stringConf.createWithDefault(""short"")
+  private[spark] val EVENT_LOG_CALLSITE_LONG_FORM =
+    ConfigBuilder(""spark.eventLog.longForm.enabled"").booleanConf.createWithDefault(false)
 
   private[spark] val EXECUTOR_CLASS_PATH =
     ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_CLASSPATH).stringConf.createOptional
 
+  private[spark] val EXECUTOR_HEARTBEAT_DROP_ZERO_ACCUMULATOR_UPDATES =
+    ConfigBuilder(""spark.executor.heartbeat.dropZeroAccumulatorUpdates"")
+      .internal()
+      .booleanConf
+      .createWithDefault(true)
+
+  private[spark] val EXECUTOR_HEARTBEAT_INTERVAL =
+    ConfigBuilder(""spark.executor.heartbeatInterval"")
+      .timeConf(TimeUnit.MILLISECONDS)
+      .createWithDefaultString(""10s"")
+
+  private[spark] val EXECUTOR_HEARTBEAT_MAX_FAILURES =
+    ConfigBuilder(""spark.executor.heartbeat.maxFailures"").internal().intConf.createWithDefault(60)
+
   private[spark] val EXECUTOR_JAVA_OPTIONS =
     ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_JAVA_OPTIONS).stringConf.createOptional
 
@@ -147,14 +185,18 @@ package object config {
   private[spark] val SHUFFLE_SERVICE_PORT =
     ConfigBuilder(""spark.shuffle.service.port"").intConf.createWithDefault(7337)
 
-  private[spark] val KEYTAB = ConfigBuilder(""spark.yarn.keytab"")
+  private[spark] val KEYTAB = ConfigBuilder(""spark.kerberos.keytab"")
     .doc(""Location of user's keytab."")
     .stringConf.createOptional
 
-  private[spark] val PRINCIPAL = ConfigBuilder(""spark.yarn.principal"")
+  private[spark] val PRINCIPAL = ConfigBuilder(""spark.kerberos.principal"")
     .doc(""Name of the Kerberos principal."")
     .stringConf.createOptional
 
+  private[spark] val KERBEROS_RELOGIN_PERIOD = ConfigBuilder(""spark.kerberos.relogin.period"")
+    .timeConf(TimeUnit.SECONDS)
+    .createWithDefaultString(""1m"")
+
   private[spark] val EXECUTOR_INSTANCES = ConfigBuilder(""spark.executor.instances"")
     .intConf
     .createOptional
@@ -239,7 +281,7 @@ package object config {
   private[spark] val LISTENER_BUS_EVENT_QUEUE_CAPACITY =
     ConfigBuilder(""spark.scheduler.listenerbus.eventqueue.capacity"")
       .intConf
-      .checkValue(_ > 0, ""The capacity of listener bus event queue must not be negative"")
+      .checkValue(_ > 0, ""The capacity of listener bus event queue must be positive"")
       .createWithDefault(10000)
 
   private[spark] val LISTENER_BUS_METRICS_MAX_LISTENER_CLASSES_TIMED =
@@ -387,8 +429,9 @@ package object config {
       .internal()
       .doc(""The chunk size in bytes during writing out the bytes of ChunkedByteBuffer."")
       .bytesConf(ByteUnit.BYTE)
-      .checkValue(_ <= Int.MaxValue, ""The chunk size during writing out the bytes of"" +
-        "" ChunkedByteBuffer should not larger than Int.MaxValue."")
+      .checkValue(_ <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,
+        ""The chunk size during writing out the bytes of ChunkedByteBuffer should"" +
+          s"" be less than or equal to ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}."")
       .createWithDefault(64 * 1024 * 1024)
 
   private[spark] val CHECKPOINT_COMPRESS =
@@ -459,8 +502,9 @@ package object config {
         ""otherwise specified. These buffers reduce the number of disk seeks and system calls "" +
         ""made in creating intermediate shuffle files."")
       .bytesConf(ByteUnit.KiB)
-      .checkValue(v => v > 0 && v <= Int.MaxValue / 1024,
-        s""The file buffer size must be greater than 0 and less than ${Int.MaxValue / 1024}."")
+      .checkValue(v => v > 0 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,
+        s""The file buffer size must be positive and less than or equal to"" +
+          s"" ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024}."")
       .createWithDefaultString(""32k"")
 
   private[spark] val SHUFFLE_UNSAFE_FILE_OUTPUT_BUFFER_SIZE =
@@ -468,16 +512,18 @@ package object config {
       .doc(""The file system for this buffer size after each partition "" +
         ""is written in unsafe shuffle writer. In KiB unless otherwise specified."")
       .bytesConf(ByteUnit.KiB)
-      .checkValue(v => v > 0 && v <= Int.MaxValue / 1024,
-        s""The buffer size must be greater than 0 and less than ${Int.MaxValue / 1024}."")
+      .checkValue(v => v > 0 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,
+        s""The buffer size must be positive and less than or equal to"" +
+          s"" ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024}."")
       .createWithDefaultString(""32k"")
 
   private[spark] val SHUFFLE_DISK_WRITE_BUFFER_SIZE =
     ConfigBuilder(""spark.shuffle.spill.diskWriteBufferSize"")
       .doc(""The buffer size, in bytes, to use when writing the sorted records to an on-disk file."")
       .bytesConf(ByteUnit.BYTE)
-      .checkValue(v => v > 0 && v <= Int.MaxValue,
-        s""The buffer size must be greater than 0 and less than ${Int.MaxValue}."")
+      .checkValue(v => v > 12 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,
+        s""The buffer size must be greater than 12 and less than or equal to "" +
+          s""${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}."")
       .createWithDefault(1024 * 1024)
 
   private[spark] val UNROLL_MEMORY_CHECK_PERIOD =
@@ -524,6 +570,12 @@ package object config {
       .stringConf
       .createOptional
 
+  private[spark] val UI_REQUEST_HEADER_SIZE =
+    ConfigBuilder(""spark.ui.requestHeaderSize"")
+      .doc(""Value for HTTP request header size in bytes."")
+      .bytesConf(ByteUnit.BYTE)
+      .createWithDefaultString(""8k"")
+
   private[spark] val EXTRA_LISTENERS = ConfigBuilder(""spark.extraListeners"")
     .doc(""Class names of listeners to add to SparkContext during initialization."")
     .stringConf
@@ -580,7 +632,7 @@ package object config {
       .internal()
       .doc(""For testing only, controls the size of chunks when memory mapping a file"")
       .bytesConf(ByteUnit.BYTE)
-      .createWithDefault(Int.MaxValue)
+      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)
 
   private[spark] val BARRIER_SYNC_TIMEOUT =
     ConfigBuilder(""spark.barrier.sync.timeout"")
@@ -592,6 +644,14 @@ package object config {
       .checkValue(v => v > 0, ""The value should be a positive time value."")
       .createWithDefaultString(""365d"")
 
+  private[spark] val UNSCHEDULABLE_TASKSET_TIMEOUT =
+    ConfigBuilder(""spark.scheduler.blacklist.unschedulableTaskSetTimeout"")
+      .doc(""The timeout in seconds to wait to acquire a new executor and schedule a task "" +
+        ""before aborting a TaskSet which is unschedulable because of being completely blacklisted."")
+      .timeConf(TimeUnit.SECONDS)
+      .checkValue(v => v >= 0, ""The value should be a non negative time value."")
+      .createWithDefault(120)
+
   private[spark] val BARRIER_MAX_CONCURRENT_TASKS_CHECK_INTERVAL =
     ConfigBuilder(""spark.scheduler.barrier.maxConcurrentTasksCheck.interval"")
       .doc(""Time in seconds to wait between a max concurrent tasks check failure and the next "" +
@@ -618,4 +678,14 @@ package object config {
       .intConf
       .checkValue(v => v > 0, ""The max failures should be a positive value."")
       .createWithDefault(40)
+
+  private[spark] val EXECUTOR_PLUGINS =
+    ConfigBuilder(""spark.executor.plugins"")
+      .doc(""Comma-separated list of class names for \""plugins\"" implementing "" +
+        ""org.apache.spark.ExecutorPlugin.  Plugins have the same privileges as any task "" +
+        ""in a Spark executor.  They can also interfere with task execution and fail in "" +
+        ""unexpected ways.  So be sure to only use this for trusted plugins."")
+      .stringConf
+      .toSequence
+      .createWithDefault(Nil)
 }
diff --git a/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala b/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala
index 3e60c50ada59b..7477e03bfaa76 100644
--- a/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala
+++ b/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala
@@ -91,7 +91,7 @@ class HadoopMapReduceCommitProtocol(
   private def stagingDir = new Path(path, "".spark-staging-"" + jobId)
 
   protected def setupCommitter(context: TaskAttemptContext): OutputCommitter = {
-    val format = context.getOutputFormatClass.newInstance()
+    val format = context.getOutputFormatClass.getConstructor().newInstance()
     // If OutputFormat is Configurable, we should set conf to it.
     format match {
       case c: Configurable => c.setConf(context.getConfiguration)
diff --git a/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala b/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala
index 9ebd0aa301592..3a58ea816937b 100644
--- a/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala
+++ b/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala
@@ -256,7 +256,7 @@ class HadoopMapRedWriteConfigUtil[K, V: ClassTag](conf: SerializableJobConf)
   private def getOutputFormat(): OutputFormat[K, V] = {
     require(outputFormat != null, ""Must call initOutputFormat first."")
 
-    outputFormat.newInstance()
+    outputFormat.getConstructor().newInstance()
   }
 
   // --------------------------------------------------------------------------
@@ -379,7 +379,7 @@ class HadoopMapReduceWriteConfigUtil[K, V: ClassTag](conf: SerializableConfigura
   private def getOutputFormat(): NewOutputFormat[K, V] = {
     require(outputFormat != null, ""Must call initOutputFormat first."")
 
-    outputFormat.newInstance()
+    outputFormat.getConstructor().newInstance()
   }
 
   // --------------------------------------------------------------------------
diff --git a/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala b/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala
index 7722db56ee297..0664c5ac752c1 100644
--- a/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala
+++ b/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala
@@ -154,72 +154,19 @@ class LZFCompressionCodec(conf: SparkConf) extends CompressionCodec {
  */
 @DeveloperApi
 class SnappyCompressionCodec(conf: SparkConf) extends CompressionCodec {
-  val version = SnappyCompressionCodec.version
 
-  override def compressedOutputStream(s: OutputStream): OutputStream = {
-    val blockSize = conf.getSizeAsBytes(""spark.io.compression.snappy.blockSize"", ""32k"").toInt
-    new SnappyOutputStreamWrapper(new SnappyOutputStream(s, blockSize))
-  }
-
-  override def compressedInputStream(s: InputStream): InputStream = new SnappyInputStream(s)
-}
-
-/**
- * Object guards against memory leak bug in snappy-java library:
- * (https://github.com/xerial/snappy-java/issues/131).
- * Before a new version of the library, we only call the method once and cache the result.
- */
-private final object SnappyCompressionCodec {
-  private lazy val version: String = try {
+  try {
     Snappy.getNativeLibraryVersion
   } catch {
     case e: Error => throw new IllegalArgumentException(e)
   }
-}
 
-/**
- * Wrapper over `SnappyOutputStream` which guards against write-after-close and double-close
- * issues. See SPARK-7660 for more details. This wrapping can be removed if we upgrade to a version
- * of snappy-java that contains the fix for https://github.com/xerial/snappy-java/issues/107.
- */
-private final class SnappyOutputStreamWrapper(os: SnappyOutputStream) extends OutputStream {
-
-  private[this] var closed: Boolean = false
-
-  override def write(b: Int): Unit = {
-    if (closed) {
-      throw new IOException(""Stream is closed"")
-    }
-    os.write(b)
-  }
-
-  override def write(b: Array[Byte]): Unit = {
-    if (closed) {
-      throw new IOException(""Stream is closed"")
-    }
-    os.write(b)
-  }
-
-  override def write(b: Array[Byte], off: Int, len: Int): Unit = {
-    if (closed) {
-      throw new IOException(""Stream is closed"")
-    }
-    os.write(b, off, len)
-  }
-
-  override def flush(): Unit = {
-    if (closed) {
-      throw new IOException(""Stream is closed"")
-    }
-    os.flush()
+  override def compressedOutputStream(s: OutputStream): OutputStream = {
+    val blockSize = conf.getSizeAsBytes(""spark.io.compression.snappy.blockSize"", ""32k"").toInt
+    new SnappyOutputStream(s, blockSize)
   }
 
-  override def close(): Unit = {
-    if (!closed) {
-      closed = true
-      os.close()
-    }
-  }
+  override def compressedInputStream(s: InputStream): InputStream = new SnappyInputStream(s)
 }
 
 /**
diff --git a/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala b/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala
index 0641adc2ab699..4fde2d0beaa71 100644
--- a/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala
+++ b/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala
@@ -180,6 +180,34 @@ private[spark] abstract class MemoryManager(
     onHeapStorageMemoryPool.memoryUsed + offHeapStorageMemoryPool.memoryUsed
   }
 
+  /**
+   *  On heap execution memory currently in use, in bytes.
+   */
+  final def onHeapExecutionMemoryUsed: Long = synchronized {
+    onHeapExecutionMemoryPool.memoryUsed
+  }
+
+  /**
+   *  Off heap execution memory currently in use, in bytes.
+   */
+  final def offHeapExecutionMemoryUsed: Long = synchronized {
+    offHeapExecutionMemoryPool.memoryUsed
+  }
+
+  /**
+   *  On heap storage memory currently in use, in bytes.
+   */
+  final def onHeapStorageMemoryUsed: Long = synchronized {
+    onHeapStorageMemoryPool.memoryUsed
+  }
+
+  /**
+   *  Off heap storage memory currently in use, in bytes.
+   */
+  final def offHeapStorageMemoryUsed: Long = synchronized {
+    offHeapStorageMemoryPool.memoryUsed
+  }
+
   /**
    * Returns the execution memory consumption, in bytes, for the given task.
    */
diff --git a/core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala b/core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala
new file mode 100644
index 0000000000000..cd10dad25e87b
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.metrics
+
+import java.lang.management.{BufferPoolMXBean, ManagementFactory}
+import javax.management.ObjectName
+
+import org.apache.spark.memory.MemoryManager
+
+/**
+ * Executor metric types for executor-level metrics stored in ExecutorMetrics.
+ */
+sealed trait ExecutorMetricType {
+  private[spark] def getMetricValue(memoryManager: MemoryManager): Long
+  private[spark] val name = getClass().getName().stripSuffix(""$"").split(""""""\."""""").last
+}
+
+private[spark] abstract class MemoryManagerExecutorMetricType(
+    f: MemoryManager => Long) extends ExecutorMetricType {
+  override private[spark] def getMetricValue(memoryManager: MemoryManager): Long = {
+    f(memoryManager)
+  }
+}
+
+private[spark] abstract class MBeanExecutorMetricType(mBeanName: String)
+  extends ExecutorMetricType {
+  private val bean = ManagementFactory.newPlatformMXBeanProxy(
+    ManagementFactory.getPlatformMBeanServer,
+    new ObjectName(mBeanName).toString, classOf[BufferPoolMXBean])
+
+  override private[spark] def getMetricValue(memoryManager: MemoryManager): Long = {
+    bean.getMemoryUsed
+  }
+}
+
+case object JVMHeapMemory extends ExecutorMetricType {
+  override private[spark] def getMetricValue(memoryManager: MemoryManager): Long = {
+    ManagementFactory.getMemoryMXBean.getHeapMemoryUsage().getUsed()
+  }
+}
+
+case object JVMOffHeapMemory extends ExecutorMetricType {
+  override private[spark] def getMetricValue(memoryManager: MemoryManager): Long = {
+    ManagementFactory.getMemoryMXBean.getNonHeapMemoryUsage().getUsed()
+  }
+}
+
+case object OnHeapExecutionMemory extends MemoryManagerExecutorMetricType(
+  _.onHeapExecutionMemoryUsed)
+
+case object OffHeapExecutionMemory extends MemoryManagerExecutorMetricType(
+  _.offHeapExecutionMemoryUsed)
+
+case object OnHeapStorageMemory extends MemoryManagerExecutorMetricType(
+  _.onHeapStorageMemoryUsed)
+
+case object OffHeapStorageMemory extends MemoryManagerExecutorMetricType(
+  _.offHeapStorageMemoryUsed)
+
+case object OnHeapUnifiedMemory extends MemoryManagerExecutorMetricType(
+  (m => m.onHeapExecutionMemoryUsed + m.onHeapStorageMemoryUsed))
+
+case object OffHeapUnifiedMemory extends MemoryManagerExecutorMetricType(
+  (m => m.offHeapExecutionMemoryUsed + m.offHeapStorageMemoryUsed))
+
+case object DirectPoolMemory extends MBeanExecutorMetricType(
+  ""java.nio:type=BufferPool,name=direct"")
+
+case object MappedPoolMemory extends MBeanExecutorMetricType(
+  ""java.nio:type=BufferPool,name=mapped"")
+
+private[spark] object ExecutorMetricType {
+  // List of all executor metric types
+  val values = IndexedSeq(
+    JVMHeapMemory,
+    JVMOffHeapMemory,
+    OnHeapExecutionMemory,
+    OffHeapExecutionMemory,
+    OnHeapStorageMemory,
+    OffHeapStorageMemory,
+    OnHeapUnifiedMemory,
+    OffHeapUnifiedMemory,
+    DirectPoolMemory,
+    MappedPoolMemory
+  )
+
+  // Map of executor metric type to its index in values.
+  val metricIdxMap =
+    Map[ExecutorMetricType, Int](ExecutorMetricType.values.zipWithIndex: _*)
+}
diff --git a/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala b/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
index 3457a2632277d..bb7b434e9a113 100644
--- a/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
+++ b/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
@@ -179,7 +179,7 @@ private[spark] class MetricsSystem private (
     sourceConfigs.foreach { kv =>
       val classPath = kv._2.getProperty(""class"")
       try {
-        val source = Utils.classForName(classPath).newInstance()
+        val source = Utils.classForName(classPath).getConstructor().newInstance()
         registerSource(source.asInstanceOf[Source])
       } catch {
         case e: Exception => logError(""Source class "" + classPath + "" cannot be instantiated"", e)
diff --git a/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala b/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala
index 859a2f6bcd456..61e74e05169cc 100644
--- a/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala
+++ b/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala
@@ -17,7 +17,7 @@
 
 package org.apache.spark.metrics.sink
 
-import java.util.Properties
+import java.util.{Locale, Properties}
 import java.util.concurrent.TimeUnit
 
 import com.codahale.metrics.MetricRegistry
@@ -52,7 +52,8 @@ private[spark] class StatsdSink(
 
   val pollPeriod = property.getProperty(STATSD_KEY_PERIOD, STATSD_DEFAULT_PERIOD).toInt
   val pollUnit =
-    TimeUnit.valueOf(property.getProperty(STATSD_KEY_UNIT, STATSD_DEFAULT_UNIT).toUpperCase)
+    TimeUnit.valueOf(
+      property.getProperty(STATSD_KEY_UNIT, STATSD_DEFAULT_UNIT).toUpperCase(Locale.ROOT))
 
   val prefix = property.getProperty(STATSD_KEY_PREFIX, STATSD_DEFAULT_PREFIX)
 
diff --git a/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala b/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala
index 1d8a266d0079c..a58c8fa2e763f 100644
--- a/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala
+++ b/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala
@@ -26,8 +26,8 @@ import scala.reflect.ClassTag
 
 import org.apache.spark.internal.Logging
 import org.apache.spark.network.buffer.{FileSegmentManagedBuffer, ManagedBuffer, NioManagedBuffer}
-import org.apache.spark.network.shuffle.{BlockFetchingListener, ShuffleClient, TempFileManager}
-import org.apache.spark.storage.{BlockId, StorageLevel}
+import org.apache.spark.network.shuffle.{BlockFetchingListener, DownloadFileManager, ShuffleClient}
+import org.apache.spark.storage.{BlockId, EncryptedManagedBuffer, StorageLevel}
 import org.apache.spark.util.ThreadUtils
 
 private[spark]
@@ -68,7 +68,7 @@ abstract class BlockTransferService extends ShuffleClient with Closeable with Lo
       execId: String,
       blockIds: Array[String],
       listener: BlockFetchingListener,
-      tempFileManager: TempFileManager): Unit
+      tempFileManager: DownloadFileManager): Unit
 
   /**
    * Upload a single block to a remote node, available only after [[init]] is invoked.
@@ -92,7 +92,7 @@ abstract class BlockTransferService extends ShuffleClient with Closeable with Lo
       port: Int,
       execId: String,
       blockId: String,
-      tempFileManager: TempFileManager): ManagedBuffer = {
+      tempFileManager: DownloadFileManager): ManagedBuffer = {
     // A monitor for the thread to wait on.
     val result = Promise[ManagedBuffer]()
     fetchBlocks(host, port, execId, Array(blockId),
@@ -104,6 +104,8 @@ abstract class BlockTransferService extends ShuffleClient with Closeable with Lo
           data match {
             case f: FileSegmentManagedBuffer =>
               result.success(f)
+            case e: EncryptedManagedBuffer =>
+              result.success(e)
             case _ =>
               val ret = ByteBuffer.allocate(data.size.toInt)
               ret.put(data.nioByteBuffer())
diff --git a/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala b/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala
index 1905632a936d3..dc55685b1e7bd 100644
--- a/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala
+++ b/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala
@@ -33,7 +33,7 @@ import org.apache.spark.network.buffer.{ManagedBuffer, NioManagedBuffer}
 import org.apache.spark.network.client.{RpcResponseCallback, TransportClientBootstrap, TransportClientFactory}
 import org.apache.spark.network.crypto.{AuthClientBootstrap, AuthServerBootstrap}
 import org.apache.spark.network.server._
-import org.apache.spark.network.shuffle.{BlockFetchingListener, OneForOneBlockFetcher, RetryingBlockFetcher, TempFileManager}
+import org.apache.spark.network.shuffle.{BlockFetchingListener, DownloadFileManager, OneForOneBlockFetcher, RetryingBlockFetcher}
 import org.apache.spark.network.shuffle.protocol.{UploadBlock, UploadBlockStream}
 import org.apache.spark.network.util.JavaUtils
 import org.apache.spark.serializer.JavaSerializer
@@ -106,7 +106,7 @@ private[spark] class NettyBlockTransferService(
       execId: String,
       blockIds: Array[String],
       listener: BlockFetchingListener,
-      tempFileManager: TempFileManager): Unit = {
+      tempFileManager: DownloadFileManager): Unit = {
     logTrace(s""Fetch blocks from $host:$port (executor id $execId)"")
     try {
       val blockFetchStarter = new RetryingBlockFetcher.BlockFetchStarter {
diff --git a/core/src/main/scala/org/apache/spark/package.scala b/core/src/main/scala/org/apache/spark/package.scala
index 8058a4d5dbdea..5d0639e92c36a 100644
--- a/core/src/main/scala/org/apache/spark/package.scala
+++ b/core/src/main/scala/org/apache/spark/package.scala
@@ -19,6 +19,8 @@ package org.apache
 
 import java.util.Properties
 
+import org.apache.spark.util.VersionUtils
+
 /**
  * Core Spark functionality. [[org.apache.spark.SparkContext]] serves as the main entry point to
  * Spark, while [[org.apache.spark.rdd.RDD]] is the data type representing a distributed collection,
@@ -89,6 +91,7 @@ package object spark {
   }
 
   val SPARK_VERSION = SparkBuildInfo.spark_version
+  val SPARK_VERSION_SHORT = VersionUtils.shortVersion(SparkBuildInfo.spark_version)
   val SPARK_BRANCH = SparkBuildInfo.spark_branch
   val SPARK_REVISION = SparkBuildInfo.spark_revision
   val SPARK_BUILD_USER = SparkBuildInfo.spark_build_user
diff --git a/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala b/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala
index a14bad47dfe10..039dbcbd5e035 100644
--- a/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala
@@ -41,7 +41,7 @@ private[spark] class BinaryFileRDD[T](
     // traversing a large number of directories and files. Parallelize it.
     conf.setIfUnset(FileInputFormat.LIST_STATUS_NUM_THREADS,
       Runtime.getRuntime.availableProcessors().toString)
-    val inputFormat = inputFormatClass.newInstance
+    val inputFormat = inputFormatClass.getConstructor().newInstance()
     inputFormat match {
       case configurable: Configurable =>
         configurable.setConf(conf)
diff --git a/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala b/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
index 4574c3724962e..7e76731f5e454 100644
--- a/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
@@ -143,8 +143,10 @@ class CoGroupedRDD[K: ClassTag](
 
       case shuffleDependency: ShuffleDependency[_, _, _] =>
         // Read map outputs of shuffle
+        val metrics = context.taskMetrics().createTempShuffleReadMetrics()
         val it = SparkEnv.get.shuffleManager
-          .getReader(shuffleDependency.shuffleHandle, split.index, split.index + 1, context)
+          .getReader(
+            shuffleDependency.shuffleHandle, split.index, split.index + 1, context, metrics)
           .read()
         rddIterators += ((it, depNum))
     }
diff --git a/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala b/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala
index 2d66d25ba39fa..483de28d92ab7 100644
--- a/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala
@@ -120,7 +120,7 @@ class NewHadoopRDD[K, V](
   }
 
   override def getPartitions: Array[Partition] = {
-    val inputFormat = inputFormatClass.newInstance
+    val inputFormat = inputFormatClass.getConstructor().newInstance()
     inputFormat match {
       case configurable: Configurable =>
         configurable.setConf(_conf)
@@ -183,7 +183,7 @@ class NewHadoopRDD[K, V](
         }
       }
 
-      private val format = inputFormatClass.newInstance
+      private val format = inputFormatClass.getConstructor().newInstance()
       format match {
         case configurable: Configurable =>
           configurable.setConf(conf)
diff --git a/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala b/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala
index a5992022d0832..5b1c024257529 100644
--- a/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala
@@ -35,7 +35,8 @@ import org.apache.spark.internal.Logging
  *
  *   val rdd: RDD[(String, Int)] = ...
  *   implicit val caseInsensitiveOrdering = new Ordering[String] {
- *     override def compare(a: String, b: String) = a.toLowerCase.compare(b.toLowerCase)
+ *     override def compare(a: String, b: String) =
+ *       a.toLowerCase(Locale.ROOT).compare(b.toLowerCase(Locale.ROOT))
  *   }
  *
  *   // Sort by key, using the above case insensitive ordering.
diff --git a/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala b/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala
index e68c6b1366c7f..4bf4f082d0382 100644
--- a/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala
@@ -394,7 +394,7 @@ class PairRDDFunctions[K, V](self: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is
    * greater than `p`) would trigger sparse representation of registers, which may reduce the
@@ -436,7 +436,7 @@ class PairRDDFunctions[K, V](self: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
@@ -456,7 +456,7 @@ class PairRDDFunctions[K, V](self: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
@@ -473,7 +473,7 @@ class PairRDDFunctions[K, V](self: RDD[(K, V)])
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
diff --git a/core/src/main/scala/org/apache/spark/rdd/RDD.scala b/core/src/main/scala/org/apache/spark/rdd/RDD.scala
index 61ad6dfdb2215..6a25ee20b2c68 100644
--- a/core/src/main/scala/org/apache/spark/rdd/RDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/RDD.scala
@@ -42,7 +42,8 @@ import org.apache.spark.partial.GroupedCountEvaluator
 import org.apache.spark.partial.PartialResult
 import org.apache.spark.storage.{RDDBlockId, StorageLevel}
 import org.apache.spark.util.{BoundedPriorityQueue, Utils}
-import org.apache.spark.util.collection.{OpenHashMap, Utils => collectionUtils}
+import org.apache.spark.util.collection.{ExternalAppendOnlyMap, OpenHashMap,
+  Utils => collectionUtils}
 import org.apache.spark.util.random.{BernoulliCellSampler, BernoulliSampler, PoissonSampler,
   SamplingUtils}
 
@@ -396,7 +397,20 @@ abstract class RDD[T: ClassTag](
    * Return a new RDD containing the distinct elements in this RDD.
    */
   def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
-    map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(_._1)
+    def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = {
+      // Create an instance of external append only map which ignores values.
+      val map = new ExternalAppendOnlyMap[T, Null, Null](
+        createCombiner = value => null,
+        mergeValue = (a, b) => a,
+        mergeCombiners = (a, b) => a)
+      map.insertAll(partition.map(_ -> null))
+      map.iterator.map(_._1)
+    }
+    partitioner match {
+      case Some(p) if numPartitions == partitions.length =>
+        mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true)
+      case _ => map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(_._1)
+    }
   }
 
   /**
@@ -1244,7 +1258,7 @@ abstract class RDD[T: ClassTag](
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater
    * than `p`) would trigger sparse representation of registers, which may reduce the memory
@@ -1276,7 +1290,7 @@ abstract class RDD[T: ClassTag](
    *
    * The algorithm used is based on streamlib's implementation of ""HyperLogLog in Practice:
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm"", available
-   * <a href=""http://dx.doi.org/10.1145/2452376.2452456"">here</a>.
+   * <a href=""https://doi.org/10.1145/2452376.2452456"">here</a>.
    *
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
    *                   It must be greater than 0.000017.
diff --git a/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala b/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala
index 53d69ba26811f..3abb2d8a11f35 100644
--- a/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala
@@ -41,7 +41,7 @@ import org.apache.spark.internal.Logging
  * There is no particular relationship between an operation scope and a stage or a job.
  * A scope may live inside one stage (e.g. map) or span across multiple jobs (e.g. take).
  */
-@JsonInclude(Include.NON_NULL)
+@JsonInclude(Include.NON_ABSENT)
 @JsonPropertyOrder(Array(""id"", ""name"", ""parent""))
 private[spark] class RDDOperationScope(
     val name: String,
diff --git a/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala b/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala
index e8f9b27b7eb55..5ec99b7f4f3ab 100644
--- a/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala
@@ -101,7 +101,9 @@ class ShuffledRDD[K: ClassTag, V: ClassTag, C: ClassTag](
 
   override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {
     val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]
-    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)
+    val metrics = context.taskMetrics().createTempShuffleReadMetrics()
+    SparkEnv.get.shuffleManager.getReader(
+      dep.shuffleHandle, split.index, split.index + 1, context, metrics)
       .read()
       .asInstanceOf[Iterator[(K, C)]]
   }
diff --git a/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala b/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala
index a733eaa5d7e53..42d190377f104 100644
--- a/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala
@@ -107,9 +107,14 @@ private[spark] class SubtractedRDD[K: ClassTag, V: ClassTag, W: ClassTag](
             .asInstanceOf[Iterator[Product2[K, V]]].foreach(op)
 
         case shuffleDependency: ShuffleDependency[_, _, _] =>
+          val metrics = context.taskMetrics().createTempShuffleReadMetrics()
           val iter = SparkEnv.get.shuffleManager
             .getReader(
-              shuffleDependency.shuffleHandle, partition.index, partition.index + 1, context)
+              shuffleDependency.shuffleHandle,
+              partition.index,
+              partition.index + 1,
+              context,
+              metrics)
             .read()
           iter.foreach(op)
       }
diff --git a/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala b/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala
index 9f3d0745c33c9..eada762b99c8e 100644
--- a/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala
@@ -44,7 +44,7 @@ private[spark] class WholeTextFileRDD(
     // traversing a large number of directories and files. Parallelize it.
     conf.setIfUnset(FileInputFormat.LIST_STATUS_NUM_THREADS,
       Runtime.getRuntime.availableProcessors().toString)
-    val inputFormat = inputFormatClass.newInstance
+    val inputFormat = inputFormatClass.getConstructor().newInstance()
     inputFormat match {
       case configurable: Configurable =>
         configurable.setConf(conf)
diff --git a/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala b/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala
index 0a5fe5a1d3ee1..bd0fe90b1f3b6 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala
@@ -22,7 +22,7 @@ import org.apache.spark.annotation.DeveloperApi
 
 /**
  * :: DeveloperApi ::
- * Information about an [[org.apache.spark.Accumulable]] modified during a task or stage.
+ * Information about an [[org.apache.spark.util.AccumulatorV2]] modified during a task or stage.
  *
  * @param id accumulator ID
  * @param name accumulator name
@@ -47,33 +47,3 @@ case class AccumulableInfo private[spark] (
     private[spark] val countFailedValues: Boolean,
     // TODO: use this to identify internal task metrics instead of encoding it in the name
     private[spark] val metadata: Option[String] = None)
-
-
-/**
- * A collection of deprecated constructors. This will be removed soon.
- */
-object AccumulableInfo {
-
-  @deprecated(""do not create AccumulableInfo"", ""2.0.0"")
-  def apply(
-      id: Long,
-      name: String,
-      update: Option[String],
-      value: String,
-      internal: Boolean): AccumulableInfo = {
-    new AccumulableInfo(
-      id, Option(name), update, Option(value), internal, countFailedValues = false)
-  }
-
-  @deprecated(""do not create AccumulableInfo"", ""2.0.0"")
-  def apply(id: Long, name: String, update: Option[String], value: String): AccumulableInfo = {
-    new AccumulableInfo(
-      id, Option(name), update, Option(value), internal = false, countFailedValues = false)
-  }
-
-  @deprecated(""do not create AccumulableInfo"", ""2.0.0"")
-  def apply(id: Long, name: String, value: String): AccumulableInfo = {
-    new AccumulableInfo(
-      id, Option(name), None, Option(value), internal = false, countFailedValues = false)
-  }
-}
diff --git a/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala b/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala
index e2b6df4600590..7cd2b862216ee 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala
@@ -169,7 +169,8 @@ private class AsyncEventQueue(
           val prevLastReportTimestamp = lastReportTimestamp
           lastReportTimestamp = System.currentTimeMillis()
           val previous = new java.util.Date(prevLastReportTimestamp)
-          logWarning(s""Dropped $droppedCount events from $name since $previous."")
+          logWarning(s""Dropped $droppedCount events from $name since "" +
+            s""${if (prevLastReportTimestamp == 0) ""the application started"" else s""$previous""}."")
         }
       }
     }
diff --git a/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala b/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala
index 980fbbe516b91..ef6d02d85c27b 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala
@@ -146,21 +146,31 @@ private[scheduler] class BlacklistTracker (
     nextExpiryTime = math.min(execMinExpiry, nodeMinExpiry)
   }
 
+  private def killExecutor(exec: String, msg: String): Unit = {
+    allocationClient match {
+      case Some(a) =>
+        logInfo(msg)
+        a.killExecutors(Seq(exec), adjustTargetNumExecutors = false, countFailures = false,
+          force = true)
+      case None =>
+        logInfo(s""Not attempting to kill blacklisted executor id $exec "" +
+          s""since allocation client is not defined."")
+    }
+  }
+
   private def killBlacklistedExecutor(exec: String): Unit = {
     if (conf.get(config.BLACKLIST_KILL_ENABLED)) {
-      allocationClient match {
-        case Some(a) =>
-          logInfo(s""Killing blacklisted executor id $exec "" +
-            s""since ${config.BLACKLIST_KILL_ENABLED.key} is set."")
-          a.killExecutors(Seq(exec), adjustTargetNumExecutors = false, countFailures = false,
-            force = true)
-        case None =>
-          logWarning(s""Not attempting to kill blacklisted executor id $exec "" +
-            s""since allocation client is not defined."")
-      }
+      killExecutor(exec,
+        s""Killing blacklisted executor id $exec since ${config.BLACKLIST_KILL_ENABLED.key} is set."")
     }
   }
 
+  private[scheduler] def killBlacklistedIdleExecutor(exec: String): Unit = {
+    killExecutor(exec,
+      s""Killing blacklisted idle executor id $exec because of task unschedulability and trying "" +
+        ""to acquire a new executor."")
+  }
+
   private def killExecutorsOnBlacklistedNode(node: String): Unit = {
     if (conf.get(config.BLACKLIST_KILL_ENABLED)) {
       allocationClient match {
diff --git a/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala b/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
index 50c91da8b13d1..06966e77db81e 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
@@ -35,7 +35,7 @@ import org.apache.commons.lang3.SerializationUtils
 
 import org.apache.spark._
 import org.apache.spark.broadcast.Broadcast
-import org.apache.spark.executor.TaskMetrics
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config
 import org.apache.spark.network.util.JavaUtils
@@ -264,8 +264,11 @@ private[spark] class DAGScheduler(
       execId: String,
       // (taskId, stageId, stageAttemptId, accumUpdates)
       accumUpdates: Array[(Long, Int, Int, Seq[AccumulableInfo])],
-      blockManagerId: BlockManagerId): Boolean = {
-    listenerBus.post(SparkListenerExecutorMetricsUpdate(execId, accumUpdates))
+      blockManagerId: BlockManagerId,
+      // executor metrics indexed by ExecutorMetricType.values
+      executorUpdates: ExecutorMetrics): Boolean = {
+    listenerBus.post(SparkListenerExecutorMetricsUpdate(execId, accumUpdates,
+      Some(executorUpdates)))
     blockManagerMaster.driverEndpoint.askSync[Boolean](
       BlockManagerHeartbeat(blockManagerId), new RpcTimeout(600 seconds, ""BlockManagerHeartbeat""))
   }
@@ -1242,9 +1245,10 @@ private[spark] class DAGScheduler(
   private def updateAccumulators(event: CompletionEvent): Unit = {
     val task = event.task
     val stage = stageIdToStage(task.stageId)
-    try {
-      event.accumUpdates.foreach { updates =>
-        val id = updates.id
+
+    event.accumUpdates.foreach { updates =>
+      val id = updates.id
+      try {
         // Find the corresponding accumulator on the driver and update it
         val acc: AccumulatorV2[Any, Any] = AccumulatorContext.get(id) match {
           case Some(accum) => accum.asInstanceOf[AccumulatorV2[Any, Any]]
@@ -1258,10 +1262,17 @@ private[spark] class DAGScheduler(
           event.taskInfo.setAccumulables(
             acc.toInfo(Some(updates.value), Some(acc.value)) +: event.taskInfo.accumulables)
         }
+      } catch {
+        case NonFatal(e) =>
+          // Log the class name to make it easy to find the bad implementation
+          val accumClassName = AccumulatorContext.get(id) match {
+            case Some(accum) => accum.getClass.getName
+            case None => ""Unknown class""
+          }
+          logError(
+            s""Failed to update accumulator $id ($accumClassName) for task ${task.partitionId}"",
+            e)
       }
-    } catch {
-      case NonFatal(e) =>
-        logError(s""Failed to update accumulators for task ${task.partitionId}"", e)
     }
   }
 
@@ -1284,6 +1295,27 @@ private[spark] class DAGScheduler(
       Utils.getFormattedClassName(event.task), event.reason, event.taskInfo, taskMetrics))
   }
 
+  /**
+   * Check [[SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL]] in job properties to see if we should
+   * interrupt running tasks. Returns `false` if the property value is not a boolean value
+   */
+  private def shouldInterruptTaskThread(job: ActiveJob): Boolean = {
+    if (job.properties == null) {
+      false
+    } else {
+      val shouldInterruptThread =
+        job.properties.getProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, ""false"")
+      try {
+        shouldInterruptThread.toBoolean
+      } catch {
+        case e: IllegalArgumentException =>
+          logWarning(s""${SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL} in Job ${job.jobId} "" +
+            s""is invalid: $shouldInterruptThread. Using 'false' instead"", e)
+          false
+      }
+    }
+  }
+
   /**
    * Responds to a task finishing. This is called inside the event loop so it assumes that it can
    * modify the scheduler's internal state. Use taskEnded() to post a task end event from outside.
@@ -1353,6 +1385,21 @@ private[spark] class DAGScheduler(
                   if (job.numFinished == job.numPartitions) {
                     markStageAsFinished(resultStage)
                     cleanupStateForJobAndIndependentStages(job)
+                    try {
+                      // killAllTaskAttempts will fail if a SchedulerBackend does not implement
+                      // killTask.
+                      logInfo(s""Job ${job.jobId} is finished. Cancelling potential speculative "" +
+                        ""or zombie tasks for this job"")
+                      // ResultStage is only used by this job. It's safe to kill speculative or
+                      // zombie tasks in this stage.
+                      taskScheduler.killAllTaskAttempts(
+                        stageId,
+                        shouldInterruptTaskThread(job),
+                        reason = ""Stage finished"")
+                    } catch {
+                      case e: UnsupportedOperationException =>
+                        logWarning(s""Could not cancel tasks for stage $stageId"", e)
+                    }
                     listenerBus.post(
                       SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded))
                   }
@@ -1362,7 +1409,7 @@ private[spark] class DAGScheduler(
                   try {
                     job.listener.taskSucceeded(rt.outputId, event.result)
                   } catch {
-                    case e: Exception =>
+                    case e: Throwable if !Utils.isFatalError(e) =>
                       // TODO: Perhaps we want to mark the resultStage as failed?
                       job.listener.jobFailed(new SparkDriverExecutionException(e))
                   }
@@ -1879,10 +1926,6 @@ private[spark] class DAGScheduler(
     val error = new SparkException(failureReason, exception.getOrElse(null))
     var ableToCancelStages = true
 
-    val shouldInterruptThread =
-      if (job.properties == null) false
-      else job.properties.getProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, ""false"").toBoolean
-
     // Cancel all independent, running stages.
     val stages = jobIdToStageIds(job.jobId)
     if (stages.isEmpty) {
@@ -1902,12 +1945,12 @@ private[spark] class DAGScheduler(
           val stage = stageIdToStage(stageId)
           if (runningStages.contains(stage)) {
             try { // cancelTasks will fail if a SchedulerBackend does not implement killTask
-              taskScheduler.cancelTasks(stageId, shouldInterruptThread)
+              taskScheduler.cancelTasks(stageId, shouldInterruptTaskThread(job))
               markStageAsFinished(stage, Some(failureReason))
             } catch {
               case e: UnsupportedOperationException =>
-                logInfo(s""Could not cancel tasks for stage $stageId"", e)
-              ableToCancelStages = false
+                logWarning(s""Could not cancel tasks for stage $stageId"", e)
+                ableToCancelStages = false
             }
           }
         }
diff --git a/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala b/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala
index 69bc51c1ecf90..5f697fe99258d 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala
@@ -20,22 +20,19 @@ package org.apache.spark.scheduler
 import java.io._
 import java.net.URI
 import java.nio.charset.StandardCharsets
-import java.util.EnumSet
 import java.util.Locale
 
-import scala.collection.mutable
-import scala.collection.mutable.ArrayBuffer
+import scala.collection.mutable.{ArrayBuffer, Map}
 
 import org.apache.hadoop.conf.Configuration
 import org.apache.hadoop.fs.{FileSystem, FSDataOutputStream, Path}
 import org.apache.hadoop.fs.permission.FsPermission
-import org.apache.hadoop.hdfs.DFSOutputStream
-import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag
 import org.json4s.JsonAST.JValue
 import org.json4s.jackson.JsonMethods._
 
 import org.apache.spark.{SPARK_VERSION, SparkConf}
 import org.apache.spark.deploy.SparkHadoopUtil
+import org.apache.spark.executor.ExecutorMetrics
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
 import org.apache.spark.io.CompressionCodec
@@ -51,6 +48,7 @@ import org.apache.spark.util.{JsonProtocol, Utils}
  *   spark.eventLog.overwrite - Whether to overwrite any existing files.
  *   spark.eventLog.dir - Path to the directory in which events are logged.
  *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams
+ *   spark.eventLog.logStageExecutorMetrics.enabled - Whether to log stage executor metrics
  */
 private[spark] class EventLoggingListener(
     appId: String,
@@ -69,6 +67,8 @@ private[spark] class EventLoggingListener(
   private val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)
   private val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)
   private val shouldLogBlockUpdates = sparkConf.get(EVENT_LOG_BLOCK_UPDATES)
+  private val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)
+  private val shouldLogStageExecutorMetrics = sparkConf.get(EVENT_LOG_STAGE_EXECUTOR_METRICS)
   private val testing = sparkConf.get(EVENT_LOG_TESTING)
   private val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt
   private val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)
@@ -93,6 +93,9 @@ private[spark] class EventLoggingListener(
   // Visible for tests only.
   private[scheduler] val logPath = getLogPath(logBaseDir, appId, appAttemptId, compressionCodecName)
 
+  // map of (stageId, stageAttempt), to peak executor metrics for the stage
+  private val liveStageExecutorMetrics = Map.empty[(Int, Int), Map[String, ExecutorMetrics]]
+
   /**
    * Creates the log file in the configured log directory.
    */
@@ -117,7 +120,11 @@ private[spark] class EventLoggingListener(
       if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == ""file"") {
         new FileOutputStream(uri.getPath)
       } else {
-        hadoopDataStream = Some(fileSystem.create(path))
+        hadoopDataStream = Some(if (shouldAllowECLogs) {
+          fileSystem.create(path)
+        } else {
+          SparkHadoopUtil.createNonECFile(fileSystem, path)
+        })
         hadoopDataStream.get
       }
 
@@ -144,10 +151,7 @@ private[spark] class EventLoggingListener(
     // scalastyle:on println
     if (flushLogger) {
       writer.foreach(_.flush())
-      hadoopDataStream.foreach(ds => ds.getWrappedStream match {
-        case wrapped: DFSOutputStream => wrapped.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH))
-        case _ => ds.hflush()
-      })
+      hadoopDataStream.foreach(_.hflush())
     }
     if (testing) {
       loggedEvents += eventJson
@@ -155,7 +159,14 @@ private[spark] class EventLoggingListener(
   }
 
   // Events that do not trigger a flush
-  override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = logEvent(event)
+  override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = {
+    logEvent(event)
+    if (shouldLogStageExecutorMetrics) {
+      // record the peak metrics for the new stage
+      liveStageExecutorMetrics.put((event.stageInfo.stageId, event.stageInfo.attemptNumber()),
+        Map.empty[String, ExecutorMetrics])
+    }
+  }
 
   override def onTaskStart(event: SparkListenerTaskStart): Unit = logEvent(event)
 
@@ -169,6 +180,26 @@ private[spark] class EventLoggingListener(
 
   // Events that trigger a flush
   override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {
+    if (shouldLogStageExecutorMetrics) {
+      // clear out any previous attempts, that did not have a stage completed event
+      val prevAttemptId = event.stageInfo.attemptNumber() - 1
+      for (attemptId <- 0 to prevAttemptId) {
+        liveStageExecutorMetrics.remove((event.stageInfo.stageId, attemptId))
+      }
+
+      // log the peak executor metrics for the stage, for each live executor,
+      // whether or not the executor is running tasks for the stage
+      val executorOpt = liveStageExecutorMetrics.remove(
+        (event.stageInfo.stageId, event.stageInfo.attemptNumber()))
+      executorOpt.foreach { execMap =>
+        execMap.foreach { case (executorId, peakExecutorMetrics) =>
+            logEvent(new SparkListenerStageExecutorMetrics(executorId, event.stageInfo.stageId,
+              event.stageInfo.attemptNumber(), peakExecutorMetrics))
+        }
+      }
+    }
+
+    // log stage completed event
     logEvent(event, flushLogger = true)
   }
 
@@ -234,8 +265,18 @@ private[spark] class EventLoggingListener(
     }
   }
 
-  // No-op because logging every update would be overkill
-  override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = { }
+  override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = {
+    if (shouldLogStageExecutorMetrics) {
+      // For the active stages, record any new peak values for the memory metrics for the executor
+      event.executorUpdates.foreach { executorUpdates =>
+        liveStageExecutorMetrics.values.foreach { peakExecutorMetrics =>
+          val peakMetrics = peakExecutorMetrics.getOrElseUpdate(
+            event.execId, new ExecutorMetrics())
+          peakMetrics.compareAndUpdatePeakValues(executorUpdates)
+        }
+      }
+    }
+  }
 
   override def onOtherEvent(event: SparkListenerEvent): Unit = {
     if (event.logEvent) {
@@ -296,7 +337,7 @@ private[spark] object EventLoggingListener extends Logging {
   private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(""770"", 8).toShort)
 
   // A cache for compression codecs to avoid creating the same codec many times
-  private val codecMap = new mutable.HashMap[String, CompressionCodec]
+  private val codecMap = Map.empty[String, CompressionCodec]
 
   /**
    * Write metadata about an event log to the given stream.
@@ -341,19 +382,15 @@ private[spark] object EventLoggingListener extends Logging {
       appId: String,
       appAttemptId: Option[String],
       compressionCodecName: Option[String] = None): String = {
-    val base = new Path(logBaseDir).toString.stripSuffix(""/"") + ""/"" + sanitize(appId)
+    val base = new Path(logBaseDir).toString.stripSuffix(""/"") + ""/"" + Utils.sanitizeDirName(appId)
     val codec = compressionCodecName.map(""."" + _).getOrElse("""")
     if (appAttemptId.isDefined) {
-      base + ""_"" + sanitize(appAttemptId.get) + codec
+      base + ""_"" + Utils.sanitizeDirName(appAttemptId.get) + codec
     } else {
       base + codec
     }
   }
 
-  private def sanitize(str: String): String = {
-    str.replaceAll(""[ :/]"", ""-"").replaceAll(""[.${}'\""]"", ""_"").toLowerCase(Locale.ROOT)
-  }
-
   /**
    * Opens an event log file and returns an input stream that contains the event data.
    *
diff --git a/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala b/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala
index 7e1d75fe723d6..64f0a060a247c 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala
@@ -20,7 +20,6 @@ package org.apache.spark.scheduler
 import java.io.{Externalizable, ObjectInput, ObjectOutput}
 
 import scala.collection.mutable
-import scala.collection.mutable.ArrayBuffer
 
 import org.roaringbitmap.RoaringBitmap
 
@@ -31,8 +30,7 @@ import org.apache.spark.util.Utils
 
 /**
  * Result returned by a ShuffleMapTask to a scheduler. Includes the block manager address that the
- * task ran on, the sizes of outputs for each reducer, and the number of outputs of the map task,
- * for passing on to the reduce tasks.
+ * task ran on as well as the sizes of outputs for each reducer, for passing on to the reduce tasks.
  */
 private[spark] sealed trait MapStatus {
   /** Location where this task was run. */
@@ -45,23 +43,24 @@ private[spark] sealed trait MapStatus {
    * necessary for correctness, since block fetchers are allowed to skip zero-size blocks.
    */
   def getSizeForBlock(reduceId: Int): Long
-
-  /**
-   * The number of outputs for the map task.
-   */
-  def numberOfOutput: Long
 }
 
 
 private[spark] object MapStatus {
 
-  def apply(loc: BlockManagerId, uncompressedSizes: Array[Long], numOutput: Long): MapStatus = {
-    if (uncompressedSizes.length >  Option(SparkEnv.get)
-      .map(_.conf.get(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS))
-      .getOrElse(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS.defaultValue.get)) {
-      HighlyCompressedMapStatus(loc, uncompressedSizes, numOutput)
+  /**
+   * Min partition number to use [[HighlyCompressedMapStatus]]. A bit ugly here because in test
+   * code we can't assume SparkEnv.get exists.
+   */
+  private lazy val minPartitionsToUseHighlyCompressMapStatus = Option(SparkEnv.get)
+    .map(_.conf.get(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS))
+    .getOrElse(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS.defaultValue.get)
+
+  def apply(loc: BlockManagerId, uncompressedSizes: Array[Long]): MapStatus = {
+    if (uncompressedSizes.length > minPartitionsToUseHighlyCompressMapStatus) {
+      HighlyCompressedMapStatus(loc, uncompressedSizes)
     } else {
-      new CompressedMapStatus(loc, uncompressedSizes, numOutput)
+      new CompressedMapStatus(loc, uncompressedSizes)
     }
   }
 
@@ -104,34 +103,29 @@ private[spark] object MapStatus {
  */
 private[spark] class CompressedMapStatus(
     private[this] var loc: BlockManagerId,
-    private[this] var compressedSizes: Array[Byte],
-    private[this] var numOutput: Long)
+    private[this] var compressedSizes: Array[Byte])
   extends MapStatus with Externalizable {
 
-  protected def this() = this(null, null.asInstanceOf[Array[Byte]], -1)  // For deserialization only
+  protected def this() = this(null, null.asInstanceOf[Array[Byte]])  // For deserialization only
 
-  def this(loc: BlockManagerId, uncompressedSizes: Array[Long], numOutput: Long) {
-    this(loc, uncompressedSizes.map(MapStatus.compressSize), numOutput)
+  def this(loc: BlockManagerId, uncompressedSizes: Array[Long]) {
+    this(loc, uncompressedSizes.map(MapStatus.compressSize))
   }
 
   override def location: BlockManagerId = loc
 
-  override def numberOfOutput: Long = numOutput
-
   override def getSizeForBlock(reduceId: Int): Long = {
     MapStatus.decompressSize(compressedSizes(reduceId))
   }
 
   override def writeExternal(out: ObjectOutput): Unit = Utils.tryOrIOException {
     loc.writeExternal(out)
-    out.writeLong(numOutput)
     out.writeInt(compressedSizes.length)
     out.write(compressedSizes)
   }
 
   override def readExternal(in: ObjectInput): Unit = Utils.tryOrIOException {
     loc = BlockManagerId(in)
-    numOutput = in.readLong()
     val len = in.readInt()
     compressedSizes = new Array[Byte](len)
     in.readFully(compressedSizes)
@@ -154,20 +148,17 @@ private[spark] class HighlyCompressedMapStatus private (
     private[this] var numNonEmptyBlocks: Int,
     private[this] var emptyBlocks: RoaringBitmap,
     private[this] var avgSize: Long,
-    private var hugeBlockSizes: Map[Int, Byte],
-    private[this] var numOutput: Long)
+    private[this] var hugeBlockSizes: scala.collection.Map[Int, Byte])
   extends MapStatus with Externalizable {
 
   // loc could be null when the default constructor is called during deserialization
   require(loc == null || avgSize > 0 || hugeBlockSizes.size > 0 || numNonEmptyBlocks == 0,
     ""Average size can only be zero for map stages that produced no output"")
 
-  protected def this() = this(null, -1, null, -1, null, -1)  // For deserialization only
+  protected def this() = this(null, -1, null, -1, null)  // For deserialization only
 
   override def location: BlockManagerId = loc
 
-  override def numberOfOutput: Long = numOutput
-
   override def getSizeForBlock(reduceId: Int): Long = {
     assert(hugeBlockSizes != null)
     if (emptyBlocks.contains(reduceId)) {
@@ -182,7 +173,6 @@ private[spark] class HighlyCompressedMapStatus private (
 
   override def writeExternal(out: ObjectOutput): Unit = Utils.tryOrIOException {
     loc.writeExternal(out)
-    out.writeLong(numOutput)
     emptyBlocks.writeExternal(out)
     out.writeLong(avgSize)
     out.writeInt(hugeBlockSizes.size)
@@ -194,26 +184,22 @@ private[spark] class HighlyCompressedMapStatus private (
 
   override def readExternal(in: ObjectInput): Unit = Utils.tryOrIOException {
     loc = BlockManagerId(in)
-    numOutput = in.readLong()
     emptyBlocks = new RoaringBitmap()
     emptyBlocks.readExternal(in)
     avgSize = in.readLong()
     val count = in.readInt()
-    val hugeBlockSizesArray = mutable.ArrayBuffer[Tuple2[Int, Byte]]()
+    val hugeBlockSizesImpl = mutable.Map.empty[Int, Byte]
     (0 until count).foreach { _ =>
       val block = in.readInt()
       val size = in.readByte()
-      hugeBlockSizesArray += Tuple2(block, size)
+      hugeBlockSizesImpl(block) = size
     }
-    hugeBlockSizes = hugeBlockSizesArray.toMap
+    hugeBlockSizes = hugeBlockSizesImpl
   }
 }
 
 private[spark] object HighlyCompressedMapStatus {
-  def apply(
-      loc: BlockManagerId,
-      uncompressedSizes: Array[Long],
-      numOutput: Long): HighlyCompressedMapStatus = {
+  def apply(loc: BlockManagerId, uncompressedSizes: Array[Long]): HighlyCompressedMapStatus = {
     // We must keep track of which blocks are empty so that we don't report a zero-sized
     // block as being non-empty (or vice-versa) when using the average block size.
     var i = 0
@@ -228,7 +214,7 @@ private[spark] object HighlyCompressedMapStatus {
     val threshold = Option(SparkEnv.get)
       .map(_.conf.get(config.SHUFFLE_ACCURATE_BLOCK_THRESHOLD))
       .getOrElse(config.SHUFFLE_ACCURATE_BLOCK_THRESHOLD.defaultValue.get)
-    val hugeBlockSizesArray = ArrayBuffer[Tuple2[Int, Byte]]()
+    val hugeBlockSizes = mutable.Map.empty[Int, Byte]
     while (i < totalNumBlocks) {
       val size = uncompressedSizes(i)
       if (size > 0) {
@@ -239,7 +225,7 @@ private[spark] object HighlyCompressedMapStatus {
           totalSmallBlockSize += size
           numSmallBlocks += 1
         } else {
-          hugeBlockSizesArray += Tuple2(i, MapStatus.compressSize(uncompressedSizes(i)))
+          hugeBlockSizes(i) = MapStatus.compressSize(uncompressedSizes(i))
         }
       } else {
         emptyBlocks.add(i)
@@ -254,6 +240,6 @@ private[spark] object HighlyCompressedMapStatus {
     emptyBlocks.trim()
     emptyBlocks.runOptimize()
     new HighlyCompressedMapStatus(loc, numNonEmptyBlocks, emptyBlocks, avgSize,
-      hugeBlockSizesArray.toMap, numOutput)
+      hugeBlockSizes)
   }
 }
diff --git a/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala b/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala
index 226c23733c870..4c6b0c1227b18 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala
@@ -118,6 +118,8 @@ private[spark] class ReplayListenerBus extends SparkListenerBus with Logging {
       case e: HaltReplayException =>
         // Just stop replay.
       case _: EOFException if maybeTruncated =>
+      case _: IOException if maybeTruncated =>
+        logWarning(s""Failed to read Spark event log: $sourceName"")
       case ioe: IOException =>
         throw ioe
       case e: Exception =>
diff --git a/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala b/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala
index f2cd65fd523ab..5412717d61988 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala
@@ -95,7 +95,8 @@ private[spark] class ShuffleMapTask(
     var writer: ShuffleWriter[Any, Any] = null
     try {
       val manager = SparkEnv.get.shuffleManager
-      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)
+      writer = manager.getWriter[Any, Any](
+        dep.shuffleHandle, partitionId, context, context.taskMetrics().shuffleWriteMetrics)
       writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
       writer.stop(success = true).get
     } catch {
diff --git a/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala b/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala
index 8a112f6a37b96..e92b8a2718df0 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala
@@ -26,7 +26,7 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo
 
 import org.apache.spark.{SparkConf, TaskEndReason}
 import org.apache.spark.annotation.DeveloperApi
-import org.apache.spark.executor.TaskMetrics
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}
 import org.apache.spark.scheduler.cluster.ExecutorInfo
 import org.apache.spark.storage.{BlockManagerId, BlockUpdatedInfo}
 import org.apache.spark.ui.SparkUI
@@ -160,11 +160,29 @@ case class SparkListenerBlockUpdated(blockUpdatedInfo: BlockUpdatedInfo) extends
  * Periodic updates from executors.
  * @param execId executor id
  * @param accumUpdates sequence of (taskId, stageId, stageAttemptId, accumUpdates)
+ * @param executorUpdates executor level metrics updates
  */
 @DeveloperApi
 case class SparkListenerExecutorMetricsUpdate(
     execId: String,
-    accumUpdates: Seq[(Long, Int, Int, Seq[AccumulableInfo])])
+    accumUpdates: Seq[(Long, Int, Int, Seq[AccumulableInfo])],
+    executorUpdates: Option[ExecutorMetrics] = None)
+  extends SparkListenerEvent
+
+/**
+ * Peak metric values for the executor for the stage, written to the history log at stage
+ * completion.
+ * @param execId executor id
+ * @param stageId stage id
+ * @param stageAttemptId stage attempt
+ * @param executorMetrics executor level metrics, indexed by ExecutorMetricType.values
+ */
+@DeveloperApi
+case class SparkListenerStageExecutorMetrics(
+    execId: String,
+    stageId: Int,
+    stageAttemptId: Int,
+    executorMetrics: ExecutorMetrics)
   extends SparkListenerEvent
 
 @DeveloperApi
@@ -264,6 +282,13 @@ private[spark] trait SparkListenerInterface {
    */
   def onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit
 
+  /**
+   * Called with the peak memory metrics for a given (executor, stage) combination. Note that this
+   * is only present when reading from the event log (as in the history server), and is never
+   * called in a live application.
+   */
+  def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit
+
   /**
    * Called when the driver registers a new executor.
    */
@@ -361,6 +386,9 @@ abstract class SparkListener extends SparkListenerInterface {
   override def onExecutorMetricsUpdate(
       executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = { }
 
+  override def onStageExecutorMetrics(
+      executorMetrics: SparkListenerStageExecutorMetrics): Unit = { }
+
   override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = { }
 
   override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = { }
diff --git a/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala b/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala
index ff19cc65552e0..8f6b7ad309602 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala
@@ -57,6 +57,8 @@ private[spark] trait SparkListenerBus
         listener.onApplicationEnd(applicationEnd)
       case metricsUpdate: SparkListenerExecutorMetricsUpdate =>
         listener.onExecutorMetricsUpdate(metricsUpdate)
+      case stageExecutorMetrics: SparkListenerStageExecutorMetrics =>
+        listener.onStageExecutorMetrics(stageExecutorMetrics)
       case executorAdded: SparkListenerExecutorAdded =>
         listener.onExecutorAdded(executorAdded)
       case executorRemoved: SparkListenerExecutorRemoved =>
diff --git a/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala b/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala
index 903e25b7986f2..33a68f24bd53a 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala
@@ -30,7 +30,7 @@ import org.apache.spark.storage.RDDInfo
 @DeveloperApi
 class StageInfo(
     val stageId: Int,
-    @deprecated(""Use attemptNumber instead"", ""2.3.0"") val attemptId: Int,
+    private val attemptId: Int,
     val name: String,
     val numTasks: Int,
     val rddInfos: Seq[RDDInfo],
@@ -56,6 +56,8 @@ class StageInfo(
     completionTime = Some(System.currentTimeMillis)
   }
 
+  // This would just be the second constructor arg, except we need to maintain this method
+  // with parentheses for compatibility
   def attemptNumber(): Int = attemptId
 
   private[spark] def getStatusString: String = {
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala
index 95f7ae4fd39a2..94221eb0d5515 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala
@@ -17,6 +17,7 @@
 
 package org.apache.spark.scheduler
 
+import org.apache.spark.executor.ExecutorMetrics
 import org.apache.spark.scheduler.SchedulingMode.SchedulingMode
 import org.apache.spark.storage.BlockManagerId
 import org.apache.spark.util.AccumulatorV2
@@ -74,14 +75,15 @@ private[spark] trait TaskScheduler {
   def defaultParallelism(): Int
 
   /**
-   * Update metrics for in-progress tasks and let the master know that the BlockManager is still
-   * alive. Return true if the driver knows about the given block manager. Otherwise, return false,
-   * indicating that the block manager should re-register.
+   * Update metrics for in-progress tasks and executor metrics, and let the master know that the
+   * BlockManager is still alive. Return true if the driver knows about the given block manager.
+   * Otherwise, return false, indicating that the block manager should re-register.
    */
   def executorHeartbeatReceived(
       execId: String,
       accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])],
-      blockManagerId: BlockManagerId): Boolean
+      blockManagerId: BlockManagerId,
+      executorUpdates: ExecutorMetrics): Boolean
 
   /**
    * Get an application ID associated with the job.
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala
index 8b71170668639..61556ea642614 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala
@@ -28,13 +28,14 @@ import scala.util.Random
 
 import org.apache.spark._
 import org.apache.spark.TaskState.TaskState
+import org.apache.spark.executor.ExecutorMetrics
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config
 import org.apache.spark.rpc.RpcEndpoint
 import org.apache.spark.scheduler.SchedulingMode.SchedulingMode
 import org.apache.spark.scheduler.TaskLocality.TaskLocality
 import org.apache.spark.storage.BlockManagerId
-import org.apache.spark.util.{AccumulatorV2, ThreadUtils, Utils}
+import org.apache.spark.util.{AccumulatorV2, SystemClock, ThreadUtils, Utils}
 
 /**
  * Schedules tasks for multiple types of clusters by acting through a SchedulerBackend.
@@ -116,6 +117,11 @@ private[spark] class TaskSchedulerImpl(
 
   protected val executorIdToHost = new HashMap[String, String]
 
+  private val abortTimer = new Timer(true)
+  private val clock = new SystemClock
+  // Exposed for testing
+  val unschedulableTaskSetToExpiryTime = new HashMap[TaskSetManager, Long]
+
   // Listener object to pass upcalls into
   var dagScheduler: DAGScheduler = null
 
@@ -414,9 +420,53 @@ private[spark] class TaskSchedulerImpl(
             launchedAnyTask |= launchedTaskAtCurrentMaxLocality
           } while (launchedTaskAtCurrentMaxLocality)
         }
+
         if (!launchedAnyTask) {
-          taskSet.abortIfCompletelyBlacklisted(hostToExecutors)
+          taskSet.getCompletelyBlacklistedTaskIfAny(hostToExecutors).foreach { taskIndex =>
+              // If the taskSet is unschedulable we try to find an existing idle blacklisted
+              // executor. If we cannot find one, we abort immediately. Else we kill the idle
+              // executor and kick off an abortTimer which if it doesn't schedule a task within the
+              // the timeout will abort the taskSet if we were unable to schedule any task from the
+              // taskSet.
+              // Note 1: We keep track of schedulability on a per taskSet basis rather than on a per
+              // task basis.
+              // Note 2: The taskSet can still be aborted when there are more than one idle
+              // blacklisted executors and dynamic allocation is on. This can happen when a killed
+              // idle executor isn't replaced in time by ExecutorAllocationManager as it relies on
+              // pending tasks and doesn't kill executors on idle timeouts, resulting in the abort
+              // timer to expire and abort the taskSet.
+              executorIdToRunningTaskIds.find(x => !isExecutorBusy(x._1)) match {
+                case Some ((executorId, _)) =>
+                  if (!unschedulableTaskSetToExpiryTime.contains(taskSet)) {
+                    blacklistTrackerOpt.foreach(blt => blt.killBlacklistedIdleExecutor(executorId))
+
+                    val timeout = conf.get(config.UNSCHEDULABLE_TASKSET_TIMEOUT) * 1000
+                    unschedulableTaskSetToExpiryTime(taskSet) = clock.getTimeMillis() + timeout
+                    logInfo(s""Waiting for $timeout ms for completely ""
+                      + s""blacklisted task to be schedulable again before aborting $taskSet."")
+                    abortTimer.schedule(
+                      createUnschedulableTaskSetAbortTimer(taskSet, taskIndex), timeout)
+                  }
+                case None => // Abort Immediately
+                  logInfo(""Cannot schedule any task because of complete blacklisting. No idle"" +
+                    s"" executors can be found to kill. Aborting $taskSet."" )
+                  taskSet.abortSinceCompletelyBlacklisted(taskIndex)
+              }
+          }
+        } else {
+          // We want to defer killing any taskSets as long as we have a non blacklisted executor
+          // which can be used to schedule a task from any active taskSets. This ensures that the
+          // job can make progress.
+          // Note: It is theoretically possible that a taskSet never gets scheduled on a
+          // non-blacklisted executor and the abort timer doesn't kick in because of a constant
+          // submission of new TaskSets. See the PR for more details.
+          if (unschedulableTaskSetToExpiryTime.nonEmpty) {
+            logInfo(""Clearing the expiry times for all unschedulable taskSets as a task was "" +
+              ""recently scheduled."")
+            unschedulableTaskSetToExpiryTime.clear()
+          }
         }
+
         if (launchedAnyTask && taskSet.isBarrier) {
           // Check whether the barrier tasks are partially launched.
           // TODO SPARK-24818 handle the assert failure case (that can happen when some locality
@@ -452,6 +502,23 @@ private[spark] class TaskSchedulerImpl(
     return tasks
   }
 
+  private def createUnschedulableTaskSetAbortTimer(
+      taskSet: TaskSetManager,
+      taskIndex: Int): TimerTask = {
+    new TimerTask() {
+      override def run() {
+        if (unschedulableTaskSetToExpiryTime.contains(taskSet) &&
+            unschedulableTaskSetToExpiryTime(taskSet) <= clock.getTimeMillis()) {
+          logInfo(""Cannot schedule any task because of complete blacklisting. "" +
+            s""Wait time for scheduling expired. Aborting $taskSet."")
+          taskSet.abortSinceCompletelyBlacklisted(taskIndex)
+        } else {
+          this.cancel()
+        }
+      }
+    }
+  }
+
   /**
    * Shuffle offers around to avoid always placing tasks on the same workers.  Exposed to allow
    * overriding in tests, so it can be deterministic.
@@ -508,14 +575,15 @@ private[spark] class TaskSchedulerImpl(
   }
 
   /**
-   * Update metrics for in-progress tasks and let the master know that the BlockManager is still
-   * alive. Return true if the driver knows about the given block manager. Otherwise, return false,
-   * indicating that the block manager should re-register.
+   * Update metrics for in-progress tasks and executor metrics, and let the master know that the
+   * BlockManager is still alive. Return true if the driver knows about the given block manager.
+   * Otherwise, return false, indicating that the block manager should re-register.
    */
   override def executorHeartbeatReceived(
       execId: String,
       accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])],
-      blockManagerId: BlockManagerId): Boolean = {
+      blockManagerId: BlockManagerId,
+      executorMetrics: ExecutorMetrics): Boolean = {
     // (taskId, stageId, stageAttemptId, accumUpdates)
     val accumUpdatesWithTaskIds: Array[(Long, Int, Int, Seq[AccumulableInfo])] = {
       accumUpdates.flatMap { case (id, updates) =>
@@ -525,7 +593,8 @@ private[spark] class TaskSchedulerImpl(
         }
       }
     }
-    dagScheduler.executorHeartbeatReceived(execId, accumUpdatesWithTaskIds, blockManagerId)
+    dagScheduler.executorHeartbeatReceived(execId, accumUpdatesWithTaskIds, blockManagerId,
+      executorMetrics)
   }
 
   def handleTaskGettingResult(taskSetManager: TaskSetManager, tid: Long): Unit = synchronized {
@@ -587,6 +656,7 @@ private[spark] class TaskSchedulerImpl(
       barrierCoordinator.stop()
     }
     starvationTimer.cancel()
+    abortTimer.cancel()
   }
 
   override def defaultParallelism(): Int = backend.defaultParallelism()
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
index d5e85a11cb279..6bf60dd8e9dfa 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
@@ -623,8 +623,8 @@ private[spark] class TaskSetManager(
    *
    * It is possible that this taskset has become impossible to schedule *anywhere* due to the
    * blacklist.  The most common scenario would be if there are fewer executors than
-   * spark.task.maxFailures. We need to detect this so we can fail the task set, otherwise the job
-   * will hang.
+   * spark.task.maxFailures. We need to detect this so we can avoid the job from being hung.
+   * We try to acquire new executor/s by killing an existing idle blacklisted executor.
    *
    * There's a tradeoff here: we could make sure all tasks in the task set are schedulable, but that
    * would add extra time to each iteration of the scheduling loop. Here, we take the approach of
@@ -635,9 +635,9 @@ private[spark] class TaskSetManager(
    * failures (this is because the method picks one unscheduled task, and then iterates through each
    * executor until it finds one that the task isn't blacklisted on).
    */
-  private[scheduler] def abortIfCompletelyBlacklisted(
-      hostToExecutors: HashMap[String, HashSet[String]]): Unit = {
-    taskSetBlacklistHelperOpt.foreach { taskSetBlacklist =>
+  private[scheduler] def getCompletelyBlacklistedTaskIfAny(
+      hostToExecutors: HashMap[String, HashSet[String]]): Option[Int] = {
+    taskSetBlacklistHelperOpt.flatMap { taskSetBlacklist =>
       val appBlacklist = blacklistTracker.get
       // Only look for unschedulable tasks when at least one executor has registered. Otherwise,
       // task sets will be (unnecessarily) aborted in cases when no executors have registered yet.
@@ -658,11 +658,11 @@ private[spark] class TaskSetManager(
           }
         }
 
-        pendingTask.foreach { indexInTaskSet =>
+        pendingTask.find { indexInTaskSet =>
           // try to find some executor this task can run on.  Its possible that some *other*
           // task isn't schedulable anywhere, but we will discover that in some later call,
           // when that unschedulable task is the last task remaining.
-          val blacklistedEverywhere = hostToExecutors.forall { case (host, execsOnHost) =>
+          hostToExecutors.forall { case (host, execsOnHost) =>
             // Check if the task can run on the node
             val nodeBlacklisted =
               appBlacklist.isNodeBlacklisted(host) ||
@@ -679,22 +679,27 @@ private[spark] class TaskSetManager(
               }
             }
           }
-          if (blacklistedEverywhere) {
-            val partition = tasks(indexInTaskSet).partitionId
-            abort(s""""""
-              |Aborting $taskSet because task $indexInTaskSet (partition $partition)
-              |cannot run anywhere due to node and executor blacklist.
-              |Most recent failure:
-              |${taskSetBlacklist.getLatestFailureReason}
-              |
-              |Blacklisting behavior can be configured via spark.blacklist.*.
-              |"""""".stripMargin)
-          }
         }
+      } else {
+        None
       }
     }
   }
 
+  private[scheduler] def abortSinceCompletelyBlacklisted(indexInTaskSet: Int): Unit = {
+    taskSetBlacklistHelperOpt.foreach { taskSetBlacklist =>
+      val partition = tasks(indexInTaskSet).partitionId
+      abort(s""""""
+         |Aborting $taskSet because task $indexInTaskSet (partition $partition)
+         |cannot run anywhere due to node and executor blacklist.
+         |Most recent failure:
+         |${taskSetBlacklist.getLatestFailureReason}
+         |
+         |Blacklisting behavior can be configured via spark.blacklist.*.
+         |"""""".stripMargin)
+    }
+  }
+
   /**
    * Marks the task as getting result and notifies the DAG Scheduler
    */
diff --git a/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala b/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala
index de7c0d813ae65..329158a44d369 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala
@@ -18,13 +18,17 @@
 package org.apache.spark.scheduler.cluster
 
 import java.util.concurrent.TimeUnit
-import java.util.concurrent.atomic.AtomicInteger
+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}
 import javax.annotation.concurrent.GuardedBy
 
 import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}
 import scala.concurrent.Future
 
+import org.apache.hadoop.security.UserGroupInformation
+
 import org.apache.spark.{ExecutorAllocationClient, SparkEnv, SparkException, TaskState}
+import org.apache.spark.deploy.SparkHadoopUtil
+import org.apache.spark.deploy.security.HadoopDelegationTokenManager
 import org.apache.spark.internal.Logging
 import org.apache.spark.rpc._
 import org.apache.spark.scheduler._
@@ -95,6 +99,12 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
   // The num of current max ExecutorId used to re-register appMaster
   @volatile protected var currentExecutorIdCounter = 0
 
+  // Current set of delegation tokens to send to executors.
+  private val delegationTokens = new AtomicReference[Array[Byte]]()
+
+  // The token manager used to create security tokens.
+  private var delegationTokenManager: Option[HadoopDelegationTokenManager] = None
+
   private val reviveThread =
     ThreadUtils.newDaemonSingleThreadScheduledExecutor(""driver-revive-thread"")
 
@@ -152,6 +162,8 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
         }
 
       case UpdateDelegationTokens(newDelegationTokens) =>
+        SparkHadoopUtil.get.addDelegationTokens(newDelegationTokens, conf)
+        delegationTokens.set(newDelegationTokens)
         executorDataMap.values.foreach { ed =>
           ed.executorEndpoint.send(UpdateDelegationTokens(newDelegationTokens))
         }
@@ -230,7 +242,7 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
         val reply = SparkAppConfig(
           sparkProperties,
           SparkEnv.get.securityManager.getIOEncryptionKey(),
-          fetchHadoopDelegationTokens())
+          Option(delegationTokens.get()))
         context.reply(reply)
     }
 
@@ -390,6 +402,21 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
 
     // TODO (prashant) send conf instead of properties
     driverEndpoint = createDriverEndpointRef(properties)
+
+    if (UserGroupInformation.isSecurityEnabled()) {
+      delegationTokenManager = createTokenManager()
+      delegationTokenManager.foreach { dtm =>
+        dtm.setDriverRef(driverEndpoint)
+        val creds = if (dtm.renewalEnabled) {
+          dtm.start().getCredentials()
+        } else {
+          val creds = UserGroupInformation.getCurrentUser().getCredentials()
+          dtm.obtainDelegationTokens(creds)
+          creds
+        }
+        delegationTokens.set(SparkHadoopUtil.get.serialize(creds))
+      }
+    }
   }
 
   protected def createDriverEndpointRef(
@@ -416,6 +443,7 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
   override def stop() {
     reviveThread.shutdownNow()
     stopExecutors()
+    delegationTokenManager.foreach(_.stop())
     try {
       if (driverEndpoint != null) {
         driverEndpoint.askSync[Boolean](StopDriver)
@@ -684,7 +712,13 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
     true
   }
 
-  protected def fetchHadoopDelegationTokens(): Option[Array[Byte]] = { None }
+  /**
+   * Create the delegation token manager to be used for the application. This method is called
+   * once during the start of the scheduler backend (so after the object has already been
+   * fully constructed), only if security is enabled in the Hadoop configuration.
+   */
+  protected def createTokenManager(): Option[HadoopDelegationTokenManager] = None
+
 }
 
 private[spark] object CoarseGrainedSchedulerBackend {
diff --git a/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala b/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala
index 00621976b77f4..18b735b8035ab 100644
--- a/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala
+++ b/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala
@@ -16,7 +16,7 @@
  */
 package org.apache.spark.security
 
-import java.io.{InputStream, OutputStream}
+import java.io.{Closeable, InputStream, IOException, OutputStream}
 import java.nio.ByteBuffer
 import java.nio.channels.{ReadableByteChannel, WritableByteChannel}
 import java.util.Properties
@@ -54,8 +54,10 @@ private[spark] object CryptoStreamUtils extends Logging {
     val params = new CryptoParams(key, sparkConf)
     val iv = createInitializationVector(params.conf)
     os.write(iv)
-    new CryptoOutputStream(params.transformation, params.conf, os, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingOutputStream(
+      new CryptoOutputStream(params.transformation, params.conf, os, params.keySpec,
+        new IvParameterSpec(iv)),
+      os)
   }
 
   /**
@@ -70,8 +72,10 @@ private[spark] object CryptoStreamUtils extends Logging {
     val helper = new CryptoHelperChannel(channel)
 
     helper.write(ByteBuffer.wrap(iv))
-    new CryptoOutputStream(params.transformation, params.conf, helper, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingWritableChannel(
+      new CryptoOutputStream(params.transformation, params.conf, helper, params.keySpec,
+        new IvParameterSpec(iv)),
+      helper)
   }
 
   /**
@@ -84,8 +88,10 @@ private[spark] object CryptoStreamUtils extends Logging {
     val iv = new Array[Byte](IV_LENGTH_IN_BYTES)
     ByteStreams.readFully(is, iv)
     val params = new CryptoParams(key, sparkConf)
-    new CryptoInputStream(params.transformation, params.conf, is, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingInputStream(
+      new CryptoInputStream(params.transformation, params.conf, is, params.keySpec,
+        new IvParameterSpec(iv)),
+      is)
   }
 
   /**
@@ -100,8 +106,10 @@ private[spark] object CryptoStreamUtils extends Logging {
     JavaUtils.readFully(channel, buf)
 
     val params = new CryptoParams(key, sparkConf)
-    new CryptoInputStream(params.transformation, params.conf, channel, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingReadableChannel(
+      new CryptoInputStream(params.transformation, params.conf, channel, params.keySpec,
+        new IvParameterSpec(iv)),
+      channel)
   }
 
   def toCryptoConf(conf: SparkConf): Properties = {
@@ -157,6 +165,117 @@ private[spark] object CryptoStreamUtils extends Logging {
 
   }
 
+  /**
+   * SPARK-25535. The commons-cryto library will throw InternalError if something goes
+   * wrong, and leave bad state behind in the Java wrappers, so it's not safe to use them
+   * afterwards. This wrapper detects that situation and avoids further calls into the
+   * commons-crypto code, while still allowing the underlying streams to be closed.
+   *
+   * This should be removed once CRYPTO-141 is fixed (and Spark upgrades its commons-crypto
+   * dependency).
+   */
+  trait BaseErrorHandler extends Closeable {
+
+    private var closed = false
+
+    /** The encrypted stream that may get into an unhealthy state. */
+    protected def cipherStream: Closeable
+
+    /**
+     * The underlying stream that is being wrapped by the encrypted stream, so that it can be
+     * closed even if there's an error in the crypto layer.
+     */
+    protected def original: Closeable
+
+    protected def safeCall[T](fn: => T): T = {
+      if (closed) {
+        throw new IOException(""Cipher stream is closed."")
+      }
+      try {
+        fn
+      } catch {
+        case ie: InternalError =>
+          closed = true
+          original.close()
+          throw ie
+      }
+    }
+
+    override def close(): Unit = {
+      if (!closed) {
+        cipherStream.close()
+      }
+    }
+
+  }
+
+  // Visible for testing.
+  class ErrorHandlingReadableChannel(
+      protected val cipherStream: ReadableByteChannel,
+      protected val original: ReadableByteChannel)
+    extends ReadableByteChannel with BaseErrorHandler {
+
+    override def read(src: ByteBuffer): Int = safeCall {
+      cipherStream.read(src)
+    }
+
+    override def isOpen(): Boolean = cipherStream.isOpen()
+
+  }
+
+  private class ErrorHandlingInputStream(
+      protected val cipherStream: InputStream,
+      protected val original: InputStream)
+    extends InputStream with BaseErrorHandler {
+
+    override def read(b: Array[Byte]): Int = safeCall {
+      cipherStream.read(b)
+    }
+
+    override def read(b: Array[Byte], off: Int, len: Int): Int = safeCall {
+      cipherStream.read(b, off, len)
+    }
+
+    override def read(): Int = safeCall {
+      cipherStream.read()
+    }
+  }
+
+  private class ErrorHandlingWritableChannel(
+      protected val cipherStream: WritableByteChannel,
+      protected val original: WritableByteChannel)
+    extends WritableByteChannel with BaseErrorHandler {
+
+    override def write(src: ByteBuffer): Int = safeCall {
+      cipherStream.write(src)
+    }
+
+    override def isOpen(): Boolean = cipherStream.isOpen()
+
+  }
+
+  private class ErrorHandlingOutputStream(
+      protected val cipherStream: OutputStream,
+      protected val original: OutputStream)
+    extends OutputStream with BaseErrorHandler {
+
+    override def flush(): Unit = safeCall {
+      cipherStream.flush()
+    }
+
+    override def write(b: Array[Byte]): Unit = safeCall {
+      cipherStream.write(b)
+    }
+
+    override def write(b: Array[Byte], off: Int, len: Int): Unit = safeCall {
+      cipherStream.write(b, off, len)
+    }
+
+    override def write(b: Int): Unit = safeCall {
+      cipherStream.write(b)
+    }
+  }
+
   private class CryptoParams(key: Array[Byte], sparkConf: SparkConf) {
 
     val keySpec = new SecretKeySpec(key, ""AES"")
diff --git a/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala b/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala
index 72427dd6ce4d4..1e1c27c477877 100644
--- a/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala
+++ b/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala
@@ -30,6 +30,7 @@ import scala.util.control.NonFatal
 import com.esotericsoftware.kryo.{Kryo, KryoException, Serializer => KryoClassSerializer}
 import com.esotericsoftware.kryo.io.{Input => KryoInput, Output => KryoOutput}
 import com.esotericsoftware.kryo.io.{UnsafeInput => KryoUnsafeInput, UnsafeOutput => KryoUnsafeOutput}
+import com.esotericsoftware.kryo.pool.{KryoCallback, KryoFactory, KryoPool}
 import com.esotericsoftware.kryo.serializers.{JavaSerializer => KryoJavaSerializer}
 import com.twitter.chill.{AllScalaRegistrar, EmptyScalaKryoInstantiator}
 import org.apache.avro.generic.{GenericData, GenericRecord}
@@ -41,7 +42,7 @@ import org.apache.spark.internal.Logging
 import org.apache.spark.network.util.ByteUnit
 import org.apache.spark.scheduler.{CompressedMapStatus, HighlyCompressedMapStatus}
 import org.apache.spark.storage._
-import org.apache.spark.util.{BoundedPriorityQueue, SerializableConfiguration, SerializableJobConf, Utils}
+import org.apache.spark.util.{BoundedPriorityQueue, ByteBufferInputStream, SerializableConfiguration, SerializableJobConf, Utils}
 import org.apache.spark.util.collection.CompactBuffer
 
 /**
@@ -84,6 +85,7 @@ class KryoSerializer(conf: SparkConf)
   private val avroSchemas = conf.getAvroSchema
   // whether to use unsafe based IO for serialization
   private val useUnsafe = conf.getBoolean(""spark.kryo.unsafe"", false)
+  private val usePool = conf.getBoolean(""spark.kryo.pool"", true)
 
   def newKryoOutput(): KryoOutput =
     if (useUnsafe) {
@@ -92,6 +94,36 @@ class KryoSerializer(conf: SparkConf)
       new KryoOutput(bufferSize, math.max(bufferSize, maxBufferSize))
     }
 
+  @transient
+  private lazy val factory: KryoFactory = new KryoFactory() {
+    override def create: Kryo = {
+      newKryo()
+    }
+  }
+
+  private class PoolWrapper extends KryoPool {
+    private var pool: KryoPool = getPool
+
+    override def borrow(): Kryo = pool.borrow()
+
+    override def release(kryo: Kryo): Unit = pool.release(kryo)
+
+    override def run[T](kryoCallback: KryoCallback[T]): T = pool.run(kryoCallback)
+
+    def reset(): Unit = {
+      pool = getPool
+    }
+
+    private def getPool: KryoPool = {
+      new KryoPool.Builder(factory).softReferences.build
+    }
+  }
+
+  @transient
+  private lazy val internalPool = new PoolWrapper
+
+  def pool: KryoPool = internalPool
+
   def newKryo(): Kryo = {
     val instantiator = new EmptyScalaKryoInstantiator
     val kryo = instantiator.newKryo()
@@ -132,7 +164,8 @@ class KryoSerializer(conf: SparkConf)
         .foreach { className => kryo.register(Class.forName(className, true, classLoader)) }
       // Allow the user to register their own classes by setting spark.kryo.registrator.
       userRegistrators
-        .map(Class.forName(_, true, classLoader).newInstance().asInstanceOf[KryoRegistrator])
+        .map(Class.forName(_, true, classLoader).getConstructor().
+          newInstance().asInstanceOf[KryoRegistrator])
         .foreach { reg => reg.registerClasses(kryo) }
       // scalastyle:on classforname
     } catch {
@@ -182,6 +215,12 @@ class KryoSerializer(conf: SparkConf)
     // We can't load those class directly in order to avoid unnecessary jar dependencies.
     // We load them safely, ignore it if the class not found.
     Seq(
+      ""org.apache.spark.ml.attribute.Attribute"",
+      ""org.apache.spark.ml.attribute.AttributeGroup"",
+      ""org.apache.spark.ml.attribute.BinaryAttribute"",
+      ""org.apache.spark.ml.attribute.NominalAttribute"",
+      ""org.apache.spark.ml.attribute.NumericAttribute"",
+
       ""org.apache.spark.ml.feature.Instance"",
       ""org.apache.spark.ml.feature.LabeledPoint"",
       ""org.apache.spark.ml.feature.OffsetInstance"",
@@ -191,6 +230,7 @@ class KryoSerializer(conf: SparkConf)
       ""org.apache.spark.ml.linalg.SparseMatrix"",
       ""org.apache.spark.ml.linalg.SparseVector"",
       ""org.apache.spark.ml.linalg.Vector"",
+      ""org.apache.spark.ml.stat.distribution.MultivariateGaussian"",
       ""org.apache.spark.ml.tree.impl.TreePoint"",
       ""org.apache.spark.mllib.clustering.VectorWithNorm"",
       ""org.apache.spark.mllib.linalg.DenseMatrix"",
@@ -199,7 +239,8 @@ class KryoSerializer(conf: SparkConf)
       ""org.apache.spark.mllib.linalg.SparseMatrix"",
       ""org.apache.spark.mllib.linalg.SparseVector"",
       ""org.apache.spark.mllib.linalg.Vector"",
-      ""org.apache.spark.mllib.regression.LabeledPoint""
+      ""org.apache.spark.mllib.regression.LabeledPoint"",
+      ""org.apache.spark.mllib.stat.distribution.MultivariateGaussian""
     ).foreach { name =>
       try {
         val clazz = Utils.classForName(name)
@@ -214,8 +255,14 @@ class KryoSerializer(conf: SparkConf)
     kryo
   }
 
+  override def setDefaultClassLoader(classLoader: ClassLoader): Serializer = {
+    super.setDefaultClassLoader(classLoader)
+    internalPool.reset()
+    this
+  }
+
   override def newInstance(): SerializerInstance = {
-    new KryoSerializerInstance(this, useUnsafe)
+    new KryoSerializerInstance(this, useUnsafe, usePool)
   }
 
   private[spark] override lazy val supportsRelocationOfSerializedObjects: Boolean = {
@@ -298,7 +345,8 @@ class KryoDeserializationStream(
   }
 }
 
-private[spark] class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boolean)
+private[spark] class KryoSerializerInstance(
+   ks: KryoSerializer, useUnsafe: Boolean, usePool: Boolean)
   extends SerializerInstance {
   /**
    * A re-used [[Kryo]] instance. Methods will borrow this instance by calling `borrowKryo()`, do
@@ -306,22 +354,29 @@ private[spark] class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole
    * pool of size one. SerializerInstances are not thread-safe, hence accesses to this field are
    * not synchronized.
    */
-  @Nullable private[this] var cachedKryo: Kryo = borrowKryo()
+  @Nullable private[this] var cachedKryo: Kryo = if (usePool) null else borrowKryo()
 
   /**
    * Borrows a [[Kryo]] instance. If possible, this tries to re-use a cached Kryo instance;
    * otherwise, it allocates a new instance.
    */
   private[serializer] def borrowKryo(): Kryo = {
-    if (cachedKryo != null) {
-      val kryo = cachedKryo
-      // As a defensive measure, call reset() to clear any Kryo state that might have been modified
-      // by the last operation to borrow this instance (see SPARK-7766 for discussion of this issue)
+    if (usePool) {
+      val kryo = ks.pool.borrow()
       kryo.reset()
-      cachedKryo = null
       kryo
     } else {
-      ks.newKryo()
+      if (cachedKryo != null) {
+        val kryo = cachedKryo
+        // As a defensive measure, call reset() to clear any Kryo state that might have
+        // been modified by the last operation to borrow this instance
+        // (see SPARK-7766 for discussion of this issue)
+        kryo.reset()
+        cachedKryo = null
+        kryo
+      } else {
+        ks.newKryo()
+      }
     }
   }
 
@@ -331,8 +386,12 @@ private[spark] class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole
    * re-use.
    */
   private[serializer] def releaseKryo(kryo: Kryo): Unit = {
-    if (cachedKryo == null) {
-      cachedKryo = kryo
+    if (usePool) {
+      ks.pool.release(kryo)
+    } else {
+      if (cachedKryo == null) {
+        cachedKryo = kryo
+      }
     }
   }
 
@@ -358,7 +417,12 @@ private[spark] class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole
   override def deserialize[T: ClassTag](bytes: ByteBuffer): T = {
     val kryo = borrowKryo()
     try {
-      input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())
+      if (bytes.hasArray) {
+        input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())
+      } else {
+        input.setBuffer(new Array[Byte](4096))
+        input.setInputStream(new ByteBufferInputStream(bytes))
+      }
       kryo.readClassAndObject(input).asInstanceOf[T]
     } finally {
       releaseKryo(kryo)
@@ -370,7 +434,12 @@ private[spark] class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole
     val oldClassLoader = kryo.getClassLoader
     try {
       kryo.setClassLoader(loader)
-      input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())
+      if (bytes.hasArray) {
+        input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())
+      } else {
+        input.setBuffer(new Array[Byte](4096))
+        input.setInputStream(new ByteBufferInputStream(bytes))
+      }
       kryo.readClassAndObject(input).asInstanceOf[T]
     } finally {
       kryo.setClassLoader(oldClassLoader)
diff --git a/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala b/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
index 74b0e0b3a741a..27e2f98c58f0c 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
@@ -33,6 +33,7 @@ private[spark] class BlockStoreShuffleReader[K, C](
     startPartition: Int,
     endPartition: Int,
     context: TaskContext,
+    readMetrics: ShuffleReadMetricsReporter,
     serializerManager: SerializerManager = SparkEnv.get.serializerManager,
     blockManager: BlockManager = SparkEnv.get.blockManager,
     mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker)
@@ -53,7 +54,8 @@ private[spark] class BlockStoreShuffleReader[K, C](
       SparkEnv.get.conf.getInt(""spark.reducer.maxReqsInFlight"", Int.MaxValue),
       SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),
       SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),
-      SparkEnv.get.conf.getBoolean(""spark.shuffle.detectCorrupt"", true))
+      SparkEnv.get.conf.getBoolean(""spark.shuffle.detectCorrupt"", true),
+      readMetrics)
 
     val serializerInstance = dep.serializer.newInstance()
 
@@ -66,7 +68,6 @@ private[spark] class BlockStoreShuffleReader[K, C](
     }
 
     // Update the context task metrics for each record read.
-    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()
     val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](
       recordIter.map { record =>
         readMetrics.incRecordsRead(1)
diff --git a/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala b/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala
index 4ea8a7120a9cc..18a743fbfa6fc 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala
@@ -38,7 +38,11 @@ private[spark] trait ShuffleManager {
       dependency: ShuffleDependency[K, V, C]): ShuffleHandle
 
   /** Get a writer for a given partition. Called on executors by map tasks. */
-  def getWriter[K, V](handle: ShuffleHandle, mapId: Int, context: TaskContext): ShuffleWriter[K, V]
+  def getWriter[K, V](
+      handle: ShuffleHandle,
+      mapId: Int,
+      context: TaskContext,
+      metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V]
 
   /**
    * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive).
@@ -48,7 +52,8 @@ private[spark] trait ShuffleManager {
       handle: ShuffleHandle,
       startPartition: Int,
       endPartition: Int,
-      context: TaskContext): ShuffleReader[K, C]
+      context: TaskContext,
+      metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C]
 
   /**
    * Remove a shuffle's metadata from the ShuffleManager.
diff --git a/core/src/main/scala/org/apache/spark/shuffle/metrics.scala b/core/src/main/scala/org/apache/spark/shuffle/metrics.scala
new file mode 100644
index 0000000000000..33be677bc90cb
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/shuffle/metrics.scala
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.shuffle
+
+/**
+ * An interface for reporting shuffle read metrics, for each shuffle. This interface assumes
+ * all the methods are called on a single-threaded, i.e. concrete implementations would not need
+ * to synchronize.
+ *
+ * All methods have additional Spark visibility modifier to allow public, concrete implementations
+ * that still have these methods marked as private[spark].
+ */
+private[spark] trait ShuffleReadMetricsReporter {
+  private[spark] def incRemoteBlocksFetched(v: Long): Unit
+  private[spark] def incLocalBlocksFetched(v: Long): Unit
+  private[spark] def incRemoteBytesRead(v: Long): Unit
+  private[spark] def incRemoteBytesReadToDisk(v: Long): Unit
+  private[spark] def incLocalBytesRead(v: Long): Unit
+  private[spark] def incFetchWaitTime(v: Long): Unit
+  private[spark] def incRecordsRead(v: Long): Unit
+}
+
+
+/**
+ * An interface for reporting shuffle write metrics. This interface assumes all the methods are
+ * called on a single-threaded, i.e. concrete implementations would not need to synchronize.
+ *
+ * All methods have additional Spark visibility modifier to allow public, concrete implementations
+ * that still have these methods marked as private[spark].
+ */
+private[spark] trait ShuffleWriteMetricsReporter {
+  private[spark] def incBytesWritten(v: Long): Unit
+  private[spark] def incRecordsWritten(v: Long): Unit
+  private[spark] def incWriteTime(v: Long): Unit
+  private[spark] def decBytesWritten(v: Long): Unit
+  private[spark] def decRecordsWritten(v: Long): Unit
+}
diff --git a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala
index 0caf84c6050a8..b51a843a31c31 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala
@@ -114,16 +114,19 @@ private[spark] class SortShuffleManager(conf: SparkConf) extends ShuffleManager
       handle: ShuffleHandle,
       startPartition: Int,
       endPartition: Int,
-      context: TaskContext): ShuffleReader[K, C] = {
+      context: TaskContext,
+      metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = {
     new BlockStoreShuffleReader(
-      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)
+      handle.asInstanceOf[BaseShuffleHandle[K, _, C]],
+      startPartition, endPartition, context, metrics)
   }
 
   /** Get a writer for a given partition. Called on executors by map tasks. */
   override def getWriter[K, V](
       handle: ShuffleHandle,
       mapId: Int,
-      context: TaskContext): ShuffleWriter[K, V] = {
+      context: TaskContext,
+      metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V] = {
     numMapsForShuffle.putIfAbsent(
       handle.shuffleId, handle.asInstanceOf[BaseShuffleHandle[_, _, _]].numMaps)
     val env = SparkEnv.get
@@ -136,15 +139,16 @@ private[spark] class SortShuffleManager(conf: SparkConf) extends ShuffleManager
           unsafeShuffleHandle,
           mapId,
           context,
-          env.conf)
+          env.conf,
+          metrics)
       case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =>
         new BypassMergeSortShuffleWriter(
           env.blockManager,
           shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver],
           bypassMergeSortHandle,
           mapId,
-          context,
-          env.conf)
+          env.conf,
+          metrics)
       case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =>
         new SortShuffleWriter(shuffleBlockResolver, other, mapId, context)
     }
diff --git a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala
index 91fc26762e533..274399b9cc1f3 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala
@@ -70,8 +70,7 @@ private[spark] class SortShuffleWriter[K, V, C](
       val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
       val partitionLengths = sorter.writePartitionedFile(blockId, tmp)
       shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)
-      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths,
-        writeMetrics.recordsWritten)
+      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)
     } finally {
       if (tmp.exists() && !tmp.delete()) {
         logError(s""Error while deleting temp file ${tmp.getAbsolutePath}"")
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala b/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala
index 91b75e4852999..bd3f58b6182c0 100644
--- a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala
@@ -25,8 +25,9 @@ import scala.collection.JavaConverters._
 import scala.collection.mutable.HashMap
 
 import org.apache.spark._
-import org.apache.spark.executor.TaskMetrics
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}
 import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config.Status._
 import org.apache.spark.scheduler._
 import org.apache.spark.status.api.v1
 import org.apache.spark.storage._
@@ -44,10 +45,9 @@ private[spark] class AppStatusListener(
     kvstore: ElementTrackingStore,
     conf: SparkConf,
     live: Boolean,
+    appStatusSource: Option[AppStatusSource] = None,
     lastUpdateTime: Option[Long] = None) extends SparkListener with Logging {
 
-  import config._
-
   private var sparkVersion = SPARK_VERSION
   private var appInfo: v1.ApplicationInfo = null
   private var appSummary = new AppSummary(0, 0)
@@ -66,6 +66,7 @@ private[spark] class AppStatusListener(
   private val liveStages = new ConcurrentHashMap[(Int, Int), LiveStage]()
   private val liveJobs = new HashMap[Int, LiveJob]()
   private val liveExecutors = new HashMap[String, LiveExecutor]()
+  private val deadExecutors = new HashMap[String, LiveExecutor]()
   private val liveTasks = new HashMap[Long, LiveTask]()
   private val liveRDDs = new HashMap[Int, LiveRDD]()
   private val pools = new HashMap[String, SchedulerPool]()
@@ -204,6 +205,19 @@ private[spark] class AppStatusListener(
           update(rdd, now)
         }
       }
+      if (isExecutorActiveForLiveStages(exec)) {
+        // the executor was running for a currently active stage, so save it for now in
+        // deadExecutors, and remove when there are no active stages overlapping with the
+        // executor.
+        deadExecutors.put(event.executorId, exec)
+      }
+    }
+  }
+
+  /** Was the specified executor active for any currently live stages? */
+  private def isExecutorActiveForLiveStages(exec: LiveExecutor): Boolean = {
+    liveStages.values.asScala.exists { stage =>
+      stage.info.submissionTime.getOrElse(0L) < exec.removeTime.getTime
     }
   }
 
@@ -266,6 +280,11 @@ private[spark] class AppStatusListener(
   private def updateBlackListStatus(execId: String, blacklisted: Boolean): Unit = {
     liveExecutors.get(execId).foreach { exec =>
       exec.isBlacklisted = blacklisted
+      if (blacklisted) {
+        appStatusSource.foreach(_.BLACKLISTED_EXECUTORS.inc())
+      } else {
+        appStatusSource.foreach(_.UNBLACKLISTED_EXECUTORS.inc())
+      }
       liveUpdate(exec, System.nanoTime())
     }
   }
@@ -368,16 +387,40 @@ private[spark] class AppStatusListener(
       }
 
       job.status = event.jobResult match {
-        case JobSucceeded => JobExecutionStatus.SUCCEEDED
-        case JobFailed(_) => JobExecutionStatus.FAILED
+        case JobSucceeded =>
+          appStatusSource.foreach{_.SUCCEEDED_JOBS.inc()}
+          JobExecutionStatus.SUCCEEDED
+        case JobFailed(_) =>
+          appStatusSource.foreach{_.FAILED_JOBS.inc()}
+          JobExecutionStatus.FAILED
       }
 
       job.completionTime = if (event.time > 0) Some(new Date(event.time)) else None
+
+      for {
+        source <- appStatusSource
+        submissionTime <- job.submissionTime
+        completionTime <- job.completionTime
+      } {
+        source.JOB_DURATION.value.set(completionTime.getTime() - submissionTime.getTime())
+      }
+
+      // update global app status counters
+      appStatusSource.foreach { source =>
+        source.COMPLETED_STAGES.inc(job.completedStages.size)
+        source.FAILED_STAGES.inc(job.failedStages)
+        source.COMPLETED_TASKS.inc(job.completedTasks)
+        source.FAILED_TASKS.inc(job.failedTasks)
+        source.KILLED_TASKS.inc(job.killedTasks)
+        source.SKIPPED_TASKS.inc(job.skippedTasks)
+        source.SKIPPED_STAGES.inc(job.skippedStages.size)
+      }
       update(job, now, last = true)
+      if (job.status == JobExecutionStatus.SUCCEEDED) {
+        appSummary = new AppSummary(appSummary.numCompletedJobs + 1, appSummary.numCompletedStages)
+        kvstore.write(appSummary)
+      }
     }
-
-    appSummary = new AppSummary(appSummary.numCompletedJobs + 1, appSummary.numCompletedStages)
-    kvstore.write(appSummary)
   }
 
   override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = {
@@ -430,6 +473,7 @@ private[spark] class AppStatusListener(
       val locality = event.taskInfo.taskLocality.toString()
       val count = stage.localitySummary.getOrElse(locality, 0L) + 1L
       stage.localitySummary = stage.localitySummary ++ Map(locality -> count)
+      stage.activeTasksPerExecutor(event.taskInfo.executorId) += 1
       maybeUpdate(stage, now)
 
       stage.jobs.foreach { job =>
@@ -515,6 +559,7 @@ private[spark] class AppStatusListener(
       if (killedDelta > 0) {
         stage.killedSummary = killedTasksSummary(event.reason, stage.killedSummary)
       }
+      stage.activeTasksPerExecutor(event.taskInfo.executorId) -= 1
       // [SPARK-24415] Wait for all tasks to finish before removing stage from live list
       val removeStage =
         stage.activeTasks == 0 &&
@@ -539,7 +584,11 @@ private[spark] class AppStatusListener(
         if (killedDelta > 0) {
           job.killedSummary = killedTasksSummary(event.reason, job.killedSummary)
         }
-        conditionalLiveUpdate(job, now, removeStage)
+        if (removeStage) {
+          update(job, now)
+        } else {
+          maybeUpdate(job, now)
+        }
       }
 
       val esummary = stage.executorSummary(event.taskInfo.executorId)
@@ -550,7 +599,16 @@ private[spark] class AppStatusListener(
       if (metricsDelta != null) {
         esummary.metrics = LiveEntityHelpers.addMetrics(esummary.metrics, metricsDelta)
       }
-      conditionalLiveUpdate(esummary, now, removeStage)
+
+      val isLastTask = stage.activeTasksPerExecutor(event.taskInfo.executorId) == 0
+
+      // If the last task of the executor finished, then update the esummary
+      // for both live and history events.
+      if (isLastTask) {
+        update(esummary, now)
+      } else {
+        maybeUpdate(esummary, now)
+      }
 
       if (!stage.cleaning && stage.savedTasks.get() > maxTasksPerStage) {
         stage.cleaning = true
@@ -583,9 +641,14 @@ private[spark] class AppStatusListener(
         }
       }
 
-      // Force an update on live applications when the number of active tasks reaches 0. This is
-      // checked in some tests (e.g. SQLTestUtilsBase) so it needs to be reliably up to date.
-      conditionalLiveUpdate(exec, now, exec.activeTasks == 0)
+      // Force an update on both live and history applications when the number of active tasks
+      // reaches 0. This is checked in some tests (e.g. SQLTestUtilsBase) so it needs to be
+      // reliably up to date.
+      if (exec.activeTasks == 0) {
+        update(exec, now)
+      } else {
+        maybeUpdate(exec, now)
+      }
     }
   }
 
@@ -639,10 +702,14 @@ private[spark] class AppStatusListener(
       if (removeStage) {
         liveStages.remove((event.stageInfo.stageId, event.stageInfo.attemptNumber))
       }
+      if (stage.status == v1.StageStatus.COMPLETE) {
+        appSummary = new AppSummary(appSummary.numCompletedJobs, appSummary.numCompletedStages + 1)
+        kvstore.write(appSummary)
+      }
     }
 
-    appSummary = new AppSummary(appSummary.numCompletedJobs, appSummary.numCompletedStages + 1)
-    kvstore.write(appSummary)
+    // remove any dead executors that were not running for any currently active stages
+    deadExecutors.retain((execId, exec) => isExecutorActiveForLiveStages(exec))
   }
 
   private def removeBlackListedStageFrom(exec: LiveExecutor, stageId: Int, now: Long) = {
@@ -669,7 +736,37 @@ private[spark] class AppStatusListener(
   }
 
   override def onUnpersistRDD(event: SparkListenerUnpersistRDD): Unit = {
-    liveRDDs.remove(event.rddId)
+    liveRDDs.remove(event.rddId).foreach { liveRDD =>
+      val storageLevel = liveRDD.info.storageLevel
+
+      // Use RDD partition info to update executor block info.
+      liveRDD.getPartitions().foreach { case (_, part) =>
+        part.executors.foreach { executorId =>
+          liveExecutors.get(executorId).foreach { exec =>
+            exec.rddBlocks = exec.rddBlocks - 1
+          }
+        }
+      }
+
+      val now = System.nanoTime()
+
+      // Use RDD distribution to update executor memory and disk usage info.
+      liveRDD.getDistributions().foreach { case (executorId, rddDist) =>
+        liveExecutors.get(executorId).foreach { exec =>
+          if (exec.hasMemoryInfo) {
+            if (storageLevel.useOffHeap) {
+              exec.usedOffHeap = addDeltaToValue(exec.usedOffHeap, -rddDist.offHeapUsed)
+            } else {
+              exec.usedOnHeap = addDeltaToValue(exec.usedOnHeap, -rddDist.onHeapUsed)
+            }
+          }
+          exec.memoryUsed = addDeltaToValue(exec.memoryUsed, -rddDist.memoryUsed)
+          exec.diskUsed = addDeltaToValue(exec.diskUsed, -rddDist.diskUsed)
+          maybeUpdate(exec, now)
+        }
+      }
+    }
+
     kvstore.delete(classOf[RDDStorageInfoWrapper], event.rddId)
   }
 
@@ -692,6 +789,31 @@ private[spark] class AppStatusListener(
         }
       }
     }
+
+    // check if there is a new peak value for any of the executor level memory metrics
+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed
+    // for the live UI.
+    event.executorUpdates.foreach { updates =>
+      liveExecutors.get(event.execId).foreach { exec =>
+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {
+          maybeUpdate(exec, now)
+        }
+      }
+    }
+  }
+
+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {
+    val now = System.nanoTime()
+
+    // check if there is a new peak value for any of the executor level memory metrics,
+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed
+    // when reading logs.
+    liveExecutors.get(executorMetrics.execId)
+      .orElse(deadExecutors.get(executorMetrics.execId)).map { exec =>
+      if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(executorMetrics.executorMetrics)) {
+        update(exec, now)
+      }
+    }
   }
 
   override def onBlockUpdated(event: SparkListenerBlockUpdated): Unit = {
@@ -728,6 +850,11 @@ private[spark] class AppStatusListener(
       .sortBy(_.stageId)
   }
 
+  /**
+   * Apply a delta to a value, but ensure that it doesn't go negative.
+   */
+  private def addDeltaToValue(old: Long, delta: Long): Long = math.max(0, old + delta)
+
   private def updateRDDBlock(event: SparkListenerBlockUpdated, block: RDDBlockId): Unit = {
     val now = System.nanoTime()
     val executorId = event.blockUpdatedInfo.blockManagerId.executorId
@@ -737,9 +864,6 @@ private[spark] class AppStatusListener(
     val diskDelta = event.blockUpdatedInfo.diskSize * (if (storageLevel.useDisk) 1 else -1)
     val memoryDelta = event.blockUpdatedInfo.memSize * (if (storageLevel.useMemory) 1 else -1)
 
-    // Function to apply a delta to a value, but ensure that it doesn't go negative.
-    def newValue(old: Long, delta: Long): Long = math.max(0, old + delta)
-
     val updatedStorageLevel = if (storageLevel.isValid) {
       Some(storageLevel.description)
     } else {
@@ -756,13 +880,13 @@ private[spark] class AppStatusListener(
     maybeExec.foreach { exec =>
       if (exec.hasMemoryInfo) {
         if (storageLevel.useOffHeap) {
-          exec.usedOffHeap = newValue(exec.usedOffHeap, memoryDelta)
+          exec.usedOffHeap = addDeltaToValue(exec.usedOffHeap, memoryDelta)
         } else {
-          exec.usedOnHeap = newValue(exec.usedOnHeap, memoryDelta)
+          exec.usedOnHeap = addDeltaToValue(exec.usedOnHeap, memoryDelta)
         }
       }
-      exec.memoryUsed = newValue(exec.memoryUsed, memoryDelta)
-      exec.diskUsed = newValue(exec.diskUsed, diskDelta)
+      exec.memoryUsed = addDeltaToValue(exec.memoryUsed, memoryDelta)
+      exec.diskUsed = addDeltaToValue(exec.diskUsed, diskDelta)
     }
 
     // Update the block entry in the RDD info, keeping track of the deltas above so that we
@@ -790,8 +914,8 @@ private[spark] class AppStatusListener(
       // Only update the partition if it's still stored in some executor, otherwise get rid of it.
       if (executors.nonEmpty) {
         partition.update(executors, rdd.storageLevel,
-          newValue(partition.memoryUsed, memoryDelta),
-          newValue(partition.diskUsed, diskDelta))
+          addDeltaToValue(partition.memoryUsed, memoryDelta),
+          addDeltaToValue(partition.diskUsed, diskDelta))
       } else {
         rdd.removePartition(block.name)
       }
@@ -799,14 +923,14 @@ private[spark] class AppStatusListener(
       maybeExec.foreach { exec =>
         if (exec.rddBlocks + rddBlocksDelta > 0) {
           val dist = rdd.distribution(exec)
-          dist.memoryUsed = newValue(dist.memoryUsed, memoryDelta)
-          dist.diskUsed = newValue(dist.diskUsed, diskDelta)
+          dist.memoryUsed = addDeltaToValue(dist.memoryUsed, memoryDelta)
+          dist.diskUsed = addDeltaToValue(dist.diskUsed, diskDelta)
 
           if (exec.hasMemoryInfo) {
             if (storageLevel.useOffHeap) {
-              dist.offHeapUsed = newValue(dist.offHeapUsed, memoryDelta)
+              dist.offHeapUsed = addDeltaToValue(dist.offHeapUsed, memoryDelta)
             } else {
-              dist.onHeapUsed = newValue(dist.onHeapUsed, memoryDelta)
+              dist.onHeapUsed = addDeltaToValue(dist.onHeapUsed, memoryDelta)
             }
           }
           dist.lastUpdate = null
@@ -825,8 +949,8 @@ private[spark] class AppStatusListener(
         }
       }
 
-      rdd.memoryUsed = newValue(rdd.memoryUsed, memoryDelta)
-      rdd.diskUsed = newValue(rdd.diskUsed, diskDelta)
+      rdd.memoryUsed = addDeltaToValue(rdd.memoryUsed, memoryDelta)
+      rdd.diskUsed = addDeltaToValue(rdd.diskUsed, diskDelta)
       update(rdd, now)
     }
 
@@ -905,14 +1029,6 @@ private[spark] class AppStatusListener(
     }
   }
 
-  private def conditionalLiveUpdate(entity: LiveEntity, now: Long, condition: Boolean): Unit = {
-    if (condition) {
-      liveUpdate(entity, now)
-    } else {
-      maybeUpdate(entity, now)
-    }
-  }
-
   private def cleanupExecutors(count: Long): Unit = {
     // Because the limit is on the number of *dead* executors, we need to calculate whether
     // there are actually enough dead executors to be deleted.
@@ -968,16 +1084,6 @@ private[spark] class AppStatusListener(
         kvstore.delete(e.getClass(), e.id)
       }
 
-      val tasks = kvstore.view(classOf[TaskDataWrapper])
-        .index(""stage"")
-        .first(key)
-        .last(key)
-        .asScala
-
-      tasks.foreach { t =>
-        kvstore.delete(t.getClass(), t.taskId)
-      }
-
       // Check whether there are remaining attempts for the same stage. If there aren't, then
       // also delete the RDD graph data.
       val remainingAttempts = kvstore.view(classOf[StageDataWrapper])
@@ -1000,6 +1106,15 @@ private[spark] class AppStatusListener(
 
       cleanupCachedQuantiles(key)
     }
+
+    // Delete tasks for all stages in one pass, as deleting them for each stage individually is slow
+    val tasks = kvstore.view(classOf[TaskDataWrapper]).asScala
+    val keys = stages.map { s => (s.info.stageId, s.info.attemptId) }.toSet
+    tasks.foreach { t =>
+      if (keys.contains((t.stageId, t.stageAttemptId))) {
+        kvstore.delete(t.getClass(), t.taskId)
+      }
+    }
   }
 
   private def cleanupTasks(stage: LiveStage): Unit = {
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusSource.scala b/core/src/main/scala/org/apache/spark/status/AppStatusSource.scala
new file mode 100644
index 0000000000000..f6a21578ff499
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusSource.scala
@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.status
+
+import java.util.concurrent.atomic.AtomicLong
+
+import AppStatusSource.getCounter
+import com.codahale.metrics.{Counter, Gauge, MetricRegistry}
+
+import org.apache.spark.SparkConf
+import org.apache.spark.internal.config.Status.APP_STATUS_METRICS_ENABLED
+import org.apache.spark.metrics.source.Source
+
+private [spark] class JobDuration(val value: AtomicLong) extends Gauge[Long] {
+  override def getValue: Long = value.get()
+}
+
+private[spark] class AppStatusSource extends Source {
+
+  override implicit val metricRegistry = new MetricRegistry()
+
+  override val sourceName = ""appStatus""
+
+  val jobDuration = new JobDuration(new AtomicLong(0L))
+
+  // Duration of each job in milliseconds
+  val JOB_DURATION = metricRegistry
+    .register(MetricRegistry.name(""jobDuration""), jobDuration)
+
+  val FAILED_STAGES = getCounter(""stages"", ""failedStages"")
+
+  val SKIPPED_STAGES = getCounter(""stages"", ""skippedStages"")
+
+  val COMPLETED_STAGES = getCounter(""stages"", ""completedStages"")
+
+  val SUCCEEDED_JOBS = getCounter(""jobs"", ""succeededJobs"")
+
+  val FAILED_JOBS = getCounter(""jobs"", ""failedJobs"")
+
+  val COMPLETED_TASKS = getCounter(""tasks"", ""completedTasks"")
+
+  val FAILED_TASKS = getCounter(""tasks"", ""failedTasks"")
+
+  val KILLED_TASKS = getCounter(""tasks"", ""killedTasks"")
+
+  val SKIPPED_TASKS = getCounter(""tasks"", ""skippedTasks"")
+
+  val BLACKLISTED_EXECUTORS = getCounter(""tasks"", ""blackListedExecutors"")
+
+  val UNBLACKLISTED_EXECUTORS = getCounter(""tasks"", ""unblackListedExecutors"")
+}
+
+private[spark] object AppStatusSource {
+
+  def getCounter(prefix: String, name: String)(implicit metricRegistry: MetricRegistry): Counter = {
+    metricRegistry.counter(MetricRegistry.name(prefix, name))
+  }
+
+  def createSource(conf: SparkConf): Option[AppStatusSource] = {
+    Option(conf.get(APP_STATUS_METRICS_ENABLED))
+      .filter(identity)
+      .map { _ => new AppStatusSource() }
+  }
+}
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala b/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala
index e237281c552b1..5c0ed4d5d8f4c 100644
--- a/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala
@@ -112,10 +112,12 @@ private[spark] class AppStatusStore(
     }
   }
 
-  def stageAttempt(stageId: Int, stageAttemptId: Int, details: Boolean = false): v1.StageData = {
+  def stageAttempt(stageId: Int, stageAttemptId: Int,
+      details: Boolean = false): (v1.StageData, Seq[Int]) = {
     val stageKey = Array(stageId, stageAttemptId)
-    val stage = store.read(classOf[StageDataWrapper], stageKey).info
-    if (details) stageWithDetails(stage) else stage
+    val stageDataWrapper = store.read(classOf[StageDataWrapper], stageKey)
+    val stage = if (details) stageWithDetails(stageDataWrapper.info) else stageDataWrapper.info
+    (stage, stageDataWrapper.jobIds.toSeq)
   }
 
   def taskCount(stageId: Int, stageAttemptId: Int): Long = {
@@ -349,7 +351,9 @@ private[spark] class AppStatusStore(
   def taskList(stageId: Int, stageAttemptId: Int, maxTasks: Int): Seq[v1.TaskData] = {
     val stageKey = Array(stageId, stageAttemptId)
     store.view(classOf[TaskDataWrapper]).index(""stage"").first(stageKey).last(stageKey).reverse()
-      .max(maxTasks).asScala.map(_.toApi).toSeq.reverse
+      .max(maxTasks).asScala.map { taskDataWrapper =>
+      constructTaskData(taskDataWrapper)
+    }.toSeq.reverse
   }
 
   def taskList(
@@ -388,7 +392,9 @@ private[spark] class AppStatusStore(
     }
 
     val ordered = if (ascending) indexed else indexed.reverse()
-    ordered.skip(offset).max(length).asScala.map(_.toApi).toSeq
+    ordered.skip(offset).max(length).asScala.map { taskDataWrapper =>
+      constructTaskData(taskDataWrapper)
+    }.toSeq
   }
 
   def executorSummary(stageId: Int, attemptId: Int): Map[String, v1.ExecutorStageSummary] = {
@@ -494,6 +500,24 @@ private[spark] class AppStatusStore(
     store.close()
   }
 
+  def constructTaskData(taskDataWrapper: TaskDataWrapper) : v1.TaskData = {
+    val taskDataOld: v1.TaskData = taskDataWrapper.toApi
+    val executorLogs: Option[Map[String, String]] = try {
+      Some(executorSummary(taskDataOld.executorId).executorLogs)
+    } catch {
+      case e: NoSuchElementException => e.getMessage
+        None
+    }
+    new v1.TaskData(taskDataOld.taskId, taskDataOld.index,
+      taskDataOld.attempt, taskDataOld.launchTime, taskDataOld.resultFetchStart,
+      taskDataOld.duration, taskDataOld.executorId, taskDataOld.host, taskDataOld.status,
+      taskDataOld.taskLocality, taskDataOld.speculative, taskDataOld.accumulatorUpdates,
+      taskDataOld.errorMessage, taskDataOld.taskMetrics,
+      executorLogs.getOrElse(Map[String, String]()),
+      AppStatusUtils.schedulerDelay(taskDataOld),
+      AppStatusUtils.gettingResultTime(taskDataOld))
+  }
+
 }
 
 private[spark] object AppStatusStore {
@@ -503,10 +527,11 @@ private[spark] object AppStatusStore {
   /**
    * Create an in-memory store for a live application.
    */
-  def createLiveStore(conf: SparkConf): AppStatusStore = {
+  def createLiveStore(
+      conf: SparkConf,
+      appStatusSource: Option[AppStatusSource] = None): AppStatusStore = {
     val store = new ElementTrackingStore(new InMemoryStore(), conf)
-    val listener = new AppStatusListener(store, conf, true)
+    val listener = new AppStatusListener(store, conf, true, appStatusSource)
     new AppStatusStore(store, listener = Some(listener))
   }
-
 }
diff --git a/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala b/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala
index 863b0967f765e..5ec7d90bfaaba 100644
--- a/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala
+++ b/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala
@@ -24,6 +24,7 @@ import scala.collection.mutable.{HashMap, ListBuffer}
 import com.google.common.util.concurrent.MoreExecutors
 
 import org.apache.spark.SparkConf
+import org.apache.spark.internal.config.Status._
 import org.apache.spark.util.{ThreadUtils, Utils}
 import org.apache.spark.util.kvstore._
 
@@ -45,8 +46,6 @@ import org.apache.spark.util.kvstore._
  */
 private[spark] class ElementTrackingStore(store: KVStore, conf: SparkConf) extends KVStore {
 
-  import config._
-
   private val triggers = new HashMap[Class[_], Seq[Trigger[_]]]()
   private val flushTriggers = new ListBuffer[() => Unit]()
   private val executor = if (conf.get(ASYNC_TRACKING_ENABLED)) {
diff --git a/core/src/main/scala/org/apache/spark/status/KVUtils.scala b/core/src/main/scala/org/apache/spark/status/KVUtils.scala
index 99b1843d8e1c0..45348be5c98b9 100644
--- a/core/src/main/scala/org/apache/spark/status/KVUtils.scala
+++ b/core/src/main/scala/org/apache/spark/status/KVUtils.scala
@@ -42,7 +42,7 @@ private[spark] object KVUtils extends Logging {
   private[spark] class KVStoreScalaSerializer extends KVStoreSerializer {
 
     mapper.registerModule(DefaultScalaModule)
-    mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL)
+    mapper.setSerializationInclusion(JsonInclude.Include.NON_ABSENT)
 
   }
 
diff --git a/core/src/main/scala/org/apache/spark/status/LiveEntity.scala b/core/src/main/scala/org/apache/spark/status/LiveEntity.scala
index 79e3f13b826ce..47e45a66ecccb 100644
--- a/core/src/main/scala/org/apache/spark/status/LiveEntity.scala
+++ b/core/src/main/scala/org/apache/spark/status/LiveEntity.scala
@@ -26,14 +26,13 @@ import scala.collection.mutable.HashMap
 import com.google.common.collect.Interners
 
 import org.apache.spark.JobExecutionStatus
-import org.apache.spark.executor.TaskMetrics
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}
 import org.apache.spark.scheduler.{AccumulableInfo, StageInfo, TaskInfo}
 import org.apache.spark.status.api.v1
 import org.apache.spark.storage.RDDInfo
 import org.apache.spark.ui.SparkUI
 import org.apache.spark.util.AccumulatorContext
 import org.apache.spark.util.collection.OpenHashSet
-import org.apache.spark.util.kvstore.KVStore
 
 /**
  * A mutable representation of a live entity in Spark (jobs, stages, tasks, et al). Every live
@@ -62,7 +61,7 @@ private[spark] abstract class LiveEntity {
 private class LiveJob(
     val jobId: Int,
     name: String,
-    submissionTime: Option[Date],
+    val submissionTime: Option[Date],
     val stageIds: Seq[Int],
     jobGroup: Option[String],
     numTasks: Int) extends LiveEntity {
@@ -268,6 +267,9 @@ private class LiveExecutor(val executorId: String, _addTime: Long) extends LiveE
 
   def hasMemoryInfo: Boolean = totalOnHeap >= 0L
 
+  // peak values for executor level metrics
+  val peakExecutorMetrics = new ExecutorMetrics()
+
   def hostname: String = if (host != null) host else hostPort.split("":"")(0)
 
   override protected def doUpdate(): Any = {
@@ -302,10 +304,10 @@ private class LiveExecutor(val executorId: String, _addTime: Long) extends LiveE
       Option(removeReason),
       executorLogs,
       memoryMetrics,
-      blacklistedInStages)
+      blacklistedInStages,
+      Some(peakExecutorMetrics).filter(_.isSet))
     new ExecutorSummaryWrapper(info)
   }
-
 }
 
 private class LiveExecutorStageSummary(
@@ -374,6 +376,8 @@ private class LiveStage extends LiveEntity {
 
   val executorSummaries = new HashMap[String, LiveExecutorStageSummary]()
 
+  val activeTasksPerExecutor = new HashMap[String, Int]().withDefaultValue(0)
+
   var blackListedExecutors = new HashSet[String]()
 
   // Used for cleanup of tasks after they reach the configured limit. Not written to the store.
@@ -538,6 +542,10 @@ private class LiveRDD(val info: RDDInfo) extends LiveEntity {
     distributions.get(exec.executorId)
   }
 
+  def getPartitions(): scala.collection.Map[String, LiveRDDPartition] = partitions
+
+  def getDistributions(): scala.collection.Map[String, LiveRDDDistribution] = distributions
+
   override protected def doUpdate(): Any = {
     val dists = if (distributions.nonEmpty) {
       Some(distributions.values.map(_.toApi()).toSeq)
@@ -581,8 +589,7 @@ private object LiveEntityHelpers {
       .filter { acc =>
         // We don't need to store internal or SQL accumulables as their values will be shown in
         // other places, so drop them to reduce the memory usage.
-        !acc.internal && (!acc.metadata.isDefined ||
-          acc.metadata.get != Some(AccumulatorContext.SQL_ACCUM_IDENTIFIER))
+        !acc.internal && acc.metadata != Some(AccumulatorContext.SQL_ACCUM_IDENTIFIER)
       }
       .map { acc =>
         new v1.AccumulableInfo(
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala b/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala
index 4560d300cb0c8..50a286d0d3b0f 100644
--- a/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala
@@ -49,7 +49,7 @@ private[v1] class JacksonMessageWriter extends MessageBodyWriter[Object]{
   }
   mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)
   mapper.enable(SerializationFeature.INDENT_OUTPUT)
-  mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL)
+  mapper.setSerializationInclusion(JsonInclude.Include.NON_ABSENT)
   mapper.setDateFormat(JacksonMessageWriter.makeISODateFormat)
 
   override def isWriteable(
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala b/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala
index 32100c5704538..1f4082cac8f75 100644
--- a/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala
@@ -175,7 +175,7 @@ private[v1] class OneApplicationAttemptResource extends AbstractApplicationResou
   def getAttempt(): ApplicationAttemptInfo = {
     uiRoot.getApplicationInfo(appId)
       .flatMap { app =>
-        app.attempts.filter(_.attemptId == attemptId).headOption
+        app.attempts.find(_.attemptId.contains(attemptId))
       }
       .getOrElse {
         throw new NotFoundException(s""unknown app $appId, attempt $attemptId"")
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala b/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala
index 96249e4bfd5fa..f81892734c2de 100644
--- a/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala
@@ -16,15 +16,16 @@
  */
 package org.apache.spark.status.api.v1
 
-import java.util.{List => JList}
+import java.util.{HashMap, List => JList, Locale}
 import javax.ws.rs._
-import javax.ws.rs.core.MediaType
+import javax.ws.rs.core.{Context, MediaType, MultivaluedMap, UriInfo}
 
 import org.apache.spark.SparkException
 import org.apache.spark.scheduler.StageInfo
 import org.apache.spark.status.api.v1.StageStatus._
 import org.apache.spark.status.api.v1.TaskSorting._
 import org.apache.spark.ui.SparkUI
+import org.apache.spark.ui.jobs.ApiHelper._
 
 @Produces(Array(MediaType.APPLICATION_JSON))
 private[v1] class StagesResource extends BaseAppResource {
@@ -56,7 +57,7 @@ private[v1] class StagesResource extends BaseAppResource {
       @PathParam(""stageAttemptId"") stageAttemptId: Int,
       @QueryParam(""details"") @DefaultValue(""true"") details: Boolean): StageData = withUI { ui =>
     try {
-      ui.store.stageAttempt(stageId, stageAttemptId, details = details)
+      ui.store.stageAttempt(stageId, stageAttemptId, details = details)._1
     } catch {
       case _: NoSuchElementException =>
         // Change the message depending on whether there are any attempts for the requested stage.
@@ -102,4 +103,120 @@ private[v1] class StagesResource extends BaseAppResource {
     withUI(_.store.taskList(stageId, stageAttemptId, offset, length, sortBy))
   }
 
+  // This api needs to stay formatted exactly as it is below, since, it is being used by the
+  // datatables for the stages page.
+  @GET
+  @Path(""{stageId: \\d+}/{stageAttemptId: \\d+}/taskTable"")
+  def taskTable(
+    @PathParam(""stageId"") stageId: Int,
+    @PathParam(""stageAttemptId"") stageAttemptId: Int,
+    @QueryParam(""details"") @DefaultValue(""true"") details: Boolean,
+    @Context uriInfo: UriInfo):
+  HashMap[String, Object] = {
+    withUI { ui =>
+      val uriQueryParameters = uriInfo.getQueryParameters(true)
+      val totalRecords = uriQueryParameters.getFirst(""numTasks"")
+      var isSearch = false
+      var searchValue: String = null
+      var filteredRecords = totalRecords
+      // The datatables client API sends a list of query parameters to the server which contain
+      // information like the columns to be sorted, search value typed by the user in the search
+      // box, pagination index etc. For more information on these query parameters,
+      // refer https://datatables.net/manual/server-side.
+      if (uriQueryParameters.getFirst(""search[value]"") != null &&
+        uriQueryParameters.getFirst(""search[value]"").length > 0) {
+        isSearch = true
+        searchValue = uriQueryParameters.getFirst(""search[value]"")
+      }
+      val _tasksToShow: Seq[TaskData] = doPagination(uriQueryParameters, stageId, stageAttemptId,
+        isSearch, totalRecords.toInt)
+      val ret = new HashMap[String, Object]()
+      if (_tasksToShow.nonEmpty) {
+        // Performs server-side search based on input from user
+        if (isSearch) {
+          val filteredTaskList = filterTaskList(_tasksToShow, searchValue)
+          filteredRecords = filteredTaskList.length.toString
+          if (filteredTaskList.length > 0) {
+            val pageStartIndex = uriQueryParameters.getFirst(""start"").toInt
+            val pageLength = uriQueryParameters.getFirst(""length"").toInt
+            ret.put(""aaData"", filteredTaskList.slice(
+              pageStartIndex, pageStartIndex + pageLength))
+          } else {
+            ret.put(""aaData"", filteredTaskList)
+          }
+        } else {
+          ret.put(""aaData"", _tasksToShow)
+        }
+      } else {
+        ret.put(""aaData"", _tasksToShow)
+      }
+      ret.put(""recordsTotal"", totalRecords)
+      ret.put(""recordsFiltered"", filteredRecords)
+      ret
+    }
+  }
+
+  // Performs pagination on the server side
+  def doPagination(queryParameters: MultivaluedMap[String, String], stageId: Int,
+    stageAttemptId: Int, isSearch: Boolean, totalRecords: Int): Seq[TaskData] = {
+    var columnNameToSort = queryParameters.getFirst(""columnNameToSort"")
+    // Sorting on Logs column will default to Index column sort
+    if (columnNameToSort.equalsIgnoreCase(""Logs"")) {
+      columnNameToSort = ""Index""
+    }
+    val isAscendingStr = queryParameters.getFirst(""order[0][dir]"")
+    var pageStartIndex = 0
+    var pageLength = totalRecords
+    // We fetch only the desired rows upto the specified page length for all cases except when a
+    // search query is present, in that case, we need to fetch all the rows to perform the search
+    // on the entire table
+    if (!isSearch) {
+      pageStartIndex = queryParameters.getFirst(""start"").toInt
+      pageLength = queryParameters.getFirst(""length"").toInt
+    }
+    withUI(_.store.taskList(stageId, stageAttemptId, pageStartIndex, pageLength,
+      indexName(columnNameToSort), isAscendingStr.equalsIgnoreCase(""asc"")))
+  }
+
+  // Filters task list based on search parameter
+  def filterTaskList(
+    taskDataList: Seq[TaskData],
+    searchValue: String): Seq[TaskData] = {
+    val defaultOptionString: String = ""d""
+    val searchValueLowerCase = searchValue.toLowerCase(Locale.ROOT)
+    val containsValue = (taskDataParams: Any) => taskDataParams.toString.toLowerCase(
+      Locale.ROOT).contains(searchValueLowerCase)
+    val taskMetricsContainsValue = (task: TaskData) => task.taskMetrics match {
+      case None => false
+      case Some(metrics) =>
+        (containsValue(task.taskMetrics.get.executorDeserializeTime)
+        || containsValue(task.taskMetrics.get.executorRunTime)
+        || containsValue(task.taskMetrics.get.jvmGcTime)
+        || containsValue(task.taskMetrics.get.resultSerializationTime)
+        || containsValue(task.taskMetrics.get.memoryBytesSpilled)
+        || containsValue(task.taskMetrics.get.diskBytesSpilled)
+        || containsValue(task.taskMetrics.get.peakExecutionMemory)
+        || containsValue(task.taskMetrics.get.inputMetrics.bytesRead)
+        || containsValue(task.taskMetrics.get.inputMetrics.recordsRead)
+        || containsValue(task.taskMetrics.get.outputMetrics.bytesWritten)
+        || containsValue(task.taskMetrics.get.outputMetrics.recordsWritten)
+        || containsValue(task.taskMetrics.get.shuffleReadMetrics.fetchWaitTime)
+        || containsValue(task.taskMetrics.get.shuffleReadMetrics.recordsRead)
+        || containsValue(task.taskMetrics.get.shuffleWriteMetrics.bytesWritten)
+        || containsValue(task.taskMetrics.get.shuffleWriteMetrics.recordsWritten)
+        || containsValue(task.taskMetrics.get.shuffleWriteMetrics.writeTime))
+    }
+    val filteredTaskDataSequence: Seq[TaskData] = taskDataList.filter(f =>
+      (containsValue(f.taskId) || containsValue(f.index) || containsValue(f.attempt)
+        || containsValue(f.launchTime)
+        || containsValue(f.resultFetchStart.getOrElse(defaultOptionString))
+        || containsValue(f.duration.getOrElse(defaultOptionString))
+        || containsValue(f.executorId) || containsValue(f.host) || containsValue(f.status)
+        || containsValue(f.taskLocality) || containsValue(f.speculative)
+        || containsValue(f.errorMessage.getOrElse(defaultOptionString))
+        || taskMetricsContainsValue(f)
+        || containsValue(f.schedulerDelay) || containsValue(f.gettingResultTime)))
+    filteredTaskDataSequence
+  }
+
 }
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/api.scala b/core/src/main/scala/org/apache/spark/status/api/v1/api.scala
index 971d7e90fa7b8..aa21da2b66ab2 100644
--- a/core/src/main/scala/org/apache/spark/status/api/v1/api.scala
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/api.scala
@@ -22,9 +22,14 @@ import java.util.Date
 import scala.xml.{NodeSeq, Text}
 
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties
-import com.fasterxml.jackson.databind.annotation.JsonDeserialize
+import com.fasterxml.jackson.core.{JsonGenerator, JsonParser}
+import com.fasterxml.jackson.core.`type`.TypeReference
+import com.fasterxml.jackson.databind.{DeserializationContext, JsonDeserializer, JsonSerializer, SerializerProvider}
+import com.fasterxml.jackson.databind.annotation.{JsonDeserialize, JsonSerialize}
 
 import org.apache.spark.JobExecutionStatus
+import org.apache.spark.executor.ExecutorMetrics
+import org.apache.spark.metrics.ExecutorMetricType
 
 case class ApplicationInfo private[spark](
     id: String,
@@ -98,7 +103,10 @@ class ExecutorSummary private[spark](
     val removeReason: Option[String],
     val executorLogs: Map[String, String],
     val memoryMetrics: Option[MemoryMetrics],
-    val blacklistedInStages: Set[Int])
+    val blacklistedInStages: Set[Int],
+    @JsonSerialize(using = classOf[ExecutorMetricsJsonSerializer])
+    @JsonDeserialize(using = classOf[ExecutorMetricsJsonDeserializer])
+    val peakMemoryMetrics: Option[ExecutorMetrics])
 
 class MemoryMetrics private[spark](
     val usedOnHeapStorageMemory: Long,
@@ -106,6 +114,36 @@ class MemoryMetrics private[spark](
     val totalOnHeapStorageMemory: Long,
     val totalOffHeapStorageMemory: Long)
 
+/** deserializer for peakMemoryMetrics: convert map to ExecutorMetrics */
+private[spark] class ExecutorMetricsJsonDeserializer
+    extends JsonDeserializer[Option[ExecutorMetrics]] {
+  override def deserialize(
+      jsonParser: JsonParser,
+      deserializationContext: DeserializationContext): Option[ExecutorMetrics] = {
+    val metricsMap = jsonParser.readValueAs[Option[Map[String, Long]]](
+      new TypeReference[Option[Map[String, java.lang.Long]]] {})
+    metricsMap.map(metrics => new ExecutorMetrics(metrics))
+  }
+}
+/** serializer for peakMemoryMetrics: convert ExecutorMetrics to map with metric name as key */
+private[spark] class ExecutorMetricsJsonSerializer
+    extends JsonSerializer[Option[ExecutorMetrics]] {
+  override def serialize(
+      metrics: Option[ExecutorMetrics],
+      jsonGenerator: JsonGenerator,
+      serializerProvider: SerializerProvider): Unit = {
+    metrics.foreach { m: ExecutorMetrics =>
+      val metricsMap = ExecutorMetricType.values.map { metricType =>
+            metricType.name -> m.getMetricValue(metricType)
+      }.toMap
+      jsonGenerator.writeObject(metricsMap)
+    }
+  }
+
+  override def isEmpty(provider: SerializerProvider, value: Option[ExecutorMetrics]): Boolean =
+    value.isEmpty
+}
+
 class JobData private[spark](
     val jobId: Int,
     val name: String,
@@ -215,7 +253,10 @@ class TaskData private[spark](
     val speculative: Boolean,
     val accumulatorUpdates: Seq[AccumulableInfo],
     val errorMessage: Option[String] = None,
-    val taskMetrics: Option[TaskMetrics] = None)
+    val taskMetrics: Option[TaskMetrics] = None,
+    val executorLogs: Map[String, String],
+    val schedulerDelay: Long,
+    val gettingResultTime: Long)
 
 class TaskMetrics private[spark](
     val executorDeserializeTime: Long,
diff --git a/core/src/main/scala/org/apache/spark/status/storeTypes.scala b/core/src/main/scala/org/apache/spark/status/storeTypes.scala
index 646cf25880e37..ef19e86f3135f 100644
--- a/core/src/main/scala/org/apache/spark/status/storeTypes.scala
+++ b/core/src/main/scala/org/apache/spark/status/storeTypes.scala
@@ -283,7 +283,10 @@ private[spark] class TaskDataWrapper(
       speculative,
       accumulatorUpdates,
       errorMessage,
-      metrics)
+      metrics,
+      executorLogs = null,
+      schedulerDelay = 0L,
+      gettingResultTime = 0L)
   }
 
   @JsonIgnore @KVIndex(TaskIndexNames.STAGE)
diff --git a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala
index f5c69ad241e3a..1dfbc6effb346 100644
--- a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala
+++ b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala
@@ -35,7 +35,7 @@ import scala.util.control.NonFatal
 import com.codahale.metrics.{MetricRegistry, MetricSet}
 
 import org.apache.spark._
-import org.apache.spark.executor.{DataReadMethod, ShuffleWriteMetrics}
+import org.apache.spark.executor.DataReadMethod
 import org.apache.spark.internal.{config, Logging}
 import org.apache.spark.memory.{MemoryManager, MemoryMode}
 import org.apache.spark.metrics.source.Source
@@ -43,12 +43,13 @@ import org.apache.spark.network._
 import org.apache.spark.network.buffer.ManagedBuffer
 import org.apache.spark.network.client.StreamCallbackWithID
 import org.apache.spark.network.netty.SparkTransportConf
-import org.apache.spark.network.shuffle.{ExternalShuffleClient, TempFileManager}
+import org.apache.spark.network.shuffle._
 import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo
+import org.apache.spark.network.util.TransportConf
 import org.apache.spark.rpc.RpcEnv
 import org.apache.spark.scheduler.ExecutorCacheTaskLocation
 import org.apache.spark.serializer.{SerializerInstance, SerializerManager}
-import org.apache.spark.shuffle.ShuffleManager
+import org.apache.spark.shuffle.{ShuffleManager, ShuffleWriteMetricsReporter}
 import org.apache.spark.storage.memory._
 import org.apache.spark.unsafe.Platform
 import org.apache.spark.util._
@@ -131,8 +132,6 @@ private[spark] class BlockManager(
 
   private[spark] val externalShuffleServiceEnabled =
     conf.get(config.SHUFFLE_SERVICE_ENABLED)
-  private val chunkSize =
-    conf.getSizeAsBytes(""spark.storage.memoryMapLimitForTests"", Int.MaxValue.toString).toInt
   private val remoteReadNioBufferConversion =
     conf.getBoolean(""spark.network.remoteReadNioBufferConversion"", false)
 
@@ -213,11 +212,11 @@ private[spark] class BlockManager(
 
   private var blockReplicationPolicy: BlockReplicationPolicy = _
 
-  // A TempFileManager used to track all the files of remote blocks which above the
+  // A DownloadFileManager used to track all the files of remote blocks which are above the
   // specified memory threshold. Files will be deleted automatically based on weak reference.
   // Exposed for test
   private[storage] val remoteBlockTempFileManager =
-    new BlockManager.RemoteBlockTempFileManager(this)
+    new BlockManager.RemoteBlockDownloadFileManager(this)
   private val maxRemoteBlockToMem = conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM)
 
   /**
@@ -237,7 +236,7 @@ private[spark] class BlockManager(
       val priorityClass = conf.get(
         ""spark.storage.replication.policy"", classOf[RandomBlockReplicationPolicy].getName)
       val clazz = Utils.classForName(priorityClass)
-      val ret = clazz.newInstance.asInstanceOf[BlockReplicationPolicy]
+      val ret = clazz.getConstructor().newInstance().asInstanceOf[BlockReplicationPolicy]
       logInfo(s""Using $priorityClass for block replication policy"")
       ret
     }
@@ -436,10 +435,8 @@ private[spark] class BlockManager(
         // stream.
         channel.close()
         // TODO SPARK-25035 Even if we're only going to write the data to disk after this, we end up
-        // using a lot of memory here.  With encryption, we'll read the whole file into a regular
-        // byte buffer and OOM.  Without encryption, we'll memory map the file and won't get a jvm
-        // OOM, but might get killed by the OS / cluster manager.  We could at least read the tmp
-        // file as a stream in both cases.
+        // using a lot of memory here. We'll read the whole file into a regular
+        // byte buffer and OOM.  We could at least read the tmp file as a stream.
         val buffer = securityManager.getIOEncryptionKey() match {
           case Some(key) =>
             // we need to pass in the size of the unencrypted block
@@ -451,7 +448,7 @@ private[spark] class BlockManager(
             new EncryptedBlockData(tmpFile, blockSize, conf, key).toChunkedByteBuffer(allocator)
 
           case None =>
-            ChunkedByteBuffer.map(tmpFile, conf.get(config.MEMORY_MAP_LIMIT_FOR_TESTS).toInt)
+            ChunkedByteBuffer.fromFile(tmpFile)
         }
         putBytes(blockId, buffer, level)(classTag)
         tmpFile.delete()
@@ -695,9 +692,9 @@ private[spark] class BlockManager(
    */
   private def getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult] = {
     val ct = implicitly[ClassTag[T]]
-    getRemoteBytes(blockId).map { data =>
+    getRemoteManagedBuffer(blockId).map { data =>
       val values =
-        serializerManager.dataDeserializeStream(blockId, data.toInputStream(dispose = true))(ct)
+        serializerManager.dataDeserializeStream(blockId, data.createInputStream())(ct)
       new BlockResult(values, DataReadMethod.Network, data.size)
     }
   }
@@ -720,14 +717,9 @@ private[spark] class BlockManager(
   }
 
   /**
-   * Get block from remote block managers as serialized bytes.
+   * Get block from remote block managers as a ManagedBuffer.
    */
-  def getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] = {
-    // TODO if we change this method to return the ManagedBuffer, then getRemoteValues
-    // could just use the inputStream on the temp file, rather than memory-mapping the file.
-    // Until then, replication can cause the process to use too much memory and get killed
-    // by the OS / cluster manager (not a java OOM, since it's a memory-mapped file) even though
-    // we've read the data to disk.
+  private def getRemoteManagedBuffer(blockId: BlockId): Option[ManagedBuffer] = {
     logDebug(s""Getting remote block $blockId"")
     require(blockId != null, ""BlockId is null"")
     var runningFailureCount = 0
@@ -792,14 +784,13 @@ private[spark] class BlockManager(
       }
 
       if (data != null) {
-        // SPARK-24307 undocumented ""escape-hatch"" in case there are any issues in converting to
-        // ChunkedByteBuffer, to go back to old code-path.  Can be removed post Spark 2.4 if
-        // new path is stable.
-        if (remoteReadNioBufferConversion) {
-          return Some(new ChunkedByteBuffer(data.nioByteBuffer()))
-        } else {
-          return Some(ChunkedByteBuffer.fromManagedBuffer(data, chunkSize))
-        }
+        // If the ManagedBuffer is a BlockManagerManagedBuffer, the disposal of the
+        // byte buffers backing it may need to be handled after reading the bytes.
+        // In this case, since we just fetched the bytes remotely, we do not have
+        // a BlockManagerManagedBuffer. The assert here is to ensure that this holds
+        // true (or the disposal is handled).
+        assert(!data.isInstanceOf[BlockManagerManagedBuffer])
+        return Some(data)
       }
       logDebug(s""The value of block $blockId is null"")
     }
@@ -807,6 +798,22 @@ private[spark] class BlockManager(
     None
   }
 
+  /**
+   * Get block from remote block managers as serialized bytes.
+   */
+  def getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] = {
+    getRemoteManagedBuffer(blockId).map { data =>
+      // SPARK-24307 undocumented ""escape-hatch"" in case there are any issues in converting to
+      // ChunkedByteBuffer, to go back to old code-path.  Can be removed post Spark 2.4 if
+      // new path is stable.
+      if (remoteReadNioBufferConversion) {
+        new ChunkedByteBuffer(data.nioByteBuffer())
+      } else {
+        ChunkedByteBuffer.fromManagedBuffer(data)
+      }
+    }
+  }
+
   /**
    * Get a block from the block manager (either local or remote).
    *
@@ -935,7 +942,7 @@ private[spark] class BlockManager(
       file: File,
       serializerInstance: SerializerInstance,
       bufferSize: Int,
-      writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter = {
+      writeMetrics: ShuffleWriteMetricsReporter): DiskBlockObjectWriter = {
     val syncWrites = conf.getBoolean(""spark.shuffle.sync"", false)
     new DiskBlockObjectWriter(file, serializerManager, serializerInstance, bufferSize,
       syncWrites, writeMetrics, blockId)
@@ -1664,23 +1671,28 @@ private[spark] object BlockManager {
     metricRegistry.registerAll(metricSet)
   }
 
-  class RemoteBlockTempFileManager(blockManager: BlockManager)
-      extends TempFileManager with Logging {
+  class RemoteBlockDownloadFileManager(blockManager: BlockManager)
+      extends DownloadFileManager with Logging {
+    // lazy because SparkEnv is set after this
+    lazy val encryptionKey = SparkEnv.get.securityManager.getIOEncryptionKey()
 
-    private class ReferenceWithCleanup(file: File, referenceQueue: JReferenceQueue[File])
-        extends WeakReference[File](file, referenceQueue) {
-      private val filePath = file.getAbsolutePath
+    private class ReferenceWithCleanup(
+        file: DownloadFile,
+        referenceQueue: JReferenceQueue[DownloadFile]
+        ) extends WeakReference[DownloadFile](file, referenceQueue) {
+
+      val filePath = file.path()
 
       def cleanUp(): Unit = {
         logDebug(s""Clean up file $filePath"")
 
-        if (!new File(filePath).delete()) {
+        if (!file.delete()) {
           logDebug(s""Fail to delete file $filePath"")
         }
       }
     }
 
-    private val referenceQueue = new JReferenceQueue[File]
+    private val referenceQueue = new JReferenceQueue[DownloadFile]
     private val referenceBuffer = Collections.newSetFromMap[ReferenceWithCleanup](
       new ConcurrentHashMap)
 
@@ -1692,11 +1704,21 @@ private[spark] object BlockManager {
     cleaningThread.setName(""RemoteBlock-temp-file-clean-thread"")
     cleaningThread.start()
 
-    override def createTempFile(): File = {
-      blockManager.diskBlockManager.createTempLocalBlock()._2
+    override def createTempFile(transportConf: TransportConf): DownloadFile = {
+      val file = blockManager.diskBlockManager.createTempLocalBlock()._2
+      encryptionKey match {
+        case Some(key) =>
+          // encryption is enabled, so when we read the decrypted data off the network, we need to
+          // encrypt it when writing to disk.  Note that the data may have been encrypted when it
+          // was cached on disk on the remote side, but it was already decrypted by now (see
+          // EncryptedBlockData).
+          new EncryptedDownloadFile(file, key)
+        case None =>
+          new SimpleDownloadFile(file, transportConf)
+      }
     }
 
-    override def registerTempFileToClean(file: File): Boolean = {
+    override def registerTempFileToClean(file: DownloadFile): Boolean = {
       referenceBuffer.add(new ReferenceWithCleanup(file, referenceQueue))
     }
 
@@ -1724,4 +1746,39 @@ private[spark] object BlockManager {
       }
     }
   }
+
+  /**
+   * A DownloadFile that encrypts data when it is written, and decrypts when it's read.
+   */
+  private class EncryptedDownloadFile(
+      file: File,
+      key: Array[Byte]) extends DownloadFile {
+
+    private val env = SparkEnv.get
+
+    override def delete(): Boolean = file.delete()
+
+    override def openForWriting(): DownloadFileWritableChannel = {
+      new EncryptedDownloadWritableChannel()
+    }
+
+    override def path(): String = file.getAbsolutePath
+
+    private class EncryptedDownloadWritableChannel extends DownloadFileWritableChannel {
+      private val countingOutput: CountingWritableChannel = new CountingWritableChannel(
+        Channels.newChannel(env.serializerManager.wrapForEncryption(new FileOutputStream(file))))
+
+      override def closeAndRead(): ManagedBuffer = {
+        countingOutput.close()
+        val size = countingOutput.getCount
+        new EncryptedManagedBuffer(new EncryptedBlockData(file, size, env.conf, key))
+      }
+
+      override def write(src: ByteBuffer): Int = countingOutput.write(src)
+
+      override def isOpen: Boolean = countingOutput.isOpen()
+
+      override def close(): Unit = countingOutput.close()
+    }
+  }
 }
diff --git a/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala b/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala
index a024c83d8d8b7..17390f9c60e79 100644
--- a/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala
+++ b/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala
@@ -20,9 +20,9 @@ package org.apache.spark.storage
 import java.io.{BufferedOutputStream, File, FileOutputStream, OutputStream}
 import java.nio.channels.FileChannel
 
-import org.apache.spark.executor.ShuffleWriteMetrics
 import org.apache.spark.internal.Logging
 import org.apache.spark.serializer.{SerializationStream, SerializerInstance, SerializerManager}
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter
 import org.apache.spark.util.Utils
 
 /**
@@ -43,7 +43,7 @@ private[spark] class DiskBlockObjectWriter(
     syncWrites: Boolean,
     // These write metrics concurrently shared with other active DiskBlockObjectWriters who
     // are themselves performing writes. All updates must be relative.
-    writeMetrics: ShuffleWriteMetrics,
+    writeMetrics: ShuffleWriteMetricsReporter,
     val blockId: BlockId = null)
   extends OutputStream
   with Logging {
diff --git a/core/src/main/scala/org/apache/spark/storage/DiskStore.scala b/core/src/main/scala/org/apache/spark/storage/DiskStore.scala
index a820bc70b33b2..29963a95cb074 100644
--- a/core/src/main/scala/org/apache/spark/storage/DiskStore.scala
+++ b/core/src/main/scala/org/apache/spark/storage/DiskStore.scala
@@ -30,8 +30,10 @@ import io.netty.channel.DefaultFileRegion
 
 import org.apache.spark.{SecurityManager, SparkConf}
 import org.apache.spark.internal.{config, Logging}
+import org.apache.spark.network.buffer.ManagedBuffer
 import org.apache.spark.network.util.{AbstractFileRegion, JavaUtils}
 import org.apache.spark.security.CryptoStreamUtils
+import org.apache.spark.unsafe.array.ByteArrayMethods
 import org.apache.spark.util.Utils
 import org.apache.spark.util.io.ChunkedByteBuffer
 
@@ -200,7 +202,7 @@ private class DiskBlockData(
   private def open() = new FileInputStream(file).getChannel
 }
 
-private class EncryptedBlockData(
+private[spark] class EncryptedBlockData(
     file: File,
     blockSize: Long,
     conf: SparkConf,
@@ -216,7 +218,7 @@ private class EncryptedBlockData(
       var remaining = blockSize
       val chunks = new ListBuffer[ByteBuffer]()
       while (remaining > 0) {
-        val chunkSize = math.min(remaining, Int.MaxValue)
+        val chunkSize = math.min(remaining, ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)
         val chunk = allocator(chunkSize.toInt)
         remaining -= chunkSize
         JavaUtils.readFully(source, chunk)
@@ -234,7 +236,8 @@ private class EncryptedBlockData(
     // This is used by the block transfer service to replicate blocks. The upload code reads
     // all bytes into memory to send the block to the remote executor, so it's ok to do this
     // as long as the block fits in a Java array.
-    assert(blockSize <= Int.MaxValue, ""Block is too large to be wrapped in a byte buffer."")
+    assert(blockSize <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,
+      ""Block is too large to be wrapped in a byte buffer."")
     val dst = ByteBuffer.allocate(blockSize.toInt)
     val in = open()
     try {
@@ -260,7 +263,23 @@ private class EncryptedBlockData(
         throw e
     }
   }
+}
+
+private[spark] class EncryptedManagedBuffer(
+    val blockData: EncryptedBlockData) extends ManagedBuffer {
+
+  // This is the size of the decrypted data
+  override def size(): Long = blockData.size
+
+  override def nioByteBuffer(): ByteBuffer = blockData.toByteBuffer()
+
+  override def convertToNetty(): AnyRef = blockData.toNetty()
+
+  override def createInputStream(): InputStream = blockData.toInputStream()
+
+  override def retain(): ManagedBuffer = this
 
+  override def release(): ManagedBuffer = this
 }
 
 private class ReadableChannelFileRegion(source: ReadableByteChannel, blockSize: Long)
diff --git a/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala b/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala
index 9ccc8f9cc585b..917cfab1c699a 100644
--- a/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala
+++ b/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala
@@ -55,14 +55,17 @@ class RDDInfo(
 }
 
 private[spark] object RDDInfo {
-  private val callsiteForm = SparkEnv.get.conf.get(EVENT_LOG_CALLSITE_FORM)
-
   def fromRdd(rdd: RDD[_]): RDDInfo = {
     val rddName = Option(rdd.name).getOrElse(Utils.getFormattedClassName(rdd))
     val parentIds = rdd.dependencies.map(_.rdd.id)
-    val callSite = callsiteForm match {
-      case ""short"" => rdd.creationSite.shortForm
-      case ""long"" => rdd.creationSite.longForm
+    val callsiteLongForm = Option(SparkEnv.get)
+      .map(_.conf.get(EVENT_LOG_CALLSITE_LONG_FORM))
+      .getOrElse(false)
+
+    val callSite = if (callsiteLongForm) {
+      rdd.creationSite.longForm
+    } else {
+      rdd.creationSite.shortForm
     }
     new RDDInfo(rdd.id, rddName, rdd.partitions.length,
       rdd.getStorageLevel, parentIds, callSite, rdd.scope)
diff --git a/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala b/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala
index 00d01dd28afb5..86f7c08eddcb5 100644
--- a/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala
+++ b/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala
@@ -17,7 +17,7 @@
 
 package org.apache.spark.storage
 
-import java.io.{File, InputStream, IOException}
+import java.io.{InputStream, IOException}
 import java.nio.ByteBuffer
 import java.util.concurrent.LinkedBlockingQueue
 import javax.annotation.concurrent.GuardedBy
@@ -28,8 +28,9 @@ import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet, Queue}
 import org.apache.spark.{SparkException, TaskContext}
 import org.apache.spark.internal.Logging
 import org.apache.spark.network.buffer.{FileSegmentManagedBuffer, ManagedBuffer}
-import org.apache.spark.network.shuffle.{BlockFetchingListener, ShuffleClient, TempFileManager}
-import org.apache.spark.shuffle.FetchFailedException
+import org.apache.spark.network.shuffle._
+import org.apache.spark.network.util.TransportConf
+import org.apache.spark.shuffle.{FetchFailedException, ShuffleReadMetricsReporter}
 import org.apache.spark.util.Utils
 import org.apache.spark.util.io.ChunkedByteBufferOutputStream
 
@@ -50,7 +51,7 @@ import org.apache.spark.util.io.ChunkedByteBufferOutputStream
  *                        For each block we also require the size (in bytes as a long field) in
  *                        order to throttle the memory usage. Note that zero-sized blocks are
  *                        already excluded, which happened in
- *                        [[MapOutputTracker.convertMapStatuses]].
+ *                        [[org.apache.spark.MapOutputTracker.convertMapStatuses]].
  * @param streamWrapper A function to wrap the returned input stream.
  * @param maxBytesInFlight max size (in bytes) of remote blocks to fetch at any given point.
  * @param maxReqsInFlight max number of remote requests to fetch blocks at any given point.
@@ -58,6 +59,7 @@ import org.apache.spark.util.io.ChunkedByteBufferOutputStream
  *                                    for a given remote host:port.
  * @param maxReqSizeShuffleToMem max size (in bytes) of a request that can be shuffled to memory.
  * @param detectCorrupt whether to detect any corruption in fetched blocks.
+ * @param shuffleMetrics used to report shuffle metrics.
  */
 private[spark]
 final class ShuffleBlockFetcherIterator(
@@ -70,8 +72,9 @@ final class ShuffleBlockFetcherIterator(
     maxReqsInFlight: Int,
     maxBlocksInFlightPerAddress: Int,
     maxReqSizeShuffleToMem: Long,
-    detectCorrupt: Boolean)
-  extends Iterator[(BlockId, InputStream)] with TempFileManager with Logging {
+    detectCorrupt: Boolean,
+    shuffleMetrics: ShuffleReadMetricsReporter)
+  extends Iterator[(BlockId, InputStream)] with DownloadFileManager with Logging {
 
   import ShuffleBlockFetcherIterator._
 
@@ -136,8 +139,6 @@ final class ShuffleBlockFetcherIterator(
    */
   private[this] val corruptedBlocks = mutable.HashSet[BlockId]()
 
-  private[this] val shuffleMetrics = context.taskMetrics().createTempShuffleReadMetrics()
-
   /**
    * Whether the iterator is still active. If isZombie is true, the callback interface will no
    * longer place fetched blocks into [[results]].
@@ -150,7 +151,7 @@ final class ShuffleBlockFetcherIterator(
    * deleted when cleanup. This is a layer of defensiveness against disk file leaks.
    */
   @GuardedBy(""this"")
-  private[this] val shuffleFilesSet = mutable.HashSet[File]()
+  private[this] val shuffleFilesSet = mutable.HashSet[DownloadFile]()
 
   initialize()
 
@@ -164,11 +165,15 @@ final class ShuffleBlockFetcherIterator(
     currentResult = null
   }
 
-  override def createTempFile(): File = {
-    blockManager.diskBlockManager.createTempLocalBlock()._2
+  override def createTempFile(transportConf: TransportConf): DownloadFile = {
+    // we never need to do any encryption or decryption here, regardless of configs, because that
+    // is handled at another layer in the code.  When encryption is enabled, shuffle data is written
+    // to disk encrypted in the first place, and sent over the network still encrypted.
+    new SimpleDownloadFile(
+      blockManager.diskBlockManager.createTempLocalBlock()._2, transportConf)
   }
 
-  override def registerTempFileToClean(file: File): Boolean = synchronized {
+  override def registerTempFileToClean(file: DownloadFile): Boolean = synchronized {
     if (isZombie) {
       false
     } else {
@@ -204,7 +209,7 @@ final class ShuffleBlockFetcherIterator(
     }
     shuffleFilesSet.foreach { file =>
       if (!file.delete()) {
-        logWarning(""Failed to cleanup shuffle fetch temp file "" + file.getAbsolutePath())
+        logWarning(""Failed to cleanup shuffle fetch temp file "" + file.path())
       }
     }
   }
@@ -443,35 +448,35 @@ final class ShuffleBlockFetcherIterator(
               buf.release()
               throwFetchFailedException(blockId, address, e)
           }
-
-          input = streamWrapper(blockId, in)
-          // Only copy the stream if it's wrapped by compression or encryption, also the size of
-          // block is small (the decompressed block is smaller than maxBytesInFlight)
-          if (detectCorrupt && !input.eq(in) && size < maxBytesInFlight / 3) {
-            val originalInput = input
-            val out = new ChunkedByteBufferOutputStream(64 * 1024, ByteBuffer.allocate)
-            try {
+          var isStreamCopied: Boolean = false
+          try {
+            input = streamWrapper(blockId, in)
+            // Only copy the stream if it's wrapped by compression or encryption, also the size of
+            // block is small (the decompressed block is smaller than maxBytesInFlight)
+            if (detectCorrupt && !input.eq(in) && size < maxBytesInFlight / 3) {
+              isStreamCopied = true
+              val out = new ChunkedByteBufferOutputStream(64 * 1024, ByteBuffer.allocate)
               // Decompress the whole block at once to detect any corruption, which could increase
               // the memory usage tne potential increase the chance of OOM.
               // TODO: manage the memory used here, and spill it into disk in case of OOM.
-              Utils.copyStream(input, out)
-              out.close()
+              Utils.copyStream(input, out, closeStreams = true)
               input = out.toChunkedByteBuffer.toInputStream(dispose = true)
-            } catch {
-              case e: IOException =>
-                buf.release()
-                if (buf.isInstanceOf[FileSegmentManagedBuffer]
-                  || corruptedBlocks.contains(blockId)) {
-                  throwFetchFailedException(blockId, address, e)
-                } else {
-                  logWarning(s""got an corrupted block $blockId from $address, fetch again"", e)
-                  corruptedBlocks += blockId
-                  fetchRequests += FetchRequest(address, Array((blockId, size)))
-                  result = null
-                }
-            } finally {
-              // TODO: release the buf here to free memory earlier
-              originalInput.close()
+            }
+          } catch {
+            case e: IOException =>
+              buf.release()
+              if (buf.isInstanceOf[FileSegmentManagedBuffer]
+                || corruptedBlocks.contains(blockId)) {
+                throwFetchFailedException(blockId, address, e)
+              } else {
+                logWarning(s""got an corrupted block $blockId from $address, fetch again"", e)
+                corruptedBlocks += blockId
+                fetchRequests += FetchRequest(address, Array((blockId, size)))
+                result = null
+              }
+          } finally {
+            // TODO: release the buf here to free memory earlier
+            if (isStreamCopied) {
               in.close()
             }
           }
diff --git a/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala b/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala
index adc406bb1c441..1c9ea1dba97d7 100644
--- a/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala
+++ b/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala
@@ -22,9 +22,12 @@ import java.nio.{ByteBuffer, MappedByteBuffer}
 import scala.collection.Map
 import scala.collection.mutable
 
+import org.apache.commons.lang3.{JavaVersion, SystemUtils}
+import sun.misc.Unsafe
 import sun.nio.ch.DirectBuffer
 
 import org.apache.spark.internal.Logging
+import org.apache.spark.util.Utils
 
 /**
  * Storage information for each BlockManager.
@@ -193,6 +196,31 @@ private[spark] class StorageStatus(
 
 /** Helper methods for storage-related objects. */
 private[spark] object StorageUtils extends Logging {
+
+  // In Java 8, the type of DirectBuffer.cleaner() was sun.misc.Cleaner, and it was possible
+  // to access the method sun.misc.Cleaner.clean() to invoke it. The type changed to
+  // jdk.internal.ref.Cleaner in later JDKs, and the .clean() method is not accessible even with
+  // reflection. However sun.misc.Unsafe added a invokeCleaner() method in JDK 9+ and this is
+  // still accessible with reflection.
+  private val bufferCleaner: DirectBuffer => Unit =
+    if (SystemUtils.isJavaVersionAtLeast(JavaVersion.JAVA_9)) {
+      val cleanerMethod =
+        Utils.classForName(""sun.misc.Unsafe"").getMethod(""invokeCleaner"", classOf[ByteBuffer])
+      val unsafeField = classOf[Unsafe].getDeclaredField(""theUnsafe"")
+      unsafeField.setAccessible(true)
+      val unsafe = unsafeField.get(null).asInstanceOf[Unsafe]
+      buffer: DirectBuffer => cleanerMethod.invoke(unsafe, buffer)
+    } else {
+      val cleanerMethod = Utils.classForName(""sun.misc.Cleaner"").getMethod(""clean"")
+      buffer: DirectBuffer => {
+        // Careful to avoid the return type of .cleaner(), which changes with JDK
+        val cleaner: AnyRef = buffer.cleaner()
+        if (cleaner != null) {
+          cleanerMethod.invoke(cleaner)
+        }
+      }
+    }
+
   /**
    * Attempt to clean up a ByteBuffer if it is direct or memory-mapped. This uses an *unsafe* Sun
    * API that will cause errors if one attempts to read from the disposed buffer. However, neither
@@ -204,14 +232,8 @@ private[spark] object StorageUtils extends Logging {
   def dispose(buffer: ByteBuffer): Unit = {
     if (buffer != null && buffer.isInstanceOf[MappedByteBuffer]) {
       logTrace(s""Disposing of $buffer"")
-      cleanDirectBuffer(buffer.asInstanceOf[DirectBuffer])
+      bufferCleaner(buffer.asInstanceOf[DirectBuffer])
     }
   }
 
-  private def cleanDirectBuffer(buffer: DirectBuffer) = {
-    val cleaner = buffer.cleaner()
-    if (cleaner != null) {
-      cleaner.clean()
-    }
-  }
 }
diff --git a/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala b/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
index 06fd56e54d9c8..8513359934bec 100644
--- a/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
+++ b/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
@@ -34,6 +34,7 @@ import org.apache.spark.memory.{MemoryManager, MemoryMode}
 import org.apache.spark.serializer.{SerializationStream, SerializerManager}
 import org.apache.spark.storage._
 import org.apache.spark.unsafe.Platform
+import org.apache.spark.unsafe.array.ByteArrayMethods
 import org.apache.spark.util.{SizeEstimator, Utils}
 import org.apache.spark.util.collection.SizeTrackingVector
 import org.apache.spark.util.io.{ChunkedByteBuffer, ChunkedByteBufferOutputStream}
@@ -333,11 +334,11 @@ private[spark] class MemoryStore(
 
     // Initial per-task memory to request for unrolling blocks (bytes).
     val initialMemoryThreshold = unrollMemoryThreshold
-    val chunkSize = if (initialMemoryThreshold > Int.MaxValue) {
+    val chunkSize = if (initialMemoryThreshold > ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH) {
       logWarning(s""Initial memory threshold of ${Utils.bytesToString(initialMemoryThreshold)} "" +
         s""is too large to be set as chunk size. Chunk size has been capped to "" +
-        s""${Utils.bytesToString(Int.MaxValue)}"")
-      Int.MaxValue
+        s""${Utils.bytesToString(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)}"")
+      ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH
     } else {
       initialMemoryThreshold.toInt
     }
diff --git a/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala b/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala
index 52a955111231a..316af9b79d286 100644
--- a/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala
+++ b/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala
@@ -356,13 +356,15 @@ private[spark] object JettyUtils extends Logging {
 
         (connector, connector.getLocalPort())
       }
+      val httpConfig = new HttpConfiguration()
+      httpConfig.setRequestHeaderSize(conf.get(UI_REQUEST_HEADER_SIZE).toInt)
 
       // If SSL is configured, create the secure connector first.
       val securePort = sslOptions.createJettySslContextFactory().map { factory =>
         val securePort = sslOptions.port.getOrElse(if (port > 0) Utils.userPort(port, 400) else 0)
         val secureServerName = if (serverName.nonEmpty) s""$serverName (HTTPS)"" else serverName
         val connectionFactories = AbstractConnectionFactory.getFactories(factory,
-          new HttpConnectionFactory())
+          new HttpConnectionFactory(httpConfig))
 
         def sslConnect(currentPort: Int): (ServerConnector, Int) = {
           newConnector(connectionFactories, currentPort)
@@ -377,7 +379,7 @@ private[spark] object JettyUtils extends Logging {
 
       // Bind the HTTP port.
       def httpConnect(currentPort: Int): (ServerConnector, Int) = {
-        newConnector(Array(new HttpConnectionFactory()), currentPort)
+        newConnector(Array(new HttpConnectionFactory(httpConfig)), currentPort)
       }
 
       val (httpConnector, httpPort) = Utils.startServiceOnPort[ServerConnector](port, httpConnect,
diff --git a/core/src/main/scala/org/apache/spark/ui/PagedTable.scala b/core/src/main/scala/org/apache/spark/ui/PagedTable.scala
index 65fa38387b9ee..6c2c1f6827948 100644
--- a/core/src/main/scala/org/apache/spark/ui/PagedTable.scala
+++ b/core/src/main/scala/org/apache/spark/ui/PagedTable.scala
@@ -31,11 +31,7 @@ import org.apache.spark.util.Utils
  *
  * @param pageSize the number of rows in a page
  */
-private[ui] abstract class PagedDataSource[T](val pageSize: Int) {
-
-  if (pageSize <= 0) {
-    throw new IllegalArgumentException(""Page size must be positive"")
-  }
+private[spark] abstract class PagedDataSource[T](val pageSize: Int) {
 
   /**
    * Return the size of all data.
@@ -51,13 +47,24 @@ private[ui] abstract class PagedDataSource[T](val pageSize: Int) {
    * Slice the data for this page
    */
   def pageData(page: Int): PageData[T] = {
-    val totalPages = (dataSize + pageSize - 1) / pageSize
-    if (page <= 0 || page > totalPages) {
-      throw new IndexOutOfBoundsException(
-        s""Page $page is out of range. Please select a page number between 1 and $totalPages."")
+    // Display all the data in one page, if the pageSize is less than or equal to zero.
+    val pageTableSize = if (pageSize <= 0) {
+      dataSize
+    } else {
+      pageSize
+    }
+    val totalPages = (dataSize + pageTableSize - 1) / pageTableSize
+
+    val pageToShow = if (page <= 0) {
+      1
+    } else if (page > totalPages) {
+      totalPages
+    } else {
+      page
     }
-    val from = (page - 1) * pageSize
-    val to = dataSize.min(page * pageSize)
+
+    val (from, to) = ((pageToShow - 1) * pageSize, dataSize.min(pageToShow * pageTableSize))
+
     PageData(totalPages, sliceData(from, to))
   }
 
@@ -72,7 +79,7 @@ private[ui] case class PageData[T](totalPage: Int, data: Seq[T])
 /**
  * A paged table that will generate a HTML table for a specified page and also the page navigation.
  */
-private[ui] trait PagedTable[T] {
+private[spark] trait PagedTable[T] {
 
   def tableId: String
 
@@ -80,8 +87,6 @@ private[ui] trait PagedTable[T] {
 
   def pageSizeFormField: String
 
-  def prevPageSizeFormField: String
-
   def pageNumberFormField: String
 
   def dataSource: PagedDataSource[T]
@@ -94,7 +99,23 @@ private[ui] trait PagedTable[T] {
     val _dataSource = dataSource
     try {
       val PageData(totalPages, data) = _dataSource.pageData(page)
-      val pageNavi = pageNavigation(page, _dataSource.pageSize, totalPages)
+
+      val pageToShow = if (page <= 0) {
+        1
+      } else if (page > totalPages) {
+        totalPages
+      } else {
+        page
+      }
+      // Display all the data in one page, if the pageSize is less than or equal to zero.
+      val pageSize = if (_dataSource.pageSize <= 0) {
+        data.size
+      } else {
+        _dataSource.pageSize
+      }
+
+      val pageNavi = pageNavigation(pageToShow, pageSize, totalPages)
+
       <div>
         {pageNavi}
         <table class={tableCssClass} id={tableId}>
@@ -122,13 +143,9 @@ private[ui] trait PagedTable[T] {
 
   /**
    * Return a page navigation.
-   * <ul>
-   *   <li>If the totalPages is 1, the page navigation will be empty</li>
-   *   <li>
-   *     If the totalPages is more than 1, it will create a page navigation including a group of
-   *     page numbers and a form to submit the page number.
-   *   </li>
-   * </ul>
+   *
+   * It will create a page navigation including a group of page numbers and a form
+   * to submit the page number.
    *
    * Here are some examples of the page navigation:
    * {{{
@@ -154,120 +171,112 @@ private[ui] trait PagedTable[T] {
    * }}}
    */
   private[ui] def pageNavigation(page: Int, pageSize: Int, totalPages: Int): Seq[Node] = {
-    if (totalPages == 1) {
-      Nil
-    } else {
-      // A group includes all page numbers will be shown in the page navigation.
-      // The size of group is 10 means there are 10 page numbers will be shown.
-      // The first group is 1 to 10, the second is 2 to 20, and so on
-      val groupSize = 10
-      val firstGroup = 0
-      val lastGroup = (totalPages - 1) / groupSize
-      val currentGroup = (page - 1) / groupSize
-      val startPage = currentGroup * groupSize + 1
-      val endPage = totalPages.min(startPage + groupSize - 1)
-      val pageTags = (startPage to endPage).map { p =>
-        if (p == page) {
-          // The current page should be disabled so that it cannot be clicked.
-          <li class=""disabled""><a href=""#"">{p}</a></li>
-        } else {
-          <li><a href={Unparsed(pageLink(p))}>{p}</a></li>
-        }
+    // A group includes all page numbers will be shown in the page navigation.
+    // The size of group is 10 means there are 10 page numbers will be shown.
+    // The first group is 1 to 10, the second is 2 to 20, and so on
+    val groupSize = 10
+    val firstGroup = 0
+    val lastGroup = (totalPages - 1) / groupSize
+    val currentGroup = (page - 1) / groupSize
+    val startPage = currentGroup * groupSize + 1
+    val endPage = totalPages.min(startPage + groupSize - 1)
+    val pageTags = (startPage to endPage).map { p =>
+      if (p == page) {
+        // The current page should be disabled so that it cannot be clicked.
+        <li class=""disabled""><a href=""#"">{p}</a></li>
+      } else {
+        <li><a href={Unparsed(pageLink(p))}>{p}</a></li>
       }
+    }
 
-      val hiddenFormFields = {
-        if (goButtonFormPath.contains('?')) {
-          val queryString = goButtonFormPath.split(""\\?"", 2)(1)
-          val search = queryString.split(""#"")(0)
-          Splitter
-            .on('&')
-            .trimResults()
-            .omitEmptyStrings()
-            .withKeyValueSeparator(""="")
-            .split(search)
-            .asScala
-            .filterKeys(_ != pageSizeFormField)
-            .filterKeys(_ != prevPageSizeFormField)
-            .filterKeys(_ != pageNumberFormField)
-            .mapValues(URLDecoder.decode(_, ""UTF-8""))
-            .map { case (k, v) =>
-              <input type=""hidden"" name={k} value={v} />
-            }
-        } else {
-          Seq.empty
-        }
+    val hiddenFormFields = {
+      if (goButtonFormPath.contains('?')) {
+        val queryString = goButtonFormPath.split(""\\?"", 2)(1)
+        val search = queryString.split(""#"")(0)
+        Splitter
+          .on('&')
+          .trimResults()
+          .omitEmptyStrings()
+          .withKeyValueSeparator(""="")
+          .split(search)
+          .asScala
+          .filterKeys(_ != pageSizeFormField)
+          .filterKeys(_ != pageNumberFormField)
+          .mapValues(URLDecoder.decode(_, ""UTF-8""))
+          .map { case (k, v) =>
+            <input type=""hidden"" name={k} value={v} />
+          }
+      } else {
+        Seq.empty
       }
+    }
 
+    <div>
       <div>
-        <div>
-          <form id={s""form-$tableId-page""}
-                method=""get""
-                action={Unparsed(goButtonFormPath)}
-                class=""form-inline pull-right""
-                style=""margin-bottom: 0px;"">
-            <input type=""hidden""
-                   name={prevPageSizeFormField}
-                   value={pageSize.toString} />
-            {hiddenFormFields}
-            <label>{totalPages} Pages. Jump to</label>
-            <input type=""text""
-                   name={pageNumberFormField}
-                   id={s""form-$tableId-page-no""}
-                   value={page.toString} class=""span1"" />
-
-            <label>. Show </label>
-            <input type=""text""
-                   id={s""form-$tableId-page-size""}
-                   name={pageSizeFormField}
-                   value={pageSize.toString}
-                   class=""span1"" />
-            <label>items in a page.</label>
-
-            <button type=""submit"" class=""btn"">Go</button>
-          </form>
-        </div>
-        <div class=""pagination"" style=""margin-bottom: 0px;"">
-          <span style=""float: left; padding-top: 4px; padding-right: 4px;"">Page: </span>
-          <ul>
-            {if (currentGroup > firstGroup) {
-            <li>
-              <a href={Unparsed(pageLink(startPage - groupSize))} aria-label=""Previous Group"">
-                <span aria-hidden=""true"">
-                  &lt;&lt;
-                </span>
-              </a>
-            </li>
-            }}
-            {if (page > 1) {
-            <li>
-            <a href={Unparsed(pageLink(page - 1))} aria-label=""Previous"">
+        <form id={s""form-$tableId-page""}
+              method=""get""
+              action={Unparsed(goButtonFormPath)}
+              class=""form-inline pull-right""
+              style=""margin-bottom: 0px;"">
+          {hiddenFormFields}
+          <label>{totalPages} Pages. Jump to</label>
+          <input type=""text""
+                 name={pageNumberFormField}
+                 id={s""form-$tableId-page-no""}
+                 value={page.toString} class=""span1"" />
+
+          <label>. Show </label>
+          <input type=""text""
+                 id={s""form-$tableId-page-size""}
+                 name={pageSizeFormField}
+                 value={pageSize.toString}
+                 class=""span1"" />
+          <label>items in a page.</label>
+
+          <button type=""submit"" class=""btn"">Go</button>
+        </form>
+      </div>
+      <div class=""pagination"" style=""margin-bottom: 0px;"">
+        <span style=""float: left; padding-top: 4px; padding-right: 4px;"">Page: </span>
+        <ul>
+          {if (currentGroup > firstGroup) {
+          <li>
+            <a href={Unparsed(pageLink(startPage - groupSize))} aria-label=""Previous Group"">
               <span aria-hidden=""true"">
-                &lt;
+                &lt;&lt;
               </span>
             </a>
-            </li>
-            }}
-            {pageTags}
-            {if (page < totalPages) {
-            <li>
-              <a href={Unparsed(pageLink(page + 1))} aria-label=""Next"">
-                <span aria-hidden=""true"">&gt;</span>
-              </a>
-            </li>
-            }}
-            {if (currentGroup < lastGroup) {
-            <li>
-              <a href={Unparsed(pageLink(startPage + groupSize))} aria-label=""Next Group"">
-                <span aria-hidden=""true"">
-                  &gt;&gt;
-                </span>
-              </a>
-            </li>
+          </li>
           }}
-          </ul>
-        </div>
+          {if (page > 1) {
+          <li>
+          <a href={Unparsed(pageLink(page - 1))} aria-label=""Previous"">
+            <span aria-hidden=""true"">
+              &lt;
+            </span>
+          </a>
+          </li>
+          }}
+          {pageTags}
+          {if (page < totalPages) {
+          <li>
+            <a href={Unparsed(pageLink(page + 1))} aria-label=""Next"">
+              <span aria-hidden=""true"">&gt;</span>
+            </a>
+          </li>
+          }}
+          {if (currentGroup < lastGroup) {
+          <li>
+            <a href={Unparsed(pageLink(startPage + groupSize))} aria-label=""Next Group"">
+              <span aria-hidden=""true"">
+                &gt;&gt;
+              </span>
+            </a>
+          </li>
+        }}
+        </ul>
       </div>
-    }
+    </div>
   }
 
   /**
diff --git a/core/src/main/scala/org/apache/spark/ui/UIUtils.scala b/core/src/main/scala/org/apache/spark/ui/UIUtils.scala
index 732b7528f499e..60a929375baae 100644
--- a/core/src/main/scala/org/apache/spark/ui/UIUtils.scala
+++ b/core/src/main/scala/org/apache/spark/ui/UIUtils.scala
@@ -204,6 +204,8 @@ private[spark] object UIUtils extends Logging {
           href={prependBaseUri(request, ""/static/dataTables.bootstrap.css"")} type=""text/css""/>
     <link rel=""stylesheet""
           href={prependBaseUri(request, ""/static/jsonFormatter.min.css"")} type=""text/css""/>
+    <link rel=""stylesheet""
+          href={prependBaseUri(request, ""/static/webui-dataTables.css"")} type=""text/css""/>
     <script src={prependBaseUri(request, ""/static/jquery.dataTables.1.10.4.min.js"")}></script>
     <script src={prependBaseUri(request, ""/static/jquery.cookies.2.2.0.min.js"")}></script>
     <script src={prependBaseUri(request, ""/static/jquery.blockUI.min.js"")}></script>
@@ -218,7 +220,6 @@ private[spark] object UIUtils extends Logging {
       title: String,
       content: => Seq[Node],
       activeTab: SparkUITab,
-      refreshInterval: Option[Int] = None,
       helpText: Option[String] = None,
       showVisualization: Boolean = false,
       useDataTables: Boolean = false): Seq[Node] = {
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala b/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala
index 90e9a7a3630cf..2c22e0555fcb8 100644
--- a/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala
@@ -220,7 +220,6 @@ private[ui] class AllJobsPage(parent: JobsTab, store: AppStatusStore) extends We
     val parameterJobSortColumn = UIUtils.stripXSS(request.getParameter(jobTag + "".sort""))
     val parameterJobSortDesc = UIUtils.stripXSS(request.getParameter(jobTag + "".desc""))
     val parameterJobPageSize = UIUtils.stripXSS(request.getParameter(jobTag + "".pageSize""))
-    val parameterJobPrevPageSize = UIUtils.stripXSS(request.getParameter(jobTag + "".prevPageSize""))
 
     val jobPage = Option(parameterJobPage).map(_.toInt).getOrElse(1)
     val jobSortColumn = Option(parameterJobSortColumn).map { sortColumn =>
@@ -231,17 +230,7 @@ private[ui] class AllJobsPage(parent: JobsTab, store: AppStatusStore) extends We
       jobSortColumn == jobIdTitle
     )
     val jobPageSize = Option(parameterJobPageSize).map(_.toInt).getOrElse(100)
-    val jobPrevPageSize = Option(parameterJobPrevPageSize).map(_.toInt).getOrElse(jobPageSize)
-
-    val page: Int = {
-      // If the user has changed to a larger page size, then go to page 1 in order to avoid
-      // IndexOutOfBoundsException.
-      if (jobPageSize <= jobPrevPageSize) {
-        jobPage
-      } else {
-        1
-      }
-    }
+
     val currentTime = System.currentTimeMillis()
 
     try {
@@ -259,7 +248,7 @@ private[ui] class AllJobsPage(parent: JobsTab, store: AppStatusStore) extends We
         pageSize = jobPageSize,
         sortColumn = jobSortColumn,
         desc = jobSortDesc
-      ).table(page)
+      ).table(jobPage)
     } catch {
       case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =>
         <div class=""alert alert-error"">
@@ -526,8 +515,6 @@ private[ui] class JobPagedTable(
 
   override def pageSizeFormField: String = jobTag + "".pageSize""
 
-  override def prevPageSizeFormField: String = jobTag + "".prevPageSize""
-
   override def pageNumberFormField: String = jobTag + "".page""
 
   override val dataSource = new JobDataSource(
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala b/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala
deleted file mode 100644
index 0ff64f053f371..0000000000000
--- a/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.ui.jobs
-
-import scala.xml.{Node, Unparsed}
-
-import org.apache.spark.status.AppStatusStore
-import org.apache.spark.status.api.v1.StageData
-import org.apache.spark.ui.{ToolTips, UIUtils}
-import org.apache.spark.util.Utils
-
-/** Stage summary grouped by executors. */
-private[ui] class ExecutorTable(stage: StageData, store: AppStatusStore) {
-
-  import ApiHelper._
-
-  def toNodeSeq: Seq[Node] = {
-    <table class={UIUtils.TABLE_CLASS_STRIPED_SORTABLE}>
-      <thead>
-        <th id=""executorid"">Executor ID</th>
-        <th>Address</th>
-        <th>Task Time</th>
-        <th>Total Tasks</th>
-        <th>Failed Tasks</th>
-        <th>Killed Tasks</th>
-        <th>Succeeded Tasks</th>
-        {if (hasInput(stage)) {
-          <th>
-            <span data-toggle=""tooltip"" title={ToolTips.INPUT}>Input Size / Records</span>
-          </th>
-        }}
-        {if (hasOutput(stage)) {
-          <th>
-            <span data-toggle=""tooltip"" title={ToolTips.OUTPUT}>Output Size / Records</span>
-          </th>
-        }}
-        {if (hasShuffleRead(stage)) {
-          <th>
-            <span data-toggle=""tooltip"" title={ToolTips.SHUFFLE_READ}>
-            Shuffle Read Size / Records</span>
-          </th>
-        }}
-        {if (hasShuffleWrite(stage)) {
-          <th>
-            <span data-toggle=""tooltip"" title={ToolTips.SHUFFLE_WRITE}>
-            Shuffle Write Size / Records</span>
-          </th>
-        }}
-        {if (hasBytesSpilled(stage)) {
-          <th>Shuffle Spill (Memory)</th>
-          <th>Shuffle Spill (Disk)</th>
-        }}
-        <th>
-          <span data-toggle=""tooltip"" title={ToolTips.BLACKLISTED}>
-          Blacklisted
-          </span>
-        </th>
-      </thead>
-      <tbody>
-        {createExecutorTable(stage)}
-      </tbody>
-    </table>
-    <script>
-      {Unparsed {
-        """"""
-          |      window.onload = function() {
-          |        sorttable.innerSortFunction.apply(document.getElementById('executorid'), [])
-          |      };
-        """""".stripMargin
-      }}
-    </script>
-  }
-
-  private def createExecutorTable(stage: StageData) : Seq[Node] = {
-    val executorSummary = store.executorSummary(stage.stageId, stage.attemptId)
-
-    executorSummary.toSeq.sortBy(_._1).map { case (k, v) =>
-      val executor = store.asOption(store.executorSummary(k))
-      <tr>
-        <td>
-          <div style=""float: left"">{k}</div>
-          <div style=""float: right"">
-          {
-            executor.map(_.executorLogs).getOrElse(Map.empty).map {
-              case (logName, logUrl) => <div><a href={logUrl}>{logName}</a></div>
-            }
-          }
-          </div>
-        </td>
-        <td>{executor.map { e => e.hostPort }.getOrElse(""CANNOT FIND ADDRESS"")}</td>
-        <td sorttable_customkey={v.taskTime.toString}>{UIUtils.formatDuration(v.taskTime)}</td>
-        <td>{v.failedTasks + v.succeededTasks + v.killedTasks}</td>
-        <td>{v.failedTasks}</td>
-        <td>{v.killedTasks}</td>
-        <td>{v.succeededTasks}</td>
-        {if (hasInput(stage)) {
-          <td sorttable_customkey={v.inputBytes.toString}>
-            {s""${Utils.bytesToString(v.inputBytes)} / ${v.inputRecords}""}
-          </td>
-        }}
-        {if (hasOutput(stage)) {
-          <td sorttable_customkey={v.outputBytes.toString}>
-            {s""${Utils.bytesToString(v.outputBytes)} / ${v.outputRecords}""}
-          </td>
-        }}
-        {if (hasShuffleRead(stage)) {
-          <td sorttable_customkey={v.shuffleRead.toString}>
-            {s""${Utils.bytesToString(v.shuffleRead)} / ${v.shuffleReadRecords}""}
-          </td>
-        }}
-        {if (hasShuffleWrite(stage)) {
-          <td sorttable_customkey={v.shuffleWrite.toString}>
-            {s""${Utils.bytesToString(v.shuffleWrite)} / ${v.shuffleWriteRecords}""}
-          </td>
-        }}
-        {if (hasBytesSpilled(stage)) {
-          <td sorttable_customkey={v.memoryBytesSpilled.toString}>
-            {Utils.bytesToString(v.memoryBytesSpilled)}
-          </td>
-          <td sorttable_customkey={v.diskBytesSpilled.toString}>
-            {Utils.bytesToString(v.diskBytesSpilled)}
-          </td>
-        }}
-        {
-          if (executor.map(_.isBlacklisted).getOrElse(false)) {
-            <td>for application</td>
-          } else if (v.isBlacklistedForStage) {
-            <td>for stage</td>
-          } else {
-            <td>false</td>
-          }
-        }
-      </tr>
-    }
-  }
-
-}
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
index 55eb989962668..a213b764abea7 100644
--- a/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
@@ -91,7 +91,14 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
     val parameterTaskSortColumn = UIUtils.stripXSS(request.getParameter(""task.sort""))
     val parameterTaskSortDesc = UIUtils.stripXSS(request.getParameter(""task.desc""))
     val parameterTaskPageSize = UIUtils.stripXSS(request.getParameter(""task.pageSize""))
-    val parameterTaskPrevPageSize = UIUtils.stripXSS(request.getParameter(""task.prevPageSize""))
+
+    val eventTimelineParameterTaskPage = UIUtils.stripXSS(
+      request.getParameter(""task.eventTimelinePageNumber""))
+    val eventTimelineParameterTaskPageSize = UIUtils.stripXSS(
+      request.getParameter(""task.eventTimelinePageSize""))
+    var eventTimelineTaskPage = Option(eventTimelineParameterTaskPage).map(_.toInt).getOrElse(1)
+    var eventTimelineTaskPageSize = Option(
+      eventTimelineParameterTaskPageSize).map(_.toInt).getOrElse(100)
 
     val taskPage = Option(parameterTaskPage).map(_.toInt).getOrElse(1)
     val taskSortColumn = Option(parameterTaskSortColumn).map { sortColumn =>
@@ -99,13 +106,11 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
     }.getOrElse(""Index"")
     val taskSortDesc = Option(parameterTaskSortDesc).map(_.toBoolean).getOrElse(false)
     val taskPageSize = Option(parameterTaskPageSize).map(_.toInt).getOrElse(100)
-    val taskPrevPageSize = Option(parameterTaskPrevPageSize).map(_.toInt).getOrElse(taskPageSize)
-
     val stageId = parameterId.toInt
     val stageAttemptId = parameterAttempt.toInt
 
     val stageHeader = s""Details for Stage $stageId (Attempt $stageAttemptId)""
-    val stageData = parent.store
+    val (stageData, stageJobIds) = parent.store
       .asOption(parent.store.stageAttempt(stageId, stageAttemptId, details = false))
       .getOrElse {
         val content =
@@ -117,7 +122,8 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
 
     val localitySummary = store.localitySummary(stageData.stageId, stageData.attemptId)
 
-    val totalTasks = taskCount(stageData)
+    val totalTasks = stageData.numActiveTasks + stageData.numCompleteTasks +
+      stageData.numFailedTasks + stageData.numKilledTasks
     if (totalTasks == 0) {
       val content =
         <div>
@@ -132,7 +138,15 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
     val totalTasksNumStr = if (totalTasks == storedTasks) {
       s""$totalTasks""
     } else {
-      s""$storedTasks, showing ${totalTasks}""
+      s""$totalTasks, showing $storedTasks""
+    }
+    if (eventTimelineTaskPageSize < 1 || eventTimelineTaskPageSize > totalTasks) {
+      eventTimelineTaskPageSize = totalTasks
+    }
+    val eventTimelineTotalPages =
+      (totalTasks + eventTimelineTaskPageSize - 1) / eventTimelineTaskPageSize
+    if (eventTimelineTaskPage < 1 || eventTimelineTaskPage > eventTimelineTotalPages) {
+      eventTimelineTaskPage = 1
     }
 
     val summary =
@@ -154,20 +168,20 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
           }}
           {if (hasOutput(stageData)) {
             <li>
-              <strong>Output: </strong>
+              <strong>Output Size / Records: </strong>
               {s""${Utils.bytesToString(stageData.outputBytes)} / ${stageData.outputRecords}""}
             </li>
           }}
           {if (hasShuffleRead(stageData)) {
             <li>
-              <strong>Shuffle Read: </strong>
+              <strong>Shuffle Read Size / Records: </strong>
               {s""${Utils.bytesToString(stageData.shuffleReadBytes)} / "" +
                s""${stageData.shuffleReadRecords}""}
             </li>
           }}
           {if (hasShuffleWrite(stageData)) {
             <li>
-              <strong>Shuffle Write: </strong>
+              <strong>Shuffle Write Size / Records: </strong>
                {s""${Utils.bytesToString(stageData.shuffleWriteBytes)} / "" +
                s""${stageData.shuffleWriteRecords}""}
             </li>
@@ -182,74 +196,17 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
               {Utils.bytesToString(stageData.diskBytesSpilled)}
             </li>
           }}
-        </ul>
-      </div>
-
-    val showAdditionalMetrics =
-      <div>
-        <span class=""expand-additional-metrics"">
-          <span class=""expand-additional-metrics-arrow arrow-closed""></span>
-          <a>Show Additional Metrics</a>
-        </span>
-        <div class=""additional-metrics collapsed"">
-          <ul>
-            <li>
-                <input type=""checkbox"" id=""select-all-metrics""/>
-                <span class=""additional-metric-title""><em>(De)select All</em></span>
-            </li>
+          {if (!stageJobIds.isEmpty) {
             <li>
-              <span data-toggle=""tooltip""
-                    title={ToolTips.SCHEDULER_DELAY} data-placement=""right"">
-                <input type=""checkbox"" name={TaskDetailsClassNames.SCHEDULER_DELAY}/>
-                <span class=""additional-metric-title"">Scheduler Delay</span>
-              </span>
+              <strong>Associated Job Ids: </strong>
+              {stageJobIds.sorted.map { jobId =>
+                val jobURL = ""%s/jobs/job/?id=%s""
+                  .format(UIUtils.prependBaseUri(request, parent.basePath), jobId)
+                <a href={jobURL}>{jobId.toString}</a><span>&nbsp;</span>
+              }}
             </li>
-            <li>
-              <span data-toggle=""tooltip""
-                    title={ToolTips.TASK_DESERIALIZATION_TIME} data-placement=""right"">
-                <input type=""checkbox"" name={TaskDetailsClassNames.TASK_DESERIALIZATION_TIME}/>
-                <span class=""additional-metric-title"">Task Deserialization Time</span>
-              </span>
-            </li>
-            {if (stageData.shuffleReadBytes > 0) {
-              <li>
-                <span data-toggle=""tooltip""
-                      title={ToolTips.SHUFFLE_READ_BLOCKED_TIME} data-placement=""right"">
-                  <input type=""checkbox"" name={TaskDetailsClassNames.SHUFFLE_READ_BLOCKED_TIME}/>
-                  <span class=""additional-metric-title"">Shuffle Read Blocked Time</span>
-                </span>
-              </li>
-              <li>
-                <span data-toggle=""tooltip""
-                      title={ToolTips.SHUFFLE_READ_REMOTE_SIZE} data-placement=""right"">
-                  <input type=""checkbox"" name={TaskDetailsClassNames.SHUFFLE_READ_REMOTE_SIZE}/>
-                  <span class=""additional-metric-title"">Shuffle Remote Reads</span>
-                </span>
-              </li>
-            }}
-            <li>
-              <span data-toggle=""tooltip""
-                    title={ToolTips.RESULT_SERIALIZATION_TIME} data-placement=""right"">
-                <input type=""checkbox"" name={TaskDetailsClassNames.RESULT_SERIALIZATION_TIME}/>
-                <span class=""additional-metric-title"">Result Serialization Time</span>
-              </span>
-            </li>
-            <li>
-              <span data-toggle=""tooltip""
-                    title={ToolTips.GETTING_RESULT_TIME} data-placement=""right"">
-                <input type=""checkbox"" name={TaskDetailsClassNames.GETTING_RESULT_TIME}/>
-                <span class=""additional-metric-title"">Getting Result Time</span>
-              </span>
-            </li>
-            <li>
-              <span data-toggle=""tooltip""
-                    title={ToolTips.PEAK_EXECUTION_MEMORY} data-placement=""right"">
-                <input type=""checkbox"" name={TaskDetailsClassNames.PEAK_EXECUTION_MEMORY}/>
-                <span class=""additional-metric-title"">Peak Execution Memory</span>
-              </span>
-            </li>
-          </ul>
-        </div>
+          }}
+        </ul>
       </div>
 
     val stageGraph = parent.store.asOption(parent.store.operationGraphForStage(stageId))
@@ -268,17 +225,8 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
       accumulableRow,
       stageData.accumulatorUpdates.toSeq)
 
-    val page: Int = {
-      // If the user has changed to a larger page size, then go to page 1 in order to avoid
-      // IndexOutOfBoundsException.
-      if (taskPageSize <= taskPrevPageSize) {
-        taskPage
-      } else {
-        1
-      }
-    }
     val currentTime = System.currentTimeMillis()
-    val (taskTable, taskTableHTML) = try {
+    val taskTable = try {
       val _taskTable = new TaskPagedTable(
         stageData,
         UIUtils.prependBaseUri(request, parent.basePath) +
@@ -289,17 +237,10 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
         desc = taskSortDesc,
         store = parent.store
       )
-      (_taskTable, _taskTable.table(page))
+      _taskTable
     } catch {
       case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =>
-        val errorMessage =
-          <div class=""alert alert-error"">
-            <p>Error while rendering stage table:</p>
-            <pre>
-              {Utils.exceptionString(e)}
-            </pre>
-          </div>
-        (null, errorMessage)
+        null
     }
 
     val jsForScrollingDownToTaskTable =
@@ -317,190 +258,36 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
         }
       </script>
 
-    val metricsSummary = store.taskSummary(stageData.stageId, stageData.attemptId,
-      Array(0, 0.25, 0.5, 0.75, 1.0))
-
-    val summaryTable = metricsSummary.map { metrics =>
-      def timeQuantiles(data: IndexedSeq[Double]): Seq[Node] = {
-        data.map { millis =>
-          <td>{UIUtils.formatDuration(millis.toLong)}</td>
-        }
-      }
-
-      def sizeQuantiles(data: IndexedSeq[Double]): Seq[Node] = {
-        data.map { size =>
-          <td>{Utils.bytesToString(size.toLong)}</td>
-        }
-      }
-
-      def sizeQuantilesWithRecords(
-          data: IndexedSeq[Double],
-          records: IndexedSeq[Double]) : Seq[Node] = {
-        data.zip(records).map { case (d, r) =>
-          <td>{s""${Utils.bytesToString(d.toLong)} / ${r.toLong}""}</td>
-        }
-      }
-
-      def titleCell(title: String, tooltip: String): Seq[Node] = {
-        <td>
-          <span data-toggle=""tooltip"" title={tooltip} data-placement=""right"">
-            {title}
-          </span>
-        </td>
-      }
-
-      def simpleTitleCell(title: String): Seq[Node] = <td>{title}</td>
-
-      val deserializationQuantiles = titleCell(""Task Deserialization Time"",
-        ToolTips.TASK_DESERIALIZATION_TIME) ++ timeQuantiles(metrics.executorDeserializeTime)
-
-      val serviceQuantiles = simpleTitleCell(""Duration"") ++ timeQuantiles(metrics.executorRunTime)
-
-      val gcQuantiles = titleCell(""GC Time"", ToolTips.GC_TIME) ++ timeQuantiles(metrics.jvmGcTime)
-
-      val serializationQuantiles = titleCell(""Result Serialization Time"",
-        ToolTips.RESULT_SERIALIZATION_TIME) ++ timeQuantiles(metrics.resultSerializationTime)
-
-      val gettingResultQuantiles = titleCell(""Getting Result Time"", ToolTips.GETTING_RESULT_TIME) ++
-        timeQuantiles(metrics.gettingResultTime)
-
-      val peakExecutionMemoryQuantiles = titleCell(""Peak Execution Memory"",
-        ToolTips.PEAK_EXECUTION_MEMORY) ++ sizeQuantiles(metrics.peakExecutionMemory)
-
-      // The scheduler delay includes the network delay to send the task to the worker
-      // machine and to send back the result (but not the time to fetch the task result,
-      // if it needed to be fetched from the block manager on the worker).
-      val schedulerDelayQuantiles = titleCell(""Scheduler Delay"", ToolTips.SCHEDULER_DELAY) ++
-        timeQuantiles(metrics.schedulerDelay)
-
-      def inputQuantiles: Seq[Node] = {
-        simpleTitleCell(""Input Size / Records"") ++
-          sizeQuantilesWithRecords(metrics.inputMetrics.bytesRead, metrics.inputMetrics.recordsRead)
-      }
-
-      def outputQuantiles: Seq[Node] = {
-        simpleTitleCell(""Output Size / Records"") ++
-          sizeQuantilesWithRecords(metrics.outputMetrics.bytesWritten,
-            metrics.outputMetrics.recordsWritten)
-      }
-
-      def shuffleReadBlockedQuantiles: Seq[Node] = {
-        titleCell(""Shuffle Read Blocked Time"", ToolTips.SHUFFLE_READ_BLOCKED_TIME) ++
-          timeQuantiles(metrics.shuffleReadMetrics.fetchWaitTime)
-      }
-
-      def shuffleReadTotalQuantiles: Seq[Node] = {
-        titleCell(""Shuffle Read Size / Records"", ToolTips.SHUFFLE_READ) ++
-          sizeQuantilesWithRecords(metrics.shuffleReadMetrics.readBytes,
-            metrics.shuffleReadMetrics.readRecords)
-      }
-
-      def shuffleReadRemoteQuantiles: Seq[Node] = {
-        titleCell(""Shuffle Remote Reads"", ToolTips.SHUFFLE_READ_REMOTE_SIZE) ++
-          sizeQuantiles(metrics.shuffleReadMetrics.remoteBytesRead)
-      }
-
-      def shuffleWriteQuantiles: Seq[Node] = {
-        simpleTitleCell(""Shuffle Write Size / Records"") ++
-          sizeQuantilesWithRecords(metrics.shuffleWriteMetrics.writeBytes,
-            metrics.shuffleWriteMetrics.writeRecords)
-      }
-
-      def memoryBytesSpilledQuantiles: Seq[Node] = {
-        simpleTitleCell(""Shuffle spill (memory)"") ++ sizeQuantiles(metrics.memoryBytesSpilled)
-      }
-
-      def diskBytesSpilledQuantiles: Seq[Node] = {
-        simpleTitleCell(""Shuffle spill (disk)"") ++ sizeQuantiles(metrics.diskBytesSpilled)
-      }
-
-      val listings: Seq[Seq[Node]] = Seq(
-        <tr>{serviceQuantiles}</tr>,
-        <tr class={TaskDetailsClassNames.SCHEDULER_DELAY}>{schedulerDelayQuantiles}</tr>,
-        <tr class={TaskDetailsClassNames.TASK_DESERIALIZATION_TIME}>
-          {deserializationQuantiles}
-        </tr>
-        <tr>{gcQuantiles}</tr>,
-        <tr class={TaskDetailsClassNames.RESULT_SERIALIZATION_TIME}>
-          {serializationQuantiles}
-        </tr>,
-        <tr class={TaskDetailsClassNames.GETTING_RESULT_TIME}>{gettingResultQuantiles}</tr>,
-        <tr class={TaskDetailsClassNames.PEAK_EXECUTION_MEMORY}>
-          {peakExecutionMemoryQuantiles}
-        </tr>,
-        if (hasInput(stageData)) <tr>{inputQuantiles}</tr> else Nil,
-        if (hasOutput(stageData)) <tr>{outputQuantiles}</tr> else Nil,
-        if (hasShuffleRead(stageData)) {
-          <tr class={TaskDetailsClassNames.SHUFFLE_READ_BLOCKED_TIME}>
-            {shuffleReadBlockedQuantiles}
-          </tr>
-          <tr>{shuffleReadTotalQuantiles}</tr>
-          <tr class={TaskDetailsClassNames.SHUFFLE_READ_REMOTE_SIZE}>
-            {shuffleReadRemoteQuantiles}
-          </tr>
-        } else {
-          Nil
-        },
-        if (hasShuffleWrite(stageData)) <tr>{shuffleWriteQuantiles}</tr> else Nil,
-        if (hasBytesSpilled(stageData)) <tr>{memoryBytesSpilledQuantiles}</tr> else Nil,
-        if (hasBytesSpilled(stageData)) <tr>{diskBytesSpilledQuantiles}</tr> else Nil)
-
-      val quantileHeaders = Seq(""Metric"", ""Min"", ""25th percentile"", ""Median"", ""75th percentile"",
-        ""Max"")
-      // The summary table does not use CSS to stripe rows, which doesn't work with hidden
-      // rows (instead, JavaScript in table.js is used to stripe the non-hidden rows).
-      UIUtils.listingTable(
-        quantileHeaders,
-        identity[Seq[Node]],
-        listings,
-        fixedWidth = true,
-        id = Some(""task-summary-table""),
-        stripeRowsWithCss = false)
-    }
-
-    val executorTable = new ExecutorTable(stageData, parent.store)
-
-    val maybeAccumulableTable: Seq[Node] =
-      if (hasAccumulators(stageData)) { <h4>Accumulators</h4> ++ accumulableTable } else Seq()
-
-    val aggMetrics =
-      <span class=""collapse-aggregated-metrics collapse-table""
-            onClick=""collapseTable('collapse-aggregated-metrics','aggregated-metrics')"">
-        <h4>
-          <span class=""collapse-table-arrow arrow-open""></span>
-          <a>Aggregated Metrics by Executor</a>
-        </h4>
-      </span>
-      <div class=""aggregated-metrics collapsible-table"">
-        {executorTable.toNodeSeq}
-      </div>
-
     val content =
       summary ++
-      dagViz ++
-      showAdditionalMetrics ++
+      dagViz ++ <div id=""showAdditionalMetrics""></div> ++
       makeTimeline(
         // Only show the tasks in the table
-        Option(taskTable).map(_.dataSource.tasks).getOrElse(Nil),
-        currentTime) ++
-      <h4>Summary Metrics for <a href=""#tasks-section"">{numCompleted} Completed Tasks</a></h4> ++
-      <div>{summaryTable.getOrElse(""No tasks have reported metrics yet."")}</div> ++
-      aggMetrics ++
-      maybeAccumulableTable ++
-      <span id=""tasks-section"" class=""collapse-aggregated-tasks collapse-table""
-          onClick=""collapseTable('collapse-aggregated-tasks','aggregated-tasks')"">
-        <h4>
-          <span class=""collapse-table-arrow arrow-open""></span>
-          <a>Tasks ({totalTasksNumStr})</a>
-        </h4>
-      </span> ++
-      <div class=""aggregated-tasks collapsible-table"">
-        {taskTableHTML ++ jsForScrollingDownToTaskTable}
-      </div>
-    UIUtils.headerSparkPage(request, stageHeader, content, parent, showVisualization = true)
+        Option(taskTable).map({ taskPagedTable =>
+          val from = (eventTimelineTaskPage - 1) * eventTimelineTaskPageSize
+          val to = taskPagedTable.dataSource.dataSize.min(
+            eventTimelineTaskPage * eventTimelineTaskPageSize)
+          taskPagedTable.dataSource.sliceData(from, to)}).getOrElse(Nil), currentTime,
+        eventTimelineTaskPage, eventTimelineTaskPageSize, eventTimelineTotalPages, stageId,
+        stageAttemptId, totalTasks) ++
+        <div id=""parent-container"">
+          <script src={UIUtils.prependBaseUri(request, ""/static/utils.js"")}></script>
+          <script src={UIUtils.prependBaseUri(request, ""/static/stagepage.js"")}></script>
+        </div>
+        UIUtils.headerSparkPage(request, stageHeader, content, parent, showVisualization = true,
+          useDataTables = true)
+
   }
 
-  def makeTimeline(tasks: Seq[TaskData], currentTime: Long): Seq[Node] = {
+  def makeTimeline(
+      tasks: Seq[TaskData],
+      currentTime: Long,
+      page: Int,
+      pageSize: Int,
+      totalPages: Int,
+      stageId: Int,
+      stageAttemptId: Int,
+      totalTasks: Int): Seq[Node] = {
     val executorsSet = new HashSet[(String, String)]
     var minLaunchTime = Long.MaxValue
     var maxFinishTime = Long.MinValue
@@ -659,6 +446,31 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
           <input type=""checkbox""></input>
           <span>Enable zooming</span>
         </div>
+        <div>
+          <form id={s""form-event-timeline-page""}
+                method=""get""
+                action=""""
+                class=""form-inline pull-right""
+                style=""margin-bottom: 0px;"">
+            <label>Tasks: {totalTasks}. {totalPages} Pages. Jump to</label>
+            <input type=""hidden"" name=""id"" value={stageId.toString} />
+            <input type=""hidden"" name=""attempt"" value={stageAttemptId.toString} />
+            <input type=""text""
+                   name=""task.eventTimelinePageNumber""
+                   id={s""form-event-timeline-page-no""}
+                   value={page.toString} class=""span1"" />
+
+            <label>. Show </label>
+            <input type=""text""
+                   id={s""form-event-timeline-page-size""}
+                   name=""task.eventTimelinePageSize""
+                   value={pageSize.toString}
+                   class=""span1"" />
+            <label>items in a page.</label>
+
+            <button type=""submit"" class=""btn"">Go</button>
+          </form>
+        </div>
       </div>
       {TIMELINE_LEGEND}
     </div> ++
@@ -685,7 +497,7 @@ private[ui] class TaskDataSource(
 
   private var _tasksToShow: Seq[TaskData] = null
 
-  override def dataSize: Int = taskCount(stage)
+  override def dataSize: Int = store.taskCount(stage.stageId, stage.attemptId).toInt
 
   override def sliceData(from: Int, to: Int): Seq[TaskData] = {
     if (_tasksToShow == null) {
@@ -722,8 +534,6 @@ private[ui] class TaskPagedTable(
 
   override def pageSizeFormField: String = ""task.pageSize""
 
-  override def prevPageSizeFormField: String = ""task.prevPageSize""
-
   override def pageNumberFormField: String = ""task.page""
 
   override val dataSource: TaskDataSource = new TaskDataSource(
@@ -847,7 +657,7 @@ private[ui] class TaskPagedTable(
         </div>
       </td>
       <td>{UIUtils.formatDate(task.launchTime)}</td>
-      <td>{formatDuration(task.duration)}</td>
+      <td>{formatDuration(task.taskMetrics.map(_.executorRunTime))}</td>
       <td class={TaskDetailsClassNames.SCHEDULER_DELAY}>
         {UIUtils.formatDuration(AppStatusUtils.schedulerDelay(task))}
       </td>
@@ -962,7 +772,7 @@ private[ui] class TaskPagedTable(
   }
 }
 
-private[ui] object ApiHelper {
+private[spark] object ApiHelper {
 
   val HEADER_ID = ""ID""
   val HEADER_TASK_INDEX = ""Index""
@@ -1000,7 +810,9 @@ private[ui] object ApiHelper {
     HEADER_EXECUTOR -> TaskIndexNames.EXECUTOR,
     HEADER_HOST -> TaskIndexNames.HOST,
     HEADER_LAUNCH_TIME -> TaskIndexNames.LAUNCH_TIME,
-    HEADER_DURATION -> TaskIndexNames.DURATION,
+    // SPARK-26109: Duration of task as executorRunTime to make it consistent with the
+    // aggregated tasks summary metrics table and the previous versions of Spark.
+    HEADER_DURATION -> TaskIndexNames.EXEC_RUN_TIME,
     HEADER_SCHEDULER_DELAY -> TaskIndexNames.SCHEDULER_DELAY,
     HEADER_DESER_TIME -> TaskIndexNames.DESER_TIME,
     HEADER_GC_TIME -> TaskIndexNames.GC_TIME,
@@ -1047,13 +859,8 @@ private[ui] object ApiHelper {
   }
 
   def lastStageNameAndDescription(store: AppStatusStore, job: JobData): (String, String) = {
-    val stage = store.asOption(store.stageAttempt(job.stageIds.max, 0))
+    val stage = store.asOption(store.stageAttempt(job.stageIds.max, 0)._1)
     (stage.map(_.name).getOrElse(""""), stage.flatMap(_.description).getOrElse(job.name))
   }
 
-  def taskCount(stageData: StageData): Int = {
-    stageData.numActiveTasks + stageData.numCompleteTasks + stageData.numFailedTasks +
-      stageData.numKilledTasks
-  }
-
 }
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala b/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala
index d01acdae59c9f..766efc15e26ba 100644
--- a/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala
@@ -53,8 +53,6 @@ private[ui] class StageTableBase(
   val parameterStageSortColumn = UIUtils.stripXSS(request.getParameter(stageTag + "".sort""))
   val parameterStageSortDesc = UIUtils.stripXSS(request.getParameter(stageTag + "".desc""))
   val parameterStagePageSize = UIUtils.stripXSS(request.getParameter(stageTag + "".pageSize""))
-  val parameterStagePrevPageSize =
-    UIUtils.stripXSS(request.getParameter(stageTag + "".prevPageSize""))
 
   val stagePage = Option(parameterStagePage).map(_.toInt).getOrElse(1)
   val stageSortColumn = Option(parameterStageSortColumn).map { sortColumn =>
@@ -65,18 +63,7 @@ private[ui] class StageTableBase(
     stageSortColumn == ""Stage Id""
   )
   val stagePageSize = Option(parameterStagePageSize).map(_.toInt).getOrElse(100)
-  val stagePrevPageSize = Option(parameterStagePrevPageSize).map(_.toInt)
-    .getOrElse(stagePageSize)
-
-  val page: Int = {
-    // If the user has changed to a larger page size, then go to page 1 in order to avoid
-    // IndexOutOfBoundsException.
-    if (stagePageSize <= stagePrevPageSize) {
-      stagePage
-    } else {
-      1
-    }
-  }
+
   val currentTime = System.currentTimeMillis()
 
   val toNodeSeq = try {
@@ -96,7 +83,7 @@ private[ui] class StageTableBase(
       isFailedStage,
       parameterOtherTable,
       request
-    ).table(page)
+    ).table(stagePage)
   } catch {
     case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =>
       <div class=""alert alert-error"">
@@ -161,8 +148,6 @@ private[ui] class StagePagedTable(
 
   override def pageSizeFormField: String = stageTag + "".pageSize""
 
-  override def prevPageSizeFormField: String = stageTag + "".prevPageSize""
-
   override def pageNumberFormField: String = stageTag + "".page""
 
   val parameterPath = UIUtils.prependBaseUri(request, basePath) + s""/$subPath/?"" +
@@ -383,7 +368,7 @@ private[ui] class StagePagedTable(
         {if (cachedRddInfos.nonEmpty) {
           Text(""RDD: "") ++
           cachedRddInfos.map { i =>
-            <a href={s""$basePathUri/storage/rdd?id=${i.id}""}>{i.name}</a>
+            <a href={s""$basePathUri/storage/rdd/?id=${i.id}""}>{i.name}</a>
           }
         }}
         <pre>{s.details}</pre>
diff --git a/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala b/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala
index 238cd31433660..87da290c83057 100644
--- a/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala
+++ b/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala
@@ -39,13 +39,11 @@ private[ui] class RDDPage(parent: SparkUITab, store: AppStatusStore) extends Web
     val parameterBlockSortColumn = UIUtils.stripXSS(request.getParameter(""block.sort""))
     val parameterBlockSortDesc = UIUtils.stripXSS(request.getParameter(""block.desc""))
     val parameterBlockPageSize = UIUtils.stripXSS(request.getParameter(""block.pageSize""))
-    val parameterBlockPrevPageSize = UIUtils.stripXSS(request.getParameter(""block.prevPageSize""))
 
     val blockPage = Option(parameterBlockPage).map(_.toInt).getOrElse(1)
     val blockSortColumn = Option(parameterBlockSortColumn).getOrElse(""Block Name"")
     val blockSortDesc = Option(parameterBlockSortDesc).map(_.toBoolean).getOrElse(false)
     val blockPageSize = Option(parameterBlockPageSize).map(_.toInt).getOrElse(100)
-    val blockPrevPageSize = Option(parameterBlockPrevPageSize).map(_.toInt).getOrElse(blockPageSize)
 
     val rddId = parameterId.toInt
     val rddStorageInfo = try {
@@ -60,16 +58,6 @@ private[ui] class RDDPage(parent: SparkUITab, store: AppStatusStore) extends Web
     val workerTable = UIUtils.listingTable(workerHeader, workerRow,
       rddStorageInfo.dataDistribution.get, id = Some(""rdd-storage-by-worker-table""))
 
-    // Block table
-    val page: Int = {
-      // If the user has changed to a larger page size, then go to page 1 in order to avoid
-      // IndexOutOfBoundsException.
-      if (blockPageSize <= blockPrevPageSize) {
-        blockPage
-      } else {
-        1
-      }
-    }
     val blockTableHTML = try {
       val _blockTable = new BlockPagedTable(
         UIUtils.prependBaseUri(request, parent.basePath) + s""/storage/rdd/?id=${rddId}"",
@@ -78,7 +66,7 @@ private[ui] class RDDPage(parent: SparkUITab, store: AppStatusStore) extends Web
         blockSortColumn,
         blockSortDesc,
         store.executorList(true))
-      _blockTable.table(page)
+      _blockTable.table(blockPage)
     } catch {
       case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =>
         <div class=""alert alert-error"">{e.getMessage}</div>
@@ -242,8 +230,6 @@ private[ui] class BlockPagedTable(
 
   override def pageSizeFormField: String = ""block.pageSize""
 
-  override def prevPageSizeFormField: String = ""block.prevPageSize""
-
   override def pageNumberFormField: String = ""block.page""
 
   override val dataSource: BlockDataSource = new BlockDataSource(
diff --git a/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala b/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala
index 3eb546e336e99..2488197814ffd 100644
--- a/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala
+++ b/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala
@@ -78,7 +78,7 @@ private[ui] class StoragePage(parent: SparkUITab, store: AppStatusStore) extends
     <tr>
       <td>{rdd.id}</td>
       <td>
-        <a href={""%s/storage/rdd?id=%s"".format(
+        <a href={""%s/storage/rdd/?id=%s"".format(
           UIUtils.prependBaseUri(request, parent.basePath), rdd.id)}>
           {rdd.name}
         </a>
diff --git a/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala b/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala
index bf618b4afbce0..d5b3ce36e742a 100644
--- a/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala
+++ b/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala
@@ -485,34 +485,3 @@ class CollectionAccumulator[T] extends AccumulatorV2[T, java.util.List[T]] {
     _list.addAll(newValue)
   }
 }
-
-
-class LegacyAccumulatorWrapper[R, T](
-    initialValue: R,
-    param: org.apache.spark.AccumulableParam[R, T]) extends AccumulatorV2[T, R] {
-  private[spark] var _value = initialValue  // Current value on driver
-
-  @transient private lazy val _zero = param.zero(initialValue)
-
-  override def isZero: Boolean = _value.asInstanceOf[AnyRef].eq(_zero.asInstanceOf[AnyRef])
-
-  override def copy(): LegacyAccumulatorWrapper[R, T] = {
-    val acc = new LegacyAccumulatorWrapper(initialValue, param)
-    acc._value = _value
-    acc
-  }
-
-  override def reset(): Unit = {
-    _value = _zero
-  }
-
-  override def add(v: T): Unit = _value = param.addAccumulator(_value, v)
-
-  override def merge(other: AccumulatorV2[T, R]): Unit = other match {
-    case o: LegacyAccumulatorWrapper[R, T] => _value = param.addInPlace(_value, o.value)
-    case _ => throw new UnsupportedOperationException(
-      s""Cannot merge ${this.getClass.getName} with ${other.getClass.getName}"")
-  }
-
-  override def value: R = _value
-}
diff --git a/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala b/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala
index b6c300c4778b1..1b3e525644f00 100644
--- a/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala
+++ b/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala
@@ -23,8 +23,8 @@ import java.lang.invoke.SerializedLambda
 import scala.collection.mutable.{Map, Set, Stack}
 import scala.language.existentials
 
-import org.apache.xbean.asm6.{ClassReader, ClassVisitor, MethodVisitor, Type}
-import org.apache.xbean.asm6.Opcodes._
+import org.apache.xbean.asm7.{ClassReader, ClassVisitor, MethodVisitor, Type}
+import org.apache.xbean.asm7.Opcodes._
 
 import org.apache.spark.{SparkEnv, SparkException}
 import org.apache.spark.internal.Logging
@@ -175,7 +175,7 @@ private[spark] object ClosureCleaner extends Logging {
       closure.getClass.isSynthetic &&
         closure
           .getClass
-          .getInterfaces.exists(_.getName.equals(""scala.Serializable""))
+          .getInterfaces.exists(_.getName == ""scala.Serializable"")
 
     if (isClosureCandidate) {
       try {
@@ -285,8 +285,6 @@ private[spark] object ClosureCleaner extends Logging {
         innerClasses.foreach { c => logDebug(s""     ${c.getName}"") }
         logDebug(s"" + outer classes: ${outerClasses.size}"" )
         outerClasses.foreach { c => logDebug(s""     ${c.getName}"") }
-        logDebug(s"" + outer objects: ${outerObjects.size}"")
-        outerObjects.foreach { o => logDebug(s""     $o"") }
       }
 
       // Fail fast if we detect return statements in closures
@@ -318,19 +316,20 @@ private[spark] object ClosureCleaner extends Logging {
       if (outerPairs.nonEmpty) {
         val (outermostClass, outermostObject) = outerPairs.head
         if (isClosure(outermostClass)) {
-          logDebug(s"" + outermost object is a closure, so we clone it: ${outerPairs.head}"")
+          logDebug(s"" + outermost object is a closure, so we clone it: ${outermostClass}"")
         } else if (outermostClass.getName.startsWith(""$line"")) {
           // SPARK-14558: if the outermost object is a REPL line object, we should clone
           // and clean it as it may carray a lot of unnecessary information,
           // e.g. hadoop conf, spark conf, etc.
-          logDebug(s"" + outermost object is a REPL line object, so we clone it: ${outerPairs.head}"")
+          logDebug(s"" + outermost object is a REPL line object, so we clone it:"" +
+            s"" ${outermostClass}"")
         } else {
           // The closure is ultimately nested inside a class; keep the object of that
           // class without cloning it since we don't want to clone the user's objects.
           // Note that we still need to keep around the outermost object itself because
           // we need it to clone its child closure later (see below).
-          logDebug("" + outermost object is not a closure or REPL line object,"" +
-            ""so do not clone it: "" +  outerPairs.head)
+          logDebug(s"" + outermost object is not a closure or REPL line object,"" +
+            s"" so do not clone it: ${outermostClass}"")
           parent = outermostObject // e.g. SparkContext
           outerPairs = outerPairs.tail
         }
@@ -341,7 +340,7 @@ private[spark] object ClosureCleaner extends Logging {
       // Clone the closure objects themselves, nulling out any fields that are not
       // used in the closure we're working on or any of its inner closures.
       for ((cls, obj) <- outerPairs) {
-        logDebug(s"" + cloning the object $obj of class ${cls.getName}"")
+        logDebug(s"" + cloning instance of class ${cls.getName}"")
         // We null out these unused references by cloning each object and then filling in all
         // required fields from the original object. We need the parent here because the Java
         // language specification requires the first constructor parameter of any closure to be
@@ -351,7 +350,7 @@ private[spark] object ClosureCleaner extends Logging {
         // If transitive cleaning is enabled, we recursively clean any enclosing closure using
         // the already populated accessed fields map of the starting closure
         if (cleanTransitively && isClosure(clone.getClass)) {
-          logDebug(s"" + cleaning cloned closure $clone recursively (${cls.getName})"")
+          logDebug(s"" + cleaning cloned closure recursively (${cls.getName})"")
           // No need to check serializable here for the outer closures because we're
           // only interested in the serializability of the starting closure
           clean(clone, checkSerializable = false, cleanTransitively, accessedFields)
@@ -425,7 +424,7 @@ private[spark] class ReturnStatementInClosureException
   extends SparkException(""Return statements aren't allowed in Spark closures"")
 
 private class ReturnStatementFinder(targetMethodName: Option[String] = None)
-  extends ClassVisitor(ASM6) {
+  extends ClassVisitor(ASM7) {
   override def visitMethod(access: Int, name: String, desc: String,
       sig: String, exceptions: Array[String]): MethodVisitor = {
 
@@ -439,7 +438,7 @@ private class ReturnStatementFinder(targetMethodName: Option[String] = None)
       val isTargetMethod = targetMethodName.isEmpty ||
         name == targetMethodName.get || name == targetMethodName.get.stripSuffix(""$adapted"")
 
-      new MethodVisitor(ASM6) {
+      new MethodVisitor(ASM7) {
         override def visitTypeInsn(op: Int, tp: String) {
           if (op == NEW && tp.contains(""scala/runtime/NonLocalReturnControl"") && isTargetMethod) {
             throw new ReturnStatementInClosureException
@@ -447,7 +446,7 @@ private class ReturnStatementFinder(targetMethodName: Option[String] = None)
         }
       }
     } else {
-      new MethodVisitor(ASM6) {}
+      new MethodVisitor(ASM7) {}
     }
   }
 }
@@ -471,7 +470,7 @@ private[util] class FieldAccessFinder(
     findTransitively: Boolean,
     specificMethod: Option[MethodIdentifier[_]] = None,
     visitedMethods: Set[MethodIdentifier[_]] = Set.empty)
-  extends ClassVisitor(ASM6) {
+  extends ClassVisitor(ASM7) {
 
   override def visitMethod(
       access: Int,
@@ -486,7 +485,7 @@ private[util] class FieldAccessFinder(
       return null
     }
 
-    new MethodVisitor(ASM6) {
+    new MethodVisitor(ASM7) {
       override def visitFieldInsn(op: Int, owner: String, name: String, desc: String) {
         if (op == GETFIELD) {
           for (cl <- fields.keys if cl.getName == owner.replace('/', '.')) {
@@ -526,7 +525,7 @@ private[util] class FieldAccessFinder(
   }
 }
 
-private class InnerClosureFinder(output: Set[Class[_]]) extends ClassVisitor(ASM6) {
+private class InnerClosureFinder(output: Set[Class[_]]) extends ClassVisitor(ASM7) {
   var myName: String = null
 
   // TODO: Recursively find inner closures that we indirectly reference, e.g.
@@ -541,7 +540,7 @@ private class InnerClosureFinder(output: Set[Class[_]]) extends ClassVisitor(ASM
 
   override def visitMethod(access: Int, name: String, desc: String,
       sig: String, exceptions: Array[String]): MethodVisitor = {
-    new MethodVisitor(ASM6) {
+    new MethodVisitor(ASM7) {
       override def visitMethodInsn(
           op: Int, owner: String, name: String, desc: String, itf: Boolean) {
         val argTypes = Type.getArgumentTypes(desc)
diff --git a/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala b/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala
index 21acaa95c5645..f4d6c7a28d2e4 100644
--- a/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala
+++ b/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala
@@ -25,11 +25,14 @@ private[spark]
 abstract class CompletionIterator[ +A, +I <: Iterator[A]](sub: I) extends Iterator[A] {
 
   private[this] var completed = false
-  def next(): A = sub.next()
+  private[this] var iter = sub
+  def next(): A = iter.next()
   def hasNext: Boolean = {
-    val r = sub.hasNext
+    val r = iter.hasNext
     if (!r && !completed) {
       completed = true
+      // reassign to release resources of highly resource consuming iterators early
+      iter = Iterator.empty.asInstanceOf[I]
       completion()
     }
     r
diff --git a/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala b/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala
index 50c6461373dee..0cd8612b8fd1c 100644
--- a/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala
+++ b/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala
@@ -31,6 +31,7 @@ import org.json4s.jackson.JsonMethods._
 
 import org.apache.spark._
 import org.apache.spark.executor._
+import org.apache.spark.metrics.ExecutorMetricType
 import org.apache.spark.rdd.RDDOperationScope
 import org.apache.spark.scheduler._
 import org.apache.spark.scheduler.cluster.ExecutorInfo
@@ -98,6 +99,8 @@ private[spark] object JsonProtocol {
         logStartToJson(logStart)
       case metricsUpdate: SparkListenerExecutorMetricsUpdate =>
         executorMetricsUpdateToJson(metricsUpdate)
+      case stageExecutorMetrics: SparkListenerStageExecutorMetrics =>
+        stageExecutorMetricsToJson(stageExecutorMetrics)
       case blockUpdate: SparkListenerBlockUpdated =>
         blockUpdateToJson(blockUpdate)
       case _ => parse(mapper.writeValueAsString(event))
@@ -236,6 +239,7 @@ private[spark] object JsonProtocol {
   def executorMetricsUpdateToJson(metricsUpdate: SparkListenerExecutorMetricsUpdate): JValue = {
     val execId = metricsUpdate.execId
     val accumUpdates = metricsUpdate.accumUpdates
+    val executorMetrics = metricsUpdate.executorUpdates.map(executorMetricsToJson(_))
     (""Event"" -> SPARK_LISTENER_EVENT_FORMATTED_CLASS_NAMES.metricsUpdate) ~
     (""Executor ID"" -> execId) ~
     (""Metrics Updated"" -> accumUpdates.map { case (taskId, stageId, stageAttemptId, updates) =>
@@ -243,7 +247,16 @@ private[spark] object JsonProtocol {
       (""Stage ID"" -> stageId) ~
       (""Stage Attempt ID"" -> stageAttemptId) ~
       (""Accumulator Updates"" -> JArray(updates.map(accumulableInfoToJson).toList))
-    })
+    }) ~
+    (""Executor Metrics Updated"" -> executorMetrics)
+  }
+
+  def stageExecutorMetricsToJson(metrics: SparkListenerStageExecutorMetrics): JValue = {
+    (""Event"" -> SPARK_LISTENER_EVENT_FORMATTED_CLASS_NAMES.stageExecutorMetrics) ~
+    (""Executor ID"" -> metrics.execId) ~
+    (""Stage ID"" -> metrics.stageId) ~
+    (""Stage Attempt ID"" -> metrics.stageAttemptId) ~
+    (""Executor Metrics"" -> executorMetricsToJson(metrics.executorMetrics))
   }
 
   def blockUpdateToJson(blockUpdate: SparkListenerBlockUpdated): JValue = {
@@ -379,6 +392,14 @@ private[spark] object JsonProtocol {
     (""Updated Blocks"" -> updatedBlocks)
   }
 
+  /** Convert executor metrics to JSON. */
+  def executorMetricsToJson(executorMetrics: ExecutorMetrics): JValue = {
+    val metrics = ExecutorMetricType.values.map{ metricType =>
+      JField(metricType.name, executorMetrics.getMetricValue(metricType))
+     }
+    JObject(metrics: _*)
+  }
+
   def taskEndReasonToJson(taskEndReason: TaskEndReason): JValue = {
     val reason = Utils.getFormattedClassName(taskEndReason)
     val json: JObject = taskEndReason match {
@@ -531,6 +552,7 @@ private[spark] object JsonProtocol {
     val executorRemoved = Utils.getFormattedClassName(SparkListenerExecutorRemoved)
     val logStart = Utils.getFormattedClassName(SparkListenerLogStart)
     val metricsUpdate = Utils.getFormattedClassName(SparkListenerExecutorMetricsUpdate)
+    val stageExecutorMetrics = Utils.getFormattedClassName(SparkListenerStageExecutorMetrics)
     val blockUpdate = Utils.getFormattedClassName(SparkListenerBlockUpdated)
   }
 
@@ -555,6 +577,7 @@ private[spark] object JsonProtocol {
       case `executorRemoved` => executorRemovedFromJson(json)
       case `logStart` => logStartFromJson(json)
       case `metricsUpdate` => executorMetricsUpdateFromJson(json)
+      case `stageExecutorMetrics` => stageExecutorMetricsFromJson(json)
       case `blockUpdate` => blockUpdateFromJson(json)
       case other => mapper.readValue(compact(render(json)), Utils.classForName(other))
         .asInstanceOf[SparkListenerEvent]
@@ -585,6 +608,15 @@ private[spark] object JsonProtocol {
     SparkListenerTaskGettingResult(taskInfo)
   }
 
+  /** Extract the executor metrics from JSON. */
+  def executorMetricsFromJson(json: JValue): ExecutorMetrics = {
+    val metrics =
+      ExecutorMetricType.values.map { metric =>
+        metric.name -> jsonOption(json \ metric.name).map(_.extract[Long]).getOrElse(0L)
+      }.toMap
+    new ExecutorMetrics(metrics)
+  }
+
   def taskEndFromJson(json: JValue): SparkListenerTaskEnd = {
     val stageId = (json \ ""Stage ID"").extract[Int]
     val stageAttemptId =
@@ -691,7 +723,18 @@ private[spark] object JsonProtocol {
         (json \ ""Accumulator Updates"").extract[List[JValue]].map(accumulableInfoFromJson)
       (taskId, stageId, stageAttemptId, updates)
     }
-    SparkListenerExecutorMetricsUpdate(execInfo, accumUpdates)
+    val executorUpdates = jsonOption(json \ ""Executor Metrics Updated"").map {
+      executorUpdate => executorMetricsFromJson(executorUpdate)
+    }
+    SparkListenerExecutorMetricsUpdate(execInfo, accumUpdates, executorUpdates)
+  }
+
+  def stageExecutorMetricsFromJson(json: JValue): SparkListenerStageExecutorMetrics = {
+    val execId = (json \ ""Executor ID"").extract[String]
+    val stageId = (json \ ""Stage ID"").extract[Int]
+    val stageAttemptId = (json \ ""Stage Attempt ID"").extract[Int]
+    val executorMetrics = executorMetricsFromJson(json \ ""Executor Metrics"")
+    SparkListenerStageExecutorMetrics(execId, stageId, stageAttemptId, executorMetrics)
   }
 
   def blockUpdateFromJson(json: JValue): SparkListenerBlockUpdated = {
diff --git a/core/src/main/scala/org/apache/spark/util/ListenerBus.scala b/core/src/main/scala/org/apache/spark/util/ListenerBus.scala
index a8f10684d5a2c..2e517707ff774 100644
--- a/core/src/main/scala/org/apache/spark/util/ListenerBus.scala
+++ b/core/src/main/scala/org/apache/spark/util/ListenerBus.scala
@@ -60,6 +60,14 @@ private[spark] trait ListenerBus[L <: AnyRef, E] extends Logging {
     }
   }
 
+  /**
+   * Remove all listeners and they won't receive any events. This method is thread-safe and can be
+   * called in any thread.
+   */
+  final def removeAllListeners(): Unit = {
+    listenersPlusTimers.clear()
+  }
+
   /**
    * This can be overridden by subclasses if there is any extra cleanup to do when removing a
    * listener.  In particular AsyncEventQueues can clean up queues in the LiveListenerBus.
diff --git a/core/src/main/scala/org/apache/spark/util/Utils.scala b/core/src/main/scala/org/apache/spark/util/Utils.scala
index 15c958d3f511e..227c9e734f0af 100644
--- a/core/src/main/scala/org/apache/spark/util/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala
@@ -19,7 +19,6 @@ package org.apache.spark.util
 
 import java.io._
 import java.lang.{Byte => JByte}
-import java.lang.InternalError
 import java.lang.management.{LockInfo, ManagementFactory, MonitorInfo, ThreadInfo}
 import java.lang.reflect.InvocationTargetException
 import java.math.{MathContext, RoundingMode}
@@ -32,7 +31,6 @@ import java.security.SecureRandom
 import java.util.{Locale, Properties, Random, UUID}
 import java.util.concurrent._
 import java.util.concurrent.TimeUnit.NANOSECONDS
-import java.util.concurrent.atomic.AtomicBoolean
 import java.util.zip.GZIPInputStream
 
 import scala.annotation.tailrec
@@ -94,53 +92,6 @@ private[spark] object Utils extends Logging {
   private val MAX_DIR_CREATION_ATTEMPTS: Int = 10
   @volatile private var localRootDirs: Array[String] = null
 
-  /**
-   * The performance overhead of creating and logging strings for wide schemas can be large. To
-   * limit the impact, we bound the number of fields to include by default. This can be overridden
-   * by setting the 'spark.debug.maxToStringFields' conf in SparkEnv.
-   */
-  val DEFAULT_MAX_TO_STRING_FIELDS = 25
-
-  private[spark] def maxNumToStringFields = {
-    if (SparkEnv.get != null) {
-      SparkEnv.get.conf.getInt(""spark.debug.maxToStringFields"", DEFAULT_MAX_TO_STRING_FIELDS)
-    } else {
-      DEFAULT_MAX_TO_STRING_FIELDS
-    }
-  }
-
-  /** Whether we have warned about plan string truncation yet. */
-  private val truncationWarningPrinted = new AtomicBoolean(false)
-
-  /**
-   * Format a sequence with semantics similar to calling .mkString(). Any elements beyond
-   * maxNumToStringFields will be dropped and replaced by a ""... N more fields"" placeholder.
-   *
-   * @return the trimmed and formatted string.
-   */
-  def truncatedString[T](
-      seq: Seq[T],
-      start: String,
-      sep: String,
-      end: String,
-      maxNumFields: Int = maxNumToStringFields): String = {
-    if (seq.length > maxNumFields) {
-      if (truncationWarningPrinted.compareAndSet(false, true)) {
-        logWarning(
-          ""Truncated the string representation of a plan since it was too large. This "" +
-          ""behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf."")
-      }
-      val numFields = math.max(0, maxNumFields - 1)
-      seq.take(numFields).mkString(
-        start, sep, sep + ""... "" + (seq.length - numFields) + "" more fields"" + end)
-    } else {
-      seq.mkString(start, sep, end)
-    }
-  }
-
-  /** Shorthand for calling truncatedString() without start or end strings. */
-  def truncatedString[T](seq: Seq[T], sep: String): String = truncatedString(seq, """", sep, """")
-
   /** Serialize an object using Java serialization */
   def serialize[T](o: T): Array[Byte] = {
     val bos = new ByteArrayOutputStream()
@@ -240,6 +191,19 @@ private[spark] object Utils extends Logging {
     // scalastyle:on classforname
   }
 
+  /**
+   * Run a segment of code using a different context class loader in the current thread
+   */
+  def withContextClassLoader[T](ctxClassLoader: ClassLoader)(fn: => T): T = {
+    val oldClassLoader = Thread.currentThread().getContextClassLoader()
+    try {
+      Thread.currentThread().setContextClassLoader(ctxClassLoader)
+      fn
+    } finally {
+      Thread.currentThread().setContextClassLoader(oldClassLoader)
+    }
+  }
+
   /**
    * Primitive often used when writing [[java.nio.ByteBuffer]] to [[java.io.DataOutput]]
    */
@@ -2052,6 +2016,30 @@ private[spark] object Utils extends Logging {
     }
   }
 
+  /**
+   * Implements the same logic as JDK `java.lang.String#trim` by removing leading and trailing
+   * non-printable characters less or equal to '\u0020' (SPACE) but preserves natural line
+   * delimiters according to [[java.util.Properties]] load method. The natural line delimiters are
+   * removed by JDK during load. Therefore any remaining ones have been specifically provided and
+   * escaped by the user, and must not be ignored
+   *
+   * @param str
+   * @return the trimmed value of str
+   */
+  private[util] def trimExceptCRLF(str: String): String = {
+    val nonSpaceOrNaturalLineDelimiter: Char => Boolean = { ch =>
+      ch > ' ' || ch == '\r' || ch == '\n'
+    }
+
+    val firstPos = str.indexWhere(nonSpaceOrNaturalLineDelimiter)
+    val lastPos = str.lastIndexWhere(nonSpaceOrNaturalLineDelimiter)
+    if (firstPos >= 0 && lastPos >= 0) {
+      str.substring(firstPos, lastPos + 1)
+    } else {
+      """"
+    }
+  }
+
   /** Load properties present in the given file. */
   def getPropertiesFromFile(filename: String): Map[String, String] = {
     val file = new File(filename)
@@ -2062,8 +2050,10 @@ private[spark] object Utils extends Logging {
     try {
       val properties = new Properties()
       properties.load(inReader)
-      properties.stringPropertyNames().asScala.map(
-        k => (k, properties.getProperty(k).trim)).toMap
+      properties.stringPropertyNames().asScala
+        .map { k => (k, trimExceptCRLF(properties.getProperty(k))) }
+        .toMap
+
     } catch {
       case e: IOException =>
         throw new SparkException(s""Failed when loading Spark properties from $filename"", e)
@@ -2290,7 +2280,12 @@ private[spark] object Utils extends Logging {
    * configure a new log4j level
    */
   def setLogLevel(l: org.apache.log4j.Level) {
-    org.apache.log4j.Logger.getRootLogger().setLevel(l)
+    val rootLogger = org.apache.log4j.Logger.getRootLogger()
+    rootLogger.setLevel(l)
+    rootLogger.getAllAppenders().asScala.foreach {
+      case ca: org.apache.log4j.ConsoleAppender => ca.setThreshold(l)
+      case _ => // no-op
+    }
   }
 
   /**
@@ -2392,7 +2387,8 @@ private[spark] object Utils extends Logging {
       ""org.apache.spark.security.ShellBasedGroupsMappingProvider"")
     if (groupProviderClassName != """") {
       try {
-        val groupMappingServiceProvider = classForName(groupProviderClassName).newInstance.
+        val groupMappingServiceProvider = classForName(groupProviderClassName).
+          getConstructor().newInstance().
           asInstanceOf[org.apache.spark.security.GroupMappingServiceProvider]
         val currentUserGroups = groupMappingServiceProvider.getGroups(username)
         return currentUserGroups
@@ -2698,7 +2694,7 @@ private[spark] object Utils extends Logging {
     }
 
     val masterScheme = new URI(masterWithoutK8sPrefix).getScheme
-    val resolvedURL = masterScheme.toLowerCase match {
+    val resolvedURL = masterScheme.toLowerCase(Locale.ROOT) match {
       case ""https"" =>
         masterWithoutK8sPrefix
       case ""http"" =>
@@ -2795,6 +2791,44 @@ private[spark] object Utils extends Logging {
       }
     }
   }
+
+  /**
+   * Regular expression matching full width characters.
+   *
+   * Looked at all the 0x0000-0xFFFF characters (unicode) and showed them under Xshell.
+   * Found all the full width characters, then get the regular expression.
+   */
+  private val fullWidthRegex = (""""""["""""" +
+    // scalastyle:off nonascii
+    """"""\u1100-\u115F"""""" +
+    """"""\u2E80-\uA4CF"""""" +
+    """"""\uAC00-\uD7A3"""""" +
+    """"""\uF900-\uFAFF"""""" +
+    """"""\uFE10-\uFE19"""""" +
+    """"""\uFE30-\uFE6F"""""" +
+    """"""\uFF00-\uFF60"""""" +
+    """"""\uFFE0-\uFFE6"""""" +
+    // scalastyle:on nonascii
+    """"""]"""""").r
+
+  /**
+   * Return the number of half widths in a given string. Note that a full width character
+   * occupies two half widths.
+   *
+   * For a string consisting of 1 million characters, the execution of this method requires
+   * about 50ms.
+   */
+  def stringHalfWidth(str: String): Int = {
+    if (str == null) 0 else str.length + fullWidthRegex.findAllIn(str).size
+  }
+
+  def sanitizeDirName(str: String): String = {
+    str.replaceAll(""[ :/]"", ""-"").replaceAll(""[.${}'\""]"", ""_"").toLowerCase(Locale.ROOT)
+  }
+
+  def isClientMode(conf: SparkConf): Boolean = {
+    ""client"".equals(conf.get(SparkLauncher.DEPLOY_MODE, ""client""))
+  }
 }
 
 private[util] object CallerContext extends Logging {
diff --git a/core/src/main/scala/org/apache/spark/util/VersionUtils.scala b/core/src/main/scala/org/apache/spark/util/VersionUtils.scala
index 828153b868420..c0f8866dd58dc 100644
--- a/core/src/main/scala/org/apache/spark/util/VersionUtils.scala
+++ b/core/src/main/scala/org/apache/spark/util/VersionUtils.scala
@@ -23,6 +23,7 @@ package org.apache.spark.util
 private[spark] object VersionUtils {
 
   private val majorMinorRegex = """"""^(\d+)\.(\d+)(\..*)?$"""""".r
+  private val shortVersionRegex = """"""^(\d+\.\d+\.\d+)(.*)?$"""""".r
 
   /**
    * Given a Spark version string, return the major version number.
@@ -36,6 +37,19 @@ private[spark] object VersionUtils {
    */
   def minorVersion(sparkVersion: String): Int = majorMinorVersion(sparkVersion)._2
 
+  /**
+   * Given a Spark version string, return the short version string.
+   * E.g., for 3.0.0-SNAPSHOT, return '3.0.0'.
+   */
+  def shortVersion(sparkVersion: String): String = {
+    shortVersionRegex.findFirstMatchIn(sparkVersion) match {
+      case Some(m) => m.group(1)
+      case None =>
+        throw new IllegalArgumentException(s""Spark tried to parse '$sparkVersion' as a Spark"" +
+          s"" version string, but it could not find the major/minor/maintenance version numbers."")
+    }
+  }
+
   /**
    * Given a Spark version string, return the (major version number, minor version number).
    * E.g., for 2.0.1-SNAPSHOT, return (2, 0).
diff --git a/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala b/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
index b159200d79222..46279e79d78db 100644
--- a/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
+++ b/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
@@ -727,9 +727,10 @@ private[spark] class ExternalSorter[K, V, C](
     spills.clear()
     forceSpillFiles.foreach(s => s.file.delete())

  (This diff was longer than 20,000 lines, and has been truncated...)


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Dec/18 02:24;githubbot;Ngone51 opened a new pull request #23368: [SPARK-26269][YARN][BRANCH-2.4] Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource
URL: https://github.com/apache/spark/pull/23368
 
 
   ## What changes were proposed in this pull request?
   
   As I mentioned in jira [SPARK-26269](https://issues.apache.org/jira/browse/SPARK-26269), in order to maxmize the use of cluster resource,  this pr try to make `YarnAllocator` have the same blacklist behaviour with YARN.
   
   ## How was this patch tested?
   
   Added.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka source may reprocess data,SPARK-26267,13202353,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,04/Dec/18 19:25,04/Sep/19 11:11,13/Jul/23 08:48,08/Jan/19 00:54,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Structured Streaming,,,,0,correctness,,,,"Due to KAFKA-7703, when the Kafka source tries to get the latest offset, it may get an earliest offset, and then it will reprocess messages that have been processed when it gets the correct latest offset in the next batch.

This usually happens when restarting a streaming query.",,dongjoon,githubbot,gsomogyi,iamhumanbeing,kabhwan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28641,,,,KAFKA-7703,,SPARK-28174,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 21 18:53:41 UTC 2018,,,,,,,,,,"0|s015z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/18 19:27;zsxwing;KAFKA-7703 only exists in Kafka 1.1.0 and above, so a possible workaround is using an old version that doesn't have this issue. This doesn't impact Spark 2.3.x and below as we use Kafka 0.10.0.1 by default.;;;","14/Dec/18 23:19;githubbot;zsxwing opened a new pull request #23324: [SPARK-26267][SS]Retry when detecting incorrect offsets from Kafka
URL: https://github.com/apache/spark/pull/23324
 
 
   ## What changes were proposed in this pull request?
   
   Due to [KAFKA-7703](https://issues.apache.org/jira/browse/KAFKA-7703), Kafka may return an earliest offset when we are request a latest offset. This will cause Spark to reprocess data.
   
   To reduce the impact of KAFKA-7703, this PR will use the previous offsets we get to audit the result from Kafka. If we find any incorrect offset, we will retry at most `maxOffsetFetchAttempts` times. For the first batch of a new query, as we don't have any previous offsets, we simply fetch offsets twice. This should reduce the chance to hit KAFKA-7703 a lot.
   
   ## How was this patch tested?
   
   Jenkins

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Dec/18 18:45;githubbot;zsxwing closed pull request #23324: [SPARK-26267][SS]Retry when detecting incorrect offsets from Kafka
URL: https://github.com/apache/spark/pull/23324
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala
index 1753a28fba2fb..02dfb9ca2b95a 100644
--- a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala
@@ -60,7 +60,7 @@ class KafkaContinuousReadSupport(
   override def initialOffset(): Offset = {
     val offsets = initialOffsets match {
       case EarliestOffsetRangeLimit => KafkaSourceOffset(offsetReader.fetchEarliestOffsets())
-      case LatestOffsetRangeLimit => KafkaSourceOffset(offsetReader.fetchLatestOffsets())
+      case LatestOffsetRangeLimit => KafkaSourceOffset(offsetReader.fetchLatestOffsets(None))
       case SpecificOffsetRangeLimit(p) => offsetReader.fetchSpecificOffsets(p, reportDataLoss)
     }
     logInfo(s""Initial offsets: $offsets"")
@@ -107,7 +107,7 @@ class KafkaContinuousReadSupport(
 
   override def needsReconfiguration(config: ScanConfig): Boolean = {
     val knownPartitions = config.asInstanceOf[KafkaContinuousScanConfig].knownPartitions
-    offsetReader.fetchLatestOffsets().keySet != knownPartitions
+    offsetReader.fetchLatestOffsets(None).keySet != knownPartitions
   }
 
   override def toString(): String = s""KafkaSource[$offsetReader]""
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala
index bb4de674c3c72..b4f042e93a5da 100644
--- a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala
@@ -84,7 +84,7 @@ private[kafka010] class KafkaMicroBatchReadSupport(
 
   override def latestOffset(start: Offset): Offset = {
     val startPartitionOffsets = start.asInstanceOf[KafkaSourceOffset].partitionToOffsets
-    val latestPartitionOffsets = kafkaOffsetReader.fetchLatestOffsets()
+    val latestPartitionOffsets = kafkaOffsetReader.fetchLatestOffsets(Some(startPartitionOffsets))
     endPartitionOffsets = KafkaSourceOffset(maxOffsetsPerTrigger.map { maxOffsets =>
       rateLimit(maxOffsets, startPartitionOffsets, latestPartitionOffsets)
     }.getOrElse {
@@ -133,10 +133,21 @@ private[kafka010] class KafkaMicroBatchReadSupport(
     }.toSeq
     logDebug(""TopicPartitions: "" + topicPartitions.mkString("", ""))
 
+    val fromOffsets = startPartitionOffsets ++ newPartitionInitialOffsets
+    val untilOffsets = endPartitionOffsets
+    untilOffsets.foreach { case (tp, untilOffset) =>
+      fromOffsets.get(tp).foreach { fromOffset =>
+        if (untilOffset < fromOffset) {
+          reportDataLoss(s""Partition $tp's offset was changed from "" +
+            s""$fromOffset to $untilOffset, some data may have been missed"")
+        }
+      }
+    }
+
     // Calculate offset ranges
     val offsetRanges = rangeCalculator.getRanges(
-      fromOffsets = startPartitionOffsets ++ newPartitionInitialOffsets,
-      untilOffsets = endPartitionOffsets,
+      fromOffsets = fromOffsets,
+      untilOffsets = untilOffsets,
       executorLocations = getSortedExecutorList())
 
     // Reuse Kafka consumers only when all the offset ranges have distinct TopicPartitions,
@@ -186,7 +197,7 @@ private[kafka010] class KafkaMicroBatchReadSupport(
         case EarliestOffsetRangeLimit =>
           KafkaSourceOffset(kafkaOffsetReader.fetchEarliestOffsets())
         case LatestOffsetRangeLimit =>
-          KafkaSourceOffset(kafkaOffsetReader.fetchLatestOffsets())
+          KafkaSourceOffset(kafkaOffsetReader.fetchLatestOffsets(None))
         case SpecificOffsetRangeLimit(p) =>
           kafkaOffsetReader.fetchSpecificOffsets(p, reportDataLoss)
       }
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala
index fb209c724afba..6008794924052 100644
--- a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala
@@ -37,6 +37,8 @@ private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int
    * the read tasks of the skewed partitions to multiple Spark tasks.
    * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more
    * depending on rounding errors or Kafka partitions that didn't receive any new data.
+   *
+   * Empty ranges (`KafkaOffsetRange.size <= 0`) will be dropped.
    */
   def getRanges(
       fromOffsets: PartitionOffsetMap,
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala
index 82066697cb95a..fc443d22bf5a2 100644
--- a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala
@@ -21,6 +21,7 @@ import java.{util => ju}
 import java.util.concurrent.{Executors, ThreadFactory}
 
 import scala.collection.JavaConverters._
+import scala.collection.mutable.ArrayBuffer
 import scala.concurrent.{ExecutionContext, Future}
 import scala.concurrent.duration.Duration
 import scala.util.control.NonFatal
@@ -137,6 +138,12 @@ private[kafka010] class KafkaOffsetReader(
         // Poll to get the latest assigned partitions
         consumer.poll(0)
         val partitions = consumer.assignment()
+
+        // Call `position` to wait until the potential offset request triggered by `poll(0)` is
+        // done. This is a workaround for KAFKA-7703, which an async `seekToBeginning` triggered by
+        // `poll(0)` may reset offsets that should have been set by another request.
+        partitions.asScala.map(p => p -> consumer.position(p)).foreach(_ => {})
+
         consumer.pause(partitions)
         assert(partitions.asScala == partitionOffsets.keySet,
           ""If startingOffsets contains specific offsets, you must specify all TopicPartitions.\n"" +
@@ -192,19 +199,82 @@ private[kafka010] class KafkaOffsetReader(
   /**
    * Fetch the latest offsets for the topic partitions that are indicated
    * in the [[ConsumerStrategy]].
+   *
+   * Kafka may return earliest offsets when we are requesting latest offsets if `poll` is called
+   * right before `seekToEnd` (KAFKA-7703). As a workaround, we will call `position` right after
+   * `poll` to wait until the potential offset request triggered by `poll(0)` is done.
+   *
+   * In addition, to avoid other unknown issues, we also use the given `knownOffsets` to audit the
+   * latest offsets returned by Kafka. If we find some incorrect offsets (a latest offset is less
+   * than an offset in `knownOffsets`), we will retry at most `maxOffsetFetchAttempts` times. When
+   * a topic is recreated, the latest offsets may be less than offsets in `knownOffsets`. We cannot
+   * distinguish this with KAFKA-7703, so we just return whatever we get from Kafka after retrying.
    */
-  def fetchLatestOffsets(): Map[TopicPartition, Long] = runUninterruptibly {
+  def fetchLatestOffsets(
+      knownOffsets: Option[PartitionOffsetMap]): PartitionOffsetMap = runUninterruptibly {
     withRetriesWithoutInterrupt {
       // Poll to get the latest assigned partitions
       consumer.poll(0)
       val partitions = consumer.assignment()
+
+      // Call `position` to wait until the potential offset request triggered by `poll(0)` is
+      // done. This is a workaround for KAFKA-7703, which an async `seekToBeginning` triggered by
+      // `poll(0)` may reset offsets that should have been set by another request.
+      partitions.asScala.map(p => p -> consumer.position(p)).foreach(_ => {})
+
       consumer.pause(partitions)
       logDebug(s""Partitions assigned to consumer: $partitions. Seeking to the end."")
 
-      consumer.seekToEnd(partitions)
-      val partitionOffsets = partitions.asScala.map(p => p -> consumer.position(p)).toMap
-      logDebug(s""Got latest offsets for partition : $partitionOffsets"")
-      partitionOffsets
+      if (knownOffsets.isEmpty) {
+        consumer.seekToEnd(partitions)
+        partitions.asScala.map(p => p -> consumer.position(p)).toMap
+      } else {
+        var partitionOffsets: PartitionOffsetMap = Map.empty
+
+        /**
+         * Compare `knownOffsets` and `partitionOffsets`. Returns all partitions that have incorrect
+         * latest offset (offset in `knownOffsets` is great than the one in `partitionOffsets`).
+         */
+        def findIncorrectOffsets(): Seq[(TopicPartition, Long, Long)] = {
+          var incorrectOffsets = ArrayBuffer[(TopicPartition, Long, Long)]()
+          partitionOffsets.foreach { case (tp, offset) =>
+            knownOffsets.foreach(_.get(tp).foreach { knownOffset =>
+              if (knownOffset > offset) {
+                val incorrectOffset = (tp, knownOffset, offset)
+                incorrectOffsets += incorrectOffset
+              }
+            })
+          }
+          incorrectOffsets
+        }
+
+        // Retry to fetch latest offsets when detecting incorrect offsets. We don't use
+        // `withRetriesWithoutInterrupt` to retry because:
+        //
+        // - `withRetriesWithoutInterrupt` will reset the consumer for each attempt but a fresh
+        //    consumer has a much bigger chance to hit KAFKA-7703.
+        // - Avoid calling `consumer.poll(0)` which may cause KAFKA-7703.
+        var incorrectOffsets: Seq[(TopicPartition, Long, Long)] = Nil
+        var attempt = 0
+        do {
+          consumer.seekToEnd(partitions)
+          partitionOffsets = partitions.asScala.map(p => p -> consumer.position(p)).toMap
+          attempt += 1
+
+          incorrectOffsets = findIncorrectOffsets()
+          if (incorrectOffsets.nonEmpty) {
+            logWarning(""Found incorrect offsets in some partitions "" +
+              s""(partition, previous offset, fetched offset): $incorrectOffsets"")
+            if (attempt < maxOffsetFetchAttempts) {
+              logWarning(""Retrying to fetch latest offsets because of incorrect offsets"")
+              Thread.sleep(offsetFetchAttemptIntervalMs)
+            }
+          }
+        } while (incorrectOffsets.nonEmpty && attempt < maxOffsetFetchAttempts)
+
+        logDebug(s""Got latest offsets for partition : $partitionOffsets"")
+        partitionOffsets
+      }
     }
   }
 
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala
index 66ec7e0cd084a..d65b3cea632c4 100644
--- a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala
@@ -130,7 +130,7 @@ private[kafka010] class KafkaSource(
     metadataLog.get(0).getOrElse {
       val offsets = startingOffsets match {
         case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())
-        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())
+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets(None))
         case SpecificOffsetRangeLimit(p) => kafkaReader.fetchSpecificOffsets(p, reportDataLoss)
       }
       metadataLog.add(0, offsets)
@@ -148,7 +148,8 @@ private[kafka010] class KafkaSource(
     // Make sure initialPartitionOffsets is initialized
     initialPartitionOffsets
 
-    val latest = kafkaReader.fetchLatestOffsets()
+    val latest = kafkaReader.fetchLatestOffsets(
+      currentPartitionOffsets.orElse(Some(initialPartitionOffsets)))
     val offsets = maxOffsetsPerTrigger match {
       case None =>
         latest
diff --git a/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala b/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala
index 5ee76990b54f4..61cbb3285a4f0 100644
--- a/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala
+++ b/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala
@@ -329,6 +329,54 @@ abstract class KafkaMicroBatchSourceSuiteBase extends KafkaSourceSuiteBase {
     )
   }
 
+  test(""subscribe topic by pattern with topic recreation between batches"") {
+    val topicPrefix = newTopic()
+    val topic = topicPrefix + ""-good""
+    val topic2 = topicPrefix + ""-bad""
+    testUtils.createTopic(topic, partitions = 1)
+    testUtils.sendMessages(topic, Array(""1"", ""3""))
+    testUtils.createTopic(topic2, partitions = 1)
+    testUtils.sendMessages(topic2, Array(""2"", ""4""))
+
+    val reader = spark
+      .readStream
+      .format(""kafka"")
+      .option(""kafka.bootstrap.servers"", testUtils.brokerAddress)
+      .option(""kafka.metadata.max.age.ms"", ""1"")
+      .option(""kafka.default.api.timeout.ms"", ""3000"")
+      .option(""startingOffsets"", ""earliest"")
+      .option(""subscribePattern"", s""$topicPrefix-.*"")
+
+    val ds = reader.load()
+      .selectExpr(""CAST(key AS STRING)"", ""CAST(value AS STRING)"")
+      .as[(String, String)]
+      .map(kv => kv._2.toInt)
+
+    testStream(ds)(
+      StartStream(),
+      AssertOnQuery { q =>
+        q.processAllAvailable()
+        true
+      },
+      CheckAnswer(1, 2, 3, 4),
+      // Restart the stream in this test to make the test stable. When recreating a topic when a
+      // consumer is alive, it may not be able to see the recreated topic even if a fresh consumer
+      // has seen it.
+      StopStream,
+      // Recreate `topic2` and wait until it's available
+      WithOffsetSync(new TopicPartition(topic2, 0), expectedOffset = 1) { () =>
+        testUtils.deleteTopic(topic2)
+        testUtils.createTopic(topic2)
+        testUtils.sendMessages(topic2, Array(""6""))
+      },
+      StartStream(),
+      ExpectFailure[IllegalStateException](e => {
+        // The offset of `topic2` should be changed from 2 to 1
+        assert(e.getMessage.contains(""was changed from 2 to 1""))
+      })
+    )
+  }
+
   test(""ensure that initial offset are written with an extra byte in the beginning (SPARK-19517)"") {
     withTempDir { metadataPath =>
       val topic = ""kafka-initial-offset-current""


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Dec/18 18:53;githubbot;zsxwing opened a new pull request #23365: [SPARK-26267][SS] Retry when detecting incorrect offsets from Kafka (2.4)
URL: https://github.com/apache/spark/pull/23365
 
 
   ## What changes were proposed in this pull request?
   
   Backport #23324 to branch-2.4.
   
   ## How was this patch tested?
   
   Jenkins

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deadlock between TaskMemoryManager and BytesToBytesMap$MapIterator,SPARK-26265,13202243,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,qianhan,qianhan,04/Dec/18 11:31,12/Dec/22 18:11,13/Jul/23 08:48,11/Dec/18 13:13,2.3.2,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,,,,0,,,,,"The application is running on a cluster with 72000 cores and 182000G mem.

Enviroment:
|spark.dynamicAllocation.minExecutors|5|
|spark.dynamicAllocation.initialExecutors|30|
|spark.dynamicAllocation.maxExecutors|400|
|spark.executor.cores|4|
|spark.executor.memory|20g|

 

  

Stage description:

org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:364) org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422) org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357) org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:193) org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894) org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198) org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228) org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137) org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

 

jstack information as follow:

Found one Java-level deadlock: ============================= ""Thread-ScriptTransformation-Feed"": waiting to lock monitor 0x0000000000e0cb18 (object 0x00000002f1641538, a org.apache.spark.memory.TaskMemoryManager), which is held by ""Executor task launch worker for task 18899"" ""Executor task launch worker for task 18899"": waiting to lock monitor 0x0000000000e09788 (object 0x0000000302faa3b0, a org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator), which is held by ""Thread-ScriptTransformation-Feed"" Java stack information for the threads listed above: =================================================== ""Thread-ScriptTransformation-Feed"": at org.apache.spark.memory.TaskMemoryManager.freePage(TaskMemoryManager.java:332) - waiting to lock <0x00000002f1641538> (a org.apache.spark.memory.TaskMemoryManager) at org.apache.spark.memory.MemoryConsumer.freePage(MemoryConsumer.java:130) at org.apache.spark.unsafe.map.BytesToBytesMap.access$300(BytesToBytesMap.java:66) at org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:274) - locked <0x0000000302faa3b0> (a org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator) at org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:313) at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:173) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply$mcV$sp(ScriptTransformationExec.scala:281) at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformationExec.scala:270) at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformationExec.scala:270) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1995) at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread.run(ScriptTransformationExec.scala:270) ""Executor task launch worker for task 18899"": at org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.spill(BytesToBytesMap.java:345) - waiting to lock <0x0000000302faa3b0> (a org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator) at org.apache.spark.unsafe.map.BytesToBytesMap.spill(BytesToBytesMap.java:772) at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:180) - locked <0x00000002f1641538> (a org.apache.spark.memory.TaskMemoryManager) at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:283) at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117) at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:371) at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:394) at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:267) at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock.",,apachespark,dongjoon,githubbot,qianhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 15 05:55:49 UTC 2018,,,,,,,,,,"0|s015b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/18 02:58;gurwls223;Please fill more information rather than just copying and pasting the error.;;;","05/Dec/18 09:20;gurwls223;Mind if I ask:

1. reproducers
2. What kind of codes did you run?
3. can you leave some analysis about the symptoms?

Let's reopen when they are in the description.;;;","06/Dec/18 07:43;qianhan;# There are hundreds of thousand application running on our cluster per day. And this deadlock is happened only once. This cannot be reproduce easily.
 # I ran spark sql. INSERT OVERWRITE TABLE dm_abtest.rpt_live_tag_metric_daily PARTITION(date='20181129_bak') select vid, tag_name, tag_value, count(*) impr_user, avg(impr) impr_per_u, stddev_pop(impr) var_impr_per_u, avg(read) read_per_u, stddev_pop(read) var_read_per_u, avg(stay) stay_per_u, stddev_pop(stay) var_stay_per_u, sum(stay)/sum(read) stay_per_r, sum(read)/sum(impr) read_per_i, avg(finish) finish_per_u, stddev_pop(finish) var_finish_per_u from ( select vid, user_uid, user_uid_type, tag_name, tag_value, sum(impr) impr, sum(read) read, sum(stay) stay, sum(stay_count) stay_count, 0 finish from ( select transform(vid,user_uid,user_uid_type,tags,impr,read,stay,stay_count) USING 'python transform.py 111111' AS (vids,user_uid,user_uid_type,tag_name,tag_value,impr,read,stay,stay_count) from ( SELECT vid, user_uid, user_uid_type, tags, count(*) impr, sum(all_read) read, sum(video_stay) stay, sum(if(video_stay>0, 1, 0)) stay_count FROM dm_abtest.stg_live_impression_stats_daily WHERE date='20181129' and vid <> '' GROUP BY vid,user_uid,user_uid_type,tags ) t distribute by vids,user_uid,user_uid_type,tag_name,tag_value ) t lateral view explode(split(vids, ',')) b as vid group by vid,user_uid,user_uid_type,tag_name,tag_value ) t group by vid,tag_name,tag_value
 # When deadlock happen, the executor hang and do nothing.;;;","06/Dec/18 14:19;gurwls223;Thanks, [~qianhan], can you investigate and make a fix to Spark?;;;","07/Dec/18 14:12;qianhan;Okay;;;","10/Dec/18 08:18;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/23272;;;","10/Dec/18 14:54;githubbot;viirya commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240187678
 
 

 ##########
 File path: core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
 ##########
 @@ -667,4 +668,53 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
 
 Review comment:
   I've tried few ways to set a timeout logic, but don't work. The deadlock always hangs the test and timeout logic.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 14:56;githubbot;viirya commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240187678
 
 

 ##########
 File path: core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
 ##########
 @@ -667,4 +668,53 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
 
 Review comment:
   I've tried several ways to set a timeout logic, but don't work. The deadlock always hangs the test and timeout logic.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 14:56;githubbot;viirya commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240204508
 
 

 ##########
 File path: core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
 ##########
 @@ -667,4 +669,54 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
+    memoryManager.limit(PAGE_SIZE_BYTES);
+    MemoryMode mode = useOffHeapMemoryAllocator() ? MemoryMode.OFF_HEAP: MemoryMode.ON_HEAP;
+    TestMemoryConsumer c1 = new TestMemoryConsumer(taskMemoryManager, mode);
+    BytesToBytesMap map =
+      new BytesToBytesMap(taskMemoryManager, blockManager, serializerManager, 1, 0.5, 1024);
+
+    Runnable memoryConsumer = new Runnable() {
+      @Override
+      public void run() {
+        int i = 0;
+        long used = 0;
+        while (i < 10) {
+          c1.use(10000000);
+          used += 10000000;
+          i++;
+        }
+        c1.free(used);
+      }
+    };
+
+    Thread thread = new Thread(memoryConsumer);
+
+    try {
+      int i;
+      for (i = 0; i < 1024; i++) {
+        final long[] arr = new long[]{i};
+        final BytesToBytesMap.Location loc = map.lookup(arr, Platform.LONG_ARRAY_OFFSET, 8);
+        loc.append(arr, Platform.LONG_ARRAY_OFFSET, 8, arr, Platform.LONG_ARRAY_OFFSET, 8);
+      }
+
+      // Starts to require memory at another memory consumer.
+      thread.start();
+
+      BytesToBytesMap.MapIterator iter = map.destructiveIterator();
+      for (i = 0; i < 1024; i++) {
+        iter.next();
+      }
+      assertFalse(iter.hasNext());
+    } finally {
+      map.free();
+      thread.join();
 
 Review comment:
   This line just makes sure `memoryConsumer` to end and free acquired memory.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:44;githubbot;SparkQA commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445905638
 
 
   **[Test build #99914 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99914/testReport)** for PR 23272 at commit [`4c621d2`](https://github.com/apache/spark/commit/4c621d2bd36c50a10591d93ccd77bd7c0432a873).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:45;githubbot;SparkQA removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445815520
 
 
   **[Test build #99914 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99914/testReport)** for PR 23272 at commit [`4c621d2`](https://github.com/apache/spark/commit/4c621d2bd36c50a10591d93ccd77bd7c0432a873).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:46;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445906442
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:46;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445906449
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99914/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:47;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445906442
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:47;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445906449
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99914/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:54;githubbot;SparkQA commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445909073
 
 
   **[Test build #99915 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99915/testReport)** for PR 23272 at commit [`9d52320`](https://github.com/apache/spark/commit/9d52320e24077a8c94639aad6b21a4af5d3e83d9).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:55;githubbot;SparkQA removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445815525
 
 
   **[Test build #99915 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99915/testReport)** for PR 23272 at commit [`9d52320`](https://github.com/apache/spark/commit/9d52320e24077a8c94639aad6b21a4af5d3e83d9).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:56;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445909862
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:56;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445909870
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99915/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:57;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445909862
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 17:57;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-445909870
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99915/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 02:22;githubbot;cloud-fan commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240453178
 
 

 ##########
 File path: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
 ##########
 @@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
 
 Review comment:
   is it possible that this page is already freed by another consumer?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 03:28;githubbot;viirya commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446059510
 
 
   I think the page is used exclusively by the map and the iterator. So it
   could not be freed by other consumer.
   
   On Tue, Dec 11, 2018, 10:23 Wenchen Fan <notifications@github.com wrote:
   
   > *@cloud-fan* commented on this pull request.
   > ------------------------------
   >
   > In core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
   > <https://github.com/apache/spark/pull/23272#discussion_r240453178>:
   >
   > > @@ -283,6 +290,9 @@ private void advanceToNextPage() {
   >            }
   >          }
   >        }
   > +      if (pageToFree != null) {
   > +        freePage(pageToFree);
   >
   > is it possible that this page is already freed by another consumer?
   >
   > —
   > You are receiving this because you were mentioned.
   > Reply to this email directly, view it on GitHub
   > <https://github.com/apache/spark/pull/23272#pullrequestreview-183486914>,
   > or mute the thread
   > <https://github.com/notifications/unsubscribe-auth/AAEM9-_lp7w_wK9mWUYjtPT1h9o5O0rzks5u3xcSgaJpZM4ZK2_Y>
   > .
   >
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 03:33;githubbot;cloud-fan commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240463496
 
 

 ##########
 File path: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
 ##########
 @@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
 
 Review comment:
   the `MapIterator.spill` will be called by `BytesToBytesMap.spill` which will be called by other consumers.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 03:43;githubbot;dongjoon-hyun commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240464776
 
 

 ##########
 File path: core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
 ##########
 @@ -667,4 +669,54 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
+    memoryManager.limit(PAGE_SIZE_BYTES);
+    MemoryMode mode = useOffHeapMemoryAllocator() ? MemoryMode.OFF_HEAP: MemoryMode.ON_HEAP;
+    TestMemoryConsumer c1 = new TestMemoryConsumer(taskMemoryManager, mode);
+    BytesToBytesMap map =
+      new BytesToBytesMap(taskMemoryManager, blockManager, serializerManager, 1, 0.5, 1024);
+
+    Runnable memoryConsumer = new Runnable() {
+      @Override
+      public void run() {
+        int i = 0;
+        long used = 0;
+        while (i < 10) {
+          c1.use(10000000);
+          used += 10000000;
+          i++;
+        }
+        c1.free(used);
+      }
+    };
+
+    Thread thread = new Thread(memoryConsumer);
 
 Review comment:
   Shall we use a short form (10 lines) instead of line 680 ~ 694?
   ```java
       Thread thread = new Thread(() -> {
         int i = 0;
         long used = 0;
         while (i < 10) {
           c1.use(10000000);
           used += 10000000;
           i++;
         }
         c1.free(used);
       });
   ```

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 03:52;githubbot;dongjoon-hyun commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240465896
 
 

 ##########
 File path: core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
 ##########
 @@ -667,4 +669,54 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
+    memoryManager.limit(PAGE_SIZE_BYTES);
+    MemoryMode mode = useOffHeapMemoryAllocator() ? MemoryMode.OFF_HEAP: MemoryMode.ON_HEAP;
+    TestMemoryConsumer c1 = new TestMemoryConsumer(taskMemoryManager, mode);
+    BytesToBytesMap map =
+      new BytesToBytesMap(taskMemoryManager, blockManager, serializerManager, 1, 0.5, 1024);
+
+    Runnable memoryConsumer = new Runnable() {
+      @Override
+      public void run() {
+        int i = 0;
+        long used = 0;
+        while (i < 10) {
+          c1.use(10000000);
+          used += 10000000;
+          i++;
+        }
+        c1.free(used);
+      }
+    };
+
+    Thread thread = new Thread(memoryConsumer);
+
+    try {
+      int i;
+      for (i = 0; i < 1024; i++) {
 
 Review comment:
   Let's use `for (int i = 0; ...` here and line 708 because `int i` is not referenced outside of `for` loop.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:04;githubbot;dongjoon-hyun commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240465896
 
 

 ##########
 File path: core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
 ##########
 @@ -667,4 +669,54 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
+    memoryManager.limit(PAGE_SIZE_BYTES);
+    MemoryMode mode = useOffHeapMemoryAllocator() ? MemoryMode.OFF_HEAP: MemoryMode.ON_HEAP;
+    TestMemoryConsumer c1 = new TestMemoryConsumer(taskMemoryManager, mode);
+    BytesToBytesMap map =
+      new BytesToBytesMap(taskMemoryManager, blockManager, serializerManager, 1, 0.5, 1024);
+
+    Runnable memoryConsumer = new Runnable() {
+      @Override
+      public void run() {
+        int i = 0;
+        long used = 0;
+        while (i < 10) {
+          c1.use(10000000);
+          used += 10000000;
+          i++;
+        }
+        c1.free(used);
+      }
+    };
+
+    Thread thread = new Thread(memoryConsumer);
+
+    try {
+      int i;
+      for (i = 0; i < 1024; i++) {
 
 Review comment:
   ~Let's use `for (int i = 0; ...` here and line 708 because `int i` is not referenced outside of `for` loop.~
   Never mind. I found that this is the convention in this test suite.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:21;githubbot;viirya commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446067447
 
 
   Oh, you meant that the page is freed by other using this map or iterator.
   Is it a problem?
   
   I think it should not be a case that more than one consumers free the same
   page at the same time.
   
   On Tue, Dec 11, 2018, 11:34 Wenchen Fan <notifications@github.com wrote:
   
   > *@cloud-fan* commented on this pull request.
   > ------------------------------
   >
   > In core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
   > <https://github.com/apache/spark/pull/23272#discussion_r240463496>:
   >
   > > @@ -283,6 +290,9 @@ private void advanceToNextPage() {
   >            }
   >          }
   >        }
   > +      if (pageToFree != null) {
   > +        freePage(pageToFree);
   >
   > the MapIterator.spill will be called by BytesToBytesMap.spill which will
   > be called by other consumers.
   >
   > —
   > You are receiving this because you were mentioned.
   > Reply to this email directly, view it on GitHub
   > <https://github.com/apache/spark/pull/23272#discussion_r240463496>, or mute
   > the thread
   > <https://github.com/notifications/unsubscribe-auth/AAEM91AsLKsA5zS0gXEhip3GuUcVjJ7-ks5u3yfAgaJpZM4ZK2_Y>
   > .
   >
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:25;githubbot;viirya commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446067993
 
 
   If you worry that the page is to be freed by the other consumer using the
   map iterator and also the map iterator itself, because I am not in front of
   laptop so I can't check it. But I guess freePage should already cover it.
   
   On Tue, Dec 11, 2018, 11:34 Wenchen Fan <notifications@github.com wrote:
   
   > *@cloud-fan* commented on this pull request.
   > ------------------------------
   >
   > In core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
   > <https://github.com/apache/spark/pull/23272#discussion_r240463496>:
   >
   > > @@ -283,6 +290,9 @@ private void advanceToNextPage() {
   >            }
   >          }
   >        }
   > +      if (pageToFree != null) {
   > +        freePage(pageToFree);
   >
   > the MapIterator.spill will be called by BytesToBytesMap.spill which will
   > be called by other consumers.
   >
   > —
   > You are receiving this because you were mentioned.
   > Reply to this email directly, view it on GitHub
   > <https://github.com/apache/spark/pull/23272#discussion_r240463496>, or mute
   > the thread
   > <https://github.com/notifications/unsubscribe-auth/AAEM91AsLKsA5zS0gXEhip3GuUcVjJ7-ks5u3yfAgaJpZM4ZK2_Y>
   > .
   >
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 07:59;githubbot;viirya commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240503360
 
 

 ##########
 File path: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
 ##########
 @@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
 
 Review comment:
   The page to free (`currentPage`) is removed from `dataPages` and advanced to next page when locking on the `MapIterator` object. The locking will prevent the same page to be freed by calling `spill` at another consumer.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:06;githubbot;SparkQA commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446108367
 
 
   **[Test build #99956 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99956/testReport)** for PR 23272 at commit [`0405527`](https://github.com/apache/spark/commit/04055278a02800c6d3ac67ddb2d9acc2c3baa18d).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:06;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446108377
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:06;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446108381
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5959/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:07;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446108377
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:07;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446108381
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5959/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:34;githubbot;cloud-fan commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240511960
 
 

 ##########
 File path: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
 ##########
 @@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
 
 Review comment:
   ah I misread the code. Can you move `synchronized` from `spill` method and put it in method definition?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:10;githubbot;viirya commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240545803
 
 

 ##########
 File path: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
 ##########
 @@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
 
 Review comment:
   Moved.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:15;githubbot;SparkQA commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446147339
 
 
   **[Test build #99966 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99966/testReport)** for PR 23272 at commit [`0849083`](https://github.com/apache/spark/commit/08490838470d4f8e291d2d94ebadf32576a60205).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:15;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446147362
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:15;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446147370
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5966/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:16;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446147362
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:16;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446147370
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5966/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:39;githubbot;cloud-fan commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446155283
 
 
   LGTM. last question: does the test always reproduce the bug? Or it has some randomness?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:24;githubbot;SparkQA commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446184428
 
 
   **[Test build #99956 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99956/testReport)** for PR 23272 at commit [`0405527`](https://github.com/apache/spark/commit/04055278a02800c6d3ac67ddb2d9acc2c3baa18d).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:25;githubbot;SparkQA removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446108367
 
 
   **[Test build #99956 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99956/testReport)** for PR 23272 at commit [`0405527`](https://github.com/apache/spark/commit/04055278a02800c6d3ac67ddb2d9acc2c3baa18d).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:26;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446184932
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:26;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446184937
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99956/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:27;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446184932
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:27;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446184937
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99956/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:48;githubbot;viirya commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446190635
 
 
   > last question: does the test always reproduce the bug? Or it has some randomness?
   
   If without the change, as I tried it locally 10 times, the test can reproduce the bug 10 times. But I'm not sure if it is 100% to reproduce the bug.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:49;githubbot;viirya edited a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446190635
 
 
   > last question: does the test always reproduce the bug? Or it has some randomness?
   
   If without the change, as I tried it locally 10 times, the test can reproduce the bug 10 times. But I'm not sure if it is 100% to reproduce the bug. I think we can't always to reproduce a deadlock like this.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:06;githubbot;cloud-fan commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446195615
 
 
   thanks, merging to master!
   
   Can you send a new PR for 2.4 without the `synchronized` move around?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:11;githubbot;viirya commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446196909
 
 
   > Can you send a new PR for 2.4 without the synchronized move around?
   
   Ok.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:11;githubbot;viirya edited a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446196909
 
 
   > Can you send a new PR for 2.4 without the synchronized move around?
   
   Ok. Thanks.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:13;githubbot;asfgit closed pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
index 405e529464152..fbba002f1f80f 100644
--- a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
+++ b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
@@ -255,11 +255,18 @@ private MapIterator(int numRecords, Location loc, boolean destructive) {
     }
 
     private void advanceToNextPage() {
+      // SPARK-26265: We will first lock this `MapIterator` and then `TaskMemoryManager` when going
+      // to free a memory page by calling `freePage`. At the same time, it is possibly that another
+      // memory consumer first locks `TaskMemoryManager` and then this `MapIterator` when it
+      // acquires memory and causes spilling on this `MapIterator`. To avoid deadlock here, we keep
+      // reference to the page to free and free it after releasing the lock of `MapIterator`.
+      MemoryBlock pageToFree = null;
+
       synchronized (this) {
         int nextIdx = dataPages.indexOf(currentPage) + 1;
         if (destructive && currentPage != null) {
           dataPages.remove(currentPage);
-          freePage(currentPage);
+          pageToFree = currentPage;
           nextIdx --;
         }
         if (dataPages.size() > nextIdx) {
@@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
+      }
     }
 
     @Override
@@ -329,52 +339,50 @@ public Location next() {
       }
     }
 
-    public long spill(long numBytes) throws IOException {
-      synchronized (this) {
-        if (!destructive || dataPages.size() == 1) {
-          return 0L;
-        }
+    public synchronized long spill(long numBytes) throws IOException {
+      if (!destructive || dataPages.size() == 1) {
+        return 0L;
+      }
 
-        updatePeakMemoryUsed();
+      updatePeakMemoryUsed();
 
-        // TODO: use existing ShuffleWriteMetrics
-        ShuffleWriteMetrics writeMetrics = new ShuffleWriteMetrics();
+      // TODO: use existing ShuffleWriteMetrics
+      ShuffleWriteMetrics writeMetrics = new ShuffleWriteMetrics();
 
-        long released = 0L;
-        while (dataPages.size() > 0) {
-          MemoryBlock block = dataPages.getLast();
-          // The currentPage is used, cannot be released
-          if (block == currentPage) {
-            break;
-          }
+      long released = 0L;
+      while (dataPages.size() > 0) {
+        MemoryBlock block = dataPages.getLast();
+        // The currentPage is used, cannot be released
+        if (block == currentPage) {
+          break;
+        }
 
-          Object base = block.getBaseObject();
-          long offset = block.getBaseOffset();
-          int numRecords = UnsafeAlignedOffset.getSize(base, offset);
-          int uaoSize = UnsafeAlignedOffset.getUaoSize();
-          offset += uaoSize;
-          final UnsafeSorterSpillWriter writer =
-            new UnsafeSorterSpillWriter(blockManager, 32 * 1024, writeMetrics, numRecords);
-          while (numRecords > 0) {
-            int length = UnsafeAlignedOffset.getSize(base, offset);
-            writer.write(base, offset + uaoSize, length, 0);
-            offset += uaoSize + length + 8;
-            numRecords--;
-          }
-          writer.close();
-          spillWriters.add(writer);
+        Object base = block.getBaseObject();
+        long offset = block.getBaseOffset();
+        int numRecords = UnsafeAlignedOffset.getSize(base, offset);
+        int uaoSize = UnsafeAlignedOffset.getUaoSize();
+        offset += uaoSize;
+        final UnsafeSorterSpillWriter writer =
+                new UnsafeSorterSpillWriter(blockManager, 32 * 1024, writeMetrics, numRecords);
+        while (numRecords > 0) {
+          int length = UnsafeAlignedOffset.getSize(base, offset);
+          writer.write(base, offset + uaoSize, length, 0);
+          offset += uaoSize + length + 8;
+          numRecords--;
+        }
+        writer.close();
+        spillWriters.add(writer);
 
-          dataPages.removeLast();
-          released += block.size();
-          freePage(block);
+        dataPages.removeLast();
+        released += block.size();
+        freePage(block);
 
-          if (released >= numBytes) {
-            break;
-          }
+        if (released >= numBytes) {
+          break;
         }
-
-        return released;
       }
+
+      return released;
     }
 
     @Override
diff --git a/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java b/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java
index 0bbaea6b834b8..6aa577d1bf797 100644
--- a/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java
+++ b/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java
@@ -38,12 +38,12 @@ public long spill(long size, MemoryConsumer trigger) throws IOException {
     return used;
   }
 
-  void use(long size) {
+  public void use(long size) {
     long got = taskMemoryManager.acquireExecutionMemory(size, this);
     used += got;
   }
 
-  void free(long size) {
+  public void free(long size) {
     used -= size;
     taskMemoryManager.releaseExecutionMemory(size, this);
   }
diff --git a/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java b/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
index aa29232e73e13..089dda7568a73 100644
--- a/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
+++ b/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
@@ -33,6 +33,8 @@
 
 import org.apache.spark.SparkConf;
 import org.apache.spark.executor.ShuffleWriteMetrics;
+import org.apache.spark.memory.MemoryMode;
+import org.apache.spark.memory.TestMemoryConsumer;
 import org.apache.spark.memory.TaskMemoryManager;
 import org.apache.spark.memory.TestMemoryManager;
 import org.apache.spark.network.util.JavaUtils;
@@ -667,4 +669,49 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
+    memoryManager.limit(PAGE_SIZE_BYTES);
+    MemoryMode mode = useOffHeapMemoryAllocator() ? MemoryMode.OFF_HEAP: MemoryMode.ON_HEAP;
+    TestMemoryConsumer c1 = new TestMemoryConsumer(taskMemoryManager, mode);
+    BytesToBytesMap map =
+      new BytesToBytesMap(taskMemoryManager, blockManager, serializerManager, 1, 0.5, 1024);
+
+    Thread thread = new Thread(() -> {
+      int i = 0;
+      long used = 0;
+      while (i < 10) {
+        c1.use(10000000);
+        used += 10000000;
+        i++;
+      }
+      c1.free(used);
+    });
+
+    try {
+      int i;
+      for (i = 0; i < 1024; i++) {
+        final long[] arr = new long[]{i};
+        final BytesToBytesMap.Location loc = map.lookup(arr, Platform.LONG_ARRAY_OFFSET, 8);
+        loc.append(arr, Platform.LONG_ARRAY_OFFSET, 8, arr, Platform.LONG_ARRAY_OFFSET, 8);
+      }
+
+      // Starts to require memory at another memory consumer.
+      thread.start();
+
+      BytesToBytesMap.MapIterator iter = map.destructiveIterator();
+      for (i = 0; i < 1024; i++) {
+        iter.next();
+      }
+      assertFalse(iter.hasNext());
+    } finally {
+      map.free();
+      thread.join();
+      for (File spillFile : spillFilesCreated) {
+        assertFalse(""Spill file "" + spillFile.getPath() + "" was not cleaned up"",
+          spillFile.exists());
+      }
+    }
+  }
+
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:53;githubbot;SparkQA commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446209546
 
 
   **[Test build #99966 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99966/testReport)** for PR 23272 at commit [`0849083`](https://github.com/apache/spark/commit/08490838470d4f8e291d2d94ebadf32576a60205).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:54;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446209885
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:54;githubbot;AmplabJenkins commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446209891
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99966/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:54;githubbot;SparkQA removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446147339
 
 
   **[Test build #99966 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99966/testReport)** for PR 23272 at commit [`0849083`](https://github.com/apache/spark/commit/08490838470d4f8e291d2d94ebadf32576a60205).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:54;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446209885
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:55;githubbot;AmplabJenkins removed a comment on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446209891
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99966/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:08;githubbot;viirya opened a new pull request #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289
 
 
   ## What changes were proposed in this pull request?
   
   In `BytesToBytesMap.MapIterator.advanceToNextPage`, We will first lock this `MapIterator` and then `TaskMemoryManager` when going to free a memory page by calling `freePage`. At the same time, it is possibly that another memory consumer first locks `TaskMemoryManager` and then this `MapIterator` when it acquires memory and causes spilling on this `MapIterator`.
   
   So it ends with the `MapIterator` object holds lock to the `MapIterator` object and waits for lock on `TaskMemoryManager`, and the other consumer holds lock to `TaskMemoryManager` and waits for lock on the `MapIterator` object.
   
   To avoid deadlock here, this patch proposes to keep reference to the page to free and free it after releasing the lock of `MapIterator`.
   
   This backports the fix to branch-2.4.
   
   ## How was this patch tested?
   
    Added test and manually test by running the test 100 times to make sure there is no deadlock.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:08;githubbot;viirya commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446214510
 
 
   cc @cloud-fan 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:09;githubbot;viirya commented on issue #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#issuecomment-446214942
 
 
   I think the failed tests are unrelated. cc @cloud-fan 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:10;githubbot;SparkQA commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446215048
 
 
   **[Test build #99978 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99978/testReport)** for PR 23289 at commit [`e408ea6`](https://github.com/apache/spark/commit/e408ea6dfe77f65f71038a196c5bfd371b970052).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:16;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446216964
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:16;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446216975
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5977/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:17;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446216964
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:26;githubbot;SparkQA commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446220331
 
 
   **[Test build #99978 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99978/testReport)** for PR 23289 at commit [`e408ea6`](https://github.com/apache/spark/commit/e408ea6dfe77f65f71038a196c5bfd371b970052).
    * This patch **fails Java style tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:26;githubbot;SparkQA removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446215048
 
 
   **[Test build #99978 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99978/testReport)** for PR 23289 at commit [`e408ea6`](https://github.com/apache/spark/commit/e408ea6dfe77f65f71038a196c5bfd371b970052).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:26;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446220380
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:26;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446220388
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99978/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:27;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446220380
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:28;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446220388
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99978/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:44;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446226764
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:44;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446226774
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5979/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:44;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446226764
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:44;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446226774
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5979/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:44;githubbot;SparkQA commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446226854
 
 
   **[Test build #99980 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99980/testReport)** for PR 23289 at commit [`d520a97`](https://github.com/apache/spark/commit/d520a97a584bd4f17c8ff0f62d18794a214fa38f).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 18:27;githubbot;kiszk commented on a change in pull request #23272: [SPARK-26265][Core] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23272#discussion_r240735509
 
 

 ##########
 File path: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
 ##########
 @@ -283,6 +290,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
 
 Review comment:
   @viirya @cloud-fan Sorry for late comment. But, I have one question.
   
   Before this PR, `freePage(currentPage)` is always called if `if (destructive && currentPage != null)` is satisfied.  
   After this PR, `freePage(pageToFree)` may not be called if `if (dataPages.size() > nextIdx) { ...}` throw an exception. In that case, I am curious whether `pageToFree` is collected somewhere or not.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 19:21;githubbot;SparkQA commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446327823
 
 
   **[Test build #99980 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99980/testReport)** for PR 23289 at commit [`d520a97`](https://github.com/apache/spark/commit/d520a97a584bd4f17c8ff0f62d18794a214fa38f).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 19:22;githubbot;SparkQA removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446226854
 
 
   **[Test build #99980 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99980/testReport)** for PR 23289 at commit [`d520a97`](https://github.com/apache/spark/commit/d520a97a584bd4f17c8ff0f62d18794a214fa38f).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 19:22;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446328319
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 19:22;githubbot;AmplabJenkins commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446328326
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99980/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 19:23;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446328326
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99980/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 19:23;githubbot;AmplabJenkins removed a comment on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446328319
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 20:22;githubbot;dongjoon-hyun commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446348301
 
 
   Thanks. Merged to branch-2.4.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Dec/18 00:46;githubbot;viirya commented on issue #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289#issuecomment-446418768
 
 
   Thanks @dongjoon-hyun @cloud-fan 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Dec/18 00:46;githubbot;viirya closed pull request #23289: [SPARK-26265][Core][BRANCH-2.4] Fix deadlock in BytesToBytesMap.MapIterator when locking both BytesToBytesMap.MapIterator and TaskMemoryManager
URL: https://github.com/apache/spark/pull/23289
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
index 9b6cbab38cbcc..64650336c9371 100644
--- a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
+++ b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
@@ -267,11 +267,18 @@ private MapIterator(int numRecords, Location loc, boolean destructive) {
     }
 
     private void advanceToNextPage() {
+      // SPARK-26265: We will first lock this `MapIterator` and then `TaskMemoryManager` when going
+      // to free a memory page by calling `freePage`. At the same time, it is possibly that another
+      // memory consumer first locks `TaskMemoryManager` and then this `MapIterator` when it
+      // acquires memory and causes spilling on this `MapIterator`. To avoid deadlock here, we keep
+      // reference to the page to free and free it after releasing the lock of `MapIterator`.
+      MemoryBlock pageToFree = null;
+
       synchronized (this) {
         int nextIdx = dataPages.indexOf(currentPage) + 1;
         if (destructive && currentPage != null) {
           dataPages.remove(currentPage);
-          freePage(currentPage);
+          pageToFree = currentPage;
           nextIdx --;
         }
         if (dataPages.size() > nextIdx) {
@@ -295,6 +302,9 @@ private void advanceToNextPage() {
           }
         }
       }
+      if (pageToFree != null) {
+        freePage(pageToFree);
+      }
     }
 
     @Override
diff --git a/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java b/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java
index 0bbaea6b834b8..6aa577d1bf797 100644
--- a/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java
+++ b/core/src/test/java/org/apache/spark/memory/TestMemoryConsumer.java
@@ -38,12 +38,12 @@ public long spill(long size, MemoryConsumer trigger) throws IOException {
     return used;
   }
 
-  void use(long size) {
+  public void use(long size) {
     long got = taskMemoryManager.acquireExecutionMemory(size, this);
     used += got;
   }
 
-  void free(long size) {
+  public void free(long size) {
     used -= size;
     taskMemoryManager.releaseExecutionMemory(size, this);
   }
diff --git a/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java b/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
index 53a233f698c7a..278d28f7bf479 100644
--- a/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
+++ b/core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java
@@ -33,6 +33,8 @@
 
 import org.apache.spark.SparkConf;
 import org.apache.spark.executor.ShuffleWriteMetrics;
+import org.apache.spark.memory.MemoryMode;
+import org.apache.spark.memory.TestMemoryConsumer;
 import org.apache.spark.memory.TaskMemoryManager;
 import org.apache.spark.memory.TestMemoryManager;
 import org.apache.spark.network.util.JavaUtils;
@@ -667,4 +669,49 @@ public void testPeakMemoryUsed() {
     }
   }
 
+  @Test
+  public void avoidDeadlock() throws InterruptedException {
+    memoryManager.limit(PAGE_SIZE_BYTES);
+    MemoryMode mode = useOffHeapMemoryAllocator() ? MemoryMode.OFF_HEAP: MemoryMode.ON_HEAP;
+    TestMemoryConsumer c1 = new TestMemoryConsumer(taskMemoryManager, mode);
+    BytesToBytesMap map =
+      new BytesToBytesMap(taskMemoryManager, blockManager, serializerManager, 1, 0.5, 1024, false);
+
+    Thread thread = new Thread(() -> {
+      int i = 0;
+      long used = 0;
+      while (i < 10) {
+        c1.use(10000000);
+        used += 10000000;
+        i++;
+      }
+      c1.free(used);
+    });
+
+    try {
+      int i;
+      for (i = 0; i < 1024; i++) {
+        final long[] arr = new long[]{i};
+        final BytesToBytesMap.Location loc = map.lookup(arr, Platform.LONG_ARRAY_OFFSET, 8);
+        loc.append(arr, Platform.LONG_ARRAY_OFFSET, 8, arr, Platform.LONG_ARRAY_OFFSET, 8);
+      }
+
+      // Starts to require memory at another memory consumer.
+      thread.start();
+
+      BytesToBytesMap.MapIterator iter = map.destructiveIterator();
+      for (i = 0; i < 1024; i++) {
+        iter.next();
+      }
+      assertFalse(iter.hasNext());
+    } finally {
+      map.free();
+      thread.join();
+      for (File spillFile : spillFilesCreated) {
+        assertFalse(""Spill file "" + spillFile.getPath() + "" was not cleaned up"",
+          spillFile.exists());
+      }
+    }
+  }
+
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Dec/18 04:06;githubbot;viirya opened a new pull request #23294: [SPARK-26265][Core][Followup] Put freePage into a finally block
URL: https://github.com/apache/spark/pull/23294
 
 
   ## What changes were proposed in this pull request?
   
   Based on the [comment](https://github.com/apache/spark/pull/23272#discussion_r240735509), it seems to be better to put `freePage` into a `finally` block. This patch as a follow-up to do so.
   
   ## How was this patch tested?
   
   Existing tests.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Dec/18 05:55;githubbot;asfgit closed pull request #23294: [SPARK-26265][Core][Followup] Put freePage into a finally block
URL: https://github.com/apache/spark/pull/23294
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
index fbba002f1f80f..7df8aafb2b674 100644
--- a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
+++ b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java
@@ -262,36 +262,39 @@ private void advanceToNextPage() {
       // reference to the page to free and free it after releasing the lock of `MapIterator`.
       MemoryBlock pageToFree = null;
 
-      synchronized (this) {
-        int nextIdx = dataPages.indexOf(currentPage) + 1;
-        if (destructive && currentPage != null) {
-          dataPages.remove(currentPage);
-          pageToFree = currentPage;
-          nextIdx --;
-        }
-        if (dataPages.size() > nextIdx) {
-          currentPage = dataPages.get(nextIdx);
-          pageBaseObject = currentPage.getBaseObject();
-          offsetInPage = currentPage.getBaseOffset();
-          recordsInPage = UnsafeAlignedOffset.getSize(pageBaseObject, offsetInPage);
-          offsetInPage += UnsafeAlignedOffset.getUaoSize();
-        } else {
-          currentPage = null;
-          if (reader != null) {
-            handleFailedDelete();
+      try {
+        synchronized (this) {
+          int nextIdx = dataPages.indexOf(currentPage) + 1;
+          if (destructive && currentPage != null) {
+            dataPages.remove(currentPage);
+            pageToFree = currentPage;
+            nextIdx--;
           }
-          try {
-            Closeables.close(reader, /* swallowIOException = */ false);
-            reader = spillWriters.getFirst().getReader(serializerManager);
-            recordsInPage = -1;
-          } catch (IOException e) {
-            // Scala iterator does not handle exception
-            Platform.throwException(e);
+          if (dataPages.size() > nextIdx) {
+            currentPage = dataPages.get(nextIdx);
+            pageBaseObject = currentPage.getBaseObject();
+            offsetInPage = currentPage.getBaseOffset();
+            recordsInPage = UnsafeAlignedOffset.getSize(pageBaseObject, offsetInPage);
+            offsetInPage += UnsafeAlignedOffset.getUaoSize();
+          } else {
+            currentPage = null;
+            if (reader != null) {
+              handleFailedDelete();
+            }
+            try {
+              Closeables.close(reader, /* swallowIOException = */ false);
+              reader = spillWriters.getFirst().getReader(serializerManager);
+              recordsInPage = -1;
+            } catch (IOException e) {
+              // Scala iterator does not handle exception
+              Platform.throwException(e);
+            }
           }
         }
-      }
-      if (pageToFree != null) {
-        freePage(pageToFree);
+      } finally {
+        if (pageToFree != null) {
+          freePage(pageToFree);
+        }
       }
     }
 


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;"
Add proper labels when deleting pods,SPARK-26256,13201993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,skonto,skonto,03/Dec/18 13:32,17/May/20 18:23,13/Jul/23 08:48,03/Dec/18 22:57,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Kubernetes,Spark Core,,,0,,,,,"As discussed here: [https://github.com/apache/spark/pull/23136#discussion_r236463330]

we need to add proper labels to avoid killing executors belonging to other jobs.",,apachespark,skonto,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 03 22:57:41 UTC 2018,,,,,,,,,,"0|s013tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/18 13:32;skonto;Will create a PR shortly.;;;","03/Dec/18 15:29;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/23209;;;","03/Dec/18 22:57;vanzin;Issue resolved by pull request 23209
[https://github.com/apache/spark/pull/23209];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom error/exception is not thrown for the SQL tab when UI filters are added in spark-sql launch,SPARK-26255,13201959,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chakravarthi,koolsen@gmail.com,koolsen@gmail.com,03/Dec/18 10:55,12/Dec/22 18:11,13/Jul/23 08:48,17/Dec/18 17:47,2.3.2,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,Web UI,,,0,,,,,"【Detailed description】：Custom error is not thrown for the SQL tab when UI filters are added in spark-sql launch
 【Precondition】：
 1.Cluster is up and running【Test step】：
 1. Launch spark sql as below:

[spark-sql --master yarn --conf spark.ui.filters=org.apache.hadoop.security.authentication.server.AuthenticationFilter --conf spark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params=""type=simple""]

2. Go to Yarn application list UI link
 3. Launch the application master for the Spark-SQL app ID
 4. It will display an error 
 5. Append /executors, /stages, /jobs, /environment, /SQL

【Expect Output】：An error should be displayed ""An error has occurred. Please check for all the TABS 

 【Actual Output】：The error message is displayed  for all the tabs except SQL tab .","【Test Environment】：
Server OS :-SUSE 
No. of Cluster Node:-3 
Spark Version:- 2.3.2
Hadoop Version:-3.1",chakravarthi,githubbot,koolsen@gmail.com,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/18 16:15;chakravarthi;command.png;https://issues.apache.org/jira/secure/attachment/12951698/command.png","13/Dec/18 16:14;chakravarthi;logs_before_fix.png;https://issues.apache.org/jira/secure/attachment/12951697/logs_before_fix.png","13/Dec/18 16:14;chakravarthi;ui_befofre_fix.png;https://issues.apache.org/jira/secure/attachment/12951696/ui_befofre_fix.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 17 17:50:46 UTC 2018,,,,,,,,,,"0|s013m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/18 10:57;chakravarthi;I will be working on this issue.;;;","04/Dec/18 03:09;gurwls223;it would be great if some snapshots from UI is uploaded here.;;;","13/Dec/18 16:07;githubbot;chakravarthiT opened a new pull request #23312: [SPARK-26255]Custom error/exception is not thrown for the SQL tab when UI filters are added in spark-sql launch
URL: https://github.com/apache/spark/pull/23312
 
 
   
   
   ## What changes were proposed in this pull request?
   
   User specified filters are not applied to SQL tab in yarn mode, as it is overridden by the yarn AmIp filter.
   So we need to append user provided filters (spark.ui.filters) with yarn filter.
   
   ## How was this patch tested?
   
   【Test step】：
   
   1)  Launch spark sql with authentication filter as below:
   
   2)  spark-sql --master yarn --conf spark.ui.filters=org.apache.hadoop.security.authentication.server.AuthenticationFilter --conf spark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params=""type=simple""
   
   3)  Go to Yarn application list UI link
   
   4) Launch the application master for the Spark-SQL app ID and access all the tabs by appending tab name.
   
   5) It will display an error for all tabs including SQL tab.(before able to access SQL tab,as Authentication filter is not applied for SQL tab)
   
   6) Also can be verified with info logs,that Authentication filter applied to SQL tab.(before it is not applied).
   
   
   I have attached the behaviour below in following order..
   
   1) Command used
   2) Before fix (logs and UI)
   3) After fix (logs and UI)
   
   
   **1) COMMAND USED**:
   
   launching spark-sql with authentication filter.
   
   ![image](https://user-images.githubusercontent.com/45845595/49947295-e7e97400-ff16-11e8-8c9a-10659487ddee.png)
   
   
   **2) BEFORE FIX:**
   
   **UI result:**
   able to access SQL tab.
   
   ![image](https://user-images.githubusercontent.com/45845595/49948398-62b38e80-ff19-11e8-95dc-e74f9e3c2ba7.png)
   
    **logs**:
   authentication filter not applied to SQL tab.
   
   ![image](https://user-images.githubusercontent.com/45845595/49947343-ff286180-ff16-11e8-9de0-3f8db140bc32.png)
   
   
   **3) AFTER FIX:**
   
   **UI result**:
   
   Not able to access SQL tab.
   
   ![image](https://user-images.githubusercontent.com/45845595/49947360-0d767d80-ff17-11e8-9e9e-a95311949164.png)
   
   **in logs**:
   
   Both yarn filter and Authentication filter applied to SQL tab.
   
   ![image](https://user-images.githubusercontent.com/45845595/49947377-1a936c80-ff17-11e8-9f44-700eb3dc0ded.png)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Dec/18 16:16;chakravarthi;[~hyukjin.kwon] I have attached the snapshots;;;","17/Dec/18 17:47;vanzin;Issue resolved by pull request 23312
[https://github.com/apache/spark/pull/23312];;;","17/Dec/18 17:50;githubbot;asfgit closed pull request #23312: [SPARK-26255][YARN] Apply user provided UI filters  to SQL tab in yarn mode
URL: https://github.com/apache/spark/pull/23312
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala
index 67c36aac49266..1289d4be79ea4 100644
--- a/resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala
@@ -168,8 +168,10 @@ private[spark] abstract class YarnSchedulerBackend(
       filterName != null && filterName.nonEmpty &&
       filterParams != null && filterParams.nonEmpty
     if (hasFilter) {
+      // SPARK-26255: Append user provided filters(spark.ui.filters) with yarn filter.
+      val allFilters = filterName + "","" + conf.get(""spark.ui.filters"", """")
       logInfo(s""Add WebUI Filter. $filterName, $filterParams, $proxyBase"")
-      conf.set(""spark.ui.filters"", filterName)
+      conf.set(""spark.ui.filters"", allFilters)
       filterParams.foreach { case (k, v) => conf.set(s""spark.$filterName.param.$k"", v) }
       scheduler.sc.ui.foreach { ui => JettyUtils.addFilters(ui.getHandlers, conf) }
     }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task Summary Metrics Table on Stage Page shows empty table when no data is present,SPARK-26253,13201930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pgandhi,pgandhi,pgandhi,03/Dec/18 08:08,03/Dec/18 13:53,13/Jul/23 08:48,03/Dec/18 13:53,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Web UI,,,,0,,,,,Task Summary Metrics Table on Stage Page shows empty table when no data is present instead of showing a message. ,,apachespark,pgandhi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21809,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 03 13:53:32 UTC 2018,,,,,,,,,,"0|s013fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/18 08:30;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/23205;;;","03/Dec/18 13:50;srowen;This should just be a follow-up to SPARK-21089, not a new issue.;;;","03/Dec/18 13:53;srowen;Issue resolved by pull request 23205
[https://github.com/apache/spark/pull/23205];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect decimal value with java beans and first/last/max... functions,SPARK-26233,13201615,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,mcanes,mcanes,30/Nov/18 13:05,12/Dec/22 18:10,13/Jul/23 08:48,04/Dec/18 18:38,2.0.2,2.1.3,2.2.2,2.3.1,2.4.0,,,,,,,,,,,,,2.2.3,2.3.3,2.4.1,3.0.0,SQL,,,,0,correctness,,,,"Decimal values from Java beans are incorrectly scaled when used with functions like first/last/max...

This problem came because Encoders.bean always set Decimal values as _DecimalType(this.MAX_PRECISION(), 18)._

Usually it's not a problem if you use numeric functions like *sum* but for functions like *first*/*last*/*max*... it is a problem.

How to reproduce this error:

Using this class as an example:
{code:java}
public class Foo implements Serializable {

  private String group;
  private BigDecimal var;

  public BigDecimal getVar() {
    return var;
  }

  public void setVar(BigDecimal var) {
    this.var = var;
  }

  public String getGroup() {
    return group;
  }

  public void setGroup(String group) {
    this.group = group;
  }
}
{code}
 

And a dummy code to create some objects:
{code:java}
Dataset<Foo> ds = spark.range(5)
    .map(l -> {
      Foo foo = new Foo();
      foo.setGroup("""" + l);
      foo.setVar(BigDecimal.valueOf(l + 0.1111));
      return foo;
    }, Encoders.bean(Foo.class));
ds.printSchema();
ds.show();

+-----+------+
|group| var|
+-----+------+
| 0|0.1111|
| 1|1.1111|
| 2|2.1111|
| 3|3.1111|
| 4|4.1111|
+-----+------+
{code}
We can see that the DecimalType is precision 38 and 18 scale and all values are show correctly.

But if we use a first function, they are scaled incorrectly:
{code:java}
ds.groupBy(col(""group""))
    .agg(
        first(""var"")
    )
    .show();


+-----+-----------------+
|group|first(var, false)|
+-----+-----------------+
| 3| 3.1111E-14|
| 0| 1.111E-15|
| 1| 1.1111E-14|
| 4| 4.1111E-14|
| 2| 2.1111E-14|
+-----+-----------------+
{code}
This incorrect behavior cannot be reproduced if we use ""numerical ""functions like sum or if the column is cast a new Decimal Type.
{code:java}
ds.groupBy(col(""group""))
    .agg(
        sum(""var"")
    )
    .show();

+-----+--------------------+
|group| sum(var)|
+-----+--------------------+
| 3|3.111100000000000000|
| 0|0.111100000000000000|
| 1|1.111100000000000000|
| 4|4.111100000000000000|
| 2|2.111100000000000000|
+-----+--------------------+

ds.groupBy(col(""group""))
    .agg(
        first(col(""var"").cast(new DecimalType(38, 8)))
    )
    .show();

+-----+----------------------------------------+
|group|first(CAST(var AS DECIMAL(38,8)), false)|
+-----+----------------------------------------+
| 3| 3.11110000|
| 0| 0.11110000|
| 1| 1.11110000|
| 4| 4.11110000|
| 2| 2.11110000|
+-----+----------------------------------------+
{code}
   

 ",,apachespark,dongjoon,githubbot,mcanes,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 11 10:35:47 UTC 2018,,,,,,,,,,"0|s011hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/18 14:08;gurwls223;Hi [~mcanes], looks it's not specific to Java Beans. Mind if I ask to make a reproducer by normal scala collections? There are some decimal precision issues going on and minimised reproducer should be helpful to identify if it's a duplicate or not.;;;","03/Dec/18 09:19;mcanes;Hi [~hyukjin.kwon], sure, I can reproduce it using Rows. It happens when the scale of the Decimal doesn't match with the real scale value. It's worse with a Java Bean because it's not posible change the Decimal scale value of the encoder.

Using *first*, the result is wrong.
{code:java}
  List<StructField> fields = new ArrayList<>();
  fields.add(DataTypes.createStructField(""group"", DataTypes.StringType, true));
  fields.add(DataTypes.createStructField(""var"", DataTypes.createDecimalType(38, 8), true));
  ExpressionEncoder<Row> encoder = RowEncoder.apply(DataTypes.createStructType(fields));

  Dataset<Row> ds = spark.range(5)
      .map(l -> RowFactory.create("""" + l, BigDecimal.valueOf(l + 0.1111)), encoder);
  ds.show();

+-----+------+
|group| var|
+-----+------+
| 0|0.1111|
| 1|1.1111|
| 2|2.1111|
| 3|3.1111|
| 4|4.1111|
+-----+------+

  ds.groupBy(col(""group""))
      .agg(
          first(col(""var""))
      )
      .show();
}

+-----+-----------------+
|group|first(var, false)|
+-----+-----------------+
| 3| 0.00031111|
| 0| 0.00001111|
| 1| 0.00011111|
| 4| 0.00041111|
| 2| 0.00021111|
+-----+-----------------+
{code}
But it works fine again if we use *sum*
{code:java}
  ds.groupBy(col(""group""))
    .agg(
        sum(col(""var""))
    )
    .show();

+-----+----------+
|group| sum(var)|
+-----+----------+
| 3|3.11110000|
| 0|0.11110000|
| 1|1.11110000|
| 4|4.11110000|
| 2|2.11110000|
+-----+----------+
{code};;;","03/Dec/18 11:13;mgaido;I think this is related to SPARK-24957. The point is that in other function we perform a cast on the input to the desired output type. First, max, min, ... instead involve no cast. So I think we might have to add a cast when we deserialize a Java BigDecimal. In the next days I'll check the feasibility of this, now I don't have too much time to work on it unfortunately.;;;","03/Dec/18 11:23;mcanes;Hi [~mgaido], makes sense. A cast for BigDecimal can fix it for the ""problematic functions"".;;;","03/Dec/18 16:20;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23210;;;","03/Dec/18 19:59;dongjoon;Hi, [~mcanes] and [~mgaido]. This seems to exist for a long time, right?;;;","03/Dec/18 20:11;mgaido;[~dongjoon] I think so. SPARK-24957 was a long standing issue too, and the problem is analogous...;;;","03/Dec/18 20:23;dongjoon;Thanks. I raised the priority because this is a correctness issue.;;;","04/Dec/18 18:38;dongjoon;This is resolved in 3.0.0 via https://github.com/apache/spark/pull/23210 . We need to backport this with separate backporting PRs.;;;","05/Dec/18 11:48;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23232;;;","05/Dec/18 11:50;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23234;;;","05/Dec/18 11:51;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23233;;;","11/Dec/18 01:18;githubbot;cloud-fan commented on issue #23232: [SPARK-26233][SQL][BACKPORT-2.4] CheckOverflow when encoding a decimal value
URL: https://github.com/apache/spark/pull/23232#issuecomment-446036494
 
 
   Hi @mgaido91 , just one more question. Without this patch, does Spark always return wrong result if the actual decimal doesn't fix the precision? even for simple operations like `df.show`?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:35;githubbot;mgaido91 commented on issue #23232: [SPARK-26233][SQL][BACKPORT-2.4] CheckOverflow when encoding a decimal value
URL: https://github.com/apache/spark/pull/23232#issuecomment-446154049
 
 
   No @cloud-fan that case is fine, the problem happens only if the value is passed to some other operations, because they think it has a given precision and scale (based on the data type), but it may have not (before the patch).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM issue encountered when computing Gramian matrix ,SPARK-26228,13201508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,hibayesian,hibayesian,30/Nov/18 04:16,29/Mar/19 20:13,13/Jul/23 08:48,23/Jan/19 01:27,2.3.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,MLlib,,,,0,,,,,"{quote}/**

 * Computes the Gramian matrix `A^T A`.
  *

 * @note This cannot be computed on matrices with more than 65535 columns.
  */
{quote}
As the above annotation of computeGramianMatrix in RowMatrix.scala said, it supports computing on matrices with no more than 65535 columns.

However, we find that it will throw OOM(Request Array Size Exceeds VM Limit) when computing on matrices with 16000 columns.

The root casue seems that the TreeAggregate writes a  very long buffer array (16000*16000*8) which exceeds jvm limit(2^31 - 1).

Does RowMatrix really supports computing on matrices with no more than 65535 columns?

I doubt that computeGramianMatrix has a very serious performance issue.

Do anyone has done some performance expriments before?

 

 ",,hibayesian,shahid,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27069,,,,,,,,,,,,,,,,,,,"03/Dec/18 05:54;hibayesian;1.jpeg;https://issues.apache.org/jira/secure/attachment/12950346/1.jpeg",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 23 01:27:28 UTC 2019,,,,,,,,,,"0|s010u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/18 05:21;shahid;Hi [~hibayesian], could you please share the full log of the error, if you have. Thanks

(btw 16000*16000*8 < 2^31 -1 );;;","03/Dec/18 06:00;hibayesian;[~shahid]

I have upload the screenshot of log.  

I doubt there are extra costs when writing a size of 16000*16000*8 byte array.;;;","03/Dec/18 06:02;hibayesian;Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)
	at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2124)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1092)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1086)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1131)
	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeGramianMatrix(RowMatrix.scala:123)
	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:345)
	at org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)
	at org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)
	at org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:57);;;","03/Dec/18 06:24;shahid;could you please increase the driver memory and check. ;;;","03/Dec/18 06:45;hibayesian;I have tried to set spark.driver.memory from 8g to 16g.

It doesn't work.;;;","21/Jan/19 01:29;srowen;Yep this is a real problem. The issue is the 'zeroValue"" to treeAggregate. It allocates a huge dense matrix of 0s, which must then be serialized several times. This includes a simple check to see if it's serializable, which serializes with JavaSerializer, and at some point that takes so much memory that more than 2GB of bytes are needed somewhere in a ByteArrayOutputStream and it fails. It won't matter how much memory the driver has.

I think the fix is easy; the zero value should just be null, and the seqOp and combOp can easily handle this situation. At least, I tried it locally and it works fine, and doesn't fail upfront at about a 16000x16000 Gramian. It ought to be faster too.;;;","23/Jan/19 01:27;srowen;Issue resolved by pull request 23600
[https://github.com/apache/spark/pull/23600];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Results in stackOverFlowError when trying to add 3000 new columns using withColumn function of dataframe.,SPARK-26224,13201440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,dotsering,dotsering,29/Nov/18 20:49,12/Dec/22 18:11,13/Jul/23 08:48,02/Apr/19 05:13,2.3.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Reproduction step:

Run this sample code on your laptop. I am trying to add 3000 new columns to a base dataframe with 1 column.

 

 
{code:java}
import spark.implicits._

val newColumnsToBeAdded : Seq[StructField] = for (i <- 1 to 3000) yield new StructField(""field_"" + i, DataTypes.LongType)

val baseDataFrame: DataFrame = Seq((1)).toDF(""employee_id"")

val result = newColumnsToBeAdded.foldLeft(baseDataFrame)((df, newColumn) => df.withColumn(newColumn.name, lit(0)))

result.show(false)
 
{code}
Ends up with following stacktrace:

java.lang.StackOverflowError
 at scala.collection.generic.GenTraversableFactory$GenericCanBuildFrom.apply(GenTraversableFactory.scala:57)
 at scala.collection.generic.GenTraversableFactory$GenericCanBuildFrom.apply(GenTraversableFactory.scala:52)
 at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:229)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
 at scala.collection.immutable.List.map(List.scala:296)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)","On macbook, used Intellij editor. Ran the above sample code as unit test.",cloud_fan,dotsering,githubbot,maropu,mgaido,saurabhc100,viirya,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 04 07:19:42 UTC 2021,,,,,,,,,,"0|s010ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/18 02:39;viirya;I think it is not specified to withColumn. withColumn simply adds a projection on original dataframe.

I think It is because you create a very deep query plan. So the analyzer or optimizer encounters when traversing down the query plan.

Even it can traverse down such deep query plan, it might be not efficient to do that. I'd recommend not to create such deep query plan.

This should not be a bug.;;;","10/Dec/18 16:03;mgaido;[~viirya] that is true, but here it comes a question: do we want our DataFrame API to be an imperative or a declarative one? The user here is saying: I want a new column. It is Spark which is adding a new projection. One thing we can do, for instance, is to add a {{CollapseProject}} at the end of {{withColumn}}: in this way, the problem would be solved.

You can argue that we are running an optimization in a very early phase in that case, which is not a great thing, but I noticed also that this patter is often used (there have been JIRA for the inefficiency of this approach in the past IIRC, because it caused the whole plan to be re-analyzed every time). cc [~cloud_fan] [~smilegator] for their opinion on this.;;;","11/Dec/18 01:23;cloud_fan;except `withColumn`, do we have other patterns that create a deep query plan. We did optimize union in `df.union`;;;","11/Dec/18 11:27;githubbot;mgaido91 opened a new pull request #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285
 
 
   ## What changes were proposed in this pull request?
   
   We have seen many cases when users make several subsequent calls to `withColumn` on a Dataset. This leads now to the generation of a lott of `Project` nodes on the top of the plan, with serious problem which can lead also to `StackOverflowException`s.
   
   The PR proposes to run `CollapseProject` on the result of `withColumn` so that subsequent call (if possible) generate anyway a plan with a single project node on the top.
   
   ## How was this patch tested?
   
   added UT

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:28;githubbot;mgaido91 commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446169638
 
 
   cc @cloud-fan @viirya 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:33;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446170969
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:33;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446170979
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5969/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:33;githubbot;SparkQA commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446171042
 
 
   **[Test build #99969 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99969/testReport)** for PR 23285 at commit [`da2c82e`](https://github.com/apache/spark/commit/da2c82ebef5fb2a7e9279d8ce3d9364d2da61ae4).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:34;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446170969
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:34;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446170979
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5969/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:39;githubbot;cloud-fan commented on a change in pull request #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#discussion_r240575095
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
 ##########
 @@ -2164,16 +2164,16 @@ class Dataset[T] private[sql](
       columnMap.find { case (colName, _) =>
         resolver(field.name, colName)
       } match {
-        case Some((colName: String, col: Column)) => col.as(colName)
-        case _ => Column(field)
+        case Some((colName: String, col: Column)) => col.as(colName).named
+        case _ => field
       }
     }
 
-    val newColumns = columnMap.filter { case (colName, col) =>
+    val newColumns = columnMap.filter { case (colName, _) =>
       !output.exists(f => resolver(f.name, colName))
-    }.map { case (colName, col) => col.as(colName) }
+    }.map { case (colName, col) => col.as(colName).named }
 
-    select(replacedAndExistingColumns ++ newColumns : _*)
+    CollapseProject(Project(replacedAndExistingColumns ++ newColumns, logicalPlan))
 
 Review comment:
   Can we reduce the scope of this optimization? e.g. if the root node of this query is `Project`, update its project list to include `withColumns`, otherwise add a new Project.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 11:47;githubbot;mgaido91 commented on a change in pull request #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#discussion_r240577269
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
 ##########
 @@ -2164,16 +2164,16 @@ class Dataset[T] private[sql](
       columnMap.find { case (colName, _) =>
         resolver(field.name, colName)
       } match {
-        case Some((colName: String, col: Column)) => col.as(colName)
-        case _ => Column(field)
+        case Some((colName: String, col: Column)) => col.as(colName).named
+        case _ => field
       }
     }
 
-    val newColumns = columnMap.filter { case (colName, col) =>
+    val newColumns = columnMap.filter { case (colName, _) =>
       !output.exists(f => resolver(f.name, colName))
-    }.map { case (colName, col) => col.as(colName) }
+    }.map { case (colName, col) => col.as(colName).named }
 
-    select(replacedAndExistingColumns ++ newColumns : _*)
+    CollapseProject(Project(replacedAndExistingColumns ++ newColumns, logicalPlan))
 
 Review comment:
   I don't think we can do that. Imagine the case when all the columns depend on the previously added one: if we would do that, we would end up with an invalid plan. Or am I missing something?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:14;githubbot;SparkQA commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446181860
 
 
   **[Test build #99969 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99969/testReport)** for PR 23285 at commit [`da2c82e`](https://github.com/apache/spark/commit/da2c82ebef5fb2a7e9279d8ce3d9364d2da61ae4).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:14;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446181889
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:14;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446181893
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99969/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:15;githubbot;SparkQA removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446171042
 
 
   **[Test build #99969 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99969/testReport)** for PR 23285 at commit [`da2c82e`](https://github.com/apache/spark/commit/da2c82ebef5fb2a7e9279d8ce3d9364d2da61ae4).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:15;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446181889
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:16;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446181893
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99969/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:38;githubbot;viirya commented on a change in pull request #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#discussion_r240592731
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
 ##########
 @@ -2146,7 +2146,7 @@ class Dataset[T] private[sql](
    * Returns a new Dataset by adding columns or replacing the existing columns that has
    * the same names.
    */
-  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
+  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = withPlan {
 
 Review comment:
   As stated on the JIRA ticket, the problem is deep query plan. I think we can have many ways to create such deep query plan, not only for `withColumns`. For example, you can call `select` many times to do that too. This change makes `withColumns` a special case.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:19;githubbot;mgaido91 commented on a change in pull request #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#discussion_r240605608
 
 

 ##########
 File path: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
 ##########
 @@ -2146,7 +2146,7 @@ class Dataset[T] private[sql](
    * Returns a new Dataset by adding columns or replacing the existing columns that has
    * the same names.
    */
-  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
+  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = withPlan {
 
 Review comment:
   yes, but I think this is a special case. I have seen many cases when `withColumn` is used in for loops: with this change such a pattern would be better supported.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:24;githubbot;SparkQA commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446200970
 
 
   **[Test build #99975 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99975/testReport)** for PR 23285 at commit [`a40db10`](https://github.com/apache/spark/commit/a40db106d4d26966af04afb79edb35a59efd1703).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:25;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446201096
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5974/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:26;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446201089
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:53;githubbot;SparkQA commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230024
 
 
   **[Test build #99975 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99975/testReport)** for PR 23285 at commit [`a40db10`](https://github.com/apache/spark/commit/a40db106d4d26966af04afb79edb35a59efd1703).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:53;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230298
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:53;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230324
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99975/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:54;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446201096
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5974/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:54;githubbot;SparkQA removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446200970
 
 
   **[Test build #99975 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99975/testReport)** for PR 23285 at commit [`a40db10`](https://github.com/apache/spark/commit/a40db106d4d26966af04afb79edb35a59efd1703).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:54;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230298
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:54;githubbot;mgaido91 commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230611
 
 
   retest this please

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 14:55;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230324
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99975/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:02;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446233443
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:02;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446233451
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5980/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:03;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446233443
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:03;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446233451
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5980/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 17:33;githubbot;SparkQA commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446290453
 
 
   **[Test build #99981 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99981/testReport)** for PR 23285 at commit [`a40db10`](https://github.com/apache/spark/commit/a40db106d4d26966af04afb79edb35a59efd1703).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 17:33;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446290728
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 17:33;githubbot;AmplabJenkins commented on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446290736
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99981/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 17:34;githubbot;SparkQA removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446230975
 
 
   **[Test build #99981 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99981/testReport)** for PR 23285 at commit [`a40db10`](https://github.com/apache/spark/commit/a40db106d4d26966af04afb79edb35a59efd1703).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 17:34;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446290728
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 17:35;githubbot;AmplabJenkins removed a comment on issue #23285: [SPARK-26224][SQL] Avoid creating many project on subsequent calls to withColumn
URL: https://github.com/apache/spark/pull/23285#issuecomment-446290736
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99981/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/19 19:46;srowen;The most realistic thing I can imagine is exposing `withColumns`.
But for this use case, there are pretty easy workarounds, like just mapping the DataFrame Rows to contain a bunch more 0s and specifying your new schema.;;;","02/Apr/19 05:13;gurwls223;Issue resolved by pull request 23285
[https://github.com/apache/spark/pull/23285];;;","29/Apr/21 11:23;saurabhc100;[~srowen] / [~hyukjin.kwon] - Any plan for exposing the ""withColumns"", I have similar requirement  where I have to loop for withColumn. For safer side I used the reflection to access the withColumns. 
{code:java}
val dataSetClass = Class.forName(""org.apache.spark.sql.Dataset"")
val newConfigurationMethod =
  dataSetClass.getMethod(""withColumns"", classOf[Seq[String]], classOf[Seq[Column]])
newConfigurationMethod.invoke(
  baseDataFrame, columnName, columnValue).asInstanceOf[DataFrame]{code}
It will be great if we use the ""withColumns"" rather than using the reflection code here.

 ;;;","30/Apr/21 02:21;gurwls223;There's a discussion going on in the dev mailing list to expose such a API. It would be great if you have some time to put some input, cc [~yikunkero] FYI;;;","04/May/21 07:19;yikunkero;FYI, as mentioned as Hyukjin, the mailing list [1] had some discussion to expose `withColumns` in Scala/PySpark.

[1] [http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Multiple-columns-adding-replacing-support-in-PySpark-DataFrame-API-td31164.html]

 

And I personal perfer to introduce the `withColumns` because it bring more friendly development experience rather than select( * ). This is the PR to add `withColumns`: [https://github.com/apache/spark/pull/32431];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor summary is not getting updated for failure jobs in history server UI,SPARK-26219,13201410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,shahid,shahid,29/Nov/18 18:21,03/Dec/18 23:15,13/Jul/23 08:48,30/Nov/18 23:25,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,,,,0,,,,,"Test step to reproduce:

{code:java}
bin/spark-shell --master yarn --conf spark.executor.instances=3
sc.parallelize(1 to 10000, 10).map{ x => throw new RuntimeException(""Bad executor"")}.collect() 
{code}

1)Open the application from History UI 
2) Go to the executor tab

From History UI:
!Screenshot from 2018-11-29 22-13-34.png! 
From Live UI:
 !Screenshot from 2018-11-29 22-13-44.png! 
",,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/18 18:21;shahid;Screenshot from 2018-11-29 22-13-34.png;https://issues.apache.org/jira/secure/attachment/12950051/Screenshot+from+2018-11-29+22-13-34.png","29/Nov/18 18:21;shahid;Screenshot from 2018-11-29 22-13-44.png;https://issues.apache.org/jira/secure/attachment/12950050/Screenshot+from+2018-11-29+22-13-44.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 01 04:10:59 UTC 2018,,,,,,,,,,"0|s01088:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/18 18:21;shahid;I will raise a PR;;;","29/Nov/18 19:35;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23181;;;","29/Nov/18 19:35;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23181;;;","01/Dec/18 04:10;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix InSet for binary, and struct and array with null.",SPARK-26211,13201260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,29/Nov/18 07:03,30/Nov/18 11:17,13/Jul/23 08:48,29/Nov/18 15:02,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,0,,,,,"Currently {{InSet}} doesn't work properly for binary type, or struct and array type with null value in the set.
 Because, as for binary type, the {{HashSet}} doesn't work properly for {{Array[Byte]}}, and as for struct and array type with null value in the set, the {{ordering}} will throw a {{NPE}}.",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 30 11:17:40 UTC 2018,,,,,,,,,,"0|s00zb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/18 07:12;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23176;;;","30/Nov/18 11:16;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23187;;;","30/Nov/18 11:17;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23187;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty dataframe does not roundtrip for csv with header,SPARK-26208,13201168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,koertkuipers,koert,koert,28/Nov/18 18:16,12/Dec/22 18:10,13/Jul/23 08:48,02/Dec/18 09:39,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"when we write empty part file for csv and header=true we fail to write header. the result cannot be read back in.

when header=true a part file with zero rows should still have header","master branch,
commit 034ae305c33b1990b3c1a284044002874c343b4d,
date:   Sun Nov 18 16:02:15 2018 +0800
",apachespark,koert,rangareddy.avula@gmail.com,ryanleitaiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31657,,,,,,,,,SPARK-15473,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 03 10:40:10 UTC 2021,,,,,,,,,,"0|s00yqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/18 20:06;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/23173;;;","02/Dec/18 09:39;gurwls223;Fixed in https://github.com/apache/spark/pull/23173;;;","09/Aug/21 14:33;rangareddy.avula@gmail.com;Hi [~koertkuipers]

The above code will work only when dataframe created manually.

Issue still persists when when we create dataframe while reading hive table.

*Hive Table:*
{code:java}
CREATE EXTERNAL TABLE `test_empty_csv_table`( 
 `col1` bigint, 
 `col2` bigint) 
STORED AS ORC 
LOCATION '/tmp/test_empty_csv_table';{code}
*spark-shell*

 
{code:java}
val tableName = ""test_empty_csv_table""
val emptyCSVFilePath = ""/tmp/empty_csv_file""
val df = spark.sql(""select * from ""+tableName)
df.printSchema()
df.write.format(""csv"").option(""header"", true).mode(""overwrite"").save(emptyCSVFilePath)
val df2 = spark.read.option(""header"", true).csv(emptyCSVFilePath)
{code}
 
{code:java}
org.apache.spark.sql.AnalysisException: Unable to infer schema for CSV. It must be specified manually.;
 at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$9.apply(DataSource.scala:208)
 at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$9.apply(DataSource.scala:208)
 at scala.Option.getOrElse(Option.scala:121)
 at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:473)
 ... 49 elided{code};;;","03/Sep/21 10:40;rangareddy.avula@gmail.com;cc [~hyukjin.kwon];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python broadcast.value on driver fails with disk encryption enabled,SPARK-26201,13201138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,redsanket,tgraves,tgraves,28/Nov/18 15:33,30/Nov/18 18:50,13/Jul/23 08:48,30/Nov/18 18:50,2.3.2,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,PySpark,,,,0,,,,,"I was trying python with rpc and disk encryption enabled and when I tried a python broadcast variable and just read the value back on the driver side the job failed with:

 

Traceback (most recent call last): File ""broadcast.py"", line 37, in <module> words_new.value File ""/pyspark.zip/pyspark/broadcast.py"", line 137, in value File ""pyspark.zip/pyspark/broadcast.py"", line 122, in load_from_path File ""pyspark.zip/pyspark/broadcast.py"", line 128, in load EOFError: Ran out of input

To reproduce use configs: --conf spark.network.crypto.enabled=true --conf spark.io.encryption.enabled=true

 

Code:

words_new = sc.broadcast([""scala"", ""java"", ""hadoop"", ""spark"", ""akka""])
 words_new.value
 print(words_new.value)",,apachespark,roczei,sanket991,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 16:37:05 UTC 2018,,,,,,,,,,"0|s00yk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/18 15:34;tgraves;the issue here seems to be that it isn't decrypting the file before trying to read it, we will have a patch up for this shortly.;;;","28/Nov/18 15:47;sanket991;Thanks [~tgraves] will put up the patch shortly;;;","28/Nov/18 16:37;apachespark;User 'redsanket' has created a pull request for this issue:
https://github.com/apache/spark/pull/23166;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long expressions cause mutate to fail,SPARK-26199,13201085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michaelchirico,joao.rafael,joao.rafael,28/Nov/18 12:37,12/Dec/22 18:11,13/Jul/23 08:48,17/Dec/20 08:22,2.2.0,,,,,,,,,,,,,,,,,3.1.0,3.2.0,,,SparkR,,,,0,,,,,"Calling {{mutate(df, field = expr)}} fails when expr is very long.

Example:

{code:R}
df <- mutate(df, field = ifelse(
    lit(TRUE),
    lit(""A""),
    ifelse(
        lit(T),
        lit(""BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB""),
        lit(""C"")
    )
))
{code}

Stack trace:

{code:R}
FATAL subscript out of bounds
  at .handleSimpleError(function (obj) 
{
    level = sapply(class(obj), sw
  at FUN(X[[i]], ...)
  at lapply(seq_along(args), function(i) {
    if (ns[[i]] != """") {

at lapply(seq_along(args), function(i) {
    if (ns[[i]] != """") {

at mutate(df, field = ifelse(lit(TRUE), lit(""A""), ifelse(lit(T), lit(""BBB
  at #78: mutate(df, field = ifelse(lit(TRUE), lit(""A""), ifelse(lit(T
{code}

The root cause is in: [DataFrame.R#LL2182|https://github.com/apache/spark/blob/master/R/pkg/R/DataFrame.R#L2182]

When the expression is long {{deparse}} returns multiple lines, causing {{args}} to have more elements than {{ns}}. The solution could be to set {{nlines = 1}} or to collapse the lines together.

A simple work around exists, by first placing the expression in a variable and using it instead:

{code:R}
tmp <- ifelse(
    lit(TRUE),
    lit(""A""),
    ifelse(
        lit(T),
        lit(""BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB""),
        lit(""C"")
    )
)
df <- mutate(df, field = tmp)
{code}
",,apachespark,joao.rafael,michaelchirico,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31517,,SPARK-31517,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 08:22:16 UTC 2020,,,,,,,,,,"0|s00y88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/18 16:03;gurwls223;I haven't taken a close look but sounds right. Wanna go ahead for a PR with a regression test?;;;","28/Apr/20 07:18;michaelchirico;Just saw this. 

https://issues.apache.org/jira/browse/SPARK-31517

is a duplicate of this.

PR to fix it is here:

https://github.com/apache/spark/pull/28386

I'll tag this Jira as well.;;;","30/Apr/20 01:49;gurwls223;Thanks, [~michaelchirico];;;","30/Apr/20 16:53;apachespark;User 'MichaelChirico' has created a pull request for this issue:
https://github.com/apache/spark/pull/28386;;;","17/Dec/20 08:22;gurwls223;Fixed in https://github.com/apache/spark/pull/28386;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metadata serialize null values throw NPE,SPARK-26198,13201077,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,yumwang,yumwang,28/Nov/18 12:21,03/Dec/18 05:44,13/Jul/23 08:48,02/Dec/18 14:52,3.0.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,0,,,,,"How to reproduce this issue:
{code}
scala> val meta = new org.apache.spark.sql.types.MetadataBuilder().putNull(""key"").build().json
java.lang.NullPointerException
  at org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$toJsonValue(Metadata.scala:196)
  at org.apache.spark.sql.types.Metadata$$anonfun$1.apply(Metadata.scala:180)
{code}",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 02 14:52:21 UTC 2018,,,,,,,,,,"0|s00y6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/18 12:28;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/23164;;;","02/Dec/18 14:52;srowen;Issue resolved by pull request 23164
[https://github.com/apache/spark/pull/23164];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Total tasks message in the stage is incorrect, when there are failed or killed tasks",SPARK-26196,13200997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,shahid,shahid,28/Nov/18 01:50,07/Dec/18 20:34,13/Jul/23 08:48,07/Dec/18 20:34,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Web UI,,,,0,,,,,"Total tasks message in the stage page is incorrect when there are failed or killed tasks.

 ",,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 07 20:34:53 UTC 2018,,,,,,,,,,"0|s00xoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/18 01:50;shahid;I will raise a PR;;;","28/Nov/18 01:59;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23160;;;","07/Dec/18 20:34;srowen;Resolved by https://github.com/apache/spark/pull/23160;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct exception messages in some classes,SPARK-26195,13200996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lcqzte2015,lcqzte2015,lcqzte2015,28/Nov/18 01:46,02/Dec/18 02:58,13/Jul/23 08:48,02/Dec/18 02:56,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,UnsupportedOperationException messages are not the same with method name.This PR correct these messages.,,apachespark,cloud_fan,lcqzte2015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 02 02:56:10 UTC 2018,,,,,,,,,,"0|s00xog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/18 01:49;apachespark;User 'lcqzte10192193' has created a pull request for this issue:
https://github.com/apache/spark/pull/23154;;;","28/Nov/18 01:50;apachespark;User 'lcqzte10192193' has created a pull request for this issue:
https://github.com/apache/spark/pull/23154;;;","02/Dec/18 02:56;cloud_fan;Issue resolved by pull request 23154
[https://github.com/apache/spark/pull/23154];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.4.0 Partitioning behavior breaks backwards compatibility,SPARK-26188,13200932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Gengliang.Wang,ddgirard,ddgirard,27/Nov/18 18:52,26/Feb/19 19:50,13/Jul/23 08:48,30/Nov/18 04:02,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,correctness,release-notes,,,"My team uses spark to partition and output parquet files to amazon S3. We typically use 256 partitions, from 00 to ff.

We've observed that in spark 2.3.2 and prior, it reads the partitions as strings by default. However, in spark 2.4.0 and later, the type of each partition is inferred by default, and partitions such as 00 become 0 and 4d become 4.0.
 Here is a log sample of this behavior from one of our jobs:
 2.4.0:
{code:java}
18/11/27 14:02:27 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=00/part-00061-hashredacted.parquet, range: 0-662, partition values: [0]
18/11/27 14:02:28 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=ef/part-00034-hashredacted.parquet, range: 0-662, partition values: [ef]
18/11/27 14:02:29 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=4a/part-00151-hashredacted.parquet, range: 0-662, partition values: [4a]
18/11/27 14:02:30 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=74/part-00180-hashredacted.parquet, range: 0-662, partition values: [74]
18/11/27 14:02:32 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=f5/part-00156-hashredacted.parquet, range: 0-662, partition values: [f5]
18/11/27 14:02:33 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=50/part-00195-hashredacted.parquet, range: 0-662, partition values: [50]
18/11/27 14:02:34 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=70/part-00054-hashredacted.parquet, range: 0-662, partition values: [70]
18/11/27 14:02:35 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=b9/part-00012-hashredacted.parquet, range: 0-662, partition values: [b9]
18/11/27 14:02:37 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=d2/part-00016-hashredacted.parquet, range: 0-662, partition values: [d2]
18/11/27 14:02:38 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=51/part-00003-hashredacted.parquet, range: 0-662, partition values: [51]
18/11/27 14:02:39 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=84/part-00135-hashredacted.parquet, range: 0-662, partition values: [84]
18/11/27 14:02:40 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=b5/part-00190-hashredacted.parquet, range: 0-662, partition values: [b5]
18/11/27 14:02:41 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=88/part-00143-hashredacted.parquet, range: 0-662, partition values: [88]
18/11/27 14:02:42 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=4d/part-00120-hashredacted.parquet, range: 0-662, partition values: [4.0]
18/11/27 14:02:43 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=ac/part-00119-hashredacted.parquet, range: 0-662, partition values: [ac]
18/11/27 14:02:44 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=24/part-00139-hashredacted.parquet, range: 0-662, partition values: [24]
18/11/27 14:02:45 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=fd/part-00167-hashredacted.parquet, range: 0-662, partition values: [fd]
18/11/27 14:02:46 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=52/part-00033-hashredacted.parquet, range: 0-662, partition values: [52]
18/11/27 14:02:47 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=ab/part-00083-hashredacted.parquet, range: 0-662, partition values: [ab]
18/11/27 14:02:48 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=f8/part-00018-hashredacted.parquet, range: 0-662, partition values: [f8]
18/11/27 14:02:49 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=7a/part-00093-hashredacted.parquet, range: 0-662, partition values: [7a]
18/11/27 14:02:50 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=ba/part-00020-hashredacted.parquet, range: 0-662, partition values: [ba]
18/11/27 14:02:51 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=2d/part-00085-hashredacted.parquet, range: 0-662, partition values: [2.0]
18/11/27 14:02:52 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=03/part-00099-hashredacted.parquet, range: 0-662, partition values: [3]
18/11/27 14:02:53 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=57/part-00196-hashredacted.parquet, range: 0-662, partition values: [57]
18/11/27 14:02:54 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=81/part-00122-hashredacted.parquet, range: 0-662, partition values: [81]
18/11/27 14:02:55 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=6d/part-00167-hashredacted.parquet, range: 0-662, partition values: [6.0]
18/11/27 14:02:56 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=36/part-00154-hashredacted.parquet, range: 0-662, partition values: [36]
18/11/27 14:02:57 INFO FileScanRDD: Reading File path: s3a://bucketnamereadacted/ddgirard/suffix=4b/part-00093-hashredacted.parquet, range: 0-662, partition values: [4b]{code}
2.3.2:
{code:java}
18/11/27 14:09:00 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=60/part-00082-hashredacted.parquet, range: 0-662, partition values: [60]
18/11/27 14:09:01 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=00/part-00061-hashredacted.parquet, range: 0-662, partition values: [00]
18/11/27 14:09:02 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=ef/part-00034-hashredacted.parquet, range: 0-662, partition values: [ef]
18/11/27 14:09:02 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=4a/part-00151-hashredacted.parquet, range: 0-662, partition values: [4a]
18/11/27 14:09:03 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=74/part-00180-hashredacted.parquet, range: 0-662, partition values: [74]
18/11/27 14:09:04 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=f5/part-00156-hashredacted.parquet, range: 0-662, partition values: [f5]
18/11/27 14:09:04 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=50/part-00195-hashredacted.parquet, range: 0-662, partition values: [50]
18/11/27 14:09:05 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=70/part-00054-hashredacted.parquet, range: 0-662, partition values: [70]
18/11/27 14:09:05 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=b9/part-00012-hashredacted.parquet, range: 0-662, partition values: [b9]
18/11/27 14:09:06 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=d2/part-00016-hashredacted.parquet, range: 0-662, partition values: [d2]
18/11/27 14:09:06 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=51/part-00003-hashredacted.parquet, range: 0-662, partition values: [51]
18/11/27 14:09:07 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=84/part-00135-hashredacted.parquet, range: 0-662, partition values: [84]
18/11/27 14:09:08 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=b5/part-00190-hashredacted.parquet, range: 0-662, partition values: [b5]
18/11/27 14:09:08 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=88/part-00143-hashredacted.parquet, range: 0-662, partition values: [88]
18/11/27 14:09:09 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=4d/part-00120-hashredacted.parquet, range: 0-662, partition values: [4d]
18/11/27 14:09:09 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=ac/part-00119-hashredacted.parquet, range: 0-662, partition values: [ac]
18/11/27 14:09:10 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=24/part-00139-hashredacted.parquet, range: 0-662, partition values: [24]
18/11/27 14:09:11 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=fd/part-00167-hashredacted.parquet, range: 0-662, partition values: [fd]
18/11/27 14:09:11 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=52/part-00033-hashredacted.parquet, range: 0-662, partition values: [52]
18/11/27 14:09:12 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=ab/part-00083-hashredacted.parquet, range: 0-662, partition values: [ab]
18/11/27 14:09:12 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=f8/part-00018-hashredacted.parquet, range: 0-662, partition values: [f8]
18/11/27 14:09:13 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=7a/part-00093-hashredacted.parquet, range: 0-662, partition values: [7a]
18/11/27 14:09:13 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=ba/part-00020-hashredacted.parquet, range: 0-662, partition values: [ba]
18/11/27 14:09:14 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=2d/part-00085-hashredacted.parquet, range: 0-662, partition values: [2d]
18/11/27 14:09:15 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=03/part-00099-hashredacted.parquet, range: 0-662, partition values: [03]
18/11/27 14:09:15 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=57/part-00196-hashredacted.parquet, range: 0-662, partition values: [57]
18/11/27 14:09:16 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=81/part-00122-hashredacted.parquet, range: 0-662, partition values: [81]
18/11/27 14:09:17 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=6d/part-00167-hashredacted.parquet, range: 0-662, partition values: [6d]
18/11/27 14:09:17 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=36/part-00154-hashredacted.parquet, range: 0-662, partition values: [36]
18/11/27 14:09:18 INFO FileScanRDD: Reading File path: s3a://bucketnameredacted/ddgirard/suffix=4b/part-00093-hashredacted.parquet, range: 0-662, partition values: [4b]

{code}
After some investigation, we've isolated the issue to
 [https://github.com/apache/spark/blob/02b510728c31b70e6035ad541bfcdc2b59dcd79a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala#L132-L136]
  

In the inferPartitioning method, 2.3.2 sets the type inference to false by default:
{code:java}
val spec = PartitioningUtils.parsePartitions(
  leafDirs,
  typeInference = false,
  basePaths = basePaths,
  timeZoneId = timeZoneId){code}
However, in version 2.4.0, the typeInference flag has been replace with a config flag

[https://github.com/apache/spark/blob/075447b3965489ffba4e6afb2b120880bc307505/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala#L129-L133]

 
{code:java}
val inferredPartitionSpec = PartitioningUtils.parsePartitions(
  leafDirs,
  typeInference = sparkSession.sessionState.conf.partitionColumnTypeInferenceEnabled,
  basePaths = basePaths,
  timeZoneId = timeZoneId){code}
And this conf's default value is true
{code:java}
val PARTITION_COLUMN_TYPE_INFERENCE =
buildConf(""spark.sql.sources.partitionColumnTypeInference.enabled"")
.doc(""When true, automatically infer the data types for partitioned columns."")
.booleanConf
.createWithDefault(true){code}
[https://github.com/apache/spark/blob/075447b3965489ffba4e6afb2b120880bc307505/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L636-L640]
  

I was wondering if a bug report would be appropriate to preserve backwards compatibility and change the default conf value to false.

 
  ",,apachespark,cloud_fan,ddgirard,dongjoon,Gengliang.Wang,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26990,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 30 21:38:52 UTC 2018,,,,,,,,,,"0|s00xa8:",9223372036854775807,,,,,,,,,,,,,2.4.1,,,,,,,,,,"28/Nov/18 16:22;Gengliang.Wang;[~ddgirard]Thanks for the investigation. I have created https://github.com/apache/spark/pull/23165 to fix it.;;;","28/Nov/18 16:22;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23165;;;","30/Nov/18 04:02;cloud_fan;Issue resolved by pull request 23165
[https://github.com/apache/spark/pull/23165];;;","30/Nov/18 14:28;stevel@apache.org;bq. > My team uses spark to partition and output parquet files to amazon S3. We typically use 256 partitions, from 00 to ff.

Independent of the patch, you'd better be using something to deliver the consistency which commit-via-rename requires for directory listing (on S3A: S3Guard), or better, an output committer designed from the ground up for S3 (S3A committers), or you are at risk of data loss due to inconsistent listings.;;;","30/Nov/18 21:38;ddgirard;[~stevel@apache.org] thanks for the tip, I brought it up with the team and we're going to check out a solution for our existing jobs;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In progress applications with last updated time is lesser than the cleaning interval are getting removed during cleaning logs,SPARK-26186,13200918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,shahid,shahid,27/Nov/18 17:56,29/Nov/18 17:52,13/Jul/23 08:48,29/Nov/18 17:52,2.4.0,3.0.0,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,,,,0,,,,,"Inporgress applications with last updated time is withing the cleaning interval are getting deleted.

 

Added a UT to test the scenario.

{code:java}
test(""should not clean inprogress application with lastUpdated time less the maxTime"") {
    val firstFileModifiedTime = TimeUnit.DAYS.toMillis(1)
    val secondFileModifiedTime = TimeUnit.DAYS.toMillis(6)
    val maxAge = TimeUnit.DAYS.toMillis(7)
    val clock = new ManualClock(0)
    val provider = new FsHistoryProvider(
      createTestConf().set(""spark.history.fs.cleaner.maxAge"", s""${maxAge}ms""), clock)
    val log = newLogFile(""inProgressApp1"", None, inProgress = true)
    writeFile(log, true, None,
      SparkListenerApplicationStart(
        ""inProgressApp1"", Some(""inProgressApp1""), 3L, ""test"", Some(""attempt1""))
    )
    clock.setTime(firstFileModifiedTime)
    provider.checkForLogs()
    writeFile(log, true, None,
      SparkListenerApplicationStart(
        ""inProgressApp1"", Some(""inProgressApp1""), 3L, ""test"", Some(""attempt1"")),
      SparkListenerJobStart(0, 1L, Nil, null)
    )

    clock.setTime(secondFileModifiedTime)
    provider.checkForLogs()
    clock.setTime(TimeUnit.DAYS.toMillis(10))
    writeFile(log, true, None,
      SparkListenerApplicationStart(
        ""inProgressApp1"", Some(""inProgressApp1""), 3L, ""test"", Some(""attempt1"")),
      SparkListenerJobStart(0, 1L, Nil, null),
      SparkListenerJobEnd(0, 1L, JobSucceeded)
    )
    provider.checkForLogs()
    // This should not trigger any cleanup
    updateAndCheck(provider) { list =>
      list.size should be(1)
    }
  }
{code}
",,apachespark,shahid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 17:52:05 UTC 2018,,,,,,,,,,"0|s00x74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/18 17:56;shahid;I will raise a PR.;;;","27/Nov/18 18:11;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23158;;;","27/Nov/18 18:11;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23158;;;","29/Nov/18 17:52;vanzin;Issue resolved by pull request 23158
[https://github.com/apache/spark/pull/23158];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Last updated time is not getting updated in the History Server UI,SPARK-26184,13200914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,abhishek.akg,abhishek.akg,27/Nov/18 17:45,29/Nov/18 17:52,13/Jul/23 08:48,29/Nov/18 17:52,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,Web UI,,,0,,,,,"For inprogress application, last updated time is not getting updated.



 !Screenshot from 2018-11-27 23-20-11.png! 

 !Screenshot from 2018-11-27 23-22-38.png! ",,abhishek.akg,apachespark,shahid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/18 17:50;shahid;Screenshot from 2018-11-27 23-20-11.png;https://issues.apache.org/jira/secure/attachment/12949728/Screenshot+from+2018-11-27+23-20-11.png","27/Nov/18 17:52;shahid;Screenshot from 2018-11-27 23-22-38.png;https://issues.apache.org/jira/secure/attachment/12949729/Screenshot+from+2018-11-27+23-22-38.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 17:52:13 UTC 2018,,,,,,,,,,"0|s00x68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/18 17:46;shahid;I will raise a PR;;;","27/Nov/18 18:12;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23158;;;","27/Nov/18 18:12;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23158;;;","27/Nov/18 18:13;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23158;;;","29/Nov/18 17:52;vanzin;Issue resolved by pull request 23158
[https://github.com/apache/spark/pull/23158];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the `hasMinMaxStats` method of `ColumnStatsMap` is not correct,SPARK-26181,13200773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,27/Nov/18 06:25,03/Dec/18 15:57,13/Jul/23 08:48,03/Dec/18 15:57,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,,,adrian-wang,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 03 15:57:33 UTC 2018,,,,,,,,,,"0|s00wb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/18 06:31;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23152;;;","27/Nov/18 06:32;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23152;;;","03/Dec/18 15:57;cloud_fan;Issue resolved by pull request 23152
[https://github.com/apache/spark/pull/23152];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark cannot terminate worker process if user program reads from stdin,SPARK-26175,13200623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,ala.luszczak,ala.luszczak,26/Nov/18 17:12,12/Dec/22 18:10,13/Jul/23 08:48,31/Jul/19 00:11,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,,,,0,Hydrogen,,,,"PySpark worker daemon reads from stdin the worker PIDs to kill. https://github.com/apache/spark/blob/1bb60ab8392adf8b896cc04fb1d060620cf09d8a/python/pyspark/daemon.py#L127

However, the worker process is a forked process from the worker daemon process and we didn't close stdin on the child after fork. This means the child and user program can read stdin as well, which blocks daemon from receiving the PID to kill. This can cause issues because the task reaper might detect the task was not terminated and eventually kill the JVM.

Possible fix could be:
* Closing stdin of the worker process right after fork.
* Creating a new socket to receive PIDs to kill instead of using stdin.

h4. Steps to reproduce

# Paste the following code in pyspark:
{code}
import subprocess
def task(_):
  subprocess.check_output([""cat""])

sc.parallelize(range(1), 1).mapPartitions(task).count()
{code}
# Press CTRL+C to cancel the job.
# The following message is displayed:
{code}
18/11/26 17:52:51 WARN PythonRunner: Incomplete task 0.0 in stage 0 (TID 0) interrupted: Attempting to kill Python Worker
18/11/26 17:52:52 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): TaskKilled (Stage cancelled)
{code}
# Run {{ps -xf}} to see that {{cat}} process was in fact not killed:
{code}
19773 pts/2    Sl+    0:00  |   |   \_ python
19803 pts/2    Sl+    0:11  |   |       \_ /usr/lib/jvm/java-8-oracle/bin/java -cp /home/ala/Repos/apache-spark-GOOD-2/conf/:/home/ala/Repos/apache-spark-GOOD-2/assembly/target/scala-2.12/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --name PySparkShell pyspark-shell
19879 pts/2    S      0:00  |   |           \_ python -m pyspark.daemon
19895 pts/2    S      0:00  |   |               \_ python -m pyspark.daemon
19898 pts/2    S      0:00  |   |                   \_ cat
{code}",,ala.luszczak,bryanc,mengxr,smilegator,Tagar,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 31 00:11:55 UTC 2019,,,,,,,,,,"0|s00ve8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"26/Nov/18 17:29;smilegator;cc [~hyukjin.kwon] [~bryanc] [~icexelloss];;;","26/Nov/18 17:32;mengxr;This affects Hydrogen because the external training process forked from Python worker might consume stdin aggressively, e.g., Open MPI by default. It presents proper termination of Python workers.;;;","01/Dec/18 14:20;gurwls223;Thanks for pinging me - just checked. Let me take a look as well.;;;","03/Dec/18 11:23;gurwls223;[~mengxr], BTW, can't this be done by {{pipe}}?;;;","31/Jul/19 00:11;gurwls223;Issue resolved by pull request 25138
[https://github.com/apache/spark/pull/25138];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Codegen for LocalTableScanExec,SPARK-26159,13200283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,23/Nov/18 16:08,28/Nov/18 05:37,13/Jul/23 08:48,28/Nov/18 05:37,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,Do codegen for LocalTableScanExec.,,apachespark,cloud_fan,juliuszsompolski,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 05:37:52 UTC 2018,,,,,,,,,,"0|s00taw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/18 20:04;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/23127;;;","23/Nov/18 20:04;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/23127;;;","28/Nov/18 05:37;cloud_fan;Issue resolved by pull request 23127
[https://github.com/apache/spark/pull/23127];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream-stream joins - left outer join gives inconsistent output,SPARK-26154,13200176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,hari28,hari28,23/Nov/18 05:16,08/Apr/20 06:41,13/Jul/23 08:48,11/Nov/19 23:47,2.3.0,2.3.2,3.0.0,,,,,,,,,,,,,,,3.0.0,,,,Structured Streaming,,,,0,correctness,,,,"Stream-stream joins using left outer join gives inconsistent  output 

The data processed once, is being processed again and gives null value. In Batch 2, the input data  ""3"" is processed. But again in batch 6, null value is provided for same data

Steps
In spark-shell
{code:java}
scala>     import org.apache.spark.sql.functions.{col, expr}
import org.apache.spark.sql.functions.{col, expr}
scala>     import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.streaming.Trigger
scala>     val lines_stream1 = spark.readStream.
     |       format(""kafka"").
     |       option(""kafka.bootstrap.servers"", ""ip:9092"").
     |       option(""subscribe"", ""topic1"").
     |       option(""includeTimestamp"", true).
     |       load().
     |       selectExpr(""CAST (value AS String)"",""CAST(timestamp AS TIMESTAMP)"").as[(String,Timestamp)].
     |       select(col(""value"") as(""data""),col(""timestamp"") as(""recordTime"")).
     |       select(""data"",""recordTime"").
     |       withWatermark(""recordTime"", ""5 seconds "")
lines_stream1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [data: string, recordTime: timestamp]
scala>     val lines_stream2 = spark.readStream.
     |       format(""kafka"").
     |       option(""kafka.bootstrap.servers"", ""ip:9092"").
     |       option(""subscribe"", ""topic2"").
     |       option(""includeTimestamp"", value = true).
     |       load().
     |       selectExpr(""CAST (value AS String)"",""CAST(timestamp AS TIMESTAMP)"").as[(String,Timestamp)].
     |       select(col(""value"") as(""data1""),col(""timestamp"") as(""recordTime1"")).
     |       select(""data1"",""recordTime1"").
     |       withWatermark(""recordTime1"", ""10 seconds "")
lines_stream2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [data1: string, recordTime1: timestamp]
scala>     val query = lines_stream1.join(lines_stream2, expr (
     |       """"""
     |         | data == data1 and
     |         | recordTime1 >= recordTime and
     |         | recordTime1 <= recordTime + interval 5 seconds
     |       """""".stripMargin),""left"").
     |       writeStream.
     |       option(""truncate"",""false"").
     |       outputMode(""append"").
     |       format(""console"").option(""checkpointLocation"", ""/tmp/leftouter/"").
     |       trigger(Trigger.ProcessingTime (""5 seconds"")).
     |       start()
query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1a48f55b
{code}

Step2 : Start producing data

kafka-console-producer.sh --broker-list ip:9092 --topic topic1
 >1
 >2
 >3
 >4
 >5
 >aa
 >bb
 >cc

kafka-console-producer.sh --broker-list ip:9092 --topic topic2
 >2
 >2
 >3
 >4
 >5
 >aa
 >cc
 >ee
 >ee

 

Output obtained:

{code:java}
Batch: 0
-------------------------------------------
+----+----------+-----+-----------+
|data|recordTime|data1|recordTime1|
+----+----------+-----+-----------+
+----+----------+-----+-----------+

-------------------------------------------
Batch: 1
-------------------------------------------
+----+----------+-----+-----------+
|data|recordTime|data1|recordTime1|
+----+----------+-----+-----------+
+----+----------+-----+-----------+

-------------------------------------------
Batch: 2
-------------------------------------------
+----+-----------------------+-----+-----------------------+
|data|recordTime             |data1|recordTime1            |
+----+-----------------------+-----+-----------------------+
|3   |2018-11-22 20:09:35.053|3    |2018-11-22 20:09:36.506|
|2   |2018-11-22 20:09:31.613|2    |2018-11-22 20:09:33.116|
+----+-----------------------+-----+-----------------------+

-------------------------------------------
Batch: 3
-------------------------------------------
+----+-----------------------+-----+-----------------------+
|data|recordTime             |data1|recordTime1            |
+----+-----------------------+-----+-----------------------+
|4   |2018-11-22 20:09:38.654|4    |2018-11-22 20:09:39.818|
+----+-----------------------+-----+-----------------------+

-------------------------------------------
Batch: 4
-------------------------------------------
+----+-----------------------+-----+-----------------------+
|data|recordTime             |data1|recordTime1            |
+----+-----------------------+-----+-----------------------+
|5   |2018-11-22 20:09:44.809|5    |2018-11-22 20:09:47.452|
|1   |2018-11-22 20:09:22.662|null |null                   |
+----+-----------------------+-----+-----------------------+

-------------------------------------------
Batch: 5
-------------------------------------------
+----+-----------------------+-----+-----------------------+
|data|recordTime             |data1|recordTime1            |
+----+-----------------------+-----+-----------------------+
|cc  |2018-11-22 20:10:06.654|cc   |2018-11-22 20:10:08.701|
|aa  |2018-11-22 20:10:01.536|aa   |2018-11-22 20:10:03.259|
+----+-----------------------+-----+-----------------------+

-------------------------------------------
Batch: 6
-------------------------------------------
+----+-----------------------+-----+-----------+
|data|recordTime             |data1|recordTime1|
+----+-----------------------+-----+-----------+
|3   |2018-11-22 20:09:35.053|null |null       |
+----+-----------------------+-----+-----------+

{code}

 ","Spark version - Spark 2.3.2

OS- Suse 11",apachespark,binitbhaskar,ChernikovP,dongjoon,f2005870@gmail.com,hari28,jammann,jimhuang,jincheng,kabhwan,koeninger,maropu,sandeep.katta2007,SparkSiva,tgraves,vanzin,,,,,,,,,,,,,,,,,SPARK-26187,SPARK-27433,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 25 09:46:51 UTC 2020,,,,,,,,,,"0|s00sn4:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"18/Dec/18 16:43;sandeep.katta2007;CC [~cody@koeninger.org] [~tdas] . this looks like potential bug to me.

can you please look into this issue;;;","17/Jan/19 20:00;ChernikovP;Just in case a bit more reproducible  example is needed , here it's documented - https://issues.apache.org/jira/browse/SPARK-26187.;;;","24/Jan/19 09:25;kabhwan;As I tracked this with SPARK-26187, let's mark this as duplicated.;;;","24/Jan/19 10:27;hari28;Since SPARK-26154 has been reported earlier can we mark the other issue as duplicate...;;;","24/Jan/19 22:51;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/23634;;;","24/Jan/19 22:53;kabhwan;As issue reporter concerns about handling duplicated issue, I just changed my patch to point to this issue, and will mark SPARK-26187 as duplicated.;;;","28/Jan/19 20:31;kabhwan;[~tdas] [~zsxwing] [~joseph.torres]
Could we please justify whether this is correctness bug and should be prioritized?;;;","31/Jan/19 00:01;srowen;[~kabhwan] I think everyone is able to modify the labels and priority (not actually by design we just can't restrict it). I made this 'Critical' though priorities except 'Blocker' don't mean a lot. And labeled it 'correctness'.;;;","31/Jan/19 00:05;kabhwan;[~srowen]
Yeah, I just wanted to avoid changing priority by myself given I'm frequently seeing comments that 'critical' and 'blocker' are reserved to committers. Thanks for justifying and changing it!;;;","31/Jan/19 00:11;srowen;Sure, it's a judgment call. If you see people contemplating it as blocking a release, I think that change would be fair to make. You can update other JIRA elements if you're pretty sure, given your experience with the project, that it should have a different label or whatever. 

What we don't want is simply people with no experience in the project marking things Blocker and setting a bunch of unuseful flags like 'Important' or tagging it 'spark'. If you know enough to be thoughtful about it, I bet your edits would be OK. If in doubt ask.;;;","11/Apr/19 08:44;kabhwan;This issue is reported again: SPARK-27433.

I'd say it should be a blocker for Spark 3.0.0. This slips two bugfix releases, and now major release is coming which I think it will slip again even for major release unless it is marked as a blocker.

Someone could say it's not a regression so it is not necessarily a blocker, but given we don't concern about non-blocker issues when releasing, only labelling as 'correctness' never works.

[http://spark.apache.org/contributing.html] page clearly describes that correctness issues should be considered blockers, and this issue has no workaround from Spark side which doesn't fit to mark as 'critical'.
 # Blocker: pointless to release without this change as the release would be unusable to a large minority of users. Correctness and data loss issues should be considered Blockers.
 # Critical: a large minority of users are missing important functionality without this, and/or a workaround is difficult

I'll mark this as a blocker: hope the patch will be reviewed soon.;;;","12/Apr/19 09:09;binitbhaskar;Thanks [~kabhwan] for taking up this issue. I too feel it is a blocker as the basic left_outer join is not working. It is a shame as we have a very tight project delivery timeline and we could proceed further due to this bug. 

I can see the pull request for this fix is hanging since January, do you have any timeline when it can be merged.  ;;;","11/Nov/19 23:47;vanzin;Issue resolved by pull request 26108
[https://github.com/apache/spark/pull/26108];;;","22/Jan/20 23:53;kabhwan;Leaving information why this issue cannot be easily ported back to 2.x version line although it fixes the correctness.

 

We fixed the issue with ""destructive way"" on existing query as the state in existing query cannot be corrected. In 3.0 migration guide we added below content:
{quote}Spark 3.0 fixes the correctness issue on Stream-stream outer join, which changes the schema of state. (SPARK-26154 for more details) Spark 3.0 will fail the query if you start your query from checkpoint constructed from Spark 2.x which uses stream-stream outer join. Please discard the checkpoint and replay previous inputs to recalculate outputs.
{quote}
End users might not be mad if ""major"" version requires them to lose something. They still have Spark 2.x version line to deny the change and take the risk (as the issue is occurred from ""edge-case""). If we put this to 2.x version line they may have no way to deny.

Please note that unlike other states having versions which can be co-existed, for stream-stream outer join, there're only ""valid"" (state format version = 2) and ""invalid"" (state format version = 1) state format which cannot be co-existed.

We cannot still simply drop state format version 1, because stream-stream inner join is not affected by this bug and we don't want to let the case also discard the state. It will affect too many existing queries and I'd like to reduce the impact.;;;","25/Jan/20 09:46;dongjoon;Hi, All.
According to the above decision, I set `Target Version` to `3.0.0` in order to distinguish this from the other correctness issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Synchronize Worker Cleanup with Worker Shutdown,SPARK-26152,13200168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ajithshetty,dongjoon,dongjoon,23/Nov/18 02:21,12/Dec/22 18:10,13/Jul/23 08:48,14/Mar/19 14:16,2.4.0,2.4.1,2.4.2,2.4.3,3.0.0,,,,,,,,,,,,,2.4.4,3.0.0,,,Spark Core,,,,0,,,,,"- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5627 (2018-11-16)
{code}
BroadcastSuite:
- Using TorrentBroadcast locally
- Accessing TorrentBroadcast variables from multiple threads
- Accessing TorrentBroadcast variables in a local cluster (encryption = off)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@59428a1 rejected from java.util.concurrent.ThreadPoolExecutor@4096a677[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:134)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:78)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:106)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@40a5bf17 rejected from java.util.concurrent.ThreadPoolExecutor@5a73967[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:134)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:78)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:106)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@7072dd4f rejected from java.util.concurrent.ThreadPoolExecutor@35705d25[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:134)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:78)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:106)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Exception in thread ""RemoteBlock-temp-file-clean-thread"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-5"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-12"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-4"" Exception in thread ""dispatcher-event-loop-9"" java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-6"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-12"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-11"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-13"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-27"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""netty-rpc-env-timeout"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""Timer-1568"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-17"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""Spark Context Cleaner"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""dispatcher-event-loop-16"" java.lang.OutOfMemoryError: GC overhead limit exceeded
Exception in thread ""stop-spark-context"" java.lang.OutOfMemoryError: GC overhead limit exceeded
*** RUN ABORTED ***
  java.lang.OutOfMemoryError: GC overhead limit exceeded
  ...
{code}

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5645/

{code}
BroadcastSuite:
- Using TorrentBroadcast locally
- Accessing TorrentBroadcast variables from multiple threads
- Accessing TorrentBroadcast variables in a local cluster (encryption = off) *** FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, localhost, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:208)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:425)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:428)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:179)
	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:12)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:151)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$3(TorrentBroadcast.scala:231)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:211)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)
	... 13 more

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1914)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1913)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1913)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:929)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:929)
  at scala.Option.foreach(Option.scala:257)
  ...
  Cause: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)
  at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:208)
  at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
  at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
  at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
  at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)
  at org.apache.spark.scheduler.Task.run(Task.scala:121)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:425)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
  ...
  Cause: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
  at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:179)
  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:12)
  at scala.collection.immutable.List.foreach(List.scala:388)
  at org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:151)
  at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$3(TorrentBroadcast.scala:231)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:211)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)
  at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:208)
  at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
  ...
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@68c6a53e rejected from java.util.concurrent.ThreadPoolExecutor@2dc42642[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:134)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:78)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:106)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@7e69ed97 rejected from java.util.concurrent.ThreadPoolExecutor@3b6b7c3e[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:134)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:78)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:106)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2e07a335 rejected from java.util.concurrent.ThreadPoolExecutor@6b052891[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:134)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:78)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:106)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at scala.concurrent.Promise.complete(Promise.scala:49)
	at scala.concurrent.Promise.complete$(Promise.scala:48)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
11/20/18 5:35:25 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
11/20/18 5:55:13 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
11/20/18 6:14:49 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
11/20/18 6:34:28 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
11/20/18 6:54:30 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
11/20/18 7:14:35 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
11/20/18 7:34:31 PM ============================================================

-- Gauges ----------------------------------------------------------------------
master.aliveWorkers
- Accessing TorrentBroadcast variables in a local cluster (encryption = on) *** FAILED ***
  org.apache.spark.SparkException: Job 0 cancelled as part of cancellation of all jobs
...
{code}

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5809/",,ajithshetty,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/19 12:08;ajithshetty;Screenshot from 2019-03-11 17-03-40.png;https://issues.apache.org/jira/secure/attachment/12961963/Screenshot+from+2019-03-11+17-03-40.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 07 21:06:17 UTC 2019,,,,,,,,,,"0|s00slc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/18 16:34;gurwls223;[~dongjoon], btw, why is this flaky test a release blocker?;;;","02/Jan/19 00:04;dongjoon;This bug is not reported correctly as we see https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5809/ . Jenkins fails with the test report `no failures`. We need to see the actual log.

{code}
	Test Result (no failures)
{code}

{code}
BroadcastSuite:
- Using TorrentBroadcast locally
- Accessing TorrentBroadcast variables from multiple threads
- Accessing TorrentBroadcast variables in a local cluster (encryption = off) *** FAILED ***
{code}
;;;","02/Mar/19 20:06;srowen;I haven't seen this one in a while, FWIW.;;;","11/Mar/19 12:04;ajithshetty;I encountered this issue on latest master branch and see that its the race between *org.apache.spark.deploy.DeployMessages.WorkDirCleanup* event and  *org.apache.spark.deploy.worker.Worker#onStop*. Here its possible that while the WorkDirCleanup event is being processed, *org.apache.spark.deploy.worker.Worker#cleanupThreadExecutor* was shutdown. hence any submission after ThreadPoolExecutor will result in *java.util.concurrent.RejectedExecutionException*

Attaching the debug snapshot of same. I would like to work on this. Please suggest;;;","14/Mar/19 14:16;srowen;Issue resolved by pull request 24056
[https://github.com/apache/spark/pull/24056];;;","07/Aug/19 21:06;zsxwing;[~ajithshetty] does your PR fix the flaky test? If I read correctly, the failure is because of OOM. RejectedExecutionException is just side effect because OOM triggers an unexpected executor shut down. Should we try to figure out why OOM happens? Maybe we have some memory leak.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDFs in join condition fail even when using columns from only one side of join,SPARK-26147,13200080,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,ala.luszczak,ala.luszczak,22/Nov/18 13:41,28/Nov/18 12:54,13/Jul/23 08:48,28/Nov/18 12:54,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,PySpark,,,,0,,,,,"The rule {{PullOutPythonUDFInJoinCondition}} was implemented in [https://github.com/apache/spark/commit/2a8cbfddba2a59d144b32910c68c22d0199093fe]
 As far as I understand, this rule was intended to prevent the use of Python UDFs in join condition if they take arguments from both sides of the join, and this doesn't make sense in combination with the join type.

The rule {{PullOutPythonUDFInJoinCondition}} seems to make an assumption, that if a given UDF is only using columns from a single side of the join, it will be already pushed down under the join before this rule is executed.

However, this is not always the case. Here's a simple example that fails, even though it looks like it should run just fine (and it does in earlier versions of Spark):
{code:java}
from pyspark.sql import Row
from pyspark.sql.types import StringType
from pyspark.sql.functions import udf

cars_list = [ Row(""NL"", ""1234AB""), Row(""UK"", ""987654"") ]
insurance_list = [ Row(""NL-1234AB""), Row(""BE-112233"") ]

spark.createDataFrame(data = cars_list, schema = [""country"", ""plate_nr""]).createOrReplaceTempView(""cars"")
spark.createDataFrame(data = insurance_list, schema = [""insurance_code""]).createOrReplaceTempView(""insurance"")

to_insurance_code = udf(lambda x, y: x + ""-"" + y, StringType())	
sqlContext.udf.register('to_insurance_code', to_insurance_code)

spark.conf.set(""spark.sql.crossJoin.enabled"", ""true"")

# This query runs just fine.
sql(""""""
  SELECT country, plate_nr, insurance_code
  FROM cars LEFT OUTER JOIN insurance
  ON CONCAT(country, '-', plate_nr) = insurance_code
"""""").show()

# This equivalent query fails with:
# pyspark.sql.utils.AnalysisException: u'Using PythonUDF in join condition of join type LeftOuter is not supported.;'
sql(""""""
  SELECT country, plate_nr, insurance_code
  FROM cars LEFT OUTER JOIN insurance
  ON to_insurance_code(country, plate_nr) = insurance_code
"""""").show()
{code}
[~cloud_fan] [~XuanYuan] fyi",,ala.luszczak,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 12:54:27 UTC 2018,,,,,,,,,,"0|s00s1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/18 11:23;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23153;;;","28/Nov/18 12:54;cloud_fan;Issue resolved by pull request 23153
[https://github.com/apache/spark/pull/23153];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`build/mvn` should detect `scala.version` based on `scala.binary.version`,SPARK-26144,13200016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,22/Nov/18 08:51,22/Nov/18 22:50,13/Jul/23 08:48,22/Nov/18 22:50,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Build,,,,0,,,,,"Currently, `build/mvn` download and use `2.12.7` in `Scala-2.11` Jenkins job. The root cause is `build/mvn` got the first match from `pom.xml` blindly.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/6/consoleFull
{code:java}
exec: curl -s -L https://downloads.lightbend.com/zinc/0.3.15/zinc-0.3.15.tgz
exec: curl -s -L https://downloads.lightbend.com/scala/2.12.7/scala-2.12.7.tgz
exec: curl -s -L https://www.apache.org/dyn/closer.lua?action=download&filename=/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 22 22:50:30 UTC 2018,,,,,,,,,,"0|s00rnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/18 08:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/23118;;;","22/Nov/18 22:50;dongjoon;This is resolved via https://github.com/apache/spark/pull/23118;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Linux file separator is hard coded in DependencyUtils used in deploy process,SPARK-26137,13199800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,markpavey,markpavey,markpavey,21/Nov/18 10:02,28/Nov/18 15:21,13/Jul/23 08:48,28/Nov/18 15:21,2.3.0,2.3.1,2.3.2,2.4.0,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Deploy,,,,0,,,,,"During deployment, while downloading dependencies the code tries to remove multiple copies of the application jar from the driver classpath. The Linux file separator (""/"") is hard coded here so on Windows multiple copies of the jar are not removed.

This has a knock on effect when trying to use elasticsearch-spark as this library does not run if there are multiple copies of the application jar on the driver classpath.",,apachespark,markpavey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 15:21:19 UTC 2018,,,,,,,,,,"0|s00qcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/18 10:13;apachespark;User 'markpavey' has created a pull request for this issue:
https://github.com/apache/spark/pull/23102;;;","28/Nov/18 15:21;srowen;Issue resolved by pull request 23102
[https://github.com/apache/spark/pull/23102];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delegation Token seems not appropriately stored on secrets of Kubernetes/Kerberized HDFS,SPARK-26125,13199500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,kkori,kkori,20/Nov/18 03:36,17/May/20 18:25,13/Jul/23 08:48,12/Feb/19 19:49,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,Spark Core,,,0,,,,,"I tried Kerberos authentication with Kubernetes Resource Manager and an external Hadoop and KDC.
I tested built on [6c9c84f|https://github.com/apache/spark/commit/6c9c84ffb9c8d98ee2ece7ba4b010856591d383d] (master + SPARK-23257).

{code}
$ bin/spark-submit \
      --deploy-mode cluster \
      --class org.apache.spark.examples.HdfsTest \
      --master k8s://https://master01.node:6443 \
      --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
      --conf spark.app.name=spark-hdfs \
      --conf spark.executer.instances=1 \
      --conf spark.kubernetes.container.image=docker-registry/kkori/spark:6c9c84f \
      --conf spark.kubernetes.kerberos.enabled=true \
      --conf spark.kubernetes.kerberos.krb5.configMapName=krb5-conf \
      --conf spark.kubernetes.kerberos.keytab=/tmp/test.keytab \
      --conf spark.kubernetes.kerberos.principal=test@EXTERNAL.KERBEROS.REALM.COM \
      --conf spark.kubernetes.hadoop.configMapName=hadoop-conf \
      local:///opt/spark/examples/jars/spark-examples_2.11-3.0.0-SNAPSHOT.jar
{code}

I successfully submitted into Kubernetes RM and Kubernetes spawned spark-driver and executors,
but Hadoop Delegation Token seems wrongly stored into Kubernetes secrets, since that contains only header like below:

{code}
$ kubectl get secrets spark-hdfs-1542613661459-delegation-tokens -o jsonpath='{.data.hadoop-tokens}' | {base64 -d | cat -A; echo;}
HDTS^@^@^@
{code}

The result of ""kubectl get secrets"" should be like folloing(I masked the actual result):
{code}
HDTS^@^ha-hdfs:test^@^_test@EXTERNAL.KERBEROS.REALM.COM^@^XXXX@
{code}


As a result, spark-driver threw GSSException for each access of HDFS.
Full logs(submit, driver, executor) are attached.",,aajisaka,kkori,Steven Rand,tasanuma,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25815,,,,,,,,,,"20/Nov/18 03:36;kkori;spark-submit-stern.log;https://issues.apache.org/jira/secure/attachment/12948798/spark-submit-stern.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 12 19:49:50 UTC 2019,,,,,,,,,,"0|s00ok0:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"28/Nov/18 00:17;vanzin;Pretty sure this works with my patch for SPARK-25815, but since this is a different bug from that one, will keep it separate (and close together).;;;","12/Feb/19 19:49;vanzin;Pretty sure this was fixed as part of  SPARK-25815, please re-open if not.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task metrics summary in the stage page should contain only successful tasks metrics,SPARK-26119,13199402,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,abhishek.akg,abhishek.akg,19/Nov/18 18:32,04/Dec/18 19:05,13/Jul/23 08:48,04/Dec/18 19:05,2.3.2,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,Web UI,,,0,,,,,"Currently task metrics summary table in the stage tab shows summary corresponds to all the tasks. But, we should display the summary of only succeeded tasks in the tasks summary metrics table",,abhishek.akg,apachespark,shahid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 04 19:05:42 UTC 2018,,,,,,,,,,"0|s00nyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/18 18:32;shahid;Thanks. I am working on it;;;","19/Nov/18 19:52;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23088;;;","04/Dec/18 19:05;vanzin;Issue resolved by pull request 23088
[https://github.com/apache/spark/pull/23088];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak of PartitionedPairBuffer when coalescing after repartitionAndSortWithinPartitions,SPARK-26114,13199267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,szhemzhitsky,szhemzhitsky,szhemzhitsky,19/Nov/18 09:28,12/Aug/19 03:15,13/Jul/23 08:48,28/Nov/18 12:23,2.2.2,2.3.2,2.4.0,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,,,,1,,,,,"Trying to use _coalesce_ after shuffle-oriented transformations leads to OutOfMemoryErrors or _Container killed by YARN for exceeding memory limits. X GB of Y GB physical memory used. Consider boostingspark.yarn.executor.memoryOverhead_.
Discussion is [here|http://apache-spark-developers-list.1001551.n3.nabble.com/Coalesce-behaviour-td25289.html].

The error happens when trying specify pretty small number of partitions in _coalesce_ call.

*How to reproduce?*

# Start spark-shell
{code:bash}
spark-shell \ 
  --num-executors=5 \ 
  --executor-cores=2 \ 
  --master=yarn \
  --deploy-mode=client \ 
  --conf spark.executor.memoryOverhead=512 \
  --conf spark.executor.memory=1g \ 
  --conf spark.dynamicAllocation.enabled=false \
  --conf spark.executor.extraJavaOptions='-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp -Dio.netty.noUnsafe=true'
{code}
Please note using _-Dio.netty.noUnsafe=true_ property. Preventing off-heap memory usage seems to be the only way to control the amount of memory used for shuffle data transferring by now.
Also note that the total number of cores allocated for job is 5x2=10
# Then generate some test data
{code:scala}
import org.apache.hadoop.io._ 
import org.apache.hadoop.io.compress._ 
import org.apache.commons.lang._ 
import org.apache.spark._ 

// generate 100M records of sample data 
sc.makeRDD(1 to 1000, 1000) 
  .flatMap(item => (1 to 100000) 
    .map(i => new Text(RandomStringUtils.randomAlphanumeric(3).toLowerCase) -> new Text(RandomStringUtils.randomAlphanumeric(1024)))) 
  .saveAsSequenceFile(""/tmp/random-strings"", Some(classOf[GzipCodec])) 
{code}
# Run the sample job
{code:scala}
import org.apache.hadoop.io._
import org.apache.spark._
import org.apache.spark.storage._

val rdd = sc.sequenceFile(""/tmp/random-strings"", classOf[Text], classOf[Text])
rdd 
  .map(item => item._1.toString -> item._2.toString) 
  .repartitionAndSortWithinPartitions(new HashPartitioner(1000)) 
  .coalesce(10,false) 
  .count 
{code}
Note that the number of partitions is equal to the total number of cores allocated to the job.

Here is dominator tree from the heapdump
 !run1-noparams-dominator-tree.png|width=700!

4 instances of ExternalSorter, although there are only 2 concurrently running tasks per executor.
 !run1-noparams-dominator-tree-externalsorter.png|width=700! 

And paths to GC root of the already stopped ExternalSorter.
 !run1-noparams-dominator-tree-externalsorter-gc-root.png|width=700! ","Spark 3.0.0-SNAPSHOT (master branch)
Scala 2.11
Yarn 2.7",apachespark,cloud_fan,emlyn,kent2171,szhemzhitsky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/18 09:33;szhemzhitsky;run1-noparams-dominator-tree-externalsorter-gc-root.png;https://issues.apache.org/jira/secure/attachment/12948685/run1-noparams-dominator-tree-externalsorter-gc-root.png","19/Nov/18 09:31;szhemzhitsky;run1-noparams-dominator-tree-externalsorter.png;https://issues.apache.org/jira/secure/attachment/12948684/run1-noparams-dominator-tree-externalsorter.png","19/Nov/18 09:29;szhemzhitsky;run1-noparams-dominator-tree.png;https://issues.apache.org/jira/secure/attachment/12948683/run1-noparams-dominator-tree.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 12:23:52 UTC 2018,,,,,,,,,,"0|s00n4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/18 11:20;apachespark;User 'szhem' has created a pull request for this issue:
https://github.com/apache/spark/pull/23083;;;","28/Nov/18 12:23;cloud_fan;Issue resolved by pull request 23083
[https://github.com/apache/spark/pull/23083];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update since versions of new built-in functions.,SPARK-26112,13199238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,19/Nov/18 06:40,19/Nov/18 14:19,13/Jul/23 08:48,19/Nov/18 14:19,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"The following 5 functions were removed from branch-2.4:

- map_entries
- map_filter
- transform_values
- transform_keys
- map_zip_with

We should update the since version to 3.0.0.
",,apachespark,cloud_fan,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 19 14:19:32 UTC 2018,,,,,,,,,,"0|s00myg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/18 06:44;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23082;;;","19/Nov/18 14:19;cloud_fan;Issue resolved by pull request 23082
[https://github.com/apache/spark/pull/23082];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duration in the task summary metrics table and the task table are different,SPARK-26109,13199205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,shahid,shahid,18/Nov/18 23:11,21/Nov/18 15:32,13/Jul/23 08:48,21/Nov/18 15:32,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Spark Core,Web UI,,,0,,,,,"Duration time in the summary metrics table and tasks table are different even though other metrics in the table are proper.

",,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 21 15:32:57 UTC 2018,,,,,,,,,,"0|s00mr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/18 23:14;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23081;;;","18/Nov/18 23:15;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23081;;;","21/Nov/18 15:32;srowen;Issue resolved by pull request 23081
[https://github.com/apache/spark/pull/23081];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemory error with large query plans,SPARK-26103,13199107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DaveDeCaprio,DaveDeCaprio,DaveDeCaprio,17/Nov/18 20:07,11/Sep/19 01:13,13/Jul/23 08:48,13/Mar/19 16:59,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Large query plans can cause OutOfMemory errors in the Spark driver.

We are creating data frames that are not extremely large but contain lots of nested joins.  These plans execute efficiently because of caching and partitioning, but the text version of the query plans generated can be hundreds of megabytes.  Running many of these in parallel causes our driver process to fail.

{{{{Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOfRange(Arrays.java:2694) at java.lang.String.<init>(String.java:203) at java.lang.StringBuilder.toString(StringBuilder.java:405) at scala.StringContext.standardInterpolator(StringContext.scala:125) at scala.StringContext.s(StringContext.scala:90) at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:70) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:52) at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108) }}}}

 

A similar error is reported in [https://stackoverflow.com/questions/38307258/out-of-memory-error-when-writing-out-spark-dataframes-to-parquet-format]

 

Code exists to truncate the string if the number of output columns is larger than 25, but not if the rest of the query plan is huge.","Amazon EMR 5.19

1 c5.4xlarge master instance

1 c5.4xlarge core instance

2 c5.4xlarge task instances",apachespark,DaveDeCaprio,roczei,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25380,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 13 16:59:11 UTC 2019,,,,,,,,,,"0|s00m5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/18 06:39;apachespark;User 'DaveDeCaprio' has created a pull request for this issue:
https://github.com/apache/spark/pull/23076;;;","26/Nov/18 16:31;vanzin;The proposed patch would probably also fix SPARK-25380.;;;","28/Nov/18 18:06;apachespark;User 'DaveDeCaprio' has created a pull request for this issue:
https://github.com/apache/spark/pull/23169;;;","28/Nov/18 18:07;apachespark;User 'DaveDeCaprio' has created a pull request for this issue:
https://github.com/apache/spark/pull/23169;;;","15/Feb/19 20:54;toopt4;please merge;;;","13/Mar/19 16:59;vanzin;Issue resolved by pull request 23169
[https://github.com/apache/spark/pull/23169];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[History server ]Jobs table and Aggregate metrics table are showing lesser number of tasks ,SPARK-26100,13199086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,abhishek.akg,abhishek.akg,17/Nov/18 11:26,29/Nov/18 19:35,13/Jul/23 08:48,26/Nov/18 21:19,2.3.2,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,Web UI,,,0,,,,,"Test step to reproduce:

1) {{bin/spark-shell --master yarn --conf spark.executor.instances=3}}

2)\{{sc.parallelize(1 to 10000, 10).map{ x => throw new RuntimeException(""Bad executor"")}.collect() }}

 

3) Open Application from the history server UI

Jobs table and Aggregated metrics are showing lesser number of tasks.
 !Screenshot from 2018-11-17 16-55-09.png! 
 

  !Screenshot from 2018-11-17 16-54-42.png! ",,abhishek.akg,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/18 11:27;shahid;Screenshot from 2018-11-17 16-54-42.png;https://issues.apache.org/jira/secure/attachment/12948599/Screenshot+from+2018-11-17+16-54-42.png","17/Nov/18 11:28;shahid;Screenshot from 2018-11-17 16-55-09.png;https://issues.apache.org/jira/secure/attachment/12948600/Screenshot+from+2018-11-17+16-55-09.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 19:35:19 UTC 2018,,,,,,,,,,"0|s00m0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/18 11:27;shahid;Thanks. I am working on it;;;","17/Nov/18 13:33;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23038;;;","29/Nov/18 18:36;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23181;;;","29/Nov/18 19:35;shahid;Sorry, I have wrongly linked the JIRA;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make-distribution.sh is hanging in jenkins,SPARK-26095,13199040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,vanzin,vanzin,16/Nov/18 21:49,29/May/20 21:00,13/Jul/23 08:48,16/Nov/18 23:57,2.4.6,3.0.0,,,,,,,,,,,,,,,,2.4.6,3.0.0,,,Build,,,,0,,,,,"See https://github.com/apache/spark/pull/23017 for further discussion.

maven seems to get stuck here:

{noformat}
""BuilderThread 5"" #80 prio=5 os_prio=0 tid=0x00007f16b8500000 nid=0x7bcf runnable [0x00007f16882fd000]
   java.lang.Thread.State: RUNNABLE
        at org.jdom2.Element.isAncestor(Element.java:1052)
        at org.jdom2.ContentList.checkPreConditions(ContentList.java:222)
        at org.jdom2.ContentList.add(ContentList.java:244)
        at org.jdom2.Element.addContent(Element.java:950)
        at org.apache.maven.plugins.shade.pom.MavenJDOMWriter.insertAtPreferredLocation(MavenJDOMWriter.java:292)
        at org.apache.maven.plugins.shade.pom.MavenJDOMWriter.iterateExclusion(MavenJDOMWriter.java:488)
        at org.apache.maven.plugins.shade.pom.MavenJDOMWriter.updateDependency(MavenJDOMWriter.java:1335)
        at org.apache.maven.plugins.shade.pom.MavenJDOMWriter.iterateDependency(MavenJDOMWriter.java:386)
        at org.apache.maven.plugins.shade.pom.MavenJDOMWriter.updateModel(MavenJDOMWriter.java:1623)
        at org.apache.maven.plugins.shade.pom.MavenJDOMWriter.write(MavenJDOMWriter.java:2156)
        at org.apache.maven.plugins.shade.pom.PomWriter.write(PomWriter.java:75)
        at org.apache.maven.plugins.shade.mojo.ShadeMojo.rewriteDependencyReducedPomIfWeHaveReduction(ShadeMojo.java:1049)
        at org.apache.maven.plugins.shade.mojo.ShadeMojo.createDependencyReducedPom(ShadeMojo.java:978)
        at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:538)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
        at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:200)
        at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:196)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}

And in fact I see a bunch of threads stuck there. Trying a few different things.",,apachespark,maropu,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31541,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 16 23:57:56 UTC 2018,,,,,,,,,,"0|s00lqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/18 21:56;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23061;;;","16/Nov/18 23:57;vanzin;Issue resolved by pull request 23061
[https://github.com/apache/spark/pull/23061];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use CheckpointFileManager to write the streaming metadata file,SPARK-26092,13199012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,16/Nov/18 19:16,16/Nov/18 23:44,13/Jul/23 08:48,16/Nov/18 23:44,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Structured Streaming,,,,0,,,,,We should use CheckpointFileManager to write the streaming metadata file to avoid potential partial file issue.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 16 19:30:53 UTC 2018,,,,,,,,,,"0|s00lkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/18 19:30;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/23060;;;","16/Nov/18 19:30;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/23060;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Key attribute of primitive type under typed aggregation should be named as ""key"" too",SPARK-26085,13198805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,16/Nov/18 01:51,22/Nov/18 02:51,13/Jul/23 08:48,22/Nov/18 02:51,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"When doing typed aggregation on a Dataset, for complex key type, the key attribute is named as ""key"". But for primitive type, the key attribute is named as ""value"". This key attribute should also be named as ""key"" for primitive type.",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 22 02:51:54 UTC 2018,,,,,,,,,,"0|s00kag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/18 01:55;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/23054;;;","16/Nov/18 01:56;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/23054;;;","22/Nov/18 02:51;cloud_fan;Issue resolved by pull request 23054
[https://github.com/apache/spark/pull/23054];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregateExpression.references fails on unresolved expression trees,SPARK-26084,13198776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,simeons,simeons,simeons,15/Nov/18 22:56,20/Nov/18 20:59,13/Jul/23 08:48,20/Nov/18 20:59,2.3.1,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,4,aggregate,regression,sql,,"[SPARK-18394|https://issues.apache.org/jira/browse/SPARK-18394] introduced a stable ordering in {{AttributeSet.toSeq}} using expression IDs ([PR-18959|https://github.com/apache/spark/pull/18959/files#diff-75576f0ec7f9d8b5032000245217d233R128]) without noticing that {{AggregateExpression.references}} used {{AttributeSet.toSeq}} as a shortcut ([link|https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala#L132]). The net result is that {{AggregateExpression.references}} fails for unresolved aggregate functions.

{code:scala}
org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression(
  org.apache.spark.sql.catalyst.expressions.aggregate.Sum(('x + 'y).expr),
  mode = org.apache.spark.sql.catalyst.expressions.aggregate.Complete,
  isDistinct = false
).references
{code}

fails with

{code:scala}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to exprId on unresolved object, tree: 'y
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.exprId(unresolved.scala:104)
	at org.apache.spark.sql.catalyst.expressions.AttributeSet$$anonfun$toSeq$2.apply(AttributeSet.scala:128)
	at org.apache.spark.sql.catalyst.expressions.AttributeSet$$anonfun$toSeq$2.apply(AttributeSet.scala:128)
	at scala.math.Ordering$$anon$5.compare(Ordering.scala:122)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.AbstractSeq.sorted(Seq.scala:41)
	at scala.collection.SeqLike$class.sortBy(SeqLike.scala:623)
	at scala.collection.AbstractSeq.sortBy(Seq.scala:41)
	at org.apache.spark.sql.catalyst.expressions.AttributeSet.toSeq(AttributeSet.scala:128)
	at org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.references(interfaces.scala:201)
{code}

The solution is to avoid calling {{toSeq}} as ordering is not important in {{references}} and simplify (and speed up) the implementation to something like

{code:scala}
mode match {
  case Partial | Complete => aggregateFunction.references
  case PartialMerge | Final => AttributeSet(aggregateFunction.aggBufferAttributes)
}
{code}",,apachespark,hvanhovell,simeons,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 18 01:45:59 UTC 2018,,,,,,,,,,"0|s00k48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/18 22:59;simeons;/cc [~maropu] [~hvanhovell] who worked on the PR that may have caused this problem;;;","16/Nov/18 11:37;hvanhovell;[~simeons] since you have already propose a solution, do you mind opening a PR.;;;","18/Nov/18 01:44;simeons;[~hvanhovell] done [https://github.com/apache/spark/pull/23075];;;","18/Nov/18 01:45;apachespark;User 'ssimeonov' has created a pull request for this issue:
https://github.com/apache/spark/pull/23075;;;","18/Nov/18 01:45;apachespark;User 'ssimeonov' has created a pull request for this issue:
https://github.com/apache/spark/pull/23075;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark command is not working properly with default Docker Image build,SPARK-26083,13198767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qi Shao,Qi Shao,Qi Shao,15/Nov/18 22:37,17/May/20 18:24,13/Jul/23 08:48,03/Dec/18 23:37,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,Spark Core,,,0,easyfix,newbie,patch,pull-request-available,"When I try to run
{code:java}
./bin/pyspark{code}
in a pod in Kubernetes(image built without change from pyspark Dockerfile), I'm getting an error:
{code:java}
$SPARK_HOME/bin/pyspark --deploy-mode client --master k8s://https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT_HTTPS ... 
Python 2.7.15 (default, Aug 22 2018, 13:24:18) [GCC 6.4.0] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. 
Could not open PYTHONSTARTUP 
IOError: [Errno 2] No such file or directory: '/opt/spark/python/pyspark/shell.py'{code}
This is because {{pyspark}} folder doesn't exist under {{/opt/spark/python/}}",,apachespark,Qi Shao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Dec 03 23:37:06 UTC 2018,,,,,,,,,,"0|s00k28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/18 22:41;apachespark;User 'AzureQ' has created a pull request for this issue:
https://github.com/apache/spark/pull/23037;;;","15/Nov/18 22:42;apachespark;User 'AzureQ' has created a pull request for this issue:
https://github.com/apache/spark/pull/23037;;;","03/Dec/18 23:37;vanzin;Issue resolved by pull request 23037
[https://github.com/apache/spark/pull/23037];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misnaming of spark.mesos.fetch(er)Cache.enable in MesosClusterScheduler,SPARK-26082,13198749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mwlon,mwlon,mwlon,15/Nov/18 21:50,10/Feb/19 22:53,13/Jul/23 08:48,07/Feb/19 09:26,2.1.0,2.1.1,2.1.2,2.1.3,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.3.2,,,,,,,,2.3.4,2.4.1,3.0.0,,Mesos,,,,0,,,,,"Currently in [docs|https://spark.apache.org/docs/latest/running-on-mesos.html]:
{quote}spark.mesos.fetcherCache.enable / false / If set to `true`, all URIs (example: `spark.executor.uri`, `spark.mesos.uris`) will be cached by the Mesos Fetcher Cache
{quote}

Currently in {{MesosClusterScheduler.scala}} (which passes parameter to driver):
{{private val useFetchCache = conf.getBoolean(""spark.mesos.fetchCache.enable"", false)}}

Currently in {{MesosCourseGrainedSchedulerBackend.scala}} (which passes mesos caching parameter to executors):
{{private val useFetcherCache = conf.getBoolean(""spark.mesos.fetcherCache.enable"", false)}}

This naming discrepancy dates back to version 2.0.0 ([jira|http://mail-archives.apache.org/mod_mbox/spark-issues/201606.mbox/%3CJIRA.12979909.1466099309000.9921.1466101026233@Atlassian.JIRA%3E]).

This means that when {{spark.mesos.fetcherCache.enable=true}} is specified, the Mesos cache will be used only for executors, and not for drivers.

IMPACT:
Not caching these driver files (typically including at least spark binaries, custom jar, and additional dependencies) adds considerable overhead network traffic and startup time when frequently running spark Applications on a Mesos cluster. Additionally, since extracted files like {{spark-x.x.x-bin-*.tgz}} are additionally copied and left in the sandbox with the cache off (rather than extracted directly without an extra copy), this can considerably increase disk usage. Users CAN currently workaround by specifying the {{spark.mesos.fetchCache.enable}} option, but this should at least be specified in the documentation.

SUGGESTED FIX:
Add {{spark.mesos.fetchCache.enable}} to the documentation for versions 2 - 2.4, and update {{MesosClusterScheduler.scala}} to use {{spark.mesos.fetcherCache.enable}} going forward (literally a one-line change).",,apachespark,dongjoon,maropu,mwlon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15994,,,,,SPARK-26192,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 07 10:30:02 UTC 2019,,,,,,,,,,"0|s00jy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/19 09:06;apachespark;User 'mwlon' has created a pull request for this issue:
https://github.com/apache/spark/pull/23734;;;","07/Feb/19 09:15;dongjoon;Since this bug is introduced by SPARK-15994 which is added Spark 2.1.0, I removed 2.0.x from the affected versions.

BTW, Spark 2.2.x is EOL (https://spark.apache.org/versioning-policy.html).;;;","07/Feb/19 09:26;dongjoon;This is resolved via https://github.com/apache/spark/pull/23734;;;","07/Feb/19 09:28;dongjoon;Thank you, [~mwlon]. You are added to Apache Spark contributor group.;;;","07/Feb/19 10:30;dongjoon;Although this is not a blocker, cc [~maropu].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to run worker.py on Windows,SPARK-26080,13198739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,HaydenJ,HaydenJ,15/Nov/18 21:03,12/Dec/22 18:10,13/Jul/23 08:48,02/Dec/18 09:41,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,PySpark,,,,0,release-notes,,,,"Use of the resource module in python means worker.py cannot run on a windows system. This package is only available in unix based environments.
[https://github.com/apache/spark/blob/9a5fda60e532dc7203d21d5fbe385cd561906ccb/python/pyspark/worker.py#L25]

{code:python}
textFile = sc.textFile(""README.md"")
textFile.first()
{code}
When the above commands are run I receive the error 'worker failed to connect back', and I can see an exception in the console coming from worker.py saying 'ModuleNotFoundError: No module named resource'

I do not really know enough about what I'm doing to fix this myself. Apologies if there's something simple I'm missing here.",Windows 10 Education 64 bit,apachespark,HaydenJ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26670,,,,,,,,,SPARK-25004,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 02 09:41:59 UTC 2018,,,,,,,,,,"0|s00jw0:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,"16/Nov/18 01:25;gurwls223;We should fix this.;;;","16/Nov/18 02:06;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/23055;;;","02/Dec/18 09:41;gurwls223;Issue resolved by pull request 23055
[https://github.com/apache/spark/pull/23055];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: StreamingQueryListenersConfSuite,SPARK-26079,13198720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,15/Nov/18 19:36,20/Nov/18 18:31,13/Jul/23 08:48,20/Nov/18 18:31,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,Tests,,,0,,,,,"We've had this test fail a few times in our builds.

{noformat}
org.scalatest.exceptions.TestFailedException: null equaled null
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
      at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
      at org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite$$anonfun$1.apply(StreamingQueryListenersConfSuite.scala:45)
      at org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite$$anonfun$1.apply(StreamingQueryListenersConfSuite.scala:38)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
{noformat}

You can reproduce it reliably by adding a sleep in the test listener. Fix coming up.",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 20 18:31:28 UTC 2018,,,,,,,,,,"0|s00jrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/18 19:41;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23050;;;","20/Nov/18 18:31;irashid;fixed by https://github.com/apache/spark/pull/23050;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WHERE .. IN fails to filter rows when used in combination with UNION,SPARK-26078,13198671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,avoutilainen,avoutilainen,15/Nov/18 16:40,05/Jan/19 15:12,13/Jul/23 08:48,16/Dec/18 02:59,2.3.1,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,0,correctness,,,,"Hey,

We encountered a case where Spark SQL does not seem to handle WHERE .. IN correctly, when used in combination with UNION, but instead returns also rows that do not fulfill the condition. Swapping the order of the datasets in the UNION makes the problem go away. Repro below:

 
{code}
sql = SQLContext(sc)

a = spark.createDataFrame([{'id': 'a', 'num': 2}, {'id':'b', 'num':1}])
b = spark.createDataFrame([{'id': 'a', 'num': 2}, {'id':'b', 'num':1}])
a.registerTempTable('a')
b.registerTempTable('b')

bug = sql.sql(""""""
    SELECT id,num,source FROM
    (
        SELECT id, num, 'a' as source FROM a
        UNION ALL
        SELECT id, num, 'b' as source FROM b
    ) AS c
    WHERE c.id IN (SELECT id FROM b WHERE num = 2)
"""""")

no_bug = sql.sql(""""""
    SELECT id,num,source FROM
    (
        SELECT id, num, 'b' as source FROM b
        UNION ALL
        SELECT id, num, 'a' as source FROM a
    ) AS c
    WHERE c.id IN (SELECT id FROM b WHERE num = 2)
"""""")

bug.show()
no_bug.show()

bug.explain(True)
no_bug.explain(True)
{code}
This results in one extra row in the ""bug"" DF coming from DF ""b"", that should not be there as it  
{code:java}
>>> bug.show()
+---+---+------+
| id|num|source|
+---+---+------+
|  a|  2|     a|
|  a|  2|     b|
|  b|  1|     b|
+---+---+------+

>>> no_bug.show()
+---+---+------+
| id|num|source|
+---+---+------+
|  a|  2|     b|
|  a|  2|     a|
+---+---+------+
{code}
 The reason can be seen in the query plans:
{code:java}
>>> bug.explain(True)
...
== Optimized Logical Plan ==
Union
:- Project [id#0, num#1L, a AS source#136]
:  +- Join LeftSemi, (id#0 = id#4)
:     :- LogicalRDD [id#0, num#1L], false
:     +- Project [id#4]
:        +- Filter (isnotnull(num#5L) && (num#5L = 2))
:           +- LogicalRDD [id#4, num#5L], false
+- Join LeftSemi, (id#4#172 = id#4#172)
   :- Project [id#4, num#5L, b AS source#137]
   :  +- LogicalRDD [id#4, num#5L], false
   +- Project [id#4 AS id#4#172]
      +- Filter (isnotnull(num#5L) && (num#5L = 2))
         +- LogicalRDD [id#4, num#5L], false
{code}
Note the line *+- Join LeftSemi, (id#4#172 = id#4#172)* - this condition seems wrong, and I believe it causes the LeftSemi to return true for all rows in the left-hand-side table, thus failing to filter as the WHERE .. IN should. Compare with the non-buggy version, where both LeftSemi joins have distinct #-things on both sides:
{code:java}
>>> no_bug.explain()
...
== Optimized Logical Plan ==
Union
:- Project [id#4, num#5L, b AS source#142]
:  +- Join LeftSemi, (id#4 = id#4#173)
:     :- LogicalRDD [id#4, num#5L], false
:     +- Project [id#4 AS id#4#173]
:        +- Filter (isnotnull(num#5L) && (num#5L = 2))
:           +- LogicalRDD [id#4, num#5L], false
+- Project [id#0, num#1L, a AS source#143]
   +- Join LeftSemi, (id#0 = id#4#173)
      :- LogicalRDD [id#0, num#1L], false
      +- Project [id#4 AS id#4#173]
         +- Filter (isnotnull(num#5L) && (num#5L = 2))
            +- LogicalRDD [id#4, num#5L], false
{code}
 

Best,
-Arttu 

 ",,apachespark,avoutilainen,cloud_fan,githubbot,mcheah,mgaido,rxin,Steven Rand,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 16 03:02:18 UTC 2018,,,,,,,,,,"0|s00jgw:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,"15/Nov/18 18:15;mcheah;As per other correctness issues we have seen as of late and as of other discussions around these kinds of things, I'm elevating this to a blocker. [~cloud_fan] [~rxin@databricks.com]. We should look to get this into 2.4.1 and 3.0.;;;","16/Nov/18 05:16;cloud_fan;looks like a bug when we rewrite correlated subquery, cc [~viirya] [~mgaido] ;;;","16/Nov/18 10:18;mgaido;I'll investigate this immediately, thanks [~cloud_fan].;;;","16/Nov/18 10:26;viirya;I simply have a look for it, but don't have great fix yet. If [~mgaido] can come out a PR, I can help review it. Thanks.;;;","16/Nov/18 12:38;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23057;;;","16/Nov/18 12:39;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23057;;;","11/Dec/18 03:40;githubbot;cloud-fan commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240464363
 
 

 ##########
 File path: sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
 ##########
 @@ -1280,4 +1281,46 @@ class SubquerySuite extends QueryTest with SharedSQLContext {
       assert(subqueries.length == 1)
     }
   }
+
+  test(""SPARK-26078: deduplicate fake self joins for IN subqueries"") {
+    withTempView(""a"", ""b"") {
+      def genTestViewWithName(name: String): Unit = {
+        val df = spark.createDataFrame(
+          spark.sparkContext.parallelize(Seq(Row(""a"", 2), Row(""b"", 1))),
+          StructType(Seq(StructField(""id"", StringType), StructField(""num"", IntegerType))))
+        df.createOrReplaceTempView(name)
+      }
+      genTestViewWithName(""a"")
 
 Review comment:
   nit:
   ```
   Seq(""a"" -> 2, ""b"" -> 1).toDF(""id"", ""num"").createTempView(""a"")
   Seq(""a"" -> 2, ""b"" -> 1).toDF(""id"", ""num"").createTempView(""b"")
   ```

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 03:45;githubbot;cloud-fan commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240465020
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
         case (p, InSubquery(values, ListQuery(sub, conditions, _, _))) =>
-          val inConditions = values.zip(sub.output).map(EqualTo.tupled)
+          val newSub = dedupSubqueryOnSelfJoin(values, sub)
+          val inConditions = values.zip(newSub.output).map(EqualTo.tupled)
           val (joinCond, outerPlan) = rewriteExistentialExpr(inConditions ++ conditions, p)
           // Deduplicate conflicting attributes if any.
-          dedupJoin(Join(outerPlan, sub, LeftSemi, joinCond))
+          dedupJoin(Join(outerPlan, newSub, LeftSemi, joinCond))
 
 Review comment:
   do we still need to dedup here?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 03:46;githubbot;cloud-fan commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240465174
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
 
 Review comment:
   Looks like we don't need `dedupJoin`, but always dedup the subquery before putting it in a join.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:13;githubbot;mgaido91 commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240546881
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
         case (p, InSubquery(values, ListQuery(sub, conditions, _, _))) =>
-          val inConditions = values.zip(sub.output).map(EqualTo.tupled)
+          val newSub = dedupSubqueryOnSelfJoin(values, sub)
+          val inConditions = values.zip(newSub.output).map(EqualTo.tupled)
           val (joinCond, outerPlan) = rewriteExistentialExpr(inConditions ++ conditions, p)
           // Deduplicate conflicting attributes if any.
-          dedupJoin(Join(outerPlan, sub, LeftSemi, joinCond))
+          dedupJoin(Join(outerPlan, newSub, LeftSemi, joinCond))
 
 Review comment:
   I think we don't, let me remove it.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:15;githubbot;mgaido91 commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240547635
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
 
 Review comment:
   I think it makes sense to dedup the subquery only when the join condition has not been created yet (so in the case of `InSubquery`). In this case, the condition is already there, so I think we still have to use `dedupJoin` (for `Exists`)

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:21;githubbot;SparkQA commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446149132
 
 
   **[Test build #99968 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99968/testReport)** for PR 23057 at commit [`1beb40c`](https://github.com/apache/spark/commit/1beb40ce0aa21f20d46ce34ce227b0e444ace80b).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:21;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446149182
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:21;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446149189
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5968/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:22;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446149182
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 10:22;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446149189
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5968/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:00;githubbot;SparkQA commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446178058
 
 
   **[Test build #99968 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99968/testReport)** for PR 23057 at commit [`1beb40c`](https://github.com/apache/spark/commit/1beb40ce0aa21f20d46ce34ce227b0e444ace80b).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:01;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446178243
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:01;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446178252
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99968/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:01;githubbot;SparkQA removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446149132
 
 
   **[Test build #99968 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99968/testReport)** for PR 23057 at commit [`1beb40c`](https://github.com/apache/spark/commit/1beb40ce0aa21f20d46ce34ce227b0e444ace80b).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:01;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446178243
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:01;githubbot;mgaido91 commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446178436
 
 
   retest this please

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:02;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446178252
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99968/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:07;githubbot;SparkQA commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446179795
 
 
   **[Test build #99970 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99970/testReport)** for PR 23057 at commit [`1beb40c`](https://github.com/apache/spark/commit/1beb40ce0aa21f20d46ce34ce227b0e444ace80b).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:07;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446179922
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:07;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446179930
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5970/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:08;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446179922
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:08;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446179930
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5970/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:23;githubbot;cloud-fan commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240606988
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
 
 Review comment:
   `dedupJoin` will evetually dedup the subquery IIUC.
   
   What I'd like to do is to unify `dedupJoin` and `dedupSubqueryOnSelfJoin`, so that the code will be consistent for all cases:
   ```
   val newSub = dedup(sub, values)
   // create join condition if any
   Join(outerPlan, newSub, ...)
   ```

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 13:59;githubbot;mgaido91 commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240619938
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
 
 Review comment:
   the main problem is that in the other cases, so when exists is there, the condition is already created. So we would need to complicate quite a lot the method in order to handle the 2 cases and I am not sure wether it is worth. For instance, the `values`, in the `Exists` case, should be taken from the conditions as those expressions referencing attributes from one side and the join condition needs to be rewritten. So I don't think that it is a good idea to have a common rewrite for both them: it would be overcomplicated IMHO.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:00;githubbot;SparkQA commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446232594
 
 
   **[Test build #99970 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99970/testReport)** for PR 23057 at commit [`1beb40c`](https://github.com/apache/spark/commit/1beb40ce0aa21f20d46ce34ce227b0e444ace80b).
    * This patch **fails Spark unit tests**.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:01;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446232914
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:01;githubbot;AmplabJenkins commented on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446232924
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99970/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:01;githubbot;SparkQA removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446179795
 
 
   **[Test build #99970 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99970/testReport)** for PR 23057 at commit [`1beb40c`](https://github.com/apache/spark/commit/1beb40ce0aa21f20d46ce34ce227b0e444ace80b).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:01;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446232914
 
 
   Merged build finished. Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:02;githubbot;AmplabJenkins removed a comment on issue #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#issuecomment-446232924
 
 
   Test FAILed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99970/
   Test FAILed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 15:41;githubbot;cloud-fan commented on a change in pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057#discussion_r240663908
 
 

 ##########
 File path: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
 ##########
 @@ -92,18 +114,20 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // Deduplicate conflicting attributes if any.
           dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
 
 Review comment:
   ah thanks for the explanation! Makes sense to me.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Dec/18 02:59;cloud_fan;Issue resolved by pull request 23057
[https://github.com/apache/spark/pull/23057];;;","16/Dec/18 03:02;githubbot;asfgit closed pull request #23057: [SPARK-26078][SQL] Dedup self-join attributes on IN subqueries
URL: https://github.com/apache/spark/pull/23057
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
index e9b7a8b76e683..34840c6c977a6 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala
@@ -19,7 +19,7 @@ package org.apache.spark.sql.catalyst.optimizer
 
 import scala.collection.mutable.ArrayBuffer
 
-import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.AnalysisException
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.SubExprUtils._
 import org.apache.spark.sql.catalyst.expressions.aggregate._
@@ -43,31 +43,53 @@ import org.apache.spark.sql.types._
  *    condition.
  */
 object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
-  private def dedupJoin(joinPlan: LogicalPlan): LogicalPlan = joinPlan match {
+
+  private def buildJoin(
+      outerPlan: LogicalPlan,
+      subplan: LogicalPlan,
+      joinType: JoinType,
+      condition: Option[Expression]): Join = {
+    // Deduplicate conflicting attributes if any.
+    val dedupSubplan = dedupSubqueryOnSelfJoin(outerPlan, subplan, None, condition)
+    Join(outerPlan, dedupSubplan, joinType, condition)
+  }
+
+  private def dedupSubqueryOnSelfJoin(
+      outerPlan: LogicalPlan,
+      subplan: LogicalPlan,
+      valuesOpt: Option[Seq[Expression]],
+      condition: Option[Expression] = None): LogicalPlan = {
     // SPARK-21835: It is possibly that the two sides of the join have conflicting attributes,
     // the produced join then becomes unresolved and break structural integrity. We should
-    // de-duplicate conflicting attributes. We don't use transformation here because we only
-    // care about the most top join converted from correlated predicate subquery.
-    case j @ Join(left, right, joinType @ (LeftSemi | LeftAnti | ExistenceJoin(_)), joinCond) =>
-      val duplicates = right.outputSet.intersect(left.outputSet)
-      if (duplicates.nonEmpty) {
-        val aliasMap = AttributeMap(duplicates.map { dup =>
-          dup -> Alias(dup, dup.toString)()
-        }.toSeq)
-        val aliasedExpressions = right.output.map { ref =>
-          aliasMap.getOrElse(ref, ref)
-        }
-        val newRight = Project(aliasedExpressions, right)
-        val newJoinCond = joinCond.map { condExpr =>
-          condExpr transform {
-            case a: Attribute => aliasMap.getOrElse(a, a).toAttribute
+    // de-duplicate conflicting attributes.
+    // SPARK-26078: it may also happen that the subquery has conflicting attributes with the outer
+    // values. In this case, the resulting join would contain trivially true conditions (eg.
+    // id#3 = id#3) which cannot be de-duplicated after. In this method, if there are conflicting
+    // attributes in the join condition, the subquery's conflicting attributes are changed using
+    // a projection which aliases them and resolves the problem.
+    val outerReferences = valuesOpt.map(values =>
+      AttributeSet.fromAttributeSets(values.map(_.references))).getOrElse(AttributeSet.empty)
+    val outerRefs = outerPlan.outputSet ++ outerReferences
+    val duplicates = outerRefs.intersect(subplan.outputSet)
+    if (duplicates.nonEmpty) {
+      condition.foreach { e =>
+          val conflictingAttrs = e.references.intersect(duplicates)
+          if (conflictingAttrs.nonEmpty) {
+            throw new AnalysisException(""Found conflicting attributes "" +
+              s""${conflictingAttrs.mkString("","")} in the condition joining outer plan:\n  "" +
+              s""$outerPlan\nand subplan:\n  $subplan"")
           }
-        }
-        Join(left, newRight, joinType, newJoinCond)
-      } else {
-        j
       }
-    case _ => joinPlan
+      val rewrites = AttributeMap(duplicates.map { dup =>
+        dup -> Alias(dup, dup.toString)()
+      }.toSeq)
+      val aliasedExpressions = subplan.output.map { ref =>
+        rewrites.getOrElse(ref, ref)
+      }
+      Project(aliasedExpressions, subplan)
+    } else {
+      subplan
+    }
   }
 
   def apply(plan: LogicalPlan): LogicalPlan = plan transform {
@@ -85,17 +107,16 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
       withSubquery.foldLeft(newFilter) {
         case (p, Exists(sub, conditions, _)) =>
           val (joinCond, outerPlan) = rewriteExistentialExpr(conditions, p)
-          // Deduplicate conflicting attributes if any.
-          dedupJoin(Join(outerPlan, sub, LeftSemi, joinCond))
+          buildJoin(outerPlan, sub, LeftSemi, joinCond)
         case (p, Not(Exists(sub, conditions, _))) =>
           val (joinCond, outerPlan) = rewriteExistentialExpr(conditions, p)
-          // Deduplicate conflicting attributes if any.
-          dedupJoin(Join(outerPlan, sub, LeftAnti, joinCond))
+          buildJoin(outerPlan, sub, LeftAnti, joinCond)
         case (p, InSubquery(values, ListQuery(sub, conditions, _, _))) =>
-          val inConditions = values.zip(sub.output).map(EqualTo.tupled)
-          val (joinCond, outerPlan) = rewriteExistentialExpr(inConditions ++ conditions, p)
           // Deduplicate conflicting attributes if any.
-          dedupJoin(Join(outerPlan, sub, LeftSemi, joinCond))
+          val newSub = dedupSubqueryOnSelfJoin(p, sub, Some(values))
+          val inConditions = values.zip(newSub.output).map(EqualTo.tupled)
+          val (joinCond, outerPlan) = rewriteExistentialExpr(inConditions ++ conditions, p)
+          Join(outerPlan, newSub, LeftSemi, joinCond)
         case (p, Not(InSubquery(values, ListQuery(sub, conditions, _, _)))) =>
           // This is a NULL-aware (left) anti join (NAAJ) e.g. col NOT IN expr
           // Construct the condition. A NULL in one of the conditions is regarded as a positive
@@ -103,7 +124,10 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
 
           // Note that will almost certainly be planned as a Broadcast Nested Loop join.
           // Use EXISTS if performance matters to you.
-          val inConditions = values.zip(sub.output).map(EqualTo.tupled)
+
+          // Deduplicate conflicting attributes if any.
+          val newSub = dedupSubqueryOnSelfJoin(p, sub, Some(values))
+          val inConditions = values.zip(newSub.output).map(EqualTo.tupled)
           val (joinCond, outerPlan) = rewriteExistentialExpr(inConditions, p)
           // Expand the NOT IN expression with the NULL-aware semantic
           // to its full form. That is from:
@@ -118,8 +142,7 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
           // will have the final conditions in the LEFT ANTI as
           // (A.A1 = B.B1 OR ISNULL(A.A1 = B.B1)) AND (B.B2 = A.A2) AND B.B3 > 1
           val finalJoinCond = (nullAwareJoinConds ++ conditions).reduceLeft(And)
-          // Deduplicate conflicting attributes if any.
-          dedupJoin(Join(outerPlan, sub, LeftAnti, Option(finalJoinCond)))
+          Join(outerPlan, newSub, LeftAnti, Option(finalJoinCond))
         case (p, predicate) =>
           val (newCond, inputPlan) = rewriteExistentialExpr(Seq(predicate), p)
           Project(p.output, Filter(newCond.get, inputPlan))
@@ -140,16 +163,16 @@ object RewritePredicateSubquery extends Rule[LogicalPlan] with PredicateHelper {
       e transformUp {
         case Exists(sub, conditions, _) =>
           val exists = AttributeReference(""exists"", BooleanType, nullable = false)()
-          // Deduplicate conflicting attributes if any.
-          newPlan = dedupJoin(
-            Join(newPlan, sub, ExistenceJoin(exists), conditions.reduceLeftOption(And)))
+          newPlan =
+            buildJoin(newPlan, sub, ExistenceJoin(exists), conditions.reduceLeftOption(And))
           exists
         case InSubquery(values, ListQuery(sub, conditions, _, _)) =>
           val exists = AttributeReference(""exists"", BooleanType, nullable = false)()
-          val inConditions = values.zip(sub.output).map(EqualTo.tupled)
-          val newConditions = (inConditions ++ conditions).reduceLeftOption(And)
           // Deduplicate conflicting attributes if any.
-          newPlan = dedupJoin(Join(newPlan, sub, ExistenceJoin(exists), newConditions))
+          val newSub = dedupSubqueryOnSelfJoin(newPlan, sub, Some(values))
+          val inConditions = values.zip(newSub.output).map(EqualTo.tupled)
+          val newConditions = (inConditions ++ conditions).reduceLeftOption(And)
+          newPlan = Join(newPlan, newSub, ExistenceJoin(exists), newConditions)
           exists
       }
     }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
index 5088821ad7361..c95c52f1d3a9c 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
@@ -22,6 +22,7 @@ import scala.collection.mutable.ArrayBuffer
 import org.apache.spark.sql.catalyst.expressions.SubqueryExpression
 import org.apache.spark.sql.catalyst.plans.logical.{Join, LogicalPlan, Sort}
 import org.apache.spark.sql.test.SharedSQLContext
+import org.apache.spark.sql.types._
 
 class SubquerySuite extends QueryTest with SharedSQLContext {
   import testImplicits._
@@ -1280,4 +1281,40 @@ class SubquerySuite extends QueryTest with SharedSQLContext {
       assert(subqueries.length == 1)
     }
   }
+
+  test(""SPARK-26078: deduplicate fake self joins for IN subqueries"") {
+    withTempView(""a"", ""b"") {
+      Seq(""a"" -> 2, ""b"" -> 1).toDF(""id"", ""num"").createTempView(""a"")
+      Seq(""a"" -> 2, ""b"" -> 1).toDF(""id"", ""num"").createTempView(""b"")
+
+      val df1 = spark.sql(
+        """"""
+          |SELECT id,num,source FROM (
+          |  SELECT id, num, 'a' as source FROM a
+          |  UNION ALL
+          |  SELECT id, num, 'b' as source FROM b
+          |) AS c WHERE c.id IN (SELECT id FROM b WHERE num = 2)
+        """""".stripMargin)
+      checkAnswer(df1, Seq(Row(""a"", 2, ""a""), Row(""a"", 2, ""b"")))
+      val df2 = spark.sql(
+        """"""
+          |SELECT id,num,source FROM (
+          |  SELECT id, num, 'a' as source FROM a
+          |  UNION ALL
+          |  SELECT id, num, 'b' as source FROM b
+          |) AS c WHERE c.id NOT IN (SELECT id FROM b WHERE num = 2)
+        """""".stripMargin)
+      checkAnswer(df2, Seq(Row(""b"", 1, ""a""), Row(""b"", 1, ""b"")))
+      val df3 = spark.sql(
+        """"""
+          |SELECT id,num,source FROM (
+          |  SELECT id, num, 'a' as source FROM a
+          |  UNION ALL
+          |  SELECT id, num, 'b' as source FROM b
+          |) AS c WHERE c.id IN (SELECT id FROM b WHERE num = 2) OR
+          |c.id IN (SELECT id FROM b WHERE num = 3)
+        """""".stripMargin)
+      checkAnswer(df3, Seq(Row(""a"", 2, ""a""), Row(""a"", 2, ""b"")))
+    }
+  }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow map as map key,SPARK-26071,13198588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,15/Nov/18 10:50,20/Feb/20 14:10,13/Jul/23 08:48,19/Nov/18 14:43,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 19 14:43:12 UTC 2018,,,,,,,,,,"0|s00iyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/18 11:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23045;;;","15/Nov/18 11:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23045;;;","19/Nov/18 14:43;cloud_fan;Issue resolved by pull request 23045
[https://github.com/apache/spark/pull/23045];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChunkedByteBufferInputStream is truncated by empty chunk,SPARK-26068,13198511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liulinhong,liulinhong,liulinhong,15/Nov/18 05:12,19/Nov/18 14:13,13/Jul/23 08:48,19/Nov/18 14:11,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,0,,,,,"If ChunkedByteBuffer contains empty chunk in the middle of it, then the ChunkedByteBufferInputStream will be truncated. All data behind the empty chunk will not be read.

The problematic code:
{code:java}
// ChunkedByteBuffer.scala
// Assume chunks.next returns an empty chunk, then we will reach
// else branch no matter chunks.hasNext = true or not. So some data is lost.
override def read(dest: Array[Byte], offset: Int, length: Int): Int = {
  if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext)    {
    currentChunk = chunks.next()
  }
  if (currentChunk != null && currentChunk.hasRemaining) {
    val amountToGet = math.min(currentChunk.remaining(), length)
    currentChunk.get(dest, offset, amountToGet)
    amountToGet
  } else {
    close()
    -1
  }
} {code}",,apachespark,cloud_fan,liulinhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 19 14:11:11 UTC 2018,,,,,,,,,,"0|s00ihc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/18 05:16;apachespark;User 'linhong-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/23040;;;","15/Nov/18 05:16;apachespark;User 'linhong-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/23040;;;","19/Nov/18 14:11;cloud_fan;Issue resolved by pull request 23040
[https://github.com/apache/spark/pull/23040];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table joining is broken in Spark 2.4,SPARK-26057,13198253,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,legba,legba,14/Nov/18 08:02,05/Apr/19 02:02,13/Jul/23 08:48,15/Nov/18 12:11,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,"This sample works in spark-shell 2.3.1 and throws an exception in 2.4.0
{code:java}
import java.util.Arrays.asList
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

spark.createDataFrame(
  asList(
    Row(""1-1"", ""sp"", 6),
    Row(""1-1"", ""pc"", 5),
    Row(""1-2"", ""pc"", 4),
    Row(""2-1"", ""sp"", 3),
    Row(""2-2"", ""pc"", 2),
    Row(""2-2"", ""sp"", 1)
  ),
  StructType(List(StructField(""id"", StringType), StructField(""layout"", StringType), StructField(""n"", IntegerType)))
).createOrReplaceTempView(""cc"")

spark.createDataFrame(
  asList(
    Row(""sp"", 1),
    Row(""sp"", 1),
    Row(""sp"", 2),
    Row(""sp"", 3),
    Row(""sp"", 3),
    Row(""sp"", 4),
    Row(""sp"", 5),
    Row(""sp"", 5),
    Row(""pc"", 1),
    Row(""pc"", 2),
    Row(""pc"", 2),
    Row(""pc"", 3),
    Row(""pc"", 4),
    Row(""pc"", 4),
    Row(""pc"", 5)
  ),
  StructType(List(StructField(""layout"", StringType), StructField(""ts"", IntegerType)))
).createOrReplaceTempView(""p"")

spark.createDataFrame(
 asList(
    Row(""1-1"", ""sp"", 1),
    Row(""1-1"", ""sp"", 2),
    Row(""1-1"", ""pc"", 3),
    Row(""1-2"", ""pc"", 3),
    Row(""1-2"", ""pc"", 4),
    Row(""2-1"", ""sp"", 4),
    Row(""2-1"", ""sp"", 5),
    Row(""2-2"", ""pc"", 6),
    Row(""2-2"", ""sp"", 6)
  ),
  StructType(List(StructField(""id"", StringType), StructField(""layout"", StringType), StructField(""ts"", IntegerType)))
).createOrReplaceTempView(""c"")

spark.sql(""""""
SELECT cc.id, cc.layout, count(*) as m
  FROM cc
  JOIN p USING(layout)
  WHERE EXISTS(SELECT 1 FROM c WHERE c.id = cc.id AND c.layout = cc.layout AND c.ts > p.ts)
  GROUP BY cc.id, cc.layout
"""""").createOrReplaceTempView(""pcc"")

spark.sql(""SELECT * FROM pcc ORDER BY id, layout"").show

spark.sql(""""""
SELECT cc.id, cc.layout, n, m
  FROM cc
  LEFT OUTER JOIN pcc ON pcc.id = cc.id AND pcc.layout = cc.layout
"""""").createOrReplaceTempView(""k"")

spark.sql(""SELECT * FROM k ORDER BY id, layout"").show

{code}
Actually I tried to catch another bug: similar calculations with joins and nested queries have different results in Spark 2.3.1 and 2.4.0, but when I tried to create a minimal example I received exception
{code:java}
java.lang.RuntimeException: Couldn't find id#0 in [id#38,layout#39,ts#7,id#10,layout#11,ts#12]
{code}",,apachespark,asolimando,cloud_fan,legba,petertoth,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26041,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 02:02:25 UTC 2019,,,,,,,,,,"0|s00gw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/18 15:33;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23035;;;","14/Nov/18 15:34;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23035;;;","15/Nov/18 12:11;cloud_fan;Issue resolved by pull request 23035
[https://github.com/apache/spark/pull/23035];;;","05/Apr/19 02:02;legba;Thank you very much, it seems fine in 2.4.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flume connector for Spark 2.4 does not exist in Maven repository,SPARK-26048,13198193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,tanakahda,tanakahda,13/Nov/18 23:54,12/Dec/22 18:10,13/Jul/23 08:48,02/Mar/19 16:15,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Deploy,,,,0,,,,,"Flume connector for Spark 2.4 does not exist in the Maven repository.

[https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume]
 [https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume-sink]

These packages will be removed in Spark 3. But Spark 2.4 branch still has these packages.",,dongjoon,tanakahda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 02 16:15:52 UTC 2019,,,,,,,,,,"0|s00gio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/18 03:04;gurwls223;Please avoid to set target versions and Critical+ priority which are usually reserved for committers.;;;","02/Mar/19 16:15;dongjoon;This is resolved via https://github.com/apache/spark/pull/23931

Please note that the code is merged into master branch only.
`branch-2.4` release code has `-Pflume` already.

As [~vanzin] explained, we should use the release script in the `master` branch and [~dbtsai] will use the script for next 2.4.1-RC to include `flume`. So, I set `2.4.1/3.0.0` as `Fixed Versions` (although the code is merged into only `master`.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in the spark 2.4 release package with the spark-avro_2.11 depdency,SPARK-26045,13198160,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,o.garcia,o.garcia,13/Nov/18 21:38,12/Jun/19 18:17,13/Jul/23 08:48,23/May/19 20:22,2.4.0,,,,,,,,,,,,,,,,,2.4.4,3.0.0,,,Build,,,,1,,,,,"Hello I have been problems with the last spark 2.4 release, the read avro file feature does not seem to be working, I have fixed it in local building the source code and updating the *avro-1.8.2.jar* on the *$SPARK_HOME*/jars/ dependencies.

With the default spark 2.4 release when I try to read an avro file spark raise the following exception.  
{code:java}
spark-shell --packages org.apache.spark:spark-avro_2.11:2.4.0


scala> spark.read.format(""avro"").load(""file.avro"")

java.lang.NoSuchMethodError: org.apache.avro.Schema.getLogicalType()Lorg/apache/avro/LogicalType;
at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:51)
at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105
{code}
Checksum:  spark-2.4.0-bin-without-hadoop.tgz: 7670E29B 59EAE7A8 5DBC9350 085DD1E0 F056CA13 11365306 7A6A32E9 B607C68E A8DAA666 EF053350 008D0254 318B70FB DE8A8B97 6586CA19 D65BA2B3 FD7F919E

 

 

 ",4.15.0-38-generic #41-Ubuntu SMP Wed Oct 10 10:59:38 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux,dongjoon,mgaido,o.garcia,pushpendra.jaiswal90@gmail.com,Renzmeister,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 23 20:22:24 UTC 2019,,,,,,,,,,"0|s00gbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/18 09:47;mgaido;[~o.garcia] can you please create a PR for this?;;;","16/Nov/18 16:36;srowen;This looks like you are using a different version of Avro than Spark uses? I am not sure how Spark could compile with this but then fail in this way.;;;","18/Nov/18 21:41;o.garcia;Following the 2.4 spark documentation, https://spark.apache.org/docs/latest/sql-data-sources-avro.html I am launching the spark-shell as follow ""*spark-shell --packages org.apache.spark:spark-avro_2.11:2.4.0*"" and when i try to read an avro file spark.read.format(""avro"").load(path) this error is raised 
java.lang.NoSuchMethodError: org.apache.avro.Schema.getLogicalType()Lorg/apache/avro/LogicalType;
 at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:51)
 at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105)



I am using the spark assembly without hadoop, and my hadoop local version is 3.0.3.

 ;;;","11/Feb/19 16:02;pushpendra.jaiswal90@gmail.com;Its happening with spark 2.3.1 , spark 2.4.0 and hadoop 3.1.1 . Tried with avro-1.8.2;;;","02/Mar/19 20:15;srowen;Avro is already on 1.8.2 in 2.4.0. I think the problem may be that you're adding a separate copy of Avro, which might end up in a different classloader. You wouldn't want to do this, in general, but certainly won't likely work with Spark 2.3, which was on Avro 1.7.7.

Try without adding an avro jar, and make sure you don't have a different Avro dependency in your app or environment. If that's not it, my final guess is that something in hive-exec has a copy of avro. But then I am not sure how the tests would pass.;;;","22/May/19 11:24;Renzmeister;[~srowen] I think the issue here is that the Spark package without Hadoop (so spark-2.4.3-bin-without-hadoop.tgz) seems to be missing 2 files inside $SPARK_HOME/jars:

avro-1.8.2.jar
avro-ipc-1.8.2.jar

I got the same error [~o.garcia]reported. I checked the spark-2.4.3-bin-hadoop2.7.tgz package and it does have the above 2 files. After copying these files into $SPARK_HOME/jars avro was working for me again.

 

 

 

 ;;;","22/May/19 12:32;srowen;Yes this is consistent with the thread on dev@ yesterday. It's the 'without Hadoop' build which means you're using Avro 1.7 from Hadoop. That won't work with the current Spark 2.4. I think the answer may be to decide to not let Avro be 'provided' in this build, but I'm not so sure about the Avro strategy here as there are many other complex compatibility issues with Parquet version too. The fact that people report it works by manually adding Avro 1.8 jars is positive though.;;;","23/May/19 20:22;dongjoon;This is resolved via https://github.com/apache/spark/pull/24680;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregate Metrics table is not sorted properly based on executor ID,SPARK-26044,13198147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,abhishek.akg,abhishek.akg,13/Nov/18 20:27,15/Nov/18 16:28,13/Jul/23 08:48,15/Nov/18 16:28,2.3.2,,,,,,,,,,,,,,,,,3.0.0,,,,Web UI,,,,0,,,,," 

Aggregated Metrics by executor is not properly sorted based on ExecutorID

 

!image-2018-11-14-01-57-55-948.png!",,abhishek.akg,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/18 20:27;abhishek.akg;image-2018-11-14-01-57-55-948.png;https://issues.apache.org/jira/secure/attachment/12948020/image-2018-11-14-01-57-55-948.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 15 16:28:10 UTC 2018,,,,,,,,,,"0|s00g8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/18 20:28;shahid;Thanks. I will raise a PR;;;","13/Nov/18 20:38;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23024;;;","13/Nov/18 20:38;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23024;;;","15/Nov/18 16:28;srowen;Issue resolved by pull request 23024
[https://github.com/apache/spark/pull/23024];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decimal toScalaBigInt/toJavaBigInteger not work for decimals not fitting in long,SPARK-26038,13198014,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,13/Nov/18 10:38,21/Jun/19 14:58,13/Jul/23 08:48,23/Nov/18 20:08,2.2.0,2.3.0,2.4.0,,,,,,,,,,,,,,,2.4.4,3.0.0,,,SQL,,,,0,correctness,,,,Decimal toScalaBigInt/toJavaBigInteger just called toLong.,,apachespark,joshrosen,juliuszsompolski,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 14:58:15 UTC 2019,,,,,,,,,,"0|s00ff4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/18 11:01;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/23022;;;","20/Jun/19 20:47;joshrosen;We just independently rediscovered this bug. I'm adding the {{correctness}} label because the old behavior gave a wrong answer (as opposed to simply crashing). Here's an example illustrating the old, buggy behavior:
{code:java}
val largeBigInt = scala.math.BigInt(""1234567890123456789"" * 2)
val incorrectLongValue = largeBigInt.longValue()

val defaultEncoder: org.apache.spark.sql.Encoder[scala.math.BigInt] =
org.apache.spark.sql.catalyst.encoders.ExpressionEncoder[BigInt]
val ds = spark.createDataset(Seq(largeBigInt))(defaultEncoder).map(identity(_))(defaultEncoder)

val roundtrippedThroughEncoder = ds.first()
println(roundtrippedThroughEncoder == largeBigInt){code}
In Spark 2.4.0, this outputs:
{code:java}
roundtrippedThroughEncoder: scala.math.BigInt = -3191638190864629483{code}
and the equality comparison is {{false}}.

[~smilegator] and [~juliuszsompolski], how do you feel about backporting this for 2.4.4?;;;","20/Jun/19 20:58;smilegator;Yes. It sounds good to me. Feel free to submit a backport PR. ;;;","21/Jun/19 14:58;joshrosen;Backported for 2.4.4 in https://github.com/apache/spark/pull/24928;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset API: repartitionByRange(...) has inconsistent behaviour,SPARK-26024,13197907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,JulienPeloton,JulienPeloton,JulienPeloton,12/Nov/18 21:48,28/Nov/18 17:01,13/Jul/23 08:48,19/Nov/18 14:26,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,dataFrame,partitioning,repartition,spark-sql,"Hi,

I recently played with the {{repartitionByRange}} method for DataFrame introduced in SPARK-22614. For DataFrames larger than the one tested in the code (which has only 10 elements), the code sends back random results.

As a test for showing the inconsistent behaviour, I start as the unit code used to test {{repartitionByRange}} ([here|https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala#L352]) but I increase the size of the initial array to 1000, repartition using 3 partitions, and count the number of element per-partitions:

 
{code}
// Shuffle numbers from 0 to 1000, and make a DataFrame
val df = Random.shuffle(0.to(1000)).toDF(""val"")

// Repartition it using 3 partitions
// Sum up number of elements in each partition, and collect it.
// And do it several times
for (i <- 0 to 9) {
  var counts = df.repartitionByRange(3, col(""val""))
    .mapPartitions{part => Iterator(part.size)}
    .collect()
  println(counts.toList)
}
// -> the number of elements in each partition varies...
{code}
I do not know whether it is expected (I will dig further in the code), but it sounds like a bug.
 Or I just misinterpret what {{repartitionByRange}} is for?
 Any ideas?

Thanks!
 Julien",Spark version 2.3.2,apachespark,cloud_fan,JulienPeloton,mgaido,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 17:01:00 UTC 2018,,,,,,,,,,"0|s00erk:",9223372036854775807,,,,,a.ionescu,,,,,,,,,,,,,,,,,,"13/Nov/18 12:31;mgaido;I think this is the expected behavior, as Spark samples {{spark.sql.execution.rangeExchange.sampleSizePerPartition}} items in order to determine the ranges. Hence, the output may not be consistent, since sampling can return different values. You can increase {{spark.sql.execution.rangeExchange.sampleSizePerPartition}} if you want it to be more precise.;;;","13/Nov/18 13:19;JulienPeloton;Hi Marco and thanks for your quick reply.

You are absolutely right, setting a higher {{sampleSizePerPartition}} fixes the number of elements to be the same at all iterations.

I note however the lack of documentation on this. To my opinion, adding documentation appears to me quite crucial as the default {{sampleSizePerPartition}} is quite small (100). So in most of the real-life cases the same input to {{repartitionByRange}} will lead to random number of elements per partition.;;;","13/Nov/18 13:25;mgaido;I am not sure about that [~JulienPeloton]. In general we don't expose too many specific configurations, as it would become very complex for a user to check them all. Let me cc [~cloud_fan] and [~smilegator] for their opinion on this. ;;;","13/Nov/18 17:48;cloud_fan;range partitioner is usually not very accurate due to performance reasons. You are welcome to send a patch to improve the doc. But I will not increase `sampleSizePerPartition` too much, as it may hurt performance or even OOM. Why would you need a super accurate range partitioner for your (large) data set?  ;;;","13/Nov/18 19:25;JulienPeloton;[~cloud_fan] I would not advocate to increase the size of {{sampleSizePerPartition}} as well, but mentioning on the doc its impact on the result makes sense to me. I will submit a patch then, thanks.
 
??Why would you need a super accurate range partitioner for your (large) data set??

It's more about reproducibility than anything else. For a given input set of parameters, I want the same repartitioned output - partition size included. In addition, I'm working in astronomy and I am used to the low-level RDD API (using {{partitioners}}) which is more flexible and detailed for custom partitioning to my opinion. ;;;","13/Nov/18 21:03;apachespark;User 'JulienPeloton' has created a pull request for this issue:
https://github.com/apache/spark/pull/23025;;;","13/Nov/18 21:04;apachespark;User 'JulienPeloton' has created a pull request for this issue:
https://github.com/apache/spark/pull/23025;;;","19/Nov/18 14:26;cloud_fan;Issue resolved by pull request 23025
[https://github.com/apache/spark/pull/23025];;;","28/Nov/18 17:01;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/23167;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"-0.0 and 0.0 not treated consistently, doesn't match Hive",SPARK-26021,13197873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,adoron,srowen,srowen,12/Nov/18 18:10,31/Jan/20 08:34,13/Jul/23 08:48,23/Nov/18 00:56,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,correctness,,,,"Per [~adoron] and [~mccheah] and SPARK-24834, I'm splitting this out as a new issue:

The underlying issue is how Spark and Hive treat 0.0 and -0.0, which are numerically identical but not the same double value:

In hive, 0.0 and -0.0 are equal since https://issues.apache.org/jira/browse/HIVE-11174.
 That's not the case with spark sql as ""group by"" (non-codegen) treats them as different values. Since their hash is different they're put in different buckets of UnsafeFixedWidthAggregationMap.

In addition there's an inconsistency when using the codegen, for example the following unit test:
{code:java}
println(Seq(0.0d, 0.0d, -0.0d).toDF(""i"").groupBy(""i"").count().collect().mkString("", ""))
{code}
[0.0,3]
{code:java}
println(Seq(0.0d, -0.0d, 0.0d).toDF(""i"").groupBy(""i"").count().collect().mkString("", ""))
{code}
[0.0,1], [-0.0,2]
{code:java}
spark.conf.set(""spark.sql.codegen.wholeStage"", ""false"")
println(Seq(0.0d, -0.0d, 0.0d).toDF(""i"").groupBy(""i"").count().collect().mkString("", ""))
{code}
[0.0,2], [-0.0,1]

Note that the only difference between the first 2 lines is the order of the elements in the Seq.
 This inconsistency is resulted by different partitioning of the Seq and the usage of the generated fast hash map in the first, partial, aggregation.

It looks like we need to add a specific check for -0.0 before hashing (both in codegen and non-codegen modes) if we want to fix this.
",,adoron,apachespark,bduffield,cloud_fan,dongjoon,Samwel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 31 08:34:55 UTC 2020,,,,,,,,,,"0|s00ek0:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"12/Nov/18 18:11;srowen;[~smilegator] and/or [~cloud_fan] might be interested in this? ;;;","13/Nov/18 15:28;adoron;Since the hashing in UnsafeFixedWidthAggregationMap is done on the UnsafeRow as a whole without regard to types I thought maybe the fix can be done in advance by switching -0.0 with 0.0 in the code generated in BoundReference::doGenCode().

Something like
{code:java}
// existing code
var codeGen = code""$javaType ${ev.value} = $value;""
// added fix
if (javaType.code.equals(""double"") || javaType.code.equals(""float"")) {
    codeGen = codeGen + code""\nif (${ev.value} == -0.0) ${ev.value} = 0;""
}
{code};;;","15/Nov/18 08:51;apachespark;User 'adoron' has created a pull request for this issue:
https://github.com/apache/spark/pull/23043;;;","15/Nov/18 08:51;apachespark;User 'adoron' has created a pull request for this issue:
https://github.com/apache/spark/pull/23043;;;","23/Nov/18 00:56;cloud_fan;Issue resolved by pull request 23043
[https://github.com/apache/spark/pull/23043];;;","26/Nov/18 06:46;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23141;;;","26/Nov/18 06:46;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23141;;;","05/Dec/18 16:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23239;;;","09/Dec/18 06:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23265;;;","09/Dec/18 06:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23265;;;","27/Dec/18 19:28;dongjoon;This is reverted from `branch-2.4` via https://github.com/apache/spark/pull/23389 .;;;","25/Jan/20 09:52;dongjoon;According to the above decision (reverting), I set `Target Version` to `3.0.0` to distingush this issue from the other correctness issue.;;;","31/Jan/20 04:23;cloud_fan;FYI: This fix has been replaced by a better fix that retains the difference between -0.0 and 0.0, by https://github.com/apache/spark/pull/23388;;;","31/Jan/20 08:34;dongjoon;Is that backported or marked as correctness issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pyspark/accumulators.py: ""TypeError: object of type 'NoneType' has no len()"" in authenticate_and_accum_updates()",SPARK-26019,13197858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,Tagar,Tagar,12/Nov/18 16:54,12/Dec/22 18:11,13/Jul/23 08:48,03/Jan/19 03:13,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.1,,,PySpark,,,,0,,,,,"pyspark's accumulator server expects a secure py4j connection between python and the jvm.  Spark will normally create a secure connection, but there is a public api which allows you to pass in your own py4j connection.  (this is used by zeppelin, at least.)  When this happens, you get an error like:

{noformat}
pyspark/accumulators.py: ""TypeError: object of type 'NoneType' has no len()"" in authenticate_and_accum_updates()
{noformat}

We should change pyspark to
1) warn loudly if a user passes in an insecure connection
1a) I'd like to suggest that we even error out, unless the user actively opts-in with a config like ""spark.python.allowInsecurePy4j=true""
2) The accumulator server should be changed to allow insecure connections.

note that SPARK-26349 will disallow insecure connections completely in 3.0.
 
More info on how this occurs:

{code:python}
Exception happened during processing of request from ('127.0.0.1', 43418)
----------------------------------------
Traceback (most recent call last):
  File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/SocketServer.py"", line 290, in _handle_request_noblock
    self.process_request(request, client_address)
  File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/SocketServer.py"", line 318, in process_request
    self.finish_request(request, client_address)
  File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/SocketServer.py"", line 331, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/SocketServer.py"", line 652, in __init__
    self.handle()
  File ""/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/python/lib/pyspark.zip/pyspark/accumulators.py"", line 263, in handle
    poll(authenticate_and_accum_updates)
  File ""/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/python/lib/pyspark.zip/pyspark/accumulators.py"", line 238, in poll
    if func():
  File ""/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/python/lib/pyspark.zip/pyspark/accumulators.py"", line 251, in authenticate_and_accum_updates
    received_token = self.rfile.read(len(auth_token))
TypeError: object of type 'NoneType' has no len()
 
{code}
 
Error happens here:
https://github.com/apache/spark/blob/cb90617f894fd51a092710271823ec7d1cd3a668/python/pyspark/accumulators.py#L254

The PySpark code was just running a simple pipeline of 
binary_rdd = sc.binaryRecords(full_file_path, record_length).map(lambda .. )
and then converting it to a dataframe and running a count on it.

It seems error is flaky - on next rerun it didn't happen. (But accumulators don't actually work.)

",,apachespark,bersprockets,dongjoon,githubbot,irashid,saivarunvishal,Tagar,viirya,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26113,,,,,,,SPARK-26349,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 03:13:03 UTC 2019,,,,,,,,,,"0|s00ego:",9223372036854775807,,,,,,,,,,,,,2.3.3,2.4.1,,,,,,,,,"16/Nov/18 02:49;gurwls223;Are you able to make a simple reproducer? If it's about flakiness, we should be able to reproduce it when it's executed multiple times.;;;","16/Nov/18 05:47;Tagar;No, it was the only instance I had for this problem. I will ask again that user who ran into this. ;;;","16/Nov/18 23:19;Tagar;That user said he has seen this error 4-5 times, and just rerunning same code makes it disappear.

;;;","17/Nov/18 06:20;gurwls223;It should be great if we can know what condition and code could reproduce this error - should be reopen-able at any point if we're clear on that.;;;","19/Nov/18 09:34;saivarunvishal;Any help with https://issues.apache.org/jira/browse/SPARK-26113;;;","19/Nov/18 20:22;Tagar;Reproduced myself ;;;","19/Nov/18 20:28;Tagar;[~hyukjin.kwon] today I reproduced this first time .. but we still receive reports from other our users as well. 

Here's code on Spark 2.3.2 + Python 2.7.15.

Execute on a freshly created Spark session :

{code:python}

def python_major_version ():
    import sys
    return(sys.version_info[0])


print(python_major_version())

print(sc.parallelize([1]).map(lambda x: python_major_version()).collect())    # error happens here !

{code}

It always reproduces for me.

Notice that just rerunning the same code makes this error disappear.

;;;","19/Nov/18 20:36;Tagar;Might be broken by https://github.com/apache/spark/pull/22635 change ;;;","19/Nov/18 20:39;Tagar;Sorry, nope it was broken by this change - 
https://github.com/apache/spark/commit/15fc2372269159ea2556b028d4eb8860c4108650#diff-c3339bbf2b850b79445b41e9eecf57c4R249 

;;;","19/Nov/18 20:41;Tagar;cc [~lucacanali] ;;;","19/Nov/18 22:15;irashid;[~hyukjin.kwon] you think maybe these two lines need to be swapped?

https://github.com/apache/spark/blob/master/python/pyspark/accumulators.py#L274-L275

any change you can try with that change [~Tagar] since you've got a handy environment to reproduce?;;;","19/Nov/18 22:42;Tagar;[~irashid] thanks a lot for looking at this ! 
It makes sense two swap those two lines to call parent class constructor after auth_token has been initialized. 

We're using Cloudera's Spark, and pyspark dependencies are inside of a zip file, in a ""immutable"" parcel... 
Unfortunately there is no quick way to test it  as it has to be propagated into all worker nodes. [~bersprockets] any ideas how to test this? 

Thank you.
;;;","20/Nov/18 00:51;gurwls223;I don't understand how the swapping two lines solve this issue. Also, I don't get how it happens and how to reproduce this.
[~Tagar], It's you who reported the issue and it looks odd to ask an arbitrary guy for an idea to test it in Apache JIRA.;;;","20/Nov/18 00:53;gurwls223;Please reopen if this is still being reproduced. Or if there's any analysis for this issue about why this happens. Swapping two lines should be fine but I don't understand why and how.;;;","20/Nov/18 00:59;viirya;Isn't the server beginning to handle requests after calling {{serve_forever}}? It is called after {{AccumulatorServer}} initialization.;;;","20/Nov/18 06:11;Tagar;[~viirya] exception stack reads that error happened in SocketServer.py, BaseRequestHandler class constructor, excerpt from the full exception stack above :

{code:python}
...
  File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/SocketServer.py"", line 652, in __init__
    self.handle()
...
{code}

Notice constructor here calls `self.handle()`  - 
https://github.com/python/cpython/blob/2.7/Lib/SocketServer.py#L655

`handle()` is defined in derived class _UpdateRequestHandler here 
https://github.com/apache/spark/blob/master/python/pyspark/accumulators.py#L232
and expects `auth_token` to be set :
https://github.com/apache/spark/blob/master/python/pyspark/accumulators.py#L254 - that's exactly where exception happens. 

[~irashid] was right - those two lines have to be swapped.

[~hyukjin.kwon] that's odd you closed this jira, although I said it always reproduces for me (100 % of times ), 
and even [posted reproducer here|https://issues.apache.org/jira/secure/EditComment!default.jspa?id=13197858&commentId=16692219].
[~saivarunvishal] also said it happens for him in SPARK-26113 and you closed that jira as well. 
It seems not in line with https://spark.apache.org/contributing.html - ""Contributing Bug Reports"". Please let me know what I miss here.

I called out [~bersprockets] because we use Cloudera distribution of Spark and Cloudera has a few patches on top of open-source Spark. 
I wanted to make sure it's not Cloudera distro specific. Also we worked with Bruce on several other Spark issue and noticed here's in watchers list on this jira... Now I see that this issue is not Cloudera specific though. 

;;;","20/Nov/18 06:30;viirya;{{TCPServer}} begins to process requests only after we call its {{serve_forever}} method. When the server goes to process a request, it will instantiate {{RequestHandlerClass}}:

https://github.com/python/cpython/blob/2.7/Lib/SocketServer.py#L332

As {{RequestHandlerClass}} in {{AccumulatorServer}} is {{_UpdateRequestHandler}}, at the moment, {{_UpdateRequestHandler.handle}} is called and to access {{auth_token}}.

All of above should be happened after we call {{serve_forever}} in {{_start_update_server}}:

{code}
def _start_update_server(auth_token):
    """"""Start a TCP server to receive accumulator updates in a daemon thread, and returns it""""""
    server = AccumulatorServer((""localhost"", 0), _UpdateRequestHandler, auth_token)
    thread = threading.Thread(target=server.serve_forever)
{code}

And I think that we already finish instantiating {{AccumulatorServer}} and {{self.auth_token = auth_token}} is run.;;;","20/Nov/18 07:25;gurwls223;I can't reproduce the issue in my local and yarn cluster. That's why I resolved this JIRA. If it's a Cloudera distribution, please use a channel for Cloudera.;;;","20/Nov/18 07:27;gurwls223;[~Tagar], can you reproduce this in Apache master?

Also, we don't know why it causes the error. I also checked what [~viirya] checked and it doesn't make sense that's the cause.;;;","20/Nov/18 07:32;gurwls223;Also, I followed the guide:

{quote}
For issues that can’t be reproduced against master as reported, resolve as Cannot Reproduce
{quote}

You missed this.;;;","20/Nov/18 07:43;gurwls223;Reopen this if this issue is persistent in Apache master, or if the analysis for it is very clear. I vaguely suspect it's somehow related with Cloudera's parcel. If the usecase is strong enough (I guess so), and we're clear on what's the cause, switching two lines should be fine.;;;","20/Nov/18 19:03;irashid;Yeah I agree with [~viirya]'s analysis, my suggestion was from just a quick glance at the code.  I don't think swapping those lines is likely to help at all ... but I can't come up with any other explanation for how it does happen.  From SPARK-26113, it doesn't seem particular to the cloudera distribution, but we'll poke at it a bit.  SPARK-26113 also makes it sound like a race as it works after the initial failure ...
[~Tagar] are you running a pyspark shell, or with spark-submit?  the token generation is different in those two cases, so that might matter (though I don't see how yet ...)

[~hyukjin.kwon] for errors which appear to be from a race, I don't think we should close immediately because we can't reproduce it, as it can be tricky to reproduce and involve something about the user environment that we dont' immediately understand, that doesn't mean its not a real issue.  (I absolutely agree that if it appears to be related to a specific distribution, it doesn't belong as an issue here).;;;","21/Nov/18 03:55;gurwls223;[~irashid], Yup, maybe I rushed to take an action. I don't mind reopening this if it looks like an real issue to you, and if this issue will likely be resolved in any event at the end.

I have to say ideally the issue should be open after enough analysis and when we're clear it's an issue, rather then blaming unrelated changes or asking how to test to other people.
See how many people and committers spent their time on this issue. Also, I myself spent my time on this issue to check - I failed to reproduce and I failed to understand the analysis made here.

For JIRA management, I have kept it in this way because 99% of such JIRAs are not resolved at the end or actually not an issue given my experience here in JIRA.

For this one, it's okay. I'll leave this one to you.;;;","22/Nov/18 00:34;apachespark;User 'Tagar' has created a pull request for this issue:
https://github.com/apache/spark/pull/23113;;;","22/Nov/18 00:38;Tagar;Thank you [~irashid]

I confirm that swapping those two lines doesn't fix things.

Fixing race condition that happens in accumulators.py: _start_update_server()
 # SocketServer:TCPServer defaults bind_and_activate to True
 [https://github.com/python/cpython/blob/2.7/Lib/SocketServer.py#L413]

 # Also {{handle()}} is defined in derived class _UpdateRequestHandler here
 [https://github.com/apache/spark/blob/master/python/pyspark/accumulators.py#L232]

Please help review [https://github.com/apache/spark/pull/23113] 

Basically fix is to bind and activate SocketServer.TCPServer only in that dedicated thread to serve AccumulatorServer, 
 to avoid race condition that could happen if we start listening and accepting connections in main thread. 

I manually verified and it fixes things for us.

Thank you.;;;","11/Dec/18 10:07;githubbot;HyukjinKwon commented on issue #23113: [SPARK-26019][PYTHON] Fix race condition in accumulators.py: _start_update_server()
URL: https://github.com/apache/spark/pull/23113#issuecomment-446144908
 
 
   So, are we going to update this PR for issuing warning or create another PR?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Dec/18 17:31;irashid;we chatted some offline about this.  the summary here is that zeppelin provides its own py4j gateway connection, and it does not use a secure connection, but the accumulator server is expecting a secure connection.

To avoid a breaking change in 2.x, this ticket should be used to change the accumulator server to allow an insecure connection, but issue a stern warning.  I'd actually also like to suggest that users need to explicitly opt-in to allowing this, eg. you have to set ""spark.python.allowInsecurePy4j=true"".  It would still be a break in 2.4, but at least users could get back the old behavior with a conf.

I'll update the summary of this as well.;;;","12/Dec/18 18:55;githubbot;vanzin closed pull request #23113: [SPARK-26019][PYTHON] Fix race condition in accumulators.py: _start_update_server()
URL: https://github.com/apache/spark/pull/23113
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/python/pyspark/accumulators.py b/python/pyspark/accumulators.py
index 00ec094e7e3b4..5023fd3fd968c 100644
--- a/python/pyspark/accumulators.py
+++ b/python/pyspark/accumulators.py
@@ -271,8 +271,17 @@ def authenticate_and_accum_updates():
 class AccumulatorServer(SocketServer.TCPServer):
 
     def __init__(self, server_address, RequestHandlerClass, auth_token):
-        SocketServer.TCPServer.__init__(self, server_address, RequestHandlerClass)
         self.auth_token = auth_token
+        SocketServer.TCPServer.__init__(self, server_address, RequestHandlerClass, bind_and_activate=False)
+
+    def bind_and_serve(self):
+        try:
+            self.server_bind()
+            self.server_activate()
+            self.serve_forever()
+        except:
+            self.shutdown()
+            raise
 
     """"""
     A simple TCP server that intercepts shutdown() in order to interrupt
@@ -289,7 +298,7 @@ def shutdown(self):
 def _start_update_server(auth_token):
     """"""Start a TCP server to receive accumulator updates in a daemon thread, and returns it""""""
     server = AccumulatorServer((""localhost"", 0), _UpdateRequestHandler, auth_token)
-    thread = threading.Thread(target=server.serve_forever)
+    thread = threading.Thread(target=server.bind_and_serve)
     thread.daemon = True
     thread.start()
     return server


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Dec/18 19:45;dongjoon;Hi, [~irashid]. Is there a corresponding Apache Zeppelin JIRA issue to provide a secure connection?

bq.  the summary here is that zeppelin provides its own py4j gateway connection, and it does not use a secure connection, but the accumulator server is expecting a secure connection.

Allowing insecure connections on a secure server sounds like another security issue. Security is an issue of all or nothing.
bq. To avoid a breaking change in 2.x, this ticket should be used to change the accumulator server to allow an insecure connection,;;;","12/Dec/18 20:59;irashid;Hi [~dongjoon], I don't think there is a zeppelin issue, but it is fixed in the master branch, though not yet released to the best of my knowledge ([~hyukjin.kwon] maybe you know more on the zeppelin side?)

bq. Allowing insecure connections on a secure server sounds like another security issue. Security is an issue of all or nothing.

totally understand your concerns, but the PMC did discuss this and came to this decision.  The reasoning is that the user has chosen to bring in an insecure connection.  Yes, absolutely, with this, you don't really have a ""secure server"".  But we do let users run in insecure modes anyway.;;;","12/Dec/18 22:09;dongjoon;Thank you for the information. If it's considered already, then no problem.;;;","17/Dec/18 17:51;irashid;I'm working on this;;;","17/Dec/18 22:05;githubbot;squito opened a new pull request #23337: [SPARK-26019][PYSPARK] Allow insecure py4j gateways
URL: https://github.com/apache/spark/pull/23337
 
 
   Spark always creates secure py4j connections between java and python,
   but it also allows users to pass in their own connection.  This restores
   the ability for users to pass in an _insecure_ connection, though it
   forces them to set 'spark.python.allowInsecurePy4j=true' and still
   issues a warning.
   
   Added test cases verifying the failure without the extra configuration,
   and verifying things still work with an insecure configuration (in
   particular, accumulators, as those were broken with an insecure py4j
   gateway before).
   
   For the tests, I added ways to create insecure gateways, but I tried to put in protections to make sure that wouldn't get used incorrectly.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Jan/19 03:13;gurwls223;Issue resolved by pull request 23337
[https://github.com/apache/spark/pull/23337];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic partition will fail when both '' and null values are taken as dynamic partition values simultaneously.,SPARK-26012,13197699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eaton,eaton,eaton,12/Nov/18 04:57,10/Apr/19 11:59,13/Jul/23 08:48,10/Apr/19 11:55,2.3.2,2.4.0,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Dynamic partition will fail when both '' and null values are taken as dynamic partition values simultaneously.
 For example, the test bellow will fail.

test(""Null and '' values should not cause dynamic partition failure of string types"") {
 withTable(""t1"", ""t2"")

{ spark.range(3).write.saveAsTable(""t1"") spark.sql(""select id, cast(case when id = 1 then '' else null end as string) as p"" + "" from t1"").write.partitionBy(""p"").saveAsTable(""t2"") checkAnswer(spark.table(""t2"").sort(""id""), Seq(Row(0, null), Row(1, null), Row(2, null))) }

}

The error is: 'org.apache.hadoop.fs.FileAlreadyExistsException: File already exists'.

 

Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: File already exists: [file:/F:/learning/spark/spark_master/spark_compile/spark-warehouse/t2/_temporary/0/_temporary/attempt_20181111204354_0001_m_000000_0/p=__HIVE_DEFAULT_PARTITION__/part-00000-96217c96-3695-4f18-b0db-4f35a9078a3d.c000.snappy.parquet|file:///F:/learning/spark/spark_master/spark_compile/spark-warehouse/t2/_temporary/0/_temporary/attempt_20181111204354_0001_m_000000_0/p=__HIVE_DEFAULT_PARTITION__/part-00000-96217c96-3695-4f18-b0db-4f35a9078a3d.c000.snappy.parquet]
 at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:289)
 at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
 at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
 at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
 at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
 at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
 at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)
 at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)
 at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)
 at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
 at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
 at org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:236)
 at org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:260)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)
 at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)
 ... 10 more

20:43:55.460 WARN org.apache.spark.sql.execution.datasources.FileFormatWriterSuite:",,apachespark,cloud_fan,eaton,maropu,wzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 10 11:55:16 UTC 2019,,,,,,,,,,"0|s00dhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/18 04:58;apachespark;User 'eatoncys' has created a pull request for this issue:
https://github.com/apache/spark/pull/23010;;;","10/Apr/19 11:55;cloud_fan;Issue resolved by pull request 24334
[https://github.com/apache/spark/pull/24334];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pyspark app with ""spark.jars.packages"" config does not work",SPARK-26011,13197690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shanyu,shanyu,shanyu,12/Nov/18 02:52,15/Nov/18 16:32,13/Jul/23 08:48,15/Nov/18 16:32,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Spark Submit,,,,0,,,,,"Command ""pyspark --packages"" works as expected, but if submitting a livy pyspark job with ""spark.jars.packages"" config, the downloaded packages are not added to python's sys.path therefore the package is not available to use.

For example, this command works:

pyspark --packages Azure:mmlspark:0.14

However, using Jupyter notebook with sparkmagic kernel to open a pyspark session failed:

%%configure -f \{""conf"": {spark.jars.packages"": ""Azure:mmlspark:0.14""}}
 import mmlspark

The root cause is that SparkSubmit determines pyspark app by the suffix of primary resource but Livy uses ""spark-internal"" as the primary resource when calling spark-submit, therefore args.isPython is set to false in SparkSubmit.scala.",,apachespark,shanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 15 16:32:15 UTC 2018,,,,,,,,,,"0|s00dfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/18 03:12;apachespark;User 'shanyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/23009;;;","15/Nov/18 16:32;srowen;Issue resolved by pull request 23009
[https://github.com/apache/spark/pull/23009];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrameReader.csv() should respect to spark.sql.columnNameOfCorruptRecord,SPARK-26007,13197637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,11/Nov/18 12:01,12/Dec/22 18:10,13/Jul/23 08:48,13/Nov/18 04:27,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"The csv() method of DataFrameReader doesn't take into account the SQL config spark.sql.columnNameOfCorruptRecord while creating an instance of CSVOptions:
https://github.com/apache/spark/blob/2d085c13b7f715dbff23dd1f81af45ff903d1a79/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L491-L494

This should be fixed by passing sparkSession.sessionState.conf.columnNameOfCorruptRecord as a constructor parameter to CSVOptions. ",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 04:27:04 UTC 2018,,,,,,,,,,"0|s00d3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/18 12:11;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/23006;;;","11/Nov/18 12:11;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/23006;;;","13/Nov/18 04:27;gurwls223;Issue resolved by pull request 23006
[https://github.com/apache/spark/pull/23006];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ANTRL to 4.7.1,SPARK-26005,13197629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,11/Nov/18 08:47,12/Nov/18 07:22,13/Jul/23 08:48,12/Nov/18 07:22,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 11 08:53:00 UTC 2018,,,,,,,,,,"0|s00d20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/18 08:52;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/23005;;;","11/Nov/18 08:53;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/23005;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL date operators calculates with incorrect dayOfYears for dates before 1500-03-01,SPARK-26002,13197581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,10/Nov/18 13:37,25/Jan/20 09:56,13/Jul/23 08:48,08/Jan/19 17:25,1.6.3,2.0.2,2.1.3,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.3.2,2.4.0,3.0.0,,,,,,,3.0.0,,,,SQL,,,,0,correctness,,,,"Running the following SQL the result is incorrect:
{noformat}
scala> sql(""select dayOfYear('1500-01-02')"").show()
+-----------------------------------+
|dayofyear(CAST(1500-01-02 AS DATE))|
+-----------------------------------+
|                                  1|
+-----------------------------------+
{noformat}
This off by one day is more annoying right at the beginning of a year:
{noformat}
scala> sql(""select year('1500-01-01')"").show()
+------------------------------+
|year(CAST(1500-01-01 AS DATE))|
+------------------------------+
|                          1499|
+------------------------------+


scala> sql(""select month('1500-01-01')"").show()
+-------------------------------+
|month(CAST(1500-01-01 AS DATE))|
+-------------------------------+
|                             12|
+-------------------------------+


scala> sql(""select dayOfYear('1500-01-01')"").show()
+-----------------------------------+
|dayofyear(CAST(1500-01-01 AS DATE))|
+-----------------------------------+
|                                365|
+-----------------------------------+
{noformat}
 ",,apachespark,attilapiros,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 25 09:56:02 UTC 2020,,,,,,,,,,"0|s00crc:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"10/Nov/18 13:38;attilapiros;I am already working on a fix.;;;","10/Nov/18 15:26;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/23000;;;","17/Dec/18 03:46;dongjoon;Thanks, [~attilapiros]. I also confirmed that this bug exists in 2.0.2/1.6.3 while `date_format` works correctly.

{code}
scala> sql(""select dayOfYear('1500-01-01')"").show
+-----------------------------------+
|dayofyear(CAST(1500-01-01 AS DATE))|
+-----------------------------------+
|                                 10|
+-----------------------------------+

scala> sc.version
res2: String = 2.0.2
{code}

{code}
scala> sql(""select date_format('1500-01-01', 'D')"").show
+---------------------------------------------+
|date_format(CAST(1500-01-01 AS TIMESTAMP), D)|
+---------------------------------------------+
|                                            1|
+---------------------------------------------+
{code};;;","08/Jan/19 17:25;cloud_fan;Issue resolved by pull request 23000
[https://github.com/apache/spark/pull/23000];;;","25/Jan/20 09:56;dongjoon;Hi, All.
I set `Target Version` to `3.0.0` to distinguish this issue from the other correctness issue.
AFAIK, this is not backported because the situation is not common.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accumulators giving KeyError in pyspark,SPARK-25992,13197410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,AbdealiJK,AbdealiJK,09/Nov/18 13:47,12/Dec/22 18:11,13/Jul/23 08:48,16/Jan/19 15:30,2.3.1,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,PySpark,,,,0,,,,,"I am using accumulators and when I run my code, I sometimes get some warn messages. When I checked, there was nothing accumulated - not sure if I lost info from the accumulator or it worked and I can ignore this error ?

The message:
{noformat}
Exception happened during processing of request from
('127.0.0.1', 62099)
Traceback (most recent call last):
File ""/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py"", line 317, in _handle_request_noblock
    self.process_request(request, client_address)
File ""/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py"", line 348, in process_request
    self.finish_request(request, client_address)
File ""/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py"", line 361, in finish_request
    self.RequestHandlerClass(request, client_address, self)
File ""/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py"", line 696, in __init__
    self.handle()
File ""/usr/local/hadoop/spark2.3.1/python/pyspark/accumulators.py"", line 238, in handle
    _accumulatorRegistry[aid] += update
KeyError: 0
----------------------------------------
2018-11-09 19:09:08 ERROR DAGScheduler:91 - Failed to update accumulators for task 0
org.apache.spark.SparkException: EOF reached before Python server acknowledged
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:634)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1131)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1123)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1123)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{noformat}
",,AbdealiJK,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 15:30:50 UTC 2019,,,,,,,,,,"0|s00bpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/18 10:22;gurwls223;Can you share your codes to reproduce? Also please take a look similar JIRAs and describe symptoms and analysis at your best. I at least see one similar JIRA SPARK-2282.;;;","15/Nov/18 09:54;AbdealiJK;[~hyukjin.kwon] I tried a fair bit and was unable to produce a reproducible minimal example.
I posted it here hoping I could get some understanding when this could occur so I can simplify my code and figure out what is happening.

I went through the pyspark code and as it occurs in _UpdateRequestHandler - which as I understand it is run only on the applicationMaster and not on workers. So, it looks like what was registered as an accumulator on the master is suddenly not seen in the master anymore. (i.e. the accumulator was lost from accumulatorRegistry)

I am creating a single spark session and using it with celery - similar to what spark-celery tries to do https://github.com/gregbaker/spark-celery

Here is the celery loader I use if that helps:
{code:java}
class AppLoader(AppLoader):
     def on_worker_init(self):  # Initialize spark for the worker - once
        try:
            import findspark
            findspark.init()
            import pyspark
        except ImportError:
            import pyspark

        spark_builder = pyspark.sql.SparkSession.builder
        spark_builder = spark_builder.appName(""MySpark"")
        self.spark = spark_builder.getOrCreate()
{code}

I am using celery with a single worker: `--concurrency 1`

I think (and may be mistaken) celery uses subprocess/multiprocessing internally to create a worker thread and runs jobs inside that worker thread. Are there any known issues with: Creating a spark session, Creating a subprocess, Submitting the job (which uses accumulators) in a subprocess.

I can see that _accumulatorRegistry is a global module level variable in pyspark.accumulators - which means that module reimports can cause a problem (not sure if subprocess does a module reimport)

Now that I think of it - _start_update_server is called during context creation. So, does that mean that the socket is created in masterThread and if the accumulator is created in my subprocess - the master will receive updates from the workers. So, my masterThread would throw an error that this accum is unknown because it was created in the subprocess ?


Again: I do not know if celery uses subprocess or something else like asyncio/gevent ... This is pure speculation.;;;","16/Nov/18 02:46;gurwls223;Sounds unclear if it's an issue within Spark or not. Would you be interested in continuing investigation?;;;","22/Nov/18 08:59;AbdealiJK;I do need to solve this, so I will be looking into it but maybe within the next month.;;;","09/Dec/18 13:02;AbdealiJK;Here is a reproducible example in pyspark where using accumulators inside multiprocessing causes errors:

{code:python}
import pyspark
from pyspark.sql import functions as F
import multiprocessing

spark = pyspark.sql.SparkSession.builder.getOrCreate()
print(""appId="", spark.sparkContext.applicationId)


def myfunc(x):
    print(""myfunc({}): appId={}"".format(x, spark.sparkContext.applicationId))
    myaccum = spark.sparkContext.accumulator([], pyspark.accumulators.AddingAccumulatorParam([]))
    df = spark.createDataFrame(
        [['a1', 'b1', 'c1', x * 1],
         ['a2', 'b2', 'c2', x * 2],
         ['a3', 'b3', 'c3', x * 3]],
        ['a', 'b', 'c', 'x'])
    df = df.withColumn(""rnd"", F.rand(42))

    @pyspark.sql.functions.udf
    def myudf(x):
        nonlocal myaccum
        myaccum += [""myudf({})"".format(x)]
        return x

    df = df.withColumn(""x2"", myudf(df['x']))
    df.show()
    print(""Accum value:"", myaccum.value)
    return myaccum

print(""Without multiproc:"")
print(""Got return value:"", myfunc(1).value)
print(""===="")

print(""With multiproc:"")
pool = multiprocessing.Pool(1)
print(""Got return value:"", [i.value for i in pool.map(myfunc, [1, 2, 3])])
print(""===="")
{code}

In this example, I am just using a multiprocPool with 1 worker, so there is no locking issues between threads or concurrency issues between threads or anything of that sort.
;;;","16/Jan/19 07:15;gurwls223;Okay, the problem seems to be because you used multi processing pool. Can you just use thread pool {{from multiprocessing.pool import ThreadPool}}?;;;","16/Jan/19 15:30;gurwls223;Fixed at https://github.com/apache/spark/pull/23564

I documented the limitation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TRANSFORM should handle different data types correctly,SPARK-25990,13197346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,cloud_fan,cloud_fan,09/Nov/18 08:05,23/Mar/20 01:09,13/Jul/23 08:48,14/Feb/20 09:41,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,cloud_fan,Ngone51,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 09:41:57 UTC 2020,,,,,,,,,,"0|s00bb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/20 09:41;cloud_fan;Issue resolved by pull request 27556
[https://github.com/apache/spark/pull/27556];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OneVsRestModel handle empty outputCols incorrectly,SPARK-25989,13197342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,09/Nov/18 07:59,28/Nov/18 15:33,13/Jul/23 08:48,28/Nov/18 15:33,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,ML,,,,0,,,,,"{\{ml.classification.ClassificationModel}} will ignore empty output columns.

However, \{{OneVsRestModel}} still try to append new column even if its name is an empty string.
{code:java}

scala> ovrModel.setPredictionCol("""").transform(test).show
+-----+--------------------+--------------------+---+
|label| features| rawPrediction| |
+-----+--------------------+--------------------+---+
| 0.0|(4,[0,1,2,3],[-0....|[-0.0965652626152...|2.0|
| 0.0|(4,[0,1,2,3],[-0....|[0.07880609384635...|2.0|
| 0.0|(4,[0,1,2,3],[-1....|[0.01891571586984...|2.0|
| 0.0|(4,[0,1,2,3],[0.1...|[0.72409973016524...|0.0|
| 0.0|(4,[0,1,2,3],[0.1...|[0.48045978946729...|2.0|
| 0.0|(4,[0,1,2,3],[0.3...|[1.05496616040758...|0.0|
| 0.0|(4,[0,1,2,3],[0.3...|[0.79508659065535...|0.0|
| 0.0|(4,[0,1,2,3],[0.6...|[1.47437469552081...|0.0|
| 0.0|(4,[0,1,2,3],[0.6...|[1.23302929670223...|0.0|
| 0.0|(4,[0,1,2,3],[0.8...|[1.79816156359706...|0.0|
| 1.0|(4,[0,1,2,3],[-0....|[-3.1564309664080...|1.0|
| 1.0|(4,[0,1,2,3],[-0....|[-3.2217906250571...|1.0|
| 1.0|(4,[0,1,2,3],[-0....|[-2.9171126308553...|1.0|
| 1.0|(4,[0,1,2,3],[-0....|[-2.8316993051998...|1.0|
| 2.0|(4,[0,1,2,3],[-0....|[-1.6486206847760...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.9252139721697...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.9025379528484...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.8518243169707...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-1.0990190524225...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.9973479746889...|2.0|
+-----+--------------------+--------------------+---+
only showing top 20 rows


scala> ovrModel.setPredictionCol("""").setRawPredictionCol(""raw"").transform(test).show
+-----+--------------------+--------------------+---+
|label| features| raw| |
+-----+--------------------+--------------------+---+
| 0.0|(4,[0,1,2,3],[-0....|[-0.0965652626152...|2.0|
| 0.0|(4,[0,1,2,3],[-0....|[0.07880609384635...|2.0|
| 0.0|(4,[0,1,2,3],[-1....|[0.01891571586984...|2.0|
| 0.0|(4,[0,1,2,3],[0.1...|[0.72409973016524...|0.0|
| 0.0|(4,[0,1,2,3],[0.1...|[0.48045978946729...|2.0|
| 0.0|(4,[0,1,2,3],[0.3...|[1.05496616040758...|0.0|
| 0.0|(4,[0,1,2,3],[0.3...|[0.79508659065535...|0.0|
| 0.0|(4,[0,1,2,3],[0.6...|[1.47437469552081...|0.0|
| 0.0|(4,[0,1,2,3],[0.6...|[1.23302929670223...|0.0|
| 0.0|(4,[0,1,2,3],[0.8...|[1.79816156359706...|0.0|
| 1.0|(4,[0,1,2,3],[-0....|[-3.1564309664080...|1.0|
| 1.0|(4,[0,1,2,3],[-0....|[-3.2217906250571...|1.0|
| 1.0|(4,[0,1,2,3],[-0....|[-2.9171126308553...|1.0|
| 1.0|(4,[0,1,2,3],[-0....|[-2.8316993051998...|1.0|
| 2.0|(4,[0,1,2,3],[-0....|[-1.6486206847760...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.9252139721697...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.9025379528484...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.8518243169707...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-1.0990190524225...|2.0|
| 2.0|(4,[0,1,2,3],[-0....|[-0.9973479746889...|2.0|
+-----+--------------------+--------------------+---+
only showing top 20 rows
{code}",,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 15:33:51 UTC 2018,,,,,,,,,,"0|s00ba8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/18 08:02;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/22991;;;","09/Nov/18 08:03;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/22991;;;","28/Nov/18 15:33;srowen;Issue resolved by pull request 22991
[https://github.com/apache/spark/pull/22991];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keep names unchanged when deduplicating the column names in Analyzer,SPARK-25988,13197312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,09/Nov/18 05:33,09/Nov/18 17:42,13/Jul/23 08:48,09/Nov/18 17:42,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,"

{code}
    withTempView(""tmpView1"", ""tmpView2"") {
      withTable(""tab1"", ""tab2"") {
        sql(
          """"""
            |CREATE TABLE `tab1` (`col1` INT, `TDATE` DATE)
            |USING CSV
            |PARTITIONED BY (TDATE)
          """""".stripMargin)
        spark.table(""tab1"").where(""TDATE >= '2017-08-15'"").createOrReplaceTempView(""tmpView1"")
        sql(""CREATE TABLE `tab2` (`TDATE` DATE) USING parquet"")
        sql(
          """"""
            |CREATE OR REPLACE TEMPORARY VIEW tmpView2 AS
            |SELECT N.tdate, col1 AS aliasCol1
            |FROM tmpView1 N
            |JOIN tab2 Z
            |ON N.tdate = Z.tdate
          """""".stripMargin)
        withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> ""0"") {
          sql(""SELECT * FROM tmpView2 x JOIN tmpView2 y ON x.tdate = y.tdate"").collect()
        }
      }
    }
{code}

The above code will issue the following error.


{code}
Expected only partition pruning predicates: ArrayBuffer(isnotnull(tdate#11986), (cast(tdate#11986 as string) >= 2017-08-15));
org.apache.spark.sql.AnalysisException: Expected only partition pruning predicates: ArrayBuffer(isnotnull(tdate#11986), (cast(tdate#11986 as string) >= 2017-08-15));
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogUtils$.prunePartitionsByFilter(ExternalCatalogUtils.scala:146)
	at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.listPartitionsByFilter(InMemoryCatalog.scala:560)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitionsByFilter(ExternalCatalogWithListener.scala:254)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionsByFilter(SessionCatalog.scala:958)
	at org.apache.spark.sql.execution.datasources.CatalogFileIndex.filterPartitions(CatalogFileIndex.scala:73)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:63)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:27)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:256)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:27)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:86)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:86)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:78)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:85)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:147)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3248)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2684)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$144$$anonfun$apply$mcV$sp$59$$anonfun$apply$mcV$sp$60$$anonfun$apply$mcV$sp$61.apply$mcV$sp(SQLQuerySuite.scala:2880)
	at org.apache.spark.sql.catalyst.plans.SQLHelper$class.withSQLConf(SQLHelper.scala:47)
	at org.apache.spark.sql.SQLQuerySuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(SQLQuerySuite.scala:38)
	at org.apache.spark.sql.test.SQLTestUtilsBase$class.withSQLConf(SQLTestUtils.scala:181)
	at org.apache.spark.sql.SQLQuerySuite.withSQLConf(SQLQuerySuite.scala:38)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$144$$anonfun$apply$mcV$sp$59$$anonfun$apply$mcV$sp$60.apply$mcV$sp(SQLQuerySuite.scala:2879)
	at org.apache.spark.sql.test.SQLTestUtilsBase$class.withTable(SQLTestUtils.scala:288)
	at org.apache.spark.sql.SQLQuerySuite.withTable(SQLQuerySuite.scala:38)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$144$$anonfun$apply$mcV$sp$59.apply$mcV$sp(SQLQuerySuite.scala:2862)
	at org.apache.spark.sql.test.SQLTestUtilsBase$class.withTempView(SQLTestUtils.scala:262)
	at org.apache.spark.sql.SQLQuerySuite.withTempView(SQLQuerySuite.scala:38)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$144.apply$mcV$sp(SQLQuerySuite.scala:2861)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$144.apply(SQLQuerySuite.scala:2861)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$144.apply(SQLQuerySuite.scala:2861)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.apache.spark.sql.SQLQuerySuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SQLQuerySuite.scala:38)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:221)
	at org.apache.spark.sql.SQLQuerySuite.runTest(SQLQuerySuite.scala:38)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
{code}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 09 05:38:00 UTC 2018,,,,,,,,,,"0|s00b3k:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"09/Nov/18 05:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22990;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window function: allow parentheses around window reference,SPARK-25979,13197166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,08/Nov/18 16:14,09/Nov/18 17:47,13/Jul/23 08:48,09/Nov/18 17:47,3.0.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,"Very minor parser bug, but possibly problematic for code-generated queries:

Consider the following two queries:

{code}
SELECT avg(k) OVER (w) FROM kv WINDOW w AS (PARTITION BY v ORDER BY w) ORDER BY 1
{code}

and

{code}
SELECT avg(k) OVER w FROM kv WINDOW w AS (PARTITION BY v ORDER BY w) ORDER BY 1
{code}

The former, with parens around the OVER condition, fails to parse while the latter, without parens, succeeds:

{code}
Error in SQL statement: ParseException: 
mismatched input '(' expecting {<EOF>, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 19)

== SQL ==
SELECT avg(k) OVER (w) FROM kv WINDOW w AS (PARTITION BY v ORDER BY w) ORDER BY 1
-------------------^^^
{code}

This was found when running the cockroach DB tests.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 08 16:26:45 UTC 2018,,,,,,,,,,"0|s00a74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/18 16:26;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22987;;;","08/Nov/18 16:26;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22987;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Difference in featureImportances results on computed vs saved models,SPARK-25959,13196729,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,snayakm,snayakm,07/Nov/18 02:58,20/Nov/18 14:47,13/Jul/23 08:48,17/Nov/18 15:47,2.2.0,,,,,,,,,,,,,,,,,3.0.0,,,,ML,MLlib,,,0,,,,,"I tried to implement GBT and found that the feature Importance computed while the model was fit is different when the same model was saved into a storage and loaded back. 

 

I also found that once the persistent model is loaded and saved back again and loaded, the feature importance remains the same. 

 

Not sure if its bug while storing and reading the model first time or am missing some parameter that need to be set before saving the model (thus model is picking some defaults - causing feature importance to change)

 

*Below is the test code:*

val testDF = Seq(
(1, 3, 2, 1, 1),
(3, 2, 1, 2, 0),
(2, 2, 1, 1, 0),
(3, 4, 2, 2, 0),
(2, 2, 1, 3, 1)
).toDF(""a"", ""b"", ""c"", ""d"", ""e"")


val featureColumns = testDF.columns.filter(_ != ""e"")
// Assemble the features into a vector
val assembler = new VectorAssembler().setInputCols(featureColumns).setOutputCol(""features"")
// Transform the data to get the feature data set
val featureDF = assembler.transform(testDF)

// Train a GBT model.
val gbt = new GBTClassifier()
.setLabelCol(""e"")
.setFeaturesCol(""features"")
.setMaxDepth(2)
.setMaxBins(5)
.setMaxIter(10)
.setSeed(10)
.fit(featureDF)

gbt.transform(featureDF).show(false)

// Write out the model

featureColumns.zip(gbt.featureImportances.toArray).sortBy(-_._2).take(20).foreach(println)
/* Prints

(d,0.5931875075767403)
(a,0.3747184548362353)
(b,0.03209403758702444)
(c,0.0)

*/
gbt.write.overwrite().save(""file:///tmp/test123"")

println(""Reading model again"")
val gbtload = GBTClassificationModel.load(""file:///tmp/test123"")

featureColumns.zip(gbtload.featureImportances.toArray).sortBy(-_._2).take(20).foreach(println)

/*

Prints

(d,0.6455841215290767)
(a,0.3316126797964181)
(b,0.022803198674505094)
(c,0.0)

*/


gbtload.write.overwrite().save(""file:///tmp/test123_rewrite"")

val gbtload2 = GBTClassificationModel.load(""file:///tmp/test123_rewrite"")

featureColumns.zip(gbtload2.featureImportances.toArray).sortBy(-_._2).take(20).foreach(println)

/* prints
(d,0.6455841215290767)
(a,0.3316126797964181)
(b,0.022803198674505094)
(c,0.0)

*/",,apachespark,functicons,huaxingao,mgaido,shahid,snayakm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 20 14:47:02 UTC 2018,,,,,,,,,,"0|s007i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/18 15:31;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22986;;;","08/Nov/18 15:32;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22986;;;","17/Nov/18 15:47;srowen;Issue resolved by pull request 22986
[https://github.com/apache/spark/pull/22986];;;","20/Nov/18 02:37;functicons;Can we backport the fix ""[SPARK-25959][ML] GBTClassifier picks wrong impurity stats on loading"" (e00cac9) to Spark 2.2+? I tried to cherry-pick it to 2.2, but there are 2 conflicts I don't know how to resolve correctly:

 

both modified: mllib/src/main/scala/org/apache/spark/ml/classification/GBTClassifier.scala
 both modified: project/MimaExcludes.scala;;;","20/Nov/18 08:45;mgaido;[~srowen] what do you think about backporting this? Maybe 2.2 is a bit too old, I don't know if we are planning any new 2.2 release, but 2.4  - 2.3 branches may be ok. What do you think?;;;","20/Nov/18 14:47;srowen;Yes 2.2 is all but EOL. I am worried about the binary incompatibility issue, and that's why I didn't back-port. Even if the incompatibility isn't in the apparent user-visible API, I wonder if it will cause problems at link time nonetheless. I didn't test it. Is it possible to submit a job compiled from master against an older cluster and just check that it doesn't fail?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
from_json returns wrong result if corrupt record column is in the middle of schema,SPARK-25952,13196641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,06/Nov/18 17:36,12/Dec/22 18:10,13/Jul/23 08:48,08/Nov/18 06:49,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"If an user specifies a corrupt record column via spark.sql.columnNameOfCorruptRecord or JSON options columnNameOfCorruptRecord, schema with the column is propagated to Jackson parser. This breaks an assumption inside of FailureSafeParser that a row returned from Jackson Parser contains only actual data. As a consequence of that FailureSafeParser writes a bad record in wrong position.

For example:
{code:scala}
val schema = new StructType()
      .add(""a"", IntegerType)
      .add(""_unparsed"", StringType)
      .add(""b"", IntegerType)
    val badRec = """"""{""a"" 1, ""b"": 11}""""""
    val df = Seq(badRec, """"""{""a"": 2, ""b"": 12}"""""").toDS()
{code}
the collect() action below
{code:scala}
df.select(from_json($""value"", schema, Map(""columnNameOfCorruptRecord"" -> ""_unparsed""))).collect()
{code}
loses 12:
{code}
Array(Row(Row(null, ""{""a"" 1, ""b"": 11}"", null)), Row(Row(2, null, null)))
{code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 08 06:49:34 UTC 2018,,,,,,,,,,"0|s006yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/18 17:46;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22958;;;","08/Nov/18 06:49;gurwls223;Fixed in https://github.com/apache/spark/pull/22958;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
from_csv should respect to spark.sql.columnNameOfCorruptRecord,SPARK-25950,13196542,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,06/Nov/18 13:14,12/Dec/22 18:11,13/Jul/23 08:48,07/Nov/18 03:26,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,The from_csv() functions should respect to SQL config *spark.sql.columnNameOfCorruptRecord* as from_json() does. Currently it takes into account CSV option *columnNameOfCorruptRecord* only.,,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 07 03:26:47 UTC 2018,,,,,,,,,,"0|s006co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/18 13:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22956;;;","06/Nov/18 13:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22956;;;","07/Nov/18 03:26;gurwls223;Issue resolved by pull request 22956
[https://github.com/apache/spark/pull/22956];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregate expressions shouldn't be resolved on AppendColumns,SPARK-25942,13196188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,05/Nov/18 08:50,13/Nov/18 17:58,13/Jul/23 08:48,13/Nov/18 17:58,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Dataset.groupByKey will bring in new attributes from serializer. If key type is the same as original Dataset's object type, they have same serializer output and so the attribute names will conflict.

This won't be a problem at most of cases, if we don't refer conflict attributes:


{code:java}
val ds: Dataset[(ClassData, Long)] = Seq(ClassData(""one"", 1), ClassData(""two"", 2)).toDS()
  .map(c => ClassData(c.a, c.b + 1))
  .groupByKey(p => p).count()
 {code}


But if we use conflict attributes, `Analyzer` will complain about ambiguous references:

{code}
val ds = Seq(1, 2, 3).toDS()
val agg = ds.groupByKey(_ >= 2).agg(sum(""value"").as[Long], sum($""value"" + 1).as[Long])
{code}
 
{code:java}
org.apache.spark.sql.AnalysisException: Reference 'value' is ambiguous, could be: value, value.;                                          
[info]   at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:247)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:101)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$38.apply(Analyzer.scala:889)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$38.apply(Analyzer.scala:891)
...
{code}

Based on the API document and implementation details of KeyValueGroupedDataset, we should not allow aggregate expressions on KeyValueGroupedDataset to access key attributes.",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 17:58:35 UTC 2018,,,,,,,,,,"0|s0046g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/18 09:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22944;;;","05/Nov/18 09:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22944;;;","13/Nov/18 17:58;cloud_fan;Issue resolved by pull request 22944
[https://github.com/apache/spark/pull/22944];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos: SPARK_CONF_DIR should not be propogated by spark submit,SPARK-25934,13196065,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mmolek,mmolek,mmolek,03/Nov/18 19:20,16/Nov/18 16:01,13/Jul/23 08:48,16/Nov/18 16:01,2.3.2,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Mesos,,,,0,,,,,"This is very similar to how SPARK_HOME caused problems for spark on Mesos in SPARK-12345

The `spark submit` command is setting spark.mesos.driverEnv.SPARK_CONF_DIR to whatever the SPARK_CONF_DIR was for the command that submitted the job.

This is doesn't make sense for most mesos situations, and it broke spark for my team when we upgraded from 2.2.0 to 2.3.2. I haven't tested it but I think 2.4.0 will have the same issue.

It's preventing spark-env.sh from running because now SPARK_CONF_DIR points to some non-existent directory, instead of the unpacked spark binary in the Mesos sandbox like it should.

I'm not that familiar with the spark code base, but I think this could be fixed by simply adding a `&& k != ""SPARK_CONF_DIR""` clause to this filter statement: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala#L421",,apachespark,mmolek,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22466,,,,,SPARK-12345,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 16 16:01:44 UTC 2018,,,,,,,,,,"0|s003f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/18 20:30;apachespark;User 'mpmolek' has created a pull request for this issue:
https://github.com/apache/spark/pull/22937;;;","03/Nov/18 20:31;apachespark;User 'mpmolek' has created a pull request for this issue:
https://github.com/apache/spark/pull/22937;;;","16/Nov/18 16:01;srowen;Issue resolved by pull request 22937
[https://github.com/apache/spark/pull/22937];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scala version string detection when maven-help-plugin is not pre-installed,SPARK-25930,13195960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,skonto,skonto,skonto,02/Nov/18 19:11,17/May/20 18:24,13/Jul/23 08:48,05/Nov/18 14:40,2.4.0,3.0.0,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Kubernetes,Spark Core,,,0,,,,,"This line [https://github.com/apache/spark/blob/c71db43e11fb90d6675421604ad29f596f2b8bfe/resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh#L32] detects the scala version based on the line that does not contain character `[`

All lines contain text in brackets besides the scala version:

[WARNING] Failed to retrieve plugin descriptor for org.eclipse.m2e:lifecycle-mapping:1.0.0: Plugin org.eclipse.m2e:lifecycle-mapping:1.0.0 or one of its dependencies could not be resolved: Failure to find org.eclipse.m2e:lifecycle-mapping:jar:1.0.0 in [https://repo.maven.apache.org/maven2] was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced

[INFO] 
 2.11
 [INFO] ------------------------------------------------------------------------
 [INFO] Reactor Summary:
 [INFO] 
 [INFO] Spark Project Parent POM 3.0.0-SNAPSHOT ............ SUCCESS [ 0.446 s]
 [INFO] Spark Project Tags ................................. SKIPPED
 [INFO] Spark Project Sketch ............................... SKIPPED

If the plugin is not installed several lines will be added to the output :

[INFO] — maven-help-plugin:2.1.1:evaluate (default-cli) @ spark-parent_2.11 —
 Downloading from central: [https://repo.maven.apache.org/maven2/org/apache/maven/plugin-tools/maven-plugin-tools-api/2.4.3/maven-plugin-tools-api-2.4.3.pom]

This will mess with the logic above and the scala version will be wrong failing the tests. 

We should only pick the last line. Although tests are run now manually, this should be fixed.",,apachespark,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 05 14:41:00 UTC 2018,,,,,,,,,,"0|s002rs:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,"02/Nov/18 19:12;skonto;Will have a PR shortly. ;;;","02/Nov/18 19:50;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/22931;;;","02/Nov/18 19:51;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/22931;;;","05/Nov/18 14:41;srowen;Issue resolved by pull request 22931
[https://github.com/apache/spark/pull/22931];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR UT Failure (checking CRAN incoming feasibility),SPARK-25923,13195796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,dongjoon,dongjoon,02/Nov/18 01:20,12/Dec/22 18:11,13/Jul/23 08:48,25/Nov/19 21:34,3.0.0,,,,,,,,,,,,,,,,,,,,,SparkR,,,,0,,,,,"Currently, the following SparkR error blocks PR builders.

{code:java}
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) : 
  dims [product 26] do not match the length of object [0]
Execution halted
{code}

- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/98362/console
- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/98367/console
- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/98368/testReport/
- https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/4403/testReport/
",,dongjoon,felixcheung,shahid,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24152,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 25 21:34:30 UTC 2019,,,,,,,,,,"0|s001rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/18 01:22;dongjoon;Hi, [~shivaram] and [~felixcheung].
Could you take a look at this on master branch? Is it due to CRAN flakiness, or do we need to do something to fix this?;;;","02/Nov/18 01:51;gurwls223;cc [~viirya] as well.;;;","02/Nov/18 03:39;viirya;I found the cause of this error and sent an email to CRAN sysadmin.;;;","02/Nov/18 10:27;viirya;CRAN sysadmin replied me and it should be fixed now.;;;","02/Nov/18 10:31;gurwls223;Thanks. I'm leaving this resolved for now.;;;","03/Nov/18 21:20;felixcheung;thanks - what's the exchange required with CRAN admin?;;;","21/Oct/19 13:21;gurwls223;Hey [~viirya] are you available? Seems CRAN started to fail again.;;;","21/Oct/19 16:40;viirya;Sorry for late. Just requested help from CRAN sysadmin. Let's wait for fix.;;;","21/Oct/19 16:48;viirya;Got a quickly response. Seems fixed now. Please let me know if it is still a problem.;;;","21/Oct/19 16:57;gurwls223;Thanks!;;;","22/Oct/19 01:45;gurwls223;Seems fixed. Resolving.;;;","31/Oct/19 17:30;srowen;[~viirya][~hyukjin.kwon][~dongjoon] Looks like this is happening again -- I wonder if it has anything to do with the changes in master for 3.0 preview?

https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/4910/console

{code}
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) : 
  dims [product 24] do not match the length of object [0]
{code}

Is this something we can resolve on our side in any way or needs CRAN help?;;;","31/Oct/19 17:35;viirya;Noticed that and asked help from CRAN two hours ago.;;;","31/Oct/19 17:50;viirya;Got reply back now. It should be fixed now.;;;","25/Nov/19 05:24;dongjoon;This seems to happen again.
- PR: https://github.com/apache/spark/pull/26611
- Jenkins: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/114370/console;;;","25/Nov/19 05:35;viirya;Sent an email to CRAN for help.;;;","25/Nov/19 16:10;viirya;Should be fixed now.;;;","25/Nov/19 21:34;dongjoon;Yes. Thanks! After monitoring some PRs, this is resolved from CRAN side completely.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[K8] Spark Driver/Executor ""spark-app-selector"" label mismatch",SPARK-25922,13195782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,suxingfate,akhurana,akhurana,01/Nov/18 23:41,17/May/20 18:26,13/Jul/23 08:48,15/Feb/19 18:09,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Kubernetes,Spark Core,,,0,,,,,"Hi,

I have been testing Spark 2.4.0 RC4 on Kubernetes  to run Python Spark Applications and running into an issue where the AppId label on the driver and executors mis-match. I am using the [https://github.com/GoogleCloudPlatform/spark-on-k8s-operator] to run these applications. 

I see a spark.app.id of the form spark-* as  ""spark-app-selector"" label on the driver as well as in the K8 config-map which gets created for the driver via spark-submit . My guess is this is coming from [https://github.com/apache/spark/blob/f6cc354d83c2c9a757f9b507aadd4dbdc5825cca/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L211] 

But when the driver actually comes up and brings up executors etc. , I see that the ""spark-app-selector"" label on the executors as well as the spark.app.Id config within the user-code on the driver is something of the form spark-application-* ( probably from [https://github.com/apache/spark/blob/b19a28dea098c7d6188f8540429c50f42952d678/core/src/main/scala/org/apache/spark/SparkContext.scala#L511] & [https://github.com/apache/spark/blob/bfb74394a5513134ea1da9fcf4a1783b77dd64e4/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala#L26|https://github.com/apache/spark/blob/bfb74394a5513134ea1da9fcf4a1783b77dd64e4/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala#L26)] )

We were consuming this ""spark-app-selector"" label on the Driver Pod to get the App Id and use it to look-up the app in SparkHistory server (among other use-cases). but due to this mis-match, this logic no longer works. This was working fine in Spark 2.2 fork for Kubernetes which i was using earlier. Is this expected behavior and if yes, what's the correct way to fetch the applicationId from outside the application ?  

Let me know if I can provide any more details or if I am doing something wrong. Here is an example run with different *spark-app-selector* label on the driver/executor : 

 
{code:java}
Name: pyfiles-driver
Namespace: default
Priority: 0
PriorityClassName: <none>
Start Time: Thu, 01 Nov 2018 18:19:46 -0700
Labels: spark-app-selector=spark-b78bb10feebf4e2d98c11d7b6320e18f
 spark-role=driver
 sparkoperator.k8s.io/app-name=pyfiles
 sparkoperator.k8s.io/launched-by-spark-operator=true
 version=2.4.0
Status: Running



Name: pyfiles-1541121585642-exec-1
Namespace: default
Priority: 0
PriorityClassName: <none>
Start Time: Thu, 01 Nov 2018 18:24:02 -0700
Labels: spark-app-selector=spark-application-1541121829445
 spark-exec-id=1
 spark-role=executor
 sparkoperator.k8s.io/app-name=pyfiles
 sparkoperator.k8s.io/launched-by-spark-operator=true
 version=2.4.0
Status: Pending
{code}
 

 

 ",Spark 2.4.0 RC4,akhurana,githubbot,liyinan926,suxingfate,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 15 18:09:22 UTC 2019,,,,,,,,,,"0|s001og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/18 20:30;liyinan926;The application ID used to set the {{spark-app-selector}} label for the driver pod is from this line [https://github.com/apache/spark/blob/3404a73f4cf7be37e574026d08ad5cf82cfac871/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L217.] The application ID used to set the {{spark-app-selector}} label for the executor pod is from this line [https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala#L87|https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala#L87,]. Agreed that it's problematic that two different labels are used.;;;","14/Dec/18 12:23;githubbot;suxingfate opened a new pull request #23322: [SPARK-25922][K8] Spark Driver/Executor spark-app-selector label mism…
URL: https://github.com/apache/spark/pull/23322
 
 
   ## What changes were proposed in this pull request?
   
   In K8S Cluster mode, the algorithm to generate spark-app-selector/spark.app.id of spark driver is different with spark executor.
   This patch consolidated the algorithm for driver and executor to have a universal logic. This will help to monitor resource
   consumption from K8S perspective.
   In K8S Client mode, this makes sure it will also use the same algorithm to generate spark-app-selector/spark.app.id for executors.
   
   ## How was this patch tested?
   
   Manually run.""

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Dec/18 12:28;suxingfate;Hi [~liyinan926],

Recently while evaluating spark on k8s, We are setting up tools to monitor resource usage from k8s perspective and also met this problem. 

I created a pull request for this. Could you please help to review?

Thanks,;;;","17/Dec/18 21:40;githubbot;asfgit closed pull request #23322: [SPARK-25922][K8] Spark Driver/Executor ""spark-app-selector"" label mismatch
URL: https://github.com/apache/spark/pull/23322
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala
index 68f6f2e46e316..03f5da2bb0bce 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala
@@ -18,9 +18,10 @@ package org.apache.spark.scheduler.cluster.k8s
 
 import java.util.concurrent.ExecutorService
 
-import io.fabric8.kubernetes.client.KubernetesClient
 import scala.concurrent.{ExecutionContext, Future}
 
+import io.fabric8.kubernetes.client.KubernetesClient
+
 import org.apache.spark.SparkContext
 import org.apache.spark.deploy.k8s.Config._
 import org.apache.spark.deploy.k8s.Constants._
@@ -39,10 +40,10 @@ private[spark] class KubernetesClusterSchedulerBackend(
     lifecycleEventHandler: ExecutorPodsLifecycleManager,
     watchEvents: ExecutorPodsWatchSnapshotSource,
     pollEvents: ExecutorPodsPollingSnapshotSource)
-  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {
+    extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {
 
-  private implicit val requestExecutorContext = ExecutionContext.fromExecutorService(
-    requestExecutorsService)
+  private implicit val requestExecutorContext =
+    ExecutionContext.fromExecutorService(requestExecutorsService)
 
   protected override val minRegisteredRatio =
     if (conf.getOption(""spark.scheduler.minRegisteredResourcesRatio"").isEmpty) {
@@ -60,6 +61,17 @@ private[spark] class KubernetesClusterSchedulerBackend(
     removeExecutor(executorId, reason)
   }
 
+  /**
+   * Get an application ID associated with the job.
+   * This returns the string value of spark.app.id if set, otherwise
+   * the locally-generated ID from the superclass.
+   *
+   * @return The application ID
+   */
+  override def applicationId(): String = {
+    conf.getOption(""spark.app.id"").map(_.toString).getOrElse(super.applicationId)
+  }
+
   override def start(): Unit = {
     super.start()
     if (!Utils.isDynamicAllocationEnabled(conf)) {
@@ -88,7 +100,8 @@ private[spark] class KubernetesClusterSchedulerBackend(
 
     if (shouldDeleteExecutors) {
       Utils.tryLogNonFatalError {
-        kubernetesClient.pods()
+        kubernetesClient
+          .pods()
           .withLabel(SPARK_APP_ID_LABEL, applicationId())
           .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)
           .delete()
@@ -120,7 +133,8 @@ private[spark] class KubernetesClusterSchedulerBackend(
   }
 
   override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = Future[Boolean] {
-    kubernetesClient.pods()
+    kubernetesClient
+      .pods()
       .withLabel(SPARK_APP_ID_LABEL, applicationId())
       .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)
       .withLabelIn(SPARK_EXECUTOR_ID_LABEL, executorIds: _*)
@@ -133,7 +147,7 @@ private[spark] class KubernetesClusterSchedulerBackend(
   }
 
   private class KubernetesDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])
-    extends DriverEndpoint(rpcEnv, sparkProperties) {
+      extends DriverEndpoint(rpcEnv, sparkProperties) {
 
     override def onDisconnected(rpcAddress: RpcAddress): Unit = {
       // Don't do anything besides disabling the executor - allow the Kubernetes API events to
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala
index 75232f7b98b04..6e182bed459f8 100644
--- a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala
@@ -37,6 +37,7 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn
   private val requestExecutorsService = new DeterministicScheduler()
   private val sparkConf = new SparkConf(false)
     .set(""spark.executor.instances"", ""3"")
+    .set(""spark.app.id"", TEST_SPARK_APP_ID)
 
   @Mock
   private var sc: SparkContext = _
@@ -87,8 +88,10 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn
     when(sc.env).thenReturn(env)
     when(env.rpcEnv).thenReturn(rpcEnv)
     driverEndpoint = ArgumentCaptor.forClass(classOf[RpcEndpoint])
-    when(rpcEnv.setupEndpoint(
-      mockitoEq(CoarseGrainedSchedulerBackend.ENDPOINT_NAME), driverEndpoint.capture()))
+    when(
+      rpcEnv.setupEndpoint(
+        mockitoEq(CoarseGrainedSchedulerBackend.ENDPOINT_NAME),
+        driverEndpoint.capture()))
       .thenReturn(driverEndpointRef)
     when(kubernetesClient.pods()).thenReturn(podOperations)
     schedulerBackendUnderTest = new KubernetesClusterSchedulerBackend(
@@ -100,9 +103,7 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn
       podAllocator,
       lifecycleEventHandler,
       watchEvents,
-      pollEvents) {
-      override def applicationId(): String = TEST_SPARK_APP_ID
-    }
+      pollEvents)
   }
 
   test(""Start all components"") {
@@ -127,8 +128,7 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn
 
   test(""Remove executor"") {
     schedulerBackendUnderTest.start()
-    schedulerBackendUnderTest.doRemoveExecutor(
-      ""1"", ExecutorKilled)
+    schedulerBackendUnderTest.doRemoveExecutor(""1"", ExecutorKilled)
     verify(driverEndpointRef).send(RemoveExecutor(""1"", ExecutorKilled))
   }
 


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Feb/19 18:09;vanzin;Issue resolved by pull request 23779
[https://github.com/apache/spark/pull/23779];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python worker reuse causes Barrier tasks to run without BarrierTaskContext,SPARK-25921,13195765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,XuanYuan,bago.amirbekian,bago.amirbekian,01/Nov/18 22:05,11/Jan/19 09:03,13/Jul/23 08:48,13/Nov/18 09:07,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,PySpark,Spark Core,,,1,,,,,"Running a barrier job after a normal spark job causes the barrier job to run without a BarrierTaskContext. Here is some code to reproduce.

 
{code:java}
def task(*args):
  from pyspark import BarrierTaskContext
  context = BarrierTaskContext.get()
  context.barrier()
  print(""in barrier phase"")
  context.barrier()
  return []
a = sc.parallelize(list(range(4))).map(lambda x: x ** 2).collect()
assert a == [0, 1, 4, 9]
b = sc.parallelize(list(range(4)), 4).barrier().mapPartitions(task).collect()

{code}
 
Here is some of the trace

{code:java}
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(14, 0) finished unsuccessfully.
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/databricks/spark/python/pyspark/worker.py"", line 372, in main
    process()
  File ""/databricks/spark/python/pyspark/worker.py"", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2482, in func
    return f(iterator)
  File ""<command-717066991496742>"", line 4, in task
AttributeError: 'TaskContext' object has no attribute 'barrier'
{code}
 ",,apachespark,bago.amirbekian,cloud_fan,dongjoon,felixcheung,irashid,mengxr,tgraves,viirya,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 09:07:15 UTC 2018,,,,,,,,,,"0|s001ko:",9223372036854775807,,,,,,,,,,,,,2.4.1,,,,,,,,,,"01/Nov/18 22:12;bago.amirbekian;[~mengxr] [~jiangxb1987] Could you have a look.;;;","01/Nov/18 23:12;mengxr;Tested offline. This was caused by PySpark worker reuse. A workaround is to disable worker reuse. Not a blocker for 2.4, but we should list it as a known issue and fix it in 2.4.1. cc: [~cloud_fan] [~smilegator];;;","07/Nov/18 10:07;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/22962;;;","07/Nov/18 10:08;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/22962;;;","13/Nov/18 09:07;cloud_fan;Issue resolved by pull request 22962
[https://github.com/apache/spark/pull/22962];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Date value corrupts when tables are ""ParquetHiveSerDe"" formatted and target table is Partitioned",SPARK-25919,13195741,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pawanlawale,pawanlawale,01/Nov/18 20:40,04/Jan/19 10:26,13/Jul/23 08:48,05/Dec/18 19:33,2.1.0,2.2.1,,,,,,,,,,,,,,,,,,,,Spark Core,Spark Shell,Spark Submit,,0,,,,,"Hi

I found a really strange issue. Below are the steps to reproduce it. This issue occurs only when the table row format is ParquetHiveSerDe and the target table is Partitioned

*Hive:*

Login in to hive terminal on cluster and create below tables.
{code:java}
create table t_src(
name varchar(10),
dob timestamp
)
ROW FORMAT SERDE 
 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
STORED AS INPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
OUTPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
create table t_tgt(
name varchar(10),
dob timestamp
)
PARTITIONED BY (city varchar(10))
ROW FORMAT SERDE 
 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
STORED AS INPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
OUTPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';


{code}
Insert data into the source table (t_src)
{code:java}
INSERT INTO t_src VALUES ('p1', '0001-01-01 00:00:00.0'),('p2', '0002-01-01 00:00:00.0'), ('p3', '0003-01-01 00:00:00.0'),('p4', '0004-01-01 00:00:00.0');{code}
*Spark-shell:*

Get on to spark-shell. 

Execute below commands on spark shell:
{code:java}
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
val q0 = ""TRUNCATE table t_tgt""
val q1 = ""SELECT CAST(alias.name AS STRING) as a0, alias.dob as a1 FROM DEFAULT.t_src alias""
val q2 = ""INSERT INTO TABLE DEFAULT.t_tgt PARTITION (city) SELECT tbl0.a0 as c0, tbl0.a1 as c1, NULL as c2 FROM tbl0""
sqlContext.sql(q0)
sqlContext.sql(q1).select(""a0"",""a1"").createOrReplaceTempView(""tbl0"")
sqlContext.sql(q2)


{code}
 After this check the contents of target table t_tgt. You will see the date ""0001-01-01 00:00:00"" changed to ""0002-01-01 00:00:00"". Below snippets shows the contents of both the tables:
{code:java}
select * from t_src;
+-------------+------------------------+--+
| t_src.name | t_src.dob |
+-------------+------------------------+--+
| p1 | 0001-01-01 00:00:00.0 |
| p2 | 0002-01-01 00:00:00.0 |
| p3 | 0003-01-01 00:00:00.0 |
| p4 | 0004-01-01 00:00:00.0 |
+-------------+------------------------+–+
 select * from t_tgt;
+-------------+------------------------+--+
| t_src.name | t_src.dob | t_tgt.city |
+-------------+------------------------+--+
| p1 | 0002-01-01 00:00:00.0 |__HIVE_DEF |
| p2 | 0002-01-01 00:00:00.0 |__HIVE_DEF |
| p3 | 0003-01-01 00:00:00.0 |__HIVE_DEF |
| p4 | 0004-01-01 00:00:00.0 |__HIVE_DEF |
+-------------+------------------------+--+
{code}
 

Is this a known issue? Is it fixed in any subsequent releases?

Thanks & regards,

Pawan Lawale",,adrian-wang,pawanlawale,smohr003,Teng Peng,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25873,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 05 19:33:00 UTC 2018,,,,,,,,,,"0|s001fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/18 22:27;smohr003;I cannot reproduce this. 

Please note that I get an error in the spark side, regarding 
{code:java}
hive.exec.dynamic.partition.mode{code}
that should be set to nonstrict 

Having set that 
{code:java}
sqlContext.setConf(""hive.exec.dynamic.partition.mode"", ""nonstrict""){code}
, there is no problem with data in tables. I am using Hive 2.1 with Spark 2.2. ;;;","05/Dec/18 19:30;pawanlawale;I just figured out why is this the issue. Its because of the hive-exec jar packaged with Spark. The latest version which is packaged Spark-2.1.0  to Spark-2.3.1 is hive-exec-1.2.1.spark2.jar. However the parquet timestamp bug was fixed by Hive in hive-exec-2.0.0.jar, which is not available in spark packages which I mentioned earlier.

It was fixed as a part of below Hive Jira

https://issues.apache.org/jira/browse/HIVE-11771

Thanks & regards,

Pawan Lawale;;;","05/Dec/18 19:33;pawanlawale;This was fixed by Hive in later versions of Jar which are not currently used by Spark yet.

https://issues.apache.org/jira/browse/HIVE-11771;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOAD DATA LOCAL INPATH should handle a relative path,SPARK-25918,13195738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,01/Nov/18 20:26,02/Nov/18 06:18,13/Jul/23 08:48,02/Nov/18 06:18,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,SQL,,,,0,,,,,"Unfortunately, it seems that we missed this in 2.4.0. In Spark 2.4, if the default file system is not the local file system, LOAD DATA LOCAL INPATH only works in case of absolute paths. This PR aims to fix it to support relative paths. This is a regression in 2.4.0.

{code}
$ ls kv1.txt
kv1.txt

scala> spark.sql(""LOAD DATA LOCAL INPATH 'kv1.txt' INTO TABLE t"")
org.apache.spark.sql.AnalysisException: LOAD DATA input path does not exist: kv1.txt;
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 02 06:18:56 UTC 2018,,,,,,,,,,"0|s001eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/18 20:36;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22927;;;","02/Nov/18 06:18;dongjoon;Issue resolved by pull request 22927
[https://github.com/apache/spark/pull/22927];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in documentation: number of cluster managers,SPARK-25909,13195671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,jameslamb,jameslamb,jameslamb,01/Nov/18 14:25,02/Nov/18 16:06,13/Jul/23 08:48,02/Nov/18 16:06,2.3.2,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Documentation,,,,0,,,,,"The documentation [here|https://spark.apache.org/docs/latest/cluster-overview.html] states that there are ""three types of cluster manager available"", but then goes on to list four (standalone, Mesos, Yarn, and k8s).

The documentation should read ""four types of cluster manager available..."".

 

I am making this bug to document the issue. I will submit a PR immediately to resolve it.",,apachespark,jameslamb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 02 16:06:35 UTC 2018,,,,,,,,,,"0|s000zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/18 14:31;apachespark;User 'jameslamb' has created a pull request for this issue:
https://github.com/apache/spark/pull/22922;;;","02/Nov/18 16:06;srowen;It's fine, but no need to make a Jira for trivial issues. There is no difference between 'what' to fix and 'how' to fix it.;;;","02/Nov/18 16:06;srowen;Issue resolved by pull request 22922
[https://github.com/apache/spark/pull/22922];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell cannot handle `-i` option correctly,SPARK-25906,13195559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,dongjoon,dongjoon,01/Nov/18 06:25,12/Dec/22 18:11,13/Jul/23 08:48,06/Nov/18 02:41,2.4.0,,,,,,,,,,,,,,,,,2.4.1,,,,Spark Shell,SQL,,,0,,,,,"This is a regression on Spark 2.4.0.

*Spark 2.3.2*
{code:java}
$ cat test.scala
spark.version
case class Record(key: Int, value: String)
spark.sparkContext.parallelize((1 to 2).map(i => Record(i, s""val_$i""))).toDF.show

$ bin/spark-shell -i test.scala
18/10/31 23:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1541053368478).
Spark session available as 'spark'.
Loading test.scala...
res0: String = 2.3.2
defined class Record
18/10/31 23:22:56 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
+---+-----+
|key|value|
+---+-----+
|  1|val_1|
|  2|val_2|
+---+-----+
{code}
*Spark 2.4.0 RC5*
{code:java}
$ bin/spark-shell -i test.scala
2018-10-31 23:23:14 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1541053400312).
Spark session available as 'spark'.
test.scala:17: error: value toDF is not a member of org.apache.spark.rdd.RDD[Record]
Error occurred in an application involving default arguments.
       spark.sparkContext.parallelize((1 to 2).map(i => Record(i, s""val_$i""))).toDF.show
{code}


*WORKAROUND*
Add the following line at the first of the script.
{code}
import spark.implicits._
{code}",,apachespark,cloud_fan,dongjoon,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25587,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 06 02:41:23 UTC 2018,,,,,,,,,,"0|s000aw:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,"01/Nov/18 06:26;dongjoon;cc [~cloud_fan] and [~smilegator] .

Although this is a regression, I marked this as non-blocker.;;;","01/Nov/18 06:29;dongjoon;Hi, [~dbtsai] . Could you take a look at this?

This seems to be related to the recent shell changes. Only `-i` option has a problem. In the shell, it works. I guess this will be a known issue of 2.4.0
{code:java}
scala> spark.version
res1: String = 2.4.0

scala> case class Record(key: Int, value: String)
defined class Record

scala> spark.sparkContext.parallelize((1 to 2).map(i => Record(i, s""val_$i""))).toDF.show
+---+-----+
|key|value|
+---+-----+
| 1|val_1|
| 2|val_2|
+---+-----+{code};;;","01/Nov/18 06:44;gurwls223;Yea, looks this is pretty much related with REPL fix we did for Scala 2.12.;;;","01/Nov/18 06:52;dongjoon;Thank you for confirmation. The workaround is to add the following at the first line. I'll lower the priority.
{code:java}
import spark.implicits._{code};;;","01/Nov/18 07:20;cloud_fan;thanks for reporting it! I'll list it as known issue in 2.4.0;;;","01/Nov/18 07:26;dongjoon;Thanks, [~cloud_fan]!;;;","01/Nov/18 08:08;gurwls223;Another workaround is to use {{-I}} instead of {{-i}}.;;;","01/Nov/18 08:36;gurwls223;The root cause seems to be [https://github.com/scala/scala/commit/99dad60d984d3f72338f3bad4c4fe905090edd51.] They change what {{-i}} means in that change.
{{-i}} option is replaced to {{-I}}. The _newly replaced_ option {{-i}} at Scala 2.11.12 works like {{:paste}} (previously it worked like {{:load}}). but still I wonder why the newly replaced {{-i}} option does not work.

Basically here's what's going on:

{code}
scala> :paste test.scala
Pasting file test.scala...
test.scala:17: error: value toDF is not a member of org.apache.spark.rdd.RDD[Record]
Error occurred in an application involving default arguments.
       spark.sparkContext.parallelize((1 to 2).map(i => Record(i, s""val_$i""))).toDF.show
{code}

{{:paste}} itself looks not working fine in both Spark 2.3 and 2.4.

FWIW, {{./bin/spark-shell --help}} does not show this option.
;;;","01/Nov/18 09:05;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/22919;;;","01/Nov/18 14:20;dongjoon;Thank you! [~hyukjin.kwon].;;;","06/Nov/18 02:41;gurwls223;Issue resolved by pull request 22919
[https://github.com/apache/spark/pull/22919];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: BarrierTaskContextSuite.throw exception on barrier() call timeout,SPARK-25903,13195475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,vanzin,vanzin,31/Oct/18 20:08,23/Sep/19 16:16,13/Jul/23 08:48,23/Sep/19 16:16,2.4.0,3.0.0,,,,,,,,,,,,,,,,2.4.5,3.0.0,,,Tests,,,,1,,,,,"We hit this in our internal builds.

{noformat}
Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown
Stacktrace
      org.scalatest.exceptions.TestFailedException: Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
      at org.scalatest.Assertions$class.intercept(Assertions.scala:822)
      at org.scalatest.FunSuite.intercept(FunSuite.scala:1560)
      at org.apache.spark.scheduler.BarrierTaskContextSuite$$anonfun$7.apply(BarrierTaskContextSuite.scala:94)
      at org.apache.spark.scheduler.BarrierTaskContextSuite$$anonfun$7.apply(BarrierTaskContextSuite.scala:76)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
{noformat}

The problem from the logs is that the first task to call {{barrier()}} took a while, and at that point the ""slow"" task was already running, so the sleep finishes before the 2s timeout runs out.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/169/",,cloud_fan,dongjoon,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 23 16:16:16 UTC 2019,,,,,,,,,,"0|i3zv1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/19 12:32;kabhwan;Hit this again: [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110979/testReport/]

 ;;;","23/Sep/19 16:16;cloud_fan;Issue resolved by pull request 25897
[https://github.com/apache/spark/pull/25897];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Barrier mode spawns a bunch of threads that get collected on gc,SPARK-25901,13195463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,yogeshgarg,yogeshgarg,31/Oct/18 18:59,03/Nov/18 06:11,13/Jul/23 08:48,03/Nov/18 06:11,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,0,,,,,"After a barrier job is terminated (successfully or interrupted), the accompanying thread created with `Timer` in `BarrierTaskContext` shows in a waiting state until gc is called. We should probably have just one thread to schedule all such tasks, since they just log every 60 seconds.

Here's a screen shot of the threads growing with more tasks:
 !Screen Shot 2018-10-31 at 11.57.25 AM.png! 

Here's a screen shot of constant number of threads with more tasks:
 !Screen Shot 2018-10-31 at 11.57.42 AM.png! ",,apachespark,jiangxb1987,yogeshgarg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/18 18:59;yogeshgarg;Screen Shot 2018-10-31 at 11.57.25 AM.png;https://issues.apache.org/jira/secure/attachment/12946439/Screen+Shot+2018-10-31+at+11.57.25+AM.png","31/Oct/18 18:59;yogeshgarg;Screen Shot 2018-10-31 at 11.57.42 AM.png;https://issues.apache.org/jira/secure/attachment/12946438/Screen+Shot+2018-10-31+at+11.57.42+AM.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 03 06:11:57 UTC 2018,,,,,,,,,,"0|i3zuz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/18 18:59;yogeshgarg;I am working on this task in this PR: https://github.com/apache/spark/pull/22912;;;","31/Oct/18 19:08;apachespark;User 'yogeshg' has created a pull request for this issue:
https://github.com/apache/spark/pull/22912;;;","01/Nov/18 21:06;yogeshgarg;[~jiangxb1987] m thanks for approving the PR, can we assign this issue to me and merge the PR?;;;","03/Nov/18 06:11;jiangxb1987;Issue resolved by pull request 22912
[https://github.com/apache/spark/pull/22912];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Date corruption when Spark and Hive both are on different timezones,SPARK-25873,13194983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pawanlawale,pawanlawale,29/Oct/18 22:20,12/Dec/22 18:10,13/Jul/23 08:48,04/Jan/19 10:26,2.2.1,,,,,,,,,,,,,,,,,,,,,Spark Core,Spark Shell,Spark Submit,,0,,,,,"There is date alteration when loading date from one table to another in hive through spark. This happens when Hive is on a remote machine with timezone different than the one on which Spark is running. This happens only when the Source table format is 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'

Below are the steps to produce the issue:

1. Create two tables as below in hive which has a timezone, say in, EST

{code}
 CREATE TABLE t_src(
 name varchar(10),
 dob timestamp
 )
 ROW FORMAT SERDE 
 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
 STORED AS INPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
 OUTPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
{code}

{code}
INSERT INTO t_src VALUES ('p1', '0001-01-01 00:00:00.0'),('p2', '0002-01-01 00:00:00.0'), ('p3', '0003-01-01 00:00:00.0'),('p4', '0004-01-01 00:00:00.0');
{code}
 
{code}
 CREATE TABLE t_tgt(
 name varchar(10),
 dob timestamp
 );
{code}


2. Copy {{hive-site.xml}} to {{spark-2.2.1-bin-hadoop2.7/conf}} folder, so that when you create {{sqlContext}} for hive it connects to your remote hive server.

3. Start your spark-shell on some other machine whose timezone is different than that of Hive, say, PDT

4. Execute below code:
{code}
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)

val q0 = ""TRUNCATE table t_tgt""
val q1 = ""SELECT CAST(alias.name AS String) as a0, alias.dob as a1 FROM t_src alias""
val q2 = ""INSERT OVERWRITE TABLE t_tgt SELECT tbl0.a0 as c0, tbl0.a1 as c1 FROM tbl0""

sqlContext.sql(q0)
sqlContext.sql(q1).select(""a0"",""a1"").createOrReplaceTempView(""tbl0"")
sqlContext.sql(q2)
{code}

5. Now navigate to hive and check the contents of the {{TARGET table (t_tgt)}}. The dob field will have incorrect values.

 

Is this a known issue? Is there any work around on this? Can it be fixed?

 

Thanks & regards,

Pawan Lawale",,pawanlawale,planga82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25919,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 21:56:01 UTC 2019,,,,,,,,,,"0|i3zs0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/18 08:20;gurwls223;Can you show current results, expected results and why it's expected?;;;","30/Oct/18 15:14;pawanlawale;[~hyukjin.kwon]

Please find below the data in t_src and t_tgt table:
{code:java}
hive> select * from t_src;
OK
p1 0001-01-01 00:00:00
p2 0002-01-01 00:00:00
p3 0003-01-01 00:00:00
p4 0004-01-01 00:00:00{code}
{code:java}
hive> select * from t_tgt;
OK
p1 0001-12-31 15:00:00
p2 0001-12-31 15:00:00
p3 0002-12-31 15:00:00
p4 0003-12-31 15:00:00{code}
 

Data in table t_tgt is a result of execution of scala code given in description. 

Thanks & regards,

Pawan Lawale;;;","03/Jan/19 21:56;planga82;Hello, it seems duplicated with SPARK-25919 that has been resolved.
Could it be closed too?
Thank you
Regards;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update KMeans formatVersion,SPARK-25866,13194779,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,mgaido,mgaido,29/Oct/18 09:13,06/Nov/18 15:20,13/Jul/23 08:48,06/Nov/18 15:20,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,MLlib,,,,0,,,,,"KMeans's {{formatVersion}} has not been updated to 2.0, when the distanceMeasure parameter has been added. Despite this causes no issue, as {{formatVersion}} is not used anywhere, the information returned is wrong.",,apachespark,cloud_fan,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 06 15:20:36 UTC 2018,,,,,,,,,,"0|i3zqrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/18 09:16;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22873;;;","06/Nov/18 15:20;cloud_fan;Issue resolved by pull request 22873
[https://github.com/apache/spark/pull/22873];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.UnsupportedOperationException: empty.max at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.updateAndGetCompilationStats(CodeGenerator.scala:1475),SPARK-25863,13194740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,Tagar,Tagar,29/Oct/18 04:03,23/Mar/19 17:43,13/Jul/23 08:48,07/Mar/19 08:29,2.3.1,2.3.2,,,,,,,,,,,,,,,,2.3.4,2.4.1,3.0.0,,Optimizer,Spark Core,,,0,cache,catalyst,code-generation,,"Failing task : 
{noformat}
An error occurred while calling o2875.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 21413.0 failed 4 times, most recent failure: Lost task 58.3 in stage 21413.0 (TID 4057314, pc1udatahad117, executor 431): java.lang.UnsupportedOperationException: empty.max
at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)
at scala.collection.AbstractTraversable.max(Traversable.scala:104)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.updateAndGetCompilationStats(CodeGenerator.scala:1475)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1418)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1493)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1490)
at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1365)
at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:81)
at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:40)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1321)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1318)
at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:401)
at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$filteredCachedBatches$1.apply(InMemoryTableScanExec.scala:263)
at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$filteredCachedBatches$1.apply(InMemoryTableScanExec.scala:262)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)

{noformat}
 

Driver stack trace:
{noformat}
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)
        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
        at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)
        at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)
        at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)
        at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)
        at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)

{noformat}
 

Caused by:
{noformat}
Caused by: java.lang.UnsupportedOperationException: empty.max
        at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)
        at scala.collection.AbstractTraversable.max(Traversable.scala:104)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.updateAndGetCompilationStats(CodeGenerator.scala:1475)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1418)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1493)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1490)
        at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
        at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1365)
        at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:81)
        at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:40)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1321)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1318)
        at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:401)
        at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$filteredCachedBatches$1.apply(InMemoryTableScanExec.scala:263)
        at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$filteredCachedBatches$1.apply(InMemoryTableScanExec.scala:262)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more

{noformat}",,maropu,mgaido,Tagar,vfeldsher,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 10 00:00:23 UTC 2019,,,,,,,,,,"0|i3zqin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/18 04:05;Tagar;This happens only on one of our heaviest Spark jobs.. ;;;","29/Oct/18 04:11;Tagar;It seems error happens here

[https://github.com/apache/spark/blob/branch-2.3/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L1475]

but this is as far as I can go... any ideas why this happens? thanks!

 ;;;","29/Oct/18 09:02;mgaido;[~Tagar] thanks for reporting this. May you please provide a reproducer? If you can post a simplified version of your code generating the issue, it would be helpful, otherwise it is hard to investigate the issue. Thanks.;;;","29/Oct/18 20:33;Tagar;[~mgaido], I will try to get a reproducer, but it might be a tough task, not sure yet as it might depend on data I guess too. 

If this helps, I can tell that this Spark job was operating on a very wide table (thousands of columns),
 and the SQL itself was generated and have a long SELECT clause with a lot of CASE statements...
 so I can imaging Spark's Code Generation had a hard time to crank through this.

Is there is some debugging we can enable that would be helpful to get to root cause?

On that line particularly
[https://github.com/apache/spark/blob/branch-2.3/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L1475]
Would it make sense just to add an `if` codeSizes is empty, return zero. If codeSizes never supposed to be empty, add some assert before that to help with debugging. 

Thank you.;;;","30/Oct/18 09:46;mgaido;[~Tagar] thanks.

??not sure yet as it might depend on data I guess too.??

No for sure, code generation doesn't depend on the data, it depends on the query and the schema.

??Is there is some debugging we can enable that would be helpful to get to root cause\???

May you post the logs? I expect some warnings or similar to happen before the exception that might help understanding more.

??Would it make sense just to add an `if` codeSizes is empty, return zero.??

I can't say which is the right fix if I don't know the root cause.
Thanks.;;;","03/Mar/19 18:36;srowen;Returning 0 seems like the correct thing to do, locally. [~maropu] does that sound right? this particular line was added in https://github.com/apache/spark/commit/4a779bdac3e75c17b7d36c5a009ba6c948fa9fb6

It seems like it could happen if you had errors computing stats. Do you see warnings like ""Error calculating stats of compiled class""?;;;","03/Mar/19 23:48;maropu;Yea, returning 0 sounds reasonable to me, too.;;;","06/Mar/19 23:08;vfeldsher;I recently experienced similar issue while upgrading from spark 2.1.0 to 2.3.2, and it appears that this happens whenever I use spark.driver.userClassPathFirst or spark.executor.userClassPathFirst. Similarly to SPARK-20241 I see 2 different class loaders when I set this property to true: org.apache.spark.util.ChildFirstURLClassLoader and sun.misc.Launcher$AppClassLoader. This is accompanied by warning:

Error calculating stats of compiled class.
 java.lang.IllegalArgumentException: Can not set final [B field org.codehaus.janino.util.ClassFile$CodeAttribute.code to org.codehaus.janino.util.ClassFile$CodeAttribute

Followed by the empty.max error.;;;","07/Mar/19 08:29;maropu;Resolved by https://github.com/apache/spark/pull/23947;;;","07/Mar/19 08:30;maropu;[~vfeldsher] Can you check if your query works well in the current master?;;;","08/Mar/19 15:47;vfeldsher;[~maropu] yes it works now, I still see the warning but it doesn't crash. Thanks!;;;","10/Mar/19 00:00;maropu;Thanks alot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"mvn helper script always exits w/1, causing mvn builds to fail",SPARK-25854,13194453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shaneknapp,shaneknapp,shaneknapp,26/Oct/18 15:59,26/Oct/18 21:42,13/Jul/23 08:48,26/Oct/18 21:42,2.2.2,2.3.2,2.4.1,,,,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,3.0.0,Build,,,,0,,,,,"the final line in the mvn helper script in build/ attempts to shut down the zinc server.  due to the zinc server being set up w/a 30min timeout, by the time the mvn test instantiation finishes, the server times out.

this means that when the mvn script tries to shut down zinc, it returns w/an exit code of 1.  this will then automatically fail the entire build (even if the build passes).

i propose the following:

1) up the timeout to 3h

2) put some logic at the end of the script to better handle killing the zinc server

PR coming now.

[~srowen] [~cloud_fan] [~joshrosen]",,apachespark,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 26 21:42:53 UTC 2018,,,,,,,,,,"0|i3zor3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/18 16:04;shaneknapp;https://github.com/apache/spark/pull/22854;;;","26/Oct/18 16:05;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/22854;;;","26/Oct/18 16:06;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/22854;;;","26/Oct/18 21:42;srowen;Issue resolved by pull request 22854
[https://github.com/apache/spark/pull/22854];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the split threshold for the code generated method configurable,SPARK-25850,13194344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,26/Oct/18 08:00,05/Nov/18 12:10,13/Jul/23 08:48,05/Nov/18 12:10,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"As per the discussion in [https://github.com/apache/spark/pull/22823/files#r228400706,] add a new configuration spark.sql.codegen.methodSplitThreshold to make the split threshold for the code generated method configurable.",,apachespark,cloud_fan,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 05 12:10:54 UTC 2018,,,,,,,,,,"0|i3zo2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/18 08:13;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/22847;;;","26/Oct/18 08:14;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/22847;;;","05/Nov/18 12:10;cloud_fan;Issue resolved by pull request 22847
[https://github.com/apache/spark/pull/22847];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`make-distribution.sh` should not fail due to missing LICENSE-binary,SPARK-25840,13194247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Oct/18 21:06,26/Oct/18 03:27,13/Jul/23 08:48,26/Oct/18 03:26,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,,,,0,,,,,"We vote for the artifacts. All releases are in the form of the *source* materials needed to make changes to the software being released.

http://www.apache.org/legal/release-policy.html#artifacts

From Spark 2.4.0, the source artifact and binary artifact starts to contain own proper LICENSE files (LICENSE, LICENSE-binary). It's great to have them. However, unfortunately, `dev/make-distribution.sh` inside source artifacts start to fail because it expects `LICENSE-binary` and source artifact have only the LICENSE file.

https://dist.apache.org/repos/dist/dev/spark/v2.4.0-rc4-bin/spark-2.4.0.tgz

dev/make-distribution.sh is used during the voting phase because we are voting on that source artifact instead of GitHub repository. Individual contributors usually don't have the downstream repository and starts to try build the voting source artifacts to help the verification for the source artifact during voting phase. (Personally, I did before.)

This issue aims to recover that script to work in any way. This doesn't aim for source artifacts to reproduce the compiled artifacts.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 26 03:26:41 UTC 2018,,,,,,,,,,"0|i3znhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/18 21:10;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22840;;;","26/Oct/18 03:26;dongjoon;Issue resolved by pull request 22840
[https://github.com/apache/spark/pull/22840];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI does not respect spark.ui.retainedJobs in some instances,SPARK-25837,13194199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,patrick.brown,patrick.brown,patrick.brown,25/Oct/18 17:10,18/Mar/19 11:05,13/Jul/23 08:48,01/Nov/18 16:41,2.3.1,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Web UI,,,,0,,,,,"Expected Behavior: Web UI only displays 1 completed job and remains responsive.

Actual Behavior: Both during job execution and following all job completion for some non short amount of time the UI retains many completed jobs, causing limited responsiveness.

 

To reproduce:

 
 > spark-shell --conf spark.ui.retainedJobs=1
  
 scala> import scala.concurrent._
 scala> import scala.concurrent.ExecutionContext.Implicits.global
 scala> for (i <- 0 until 50000) { Future

{ println(sc.parallelize(0 until i).collect.length) }

}
  

 

 

The attached screenshot shows the state of the webui after running the repro code, you can see the ui is displaying some 43k completed jobs (takes a long time to load) after a few minutes of inactivity this will clear out, however in an application which continues to submit jobs every once in a while, the issue persists.

 

The issue seems to appear when running multiple jobs at once as well as in sequence for a while and may as well have something to do with high master CPU usage (thus the collect in the repro code). My rough guess would be whatever is managing clearing out completed jobs gets overwhelmed (on the master during repro htop reported almost full CPU usage across all 4 cores).","Reproduction Environment:

Spark 2.3.1

Dataproc 1.3-deb9

1x master 4 vCPUs, 15 GB

2x workers 4 vCPUs, 15 GB

 ",apachespark,patrick.brown,vanzin,xiaojuwu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26395,,,,,,,,,,,,,,,,,,,"25/Oct/18 17:11;patrick.brown;Screen Shot 2018-10-23 at 4.40.51 PM (1).png;https://issues.apache.org/jira/secure/attachment/12945622/Screen+Shot+2018-10-23+at+4.40.51+PM+%281%29.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 18 11:05:18 UTC 2019,,,,,,,,,,"0|i3zn6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/18 17:28;patrick.brown;I would be interested and happy to tackle this, if its an issue that the community agrees should be addressed.;;;","29/Oct/18 19:39;patrick.brown;The fundamental problem seems to be in AppStatusLisener in the cleanupStages method.

 

Using the repro code above it appears that sometimes (not always) stages and tasks get slightly backed up. When this occurs the iteration through tasks starts taking longer and longer:

 
{code:java}
val tasks = kvstore.view(classOf[TaskDataWrapper])
 .index(""stage"")
 .first(key)
 .last(key)
 .asScala{code}
 


This seems to be because for each stage we are then iterating through all the tasks (of which there can be ~400k in this repro code), which can go from taking ~10ms before the back up to ~300ms afterwards due to the large number of tasks. This causes a feedback loop in which the `cleanupStages` method cannot keep up.

 ;;;","29/Oct/18 20:25;apachespark;User 'patrickbrownsync' has created a pull request for this issue:
https://github.com/apache/spark/pull/22883;;;","01/Nov/18 16:41;vanzin;Issue resolved by pull request 22883
[https://github.com/apache/spark/pull/22883];;;","18/Mar/19 11:05;xiaojuwu;Did you verify this fix with the reproduce case above? I tried and found the issue is still there: the cleanup was still backed up but better than the version without this fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate scala 2.12 profile in k8s integration tests,SPARK-25835,13194178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,skonto,skonto,skonto,25/Oct/18 15:38,17/May/20 18:24,13/Jul/23 08:48,26/Oct/18 13:54,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"The [line|https://github.com/apache/spark/blob/master/resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh#L106] that calls k8s integration tests ignores the scala version: ",,apachespark,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25836,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 26 13:54:30 UTC 2018,,,,,,,,,,"0|i3zn1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/18 19:20;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/22838;;;","25/Oct/18 19:21;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/22838;;;","26/Oct/18 13:54;srowen;Issue resolved by pull request 22838
[https://github.com/apache/spark/pull/22838];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove newly added map related functions,SPARK-25832,13194047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,cloud_fan,cloud_fan,25/Oct/18 04:45,21/Jun/19 01:41,13/Jul/23 08:48,25/Oct/18 23:43,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25829,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 25 23:43:02 UTC 2018,,,,,,,,,,"0|i3zm8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/18 04:46;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22821;;;","25/Oct/18 16:55;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22827;;;","25/Oct/18 17:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22828;;;","25/Oct/18 17:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22828;;;","25/Oct/18 23:43;cloud_fan;Issue resolved by pull request 22827
[https://github.com/apache/spark/pull/22827];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove duplicated map keys with last wins policy,SPARK-25829,13194021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,25/Oct/18 00:48,18/Feb/20 02:11,13/Jul/23 08:48,28/Nov/18 15:43,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"In Spark SQL, we apply ""earlier entry wins"" semantic to duplicated map keys. e.g.
{code}
scala> sql(""SELECT map(1,2,1,3)[1]"").show
+------------------+
|map(1, 2, 1, 3)[1]|
+------------------+
|                 2|
+------------------+
{code}

However, this handling is not applied consistently.",,apachespark,cloud_fan,dongjoon,kevinyu98,kiszk,mgaido,Samwel,viirya,,,,,,,,,,,,,,,,,,,,,,,SPARK-25823,,,,,,SPARK-25832,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 04 12:46:16 UTC 2018,,,,,,,,,,"0|i3zm33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/18 02:19;cloud_fan;After more thoughts, both the map lookup behavior and `Dataset.collect` behavior are visible to end-users. It's hard to say which one is the official semantic as there is no doc, and we have to do behavior change for one of them.

If we want to stick with the ""earlier entry wins"" semantic, then we need to fix the 3 sub-tasks listed here.

If we want to stick with the ""later entry wins"" semantic, then we need to fix the map lookup(GetMapValue) and other related functions like `map_filter`, or deduplicate map keys at all the places that may create map. And for 2.4 we should revert these function if they are newly added, like `map_filter`.

Any ideas? cc [~rxin] [~LI,Xiao] [~dongjoon] [~viirya] [~mgaido];;;","25/Oct/18 02:48;cloud_fan;More investigation on ""later entry wins"".

If we still allow duplicated keys in map physically, following functions need to be updated:
Explode, PosExplode, GetMapValue, MapKeys, MapValues, MapEntries, TransformKeys, TransformValues, MapZipWith

If we want to forbid duplicated keys in map, following functions need to be updated:
CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat, TransformKeys, MapFilter, and also reading map from data sources.

So ""later entry wins"" semantic is more ideal but needs more works.;;;","25/Oct/18 02:51;cloud_fan;If we decide to follow ""later entry wins"", the following functions need to be reverted from 2.4
MapFilter, MapZipWith, TransformKeys, TransformValues;;;","25/Oct/18 04:41;dongjoon;Thank you for further investigation! Both tasks look not easy. For me, +1 for `later entry wins` semantics because it's Java/Scala language style and many users know those languages. Also, Spark works in that way, especially during the writing operation.;;;","25/Oct/18 07:23;kiszk;cc [~ueshin];;;","25/Oct/18 07:40;kiszk;I am curious about behavior in other systems such as Presto.
Here are test cases for [array|https://github.com/prestodb/presto/blob/master/presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java] and [map|https://github.com/prestodb/presto/blob/master/presto-main/src/test/java/com/facebook/presto/type/TestMapOperators.java].;;;","25/Oct/18 09:26;viirya;Besides Java/Scala, maybe we can be consistent with common behavior of other SQL systems, if any.

From the test case above, looks like Presto disallows duplicate keys in map. https://github.com/prestodb/presto/blob/master/presto-main/src/test/java/com/facebook/presto/type/TestMapOperators.java#L123;;;","25/Oct/18 10:11;viirya;Although I think the inconsistent handling exists for a while, if we decide ""later entry wins"" and revert some functions from 2.4, will this be a blocker for 2.4?;;;","25/Oct/18 10:28;mgaido;I think the main issue is that since this is not a SQL standard thing, every DB works in its way. Eg. Postgres just says that when duplicate keys are entered, there is no guarantee on the result (https://www.postgresql.org/docs/9.0/static/hstore.html); not a great policy, I agree. Maybe we can check Hive, since Spark takes much of its behavior from it. Anyway, I think we just need to define a coherent behavior across the codebase.

One consideration is that enforcing a policy like Presto (eg. fail in such a situation) has 2 main drawbacks:
 - We usually don't fail with bad data (most of the times we return NULL instead of throwing exceptions in other situations);
 - Checking if a key is already present, with the current {{ArrayData}} representation, is very inefficient and we can do workarounds for this, but we would need to replicate workarounds in any function which can produce keys, so it is going to be problematic to maintain.
;;;","23/Nov/18 05:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23124;;;","23/Nov/18 05:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23124;;;","28/Nov/18 15:43;cloud_fan;Issue resolved by pull request 23124
[https://github.com/apache/spark/pull/23124];;;","04/Dec/18 12:45;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23217;;;","04/Dec/18 12:46;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/23217;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replicating a block > 2gb with encryption fails,SPARK-25827,13193967,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,24/Oct/18 20:33,02/Nov/18 20:25,13/Jul/23 08:48,02/Nov/18 20:25,2.4.0,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Spark Core,,,,0,,,,,"There are a couple of issues with replicating & remote reads of large encrypted blocks, which try to create buffers where they shouldn't.  Some of this is properly limiting the size of arrays under SPARK-25904, but there are others specific to encryption & trying to convert EncryptedBlockData into a regular ByteBuffer.

*EDIT*: moved general array size stuff under SPARK-25904.",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24296,,,,,SPARK-25704,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 02 20:25:48 UTC 2018,,,,,,,,,,"0|i3zlr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/18 20:47;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22818;;;","31/Oct/18 21:22;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22916;;;","31/Oct/18 21:25;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22917;;;","31/Oct/18 21:26;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22917;;;","02/Nov/18 20:25;vanzin;Issue resolved by pull request 22917
[https://github.com/apache/spark/pull/22917];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a race condition when releasing a Python worker,SPARK-25822,13193926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,24/Oct/18 17:22,26/Oct/18 04:56,13/Jul/23 08:48,26/Oct/18 04:56,2.3.2,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,PySpark,,,,0,,,,,"There is a race condition when releasing a Python worker. If ""ReaderIterator.handleEndOfDataSection"" is not running in the task thread, when a task is early terminated (such as ""take(N)""), the task completion listener may close the worker but ""handleEndOfDataSection"" can still put the worker into the worker pool to reuse.

https://github.com/zsxwing/spark/commit/0e07b483d2e7c68f3b5c3c118d0bf58c501041b7 is a patch to reproduce this issue.

I also found a user reported this in the mail list: http://mail-archives.apache.org/mod_mbox/spark-user/201610.mbox/%3CCAAUq=H+YLUEpd23nwvq13Ms5hOStkhX3ao4f4zQV6sgO5zM-xA@mail.gmail.com%3E",,apachespark,ueshin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 26 04:56:37 UTC 2018,,,,,,,,,,"0|i3zlhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/18 17:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22816;;;","24/Oct/18 17:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22816;;;","26/Oct/18 04:56;ueshin;Issue resolved by pull request 22816
https://github.com/apache/spark/pull/22816;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset encoder should support combination of map and product type,SPARK-25817,13193752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,24/Oct/18 04:21,28/Oct/18 05:37,13/Jul/23 08:48,28/Oct/18 05:37,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 28 05:37:35 UTC 2018,,,,,,,,,,"0|i3zkfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/18 04:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22812;;;","24/Oct/18 04:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22812;;;","28/Oct/18 05:37;cloud_fan;Issue resolved by pull request 22812
[https://github.com/apache/spark/pull/22812];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Functions does not resolve Columns correctly,SPARK-25816,13193716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,petertoth,bzhang,bzhang,23/Oct/18 23:41,29/Oct/18 18:10,13/Jul/23 08:48,29/Oct/18 00:57,2.3.0,2.3.1,2.3.2,2.4.0,,,,,,,,,,,,,,2.3.3,2.4.0,,,SQL,,,,1,,,,,"When there is a duplicate column name in the current Dataframe and orginal Dataframe where current df is selected from, Spark in 2.3.0 and 2.3.1 does not resolve the column correctly when using it in the expression, hence causing casting issue. The same code is working in Spark 2.2.1

Please see below code to reproduce the issue

import org.apache.spark._
import org.apache.spark.rdd._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.sql._
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.Column

val v0 = spark.read.parquet(""/data/home/bzinfa/bz/source.snappy.parquet"")
val v00 = v0.toDF(v0.schema.fields.indices.view.map("""" + _):_*)
val v5 = v00.select($""13"".as(""0""),$""14"".as(""1""),$""15"".as(""2""))
val v5_2 = $""2""
v5.where(lit(500).<(v5_2(new Column(new MapKeys(v5_2.expr))(lit(0)))))

//v00's 3rdcolumn is binary and 16th is map<string, double>

Error:
org.apache.spark.sql.AnalysisException: cannot resolve 'map_keys(`2`)' due to data type mismatch: argument 1 requires map type, however, '`2`' is of binary type.;
 
 'Project [0#1591, 1#1592, 2#1593] +- 'Filter (500 < {color:#FF0000}2#1593{color}[map_keys({color:#FF0000}2#1561{color})[0]]) +- Project [13#1572 AS 0#1591, 14#1573 AS 1#1592, 15#1574 AS 2#1593, 2#1561] +- Project [c_bytes#1527 AS 0#1559, c_union#1528 AS 1#1560, c_fixed#1529 AS 2#1561, c_boolean#1530 AS 3#1562, c_float#1531 AS 4#1563, c_double#1532 AS 5#1564, c_int#1533 AS 6#1565, c_long#1534L AS 7#1566L, c_string#1535 AS 8#1567, c_decimal_18_2#1536 AS 9#1568, c_decimal_28_2#1537 AS 10#1569, c_decimal_38_2#1538 AS 11#1570, c_date#1539 AS 12#1571, simple_struct#1540 AS 13#1572, simple_array#1541 AS 14#1573, simple_map#1542 AS 15#1574] +- Relation[c_bytes#1527,c_union#1528,c_fixed#1529,c_boolean#1530,c_float#1531,c_double#1532,c_int#1533,c_long#1534L,c_string#1535,c_decimal_18_2#1536,c_decimal_28_2#1537,c_decimal_38_2#1538,c_date#1539,simple_struct#1540,simple_array#1541,simple_map#1542] parquet",,apachespark,bzhang,dongjoon,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/18 20:09;bzhang;final_allDatatypes_Spark.avro;https://issues.apache.org/jira/secure/attachment/12945822/final_allDatatypes_Spark.avro","23/Oct/18 23:42;bzhang;source.snappy.parquet;https://issues.apache.org/jira/secure/attachment/12945303/source.snappy.parquet",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 29 18:10:35 UTC 2018,,,,,,,,,,"0|i3zk87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/18 19:04;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/22817;;;","26/Oct/18 20:09;bzhang;Here is another reproduce that should be related to this same issue:

val v0 = sqlContext.read.avro(""final_allDatatypes_Spark.avro"");
val v00 = v0.toDF(v0.schema.fields.indices.view.map("""" + _):_*)
val v001 = v00.select($""0"".as(""0""), $""1"".as(""1""),$""2"".as(""2""),$""3"".as(""3""),$""4"".as(""4""),$""5"".as(""5""),$""6"".as(""6""),$""7"".as(""7""),$""8"".as(""8""))
val v013 = $""8""
val v010 = map(v013, v013)
 
v001.where(map(v013, v010)(v013)(v013)===""dummy"")

 

org.apache.spark.sql.AnalysisException: Reference '8' is ambiguous, could be: 8, 8.;
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:213)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:97)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$36.apply(Analyzer.scala:822)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$36.apply(Analyzer.scala:824)
 at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:821)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:830)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:830)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:891)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:891)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:891)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:833)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:833)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:690)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
 at scala.collection.immutable.List.foldLeft(List.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)
 at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
 at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)
 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)
 at org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)
 at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3301)
 at org.apache.spark.sql.Dataset.filter(Dataset.scala:1458)
 at org.apache.spark.sql.Dataset.where(Dataset.scala:1486)

 ;;;","27/Oct/18 12:10;petertoth;Thanks [~bzhang], It seems both are regressions from 2.2 to 2.3 for the same reason. My submitted PR fixes them.;;;","28/Oct/18 19:06;dongjoon;Thank you, [~bzhang] and [~petertoth]. I also confirmed that this is a bug in 2.3.x and 2.4.0 RC4 (and master). Thanks to [~petertoth], it looks like we can have the fix this for 2.3.3 and 2.4.0 RC5.

cc [~cloud_fan];;;","29/Oct/18 18:10;bzhang;Thanks [~petertoth] and everyone else for the quick fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: PagedTableSuite.pageNavigation,SPARK-25812,13193612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,dongjoon,dongjoon,23/Oct/18 16:27,24/Oct/18 18:13,13/Jul/23 08:48,23/Oct/18 19:38,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,Tests,,,0,,,,,"- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5074/testReport/

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5073/testReport/

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5072/testReport/

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/5070/testReport/

- [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/97878/testReport/org.apache.spark.ui/PagedTableSuite/pageNavigation/]

{code:java}
[info] PagedTableSuite:
[info] - pageNavigation *** FAILED *** (2 milliseconds)
[info]   <div>
[info]         <div>
[info]           <form id=""form--page"" method=""get"" action="""" class=""form-inline pull-right"" style=""margin-bottom: 0px;"">
[info]             <input type=""hidden"" name=""prevPageSize"" value=""10""/>
[info]             
[info]             <label>1 Pages. Jump to</label>
[info]             <input type=""text"" name=""page"" id=""form--page-no"" value=""1"" class=""span1""/>
[info]   
[info]             <label>. Show </label>
[info]             <input type=""text"" id=""form--page-size"" name=""pageSize"" value=""10"" class=""span1""/>
[info]             <label>items in a page.</label>
[info]   
[info]             <button type=""submit"" class=""btn"">Go</button>
[info]           </form>
[info]         </div>
[info]         <div class=""pagination"" style=""margin-bottom: 0px;"">
[info]           <span style=""float: left; padding-top: 4px; padding-right: 4px;"">Page: </span>
[info]           <ul>
[info]             
[info]             
[info]             <li class=""disabled""><a href=""#"">1</a></li>
[info]             
[info]             
[info]           </ul>
[info]         </div>
[info]       </div> did not equal List() (PagedTableSuite.scala:76)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
[info]   at org.apache.spark.ui.PagedTableSuite$$anonfun$4.apply(PagedTableSuite.scala:76)
[info]   at org.apache.spark.ui.PagedTableSuite$$anonfun$4.apply(PagedTableSuite.scala:52)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 19:38:08 UTC 2018,,,,,,,,,,"0|i3zjlb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/18 16:52;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22808;;;","23/Oct/18 19:38;dongjoon;Issue resolved by pull request 22808
[https://github.com/apache/spark/pull/22808];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The -n option to docker-image-tool.sh causes other options to be ignored,SPARK-25803,13193365,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,slarkin,slarkin,slarkin,22/Oct/18 21:13,17/May/20 18:23,13/Jul/23 08:48,25/Oct/18 20:03,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"To reproduce:-

1. Build spark
 $ ./build/mvn -Pkubernetes -DskipTests clean package

2. Create a Dockerfile (a simple one, just for demonstration)
 $ cat > hello-world.dockerfile <<EOF
 > FROM hello-world
 > EOF

3. Build container images with our Dockerfile
 $ ./bin/docker-image-tool.sh -R hello-world.dockerfile -r docker.io/myrepo -t myversion build

The result is that the -R option is honoured and the hello-world image is built for spark-r, as expected.

4. Build container images with our Dockerfile and the -n option
 $ ./bin/docker-image-tool.sh -n -R hello-world.dockerfile -r docker.io/myrepo -t myversion build

The result is that the -R option is ignored and the default container image for R is built

docker-image-tool.sh, uses [getopts|http://pubs.opengroup.org/onlinepubs/9699919799/utilities/getopts.html] in which a colon, ':', signifies that an option takes an argument.  Since -n does not take an argument it should not have a colon.","* OS X 10.14
 * iTerm2
 * bash3
 * Docker 2.0.0.0-beta1-mac75 (27117)

(NB: I don't believe the above has a bearing; I imagine this issue is present also on linux and can confirm if needed.)",apachespark,slarkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 22 21:25:52 UTC 2018,,,,,,,,,,"0|i3zi2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/18 21:25;apachespark;User 'sel' has created a pull request for this issue:
https://github.com/apache/spark/pull/22798;;;","22/Oct/18 21:25;slarkin;Created a PR:

https://github.com/apache/spark/pull/22798;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pandas_udf grouped_map fails with input dataframe with more than 255 columns,SPARK-25801,13193277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Toekan,Toekan,22/Oct/18 14:31,23/Oct/18 21:08,13/Jul/23 08:48,23/Oct/18 21:08,2.3.0,,,,,,,,,,,,,,,,,2.4.0,,,,PySpark,,,,0,,,,,"Hi,

I'm using a pandas_udf to deploy a model to predict all samples in a spark dataframe,

for this I use a udf as follows:
@pandas_udf(""scores double"", PandasUDFType.GROUPED_MAP) def predict_scores(pdf):  score_values = model.predict_proba(pdf)[:,1]  return pd.DataFrame({'scores': score_values})
So it takes a dataframe and predicts the probability of being positive according to an sklearn model for each row and returns this as single column. This works great on a random groupBy, e.g.:
sdf_to_score.groupBy(sf.col('age')).apply(predict_scores)
as long as the dataframe has <255 columns. When the input dataframe has more than 255 columns (thus features in my model), I get:
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""path/to/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py"", line 219, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File ""path/to/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py"", line 148, in read_udfs
    mapper = eval(mapper_str, udfs)
  File ""<string>"", line 1
SyntaxError: more than 255 arguments
Which seems to be related with Python's general limitation of having not allowing more than 255 arguments for a function?

 

Is this a bug or is there a straightforward way around this problem?

 

Regards,

Frederik","python 2.7

pyspark 2.3.0",bryanc,Toekan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 21:06:51 UTC 2018,,,,,,,,,,"0|i3zhjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/18 18:38;bryanc;I was able to reproduce with Spark 2.3, and yeah the problem is that each column gets applied as an arg to the function, and Python < 3.7 forbids more than 255 arguments. This isn't a problem in Spark 2.4 because SPARK-23011 changed the way the function is called for grouped-map UDFs, and I confirmed it doesn't have this problem in master. Are you able to upgrade to Spark 2.4 when it's released or use Python 3.7?;;;","23/Oct/18 09:45;Toekan;Hi Bryan,

Thanks for the quick answer!

I wasn't aware Python 3.7 doesn't have the 255 arguments limitation. Unfortunately I can't use python 3.7 (I'm on a platform where I can't change PYSPARK_DRIVER_PYTHON from 3.6 and PYSPARK_DRIVER_PYTHON and PYSPARK_PYTHON need the same minor versions) nor upgrade Spark. Think I'll use an approach with standard udf's as for example outlined here:

[https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/]

Unless there's other options?

 

 ;;;","23/Oct/18 21:06;bryanc;[~Toekan] you might try turning your features into an array of doubles, so that there is only one column. Then you could unpack them in your udf if needed. I'll mark this as fixed in Spark 2.4 and close. You can reopen if you are unable to find a workaround and want to request a fix to be backported for the next 2.3 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSourceApiV2 scan reuse does not respect options,SPARK-25799,13193250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,max.kiessling,max.kiessling,22/Oct/18 12:28,16/Nov/18 13:23,13/Jul/23 08:48,16/Nov/18 13:23,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"When creating a custom data source with the Data Source API V2 it seems that the computation of possible scan reuses is broken when the same data source is used but configured with different configuration options. In the case when both scans produce the same schema (which is always the case for count queries with column pruning enabled) the optimizer will reuse the scan produced by on of the data source instance for both branches of the query. 
This can lead to wrong results if the configuration option somehow influences the returned data.

The behavior can be reproduced with the following example:

{code:scala}
import org.apache.spark.sql.sources.v2.reader._
import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{Row, SparkSession}

import scala.tools.nsc.interpreter.JList

class AdvancedDataSourceV2 extends DataSourceV2 with ReadSupport {

  class Reader(rowCount: Int) extends DataSourceReader
    with SupportsPushDownRequiredColumns {

    var requiredSchema = new StructType().add(""i"", ""int"").add(""j"", ""int"")

    override def pruneColumns(requiredSchema: StructType): Unit = {
      this.requiredSchema = requiredSchema
    }


    override def readSchema(): StructType = {
      requiredSchema
    }

    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] = {


      val res = new java.util.ArrayList[DataReaderFactory[Row]]

      res.add(new AdvancedDataReaderFactory(0, 5, requiredSchema))
      res.add(new AdvancedDataReaderFactory(5, rowCount, requiredSchema))

      res
    }
  }

  override def createReader(options: DataSourceOptions): DataSourceReader =
    new Reader(options.get(""rows"").orElse(""10"").toInt)
}

class AdvancedDataReaderFactory(start: Int, end: Int, requiredSchema: StructType)
  extends DataReaderFactory[Row] with DataReader[Row] {

  private var current = start - 1

  override def createDataReader(): DataReader[Row] = {
    new AdvancedDataReaderFactory(start, end, requiredSchema)
  }

  override def close(): Unit = {}

  override def next(): Boolean = {
    current += 1
    current < end
  }

  override def get(): Row = {
    val values = requiredSchema.map(_.name).map {
      case ""i"" => current
      case ""j"" => -current
    }
    Row.fromSeq(values)
  }
}

object DataSourceTest extends App {
  val spark = SparkSession.builder().master(""local[*]"").getOrCreate()

  val cls = classOf[AdvancedDataSourceV2]

  val with100 = spark.read.format(cls.getName).option(""rows"", 100).load()
  val with10 = spark.read.format(cls.getName).option(""rows"", 10).load()


  assert(with100.union(with10).count == 110)
}
{code}",,max.kiessling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 16 13:22:34 UTC 2018,,,,,,,,,,"0|i3zhdj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/18 13:22;max.kiessling;Apparently this issues is already resolved with the new 2.4.0 release. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Views created via 2.1 cannot be read via 2.2+,SPARK-25797,13193180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,seancxmao,seancxmao,seancxmao,22/Oct/18 07:15,29/Oct/18 04:28,13/Jul/23 08:48,29/Oct/18 04:28,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,SQL,,,,0,,,,,"We ran into this issue when we update our Spark from 2.1 to 2.3. Below's a simple example to reproduce the issue.

Create views via Spark 2.1
{code:sql}
create view v1 as
select (cast(1 as decimal(18,0)) + cast(1 as decimal(18,0))) c1;
{code}

Query views via Spark 2.3
{code:sql}
select * from v1;
Error in query: Cannot up cast `c1` from decimal(20,0) to c1#3906: decimal(19,0) as it may truncate
{code}

After investigation, we found that this is because when a view is created via Spark 2.1, the expanded text is saved instead of the original text. Unfortunately, the blow expanded text is buggy.
{code:sql}
spark-sql> desc extended v1;
c1 decimal(19,0) NULL
Detailed Table Information
Database default
Table v1
Type VIEW
View Text SELECT `gen_attr_0` AS `c1` FROM (SELECT (CAST(CAST(1 AS DECIMAL(18,0)) AS DECIMAL(19,0)) + CAST(CAST(1 AS DECIMAL(18,0)) AS DECIMAL(19,0))) AS `gen_attr_0`) AS gen_subquery_0
{code}

We can see that c1 is decimal(19,0), however in the expanded text there is decimal(19,0) + decimal(19,0) which results in decimal(20,0). Since Spark 2.2, decimal(20,0) in query is not allowed to cast to view definition column decimal(19,0). ([https://github.com/apache/spark/pull/16561])

I further tested other decimal calculations. Only add/subtract has this issue.

Create views via 2.1:
{code:sql}
create view v1 as
select (cast(1 as decimal(18,0)) + cast(1 as decimal(18,0))) c1;
create view v2 as
select (cast(1 as decimal(18,0)) - cast(1 as decimal(18,0))) c1;
create view v3 as
select (cast(1 as decimal(18,0)) * cast(1 as decimal(18,0))) c1;
create view v4 as
select (cast(1 as decimal(18,0)) / cast(1 as decimal(18,0))) c1;
create view v5 as
select (cast(1 as decimal(18,0)) % cast(1 as decimal(18,0))) c1;
create view v6 as
select cast(1 as decimal(18,0)) c1
union
select cast(1 as decimal(19,0)) c1;
{code}

Query views via Spark 2.3
{code:sql}
select * from v1;
Error in query: Cannot up cast `c1` from decimal(20,0) to c1#3906: decimal(19,0) as it may truncate
select * from v2;
Error in query: Cannot up cast `c1` from decimal(20,0) to c1#3909: decimal(19,0) as it may truncate
select * from v3;
1
select * from v4;
1
select * from v5;
0
select * from v6;
1
{code}

Views created via Spark 2.2+ don't have this issue because Spark 2.2+ does not generate expanded text for view (https://issues.apache.org/jira/browse/SPARK-18209).",,apachespark,dongjoon,kevinyu98,seancxmao,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18209,SPARK-18801,SPARK-11012,HIVE-972,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 29 04:28:43 UTC 2018,,,,,,,,,,"0|i3zgxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/18 07:36;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22846;;;","26/Oct/18 11:43;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22851;;;","29/Oct/18 04:28;dongjoon;Issue resolved by pull request 22851
[https://github.com/apache/spark/pull/22851];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CSV SparkR SQL Example,SPARK-25795,13193164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,22/Oct/18 04:10,29/Oct/18 00:57,13/Jul/23 08:48,22/Oct/18 23:35,2.3.0,2.3.1,2.3.2,2.4.0,,,,,,,,,,,,,,2.3.3,2.4.0,,,Examples,R,,,0,,,,,"This issue aims to fix the following SparkR example in Spark 2.3.0 ~ 2.4.0.

{code}
> df <- read.df(""examples/src/main/resources/people.csv"", ""csv"")
> namesAndAges <- select(df, ""name"", ""age"")
...
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`name`' given input columns: [_c0];;
'Project ['name, 'age]
+- AnalysisBarrier
      +- Relation[_c0#97] csv
{code}
 
- https://github.com/apache/spark/blob/master/examples/src/main/r/RSparkSQLExample.R
- https://dist.apache.org/repos/dist/dev/spark/v2.4.0-rc3-docs/_site/sql-programming-guide.html#manually-specifying-options
- http://spark.apache.org/docs/2.3.2/sql-programming-guide.html#manually-specifying-options
- http://spark.apache.org/docs/2.3.1/sql-programming-guide.html#manually-specifying-options
- http://spark.apache.org/docs/2.3.0/sql-programming-guide.html#manually-specifying-options


",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/18 04:49;dongjoon;0001-SPARK-25795-R-EXAMPLE-Fix-CSV-SparkR-SQL-Example.patch;https://issues.apache.org/jira/secure/attachment/12944935/0001-SPARK-25795-R-EXAMPLE-Fix-CSV-SparkR-SQL-Example.patch",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 22 23:35:26 UTC 2018,,,,,,,,,,"0|i3zguf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/18 04:42;dongjoon;PR is disappearing due to GitHub malfunctioning. I'll make a new one again after GitHub becomes healthy.;;;","22/Oct/18 08:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22792;;;","22/Oct/18 13:43;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22791;;;","22/Oct/18 23:35;dongjoon;Issue resolved by pull request 22791
[https://github.com/apache/spark/pull/22791];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loading model bug in BisectingKMeans,SPARK-25793,13193159,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,weichenxu123,weichenxu123,22/Oct/18 02:32,12/Dec/22 18:11,13/Jul/23 08:48,26/Oct/18 03:10,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,ML,MLlib,,,0,,,,,"See this line:

[https://github.com/apache/spark/blob/fc64e83f9538d6b7e13359a4933a454ba7ed89ec/mllib/src/main/scala/org/apache/spark/mllib/clustering/BisectingKMeansModel.scala#L129]

 

This also affects `ml.clustering.BisectingKMeansModel`

 ",,apachespark,cloud_fan,dongjoon,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 26 03:10:27 UTC 2018,,,,,,,,,,"0|i3zgtb:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,"22/Oct/18 05:06;gurwls223;Please avoid to set a target version which is usually reserved for committers.;;;","22/Oct/18 08:54;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22790;;;","22/Oct/18 08:55;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22793;;;","22/Oct/18 16:53;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22793;;;","22/Oct/18 21:56;dongjoon;Hi, [~WeichenXu123]. Could you tell us why did you create this as a `Minor` and `Documentation` issue?;;;","23/Oct/18 01:57;weichenxu123;[~dongjoon] Sorry. Change type to Bug and priority Major ;;;","23/Oct/18 03:18;dongjoon;Thank you, [~WeichenXu123].;;;","26/Oct/18 03:10;cloud_fan;Issue resolved by pull request 22790
[https://github.com/apache/spark/pull/22790];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datatype of serializers in RowEncoder should be accessible,SPARK-25791,13193071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,21/Oct/18 00:53,23/Oct/18 14:03,13/Jul/23 08:48,23/Oct/18 14:03,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"The serializers of {{RowEncoder}} use few {{If}} Catalyst expression which inherits {{ComplexTypeMergingExpression}} that will check input data types.

It is possible to generate serializers which fail the check and can't to access the data type of serializers. When producing {{If}} expression, we should use the same data type at its input expressions.",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 14:03:56 UTC 2018,,,,,,,,,,"0|i3zg9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/18 01:01;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22785;;;","23/Oct/18 14:03;cloud_fan;Issue resolved by pull request 22785
[https://github.com/apache/spark/pull/22785];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If the ByteBuffer.hasArray is false , it will throw UnsupportedOperationException for Kryo",SPARK-25786,13193011,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,20/Oct/18 02:39,24/Nov/18 15:35,13/Jul/23 08:48,24/Nov/18 15:10,3.0.0,,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,Spark Core,,,,0,,,,,"`{color:#ffc66d}deserialize{color}` for kryo,  the type of input parameter is ByteBuffer, if it is not backed by an accessible byte array. it will throw UnsupportedOperationException

Exception Info:

java.lang.UnsupportedOperationException was thrown.
 java.lang.UnsupportedOperationException
     at java.nio.ByteBuffer.array(ByteBuffer.java:994)
     at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:362)

 ",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 24 15:10:38 UTC 2018,,,,,,,,,,"0|i3zfwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/18 02:48;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/22779;;;","20/Oct/18 02:48;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/22779;;;","24/Nov/18 15:10;srowen;Issue resolved by pull request 22779
[https://github.com/apache/spark/pull/22779];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The disk write buffer size must be greater than 12.,SPARK-25776,13192731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,19/Oct/18 02:12,26/Apr/19 22:25,13/Jul/23 08:48,04/Nov/18 16:58,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,0,,,,,"In {color:#205081}{{UnsafeSorterSpillWriter.java}}{color}, when we write a record to a spill file wtih {{ {color:#205081}void write(Object baseObject, long baseOffset, int recordLength, long keyPrefix{color})}}, {color:#205081}{{recordLength}} {color}and {color:#205081}{{keyPrefix}} {color}will be written the disk write buffer first, and these will take 12 bytes, so the disk write buffer size must be greater than 12.

If {color:#205081}{{diskWriteBufferSize}} {color}is 10, it will print this exception info:

_java.lang.ArrayIndexOutOfBoundsException: 10_
 _at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.writeLongToBuffer (UnsafeSorterSpillWriter.java:91)_
 _at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:123)_
 _at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:498)_
 _at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:222)_
 _at org.apache.spark.memory.MemoryConsumer.spill(MemoryConsumer.java:65)_",,10110346,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 04 17:04:25 UTC 2018,,,,,,,,,,"0|i3ze67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/18 02:16;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/22754;;;","19/Oct/18 02:17;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/22754;;;","04/Nov/18 17:04;kiszk;Issue resolved by pull request 22754
https://github.com/apache/spark/pull/22754;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java encoders - switch fields on collectAsList,SPARK-25772,13192682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vofque,tomron,dongjoon,18/Oct/18 20:43,25/Oct/18 14:36,13/Jul/23 08:48,24/Oct/18 01:30,2.1.1,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"I have the following schema in a dataset -

root
 |-- userId: string (nullable = true)
 |-- data: map (nullable = true)
 |    |-- key: string
 |    |-- value: struct (valueContainsNull = true)
 |    |    |-- startTime: long (nullable = true)
 |    |    |-- endTime: long (nullable = true)
 |-- offset: long (nullable = true)


 And I have the following classes (+ setter and getters which I omitted for simplicity) -


 
{code:java}
public class MyClass {

    private String userId;

    private Map<String, MyDTO> data;

    private Long offset;
 }

public class MyDTO {

    private long startTime;
    private long endTime;

}
{code}


I collect the result the following way - 


{code:java}
        Encoder<MyClass> myClassEncoder = Encoders.bean(MyClass.class);
        Dataset<MyClass> results = raw_df.as(myClassEncoder);
        List<MyClass> lst = results.collectAsList();

{code}
        
I do several calculations to get the result I want and the result is correct all through the way before I collect it.
This is the result for - 


{code:java}
results.select(results.col(""data"").getField(""2017-07-01"").getField(""startTime"")).show(false);

{code}

|data[2017-07-01].startTime|data[2017-07-01].endTime|
+-----------------------------+--------------+
|1498854000                |1498870800              |


This is the result after collecting the reuslts for - 


{code:java}
MyClass userData = results.collectAsList().get(0);
MyDTO userDTO = userData.getData().get(""2017-07-01"");
System.out.println(""userDTO startTime: "" + userDTO.getStartTime());
System.out.println(""userDTO endTime: "" + userDTO.getEndTime());

{code}

--
data startTime: 1498870800
data endTime: 1498854000

I tend to believe it is a spark issue. Would love any suggestions on how to bypass it.","mac os
spark 2.1.1
Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_121",apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 25 14:36:48 UTC 2018,,,,,,,,,,"0|i3zdvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/18 20:55;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22745;;;","18/Oct/18 20:55;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22745;;;","24/Oct/18 01:30;cloud_fan;Issue resolved by pull request 22745
[https://github.com/apache/spark/pull/22745];;;","25/Oct/18 14:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22825;;;","25/Oct/18 14:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix improper synchronization in PythonWorkerFactory,SPARK-25771,13192681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,18/Oct/18 20:41,22/Oct/18 17:08,13/Jul/23 08:48,22/Oct/18 17:08,2.3.2,,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,,,,0,,,,,,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 18 20:47:52 UTC 2018,,,,,,,,,,"0|i3zdv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/18 20:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22770;;;","18/Oct/18 20:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22770;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnresolvedAttribute.sql() incorrectly escapes nested columns,SPARK-25769,13192553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,simeons,simeons,18/Oct/18 14:14,12/Mar/21 03:00,13/Jul/23 08:48,12/Mar/21 02:59,2.3.2,,,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,,"{{UnresolvedAttribute.sql()}} output is incorrectly escaped for nested columns
{code:java}
import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute

// The correct output is a.b, without backticks, or `a`.`b`.
$""a.b"".expr.asInstanceOf[UnresolvedAttribute].sql
// res1: String = `a.b`

// Parsing is correct; the bug is localized to sql() 
$""a.b"".expr.asInstanceOf[UnresolvedAttribute].nameParts 
// res2: Seq[String] = ArrayBuffer(a, b)
{code}
The likely culprit is that the {{sql()}} implementation does not check for {{nameParts}} being non-empty.
{code:java}
override def sql: String = name match { 
  case ParserUtils.escapedIdentifier(_) | ParserUtils.qualifiedEscapedIdentifier(_, _) => name 
  case _ => quoteIdentifier(name) 
}
{code}
 ",,apachespark,cloud_fan,huaxingao,kevinyu98,simeons,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 02:59:12 UTC 2021,,,,,,,,,,"0|i3zd2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/18 18:29;huaxingao;I will work on this. Thanks!;;;","21/Oct/18 16:26;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22788;;;","06/Mar/21 23:56;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31754;;;","12/Mar/21 02:59;cloud_fan;Issue resolved by pull request 31754
[https://github.com/apache/spark/pull/31754];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constant argument expecting Hive UDAFs doesn't work,SPARK-25768,13192546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,18/Oct/18 13:49,19/Oct/18 13:20,13/Jul/23 08:48,19/Oct/18 13:20,2.2.0,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,SQL,,,,0,,,,," The following doesn't work since SPARK-18186 so it is a regression.
{code:java}
test(""constant argument expecting Hive UDAF"") {
  withTempView(""inputTable"") {
    spark.range(10).createOrReplaceTempView(""inputTable"")
    withUserDefinedFunction(""testGenericUDAFPercentileApprox"" -> false) {
      val numFunc = spark.catalog.listFunctions().count()
      sql(s""CREATE FUNCTION testGenericUDAFPercentileApprox AS '"" +
        s""${classOf[GenericUDAFPercentileApprox].getName}'"")
      checkAnswer(
        sql(""SELECT testGenericUDAFPercentileApprox(id, 0.5) FROM inputTable""),
        Seq(Row(4.0)))
    }
  }
}

{code}
 ",,apachespark,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 18 13:58:02 UTC 2018,,,,,,,,,,"0|i3zd13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/18 13:57;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/22766;;;","18/Oct/18 13:58;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/22766;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error reported in Spark logs when using the org.apache.spark:spark-sql_2.11:2.3.2 Java library,SPARK-25767,13192539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,onyssius,onyssius,18/Oct/18 13:27,27/Oct/20 11:01,13/Jul/23 08:48,29/Oct/18 15:50,2.2.0,2.3.2,,,,,,,,,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,0,,,,,"Hi,

Here is a bug I found using the latest version of spark-sql_2.11:2.2.0. Note that this case was also tested with spark-sql_2.11:2.3.2 and the bug is also present.

This issue is a duplicate of the SPARK-25582 issue that I had to close after an accidental manipulation from another developer (was linked to a wrong PR)

You will find attached three small sample CSV files with the minimal content to raise the bug.

Find below a reproducer code:
{code:java}
import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import scala.collection.JavaConverters;
import scala.collection.Seq;

import java.util.Arrays;


public class SparkBug {

    private static <T> Seq<T> arrayToSeq(T[] input) {
        return JavaConverters.asScalaIteratorConverter(Arrays.asList(input).iterator()).asScala().toSeq();
    }

    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf().setAppName(""SparkBug"").setMaster(""local"");
        SparkSession sparkSession = SparkSession.builder().config(conf).getOrCreate();
        Dataset<Row> df_a = sparkSession.read().option(""header"", true).csv(""local/fileA.csv"").dropDuplicates();
        Dataset<Row> df_b = sparkSession.read().option(""header"", true).csv(""local/fileB.csv"").dropDuplicates();
        Dataset<Row> df_c = sparkSession.read().option(""header"", true).csv(""local/fileC.csv"").dropDuplicates();

        String[] key_join_1 = new String[]{""colA"", ""colB"", ""colC"", ""colD"", ""colE"", ""colF""};
        String[] key_join_2 = new String[]{""colA"", ""colB"", ""colC"", ""colD"", ""colE""};

        Dataset<Row> df_inventory_1 = df_a.join(df_b, arrayToSeq(key_join_1), ""left"");
        Dataset<Row> df_inventory_2 = df_inventory_1.join(df_c, arrayToSeq(key_join_2), ""left"");

        df_inventory_2.show();
    }

}
{code}
When running this code, I can see the exception below:
{code:java}
18/10/18 09:25:49 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 202, Column 18: Expression ""agg_isNull_28"" is not an rvalue
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 202, Column 18: Expression ""agg_isNull_28"" is not an rvalue
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11821)
    at org.codehaus.janino.UnitCompiler.toRvalueOrCompileException(UnitCompiler.java:7170)
    at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:5332)
    at org.codehaus.janino.UnitCompiler.access$9400(UnitCompiler.java:212)
    at org.codehaus.janino.UnitCompiler$13$1.visitAmbiguousName(UnitCompiler.java:5287)
    at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4053)
    at org.codehaus.janino.UnitCompiler$13.visitLvalue(UnitCompiler.java:5284)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:3977)
    at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:5280)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2391)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:212)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1474)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1466)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2926)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3075)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1336)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1309)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:799)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:958)
    at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:212)
    at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:393)
    at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:385)
    at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1286)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:385)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1285)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:825)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:212)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:390)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:385)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1405)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:385)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:357)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:446)
    at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:313)
    at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:235)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:204)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1417)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1493)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1490)
    at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
    at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
    at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1365)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:579)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:578)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
    at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
    at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:723)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:682)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:691)
    at SparkBug.main(SparkBug.java:30)
{code}
 ",,apachespark,mgaido,onyssius,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33260,,,,,,,,,,"18/Oct/18 13:27;onyssius;fileA.csv;https://issues.apache.org/jira/secure/attachment/12944536/fileA.csv","18/Oct/18 13:27;onyssius;fileB.csv;https://issues.apache.org/jira/secure/attachment/12944537/fileB.csv","18/Oct/18 13:27;onyssius;fileC.csv;https://issues.apache.org/jira/secure/attachment/12944538/fileC.csv",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 24 21:31:47 UTC 2019,,,,,,,,,,"0|i3zczj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/18 14:59;mgaido;I tried on current master branch but I wasn't able to reproduce. Could you try it on master branch too? Thanks.;;;","18/Oct/18 15:04;onyssius;Hi Marco,

I have noticed the bug using the jar coming from the mavenCentral repository ([http://central.maven.org/maven2/org/apache/spark/spark-sql_2.11/2.3.2/spark-sql_2.11-2.3.2.jar)]

Find below the *build.gradle* I'm using for reference:
{code:java}
buildscript {
    ext {
        sparkVersion = '2.3.2'
    }
}

plugins {
    id 'java'
}

group 'spark'
version '1.0-SNAPSHOT'

sourceCompatibility = 1.8

repositories {
    mavenCentral()
}

dependencies {
    compile group: 'org.apache.spark', name: 'spark-sql_2.11', version: ""${sparkVersion}""
}
{code};;;","18/Oct/18 15:14;onyssius;If it can help, I have created a github repo with the problematic code: [https://github.com/onyssius/spark-troubleshooting-bug]

Once I run the SparkBug.main() function, I can see the error mentioned in the description in the application logs (search ERROR)

Note that the exception doesn't reach the main thread, but it appears in the logs;;;","18/Oct/18 15:23;mgaido;It is interesting, I can reproduce with the Java API but not with the Scala one...;;;","18/Oct/18 15:25;onyssius;It looks like it's not stopping the processing but I would like to make sure there is has no side effect in the data processing;;;","18/Oct/18 15:33;mgaido;So I tracked down the issue. The problem is that you are passing a Stream as parameter for the join keys. The easy workaround is to use a Buffer instead of it.;;;","18/Oct/18 15:36;onyssius;Please can you provide an example? I don't see any Stream in my code;;;","18/Oct/18 16:24;mgaido;Your conversion of a Java array in a Scala Seq creates a Stream.;;;","19/Oct/18 13:00;onyssius;I confirm that changing from *.toSeq()* to *.toBuffer()* in my *arrayToSeq* function fixed the issue, thanks Marco!

Is there a reason that justifies why the signature of the *join* function is:
{code:java}
def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame
{code}
and not
{code:java}
def join(right: Dataset[_], usingColumns: Buffer[String], joinType: String): DataFrame
{code}
since using a _scala.collection.immutable.Stream_ generates an exception ?;;;","19/Oct/18 14:35;mgaido;I think it is a bug (thanks for reporting this): indeed I have not closed this issue because I think it needs to be addressed. I just included my analysis here in order to provide a workaround and some hint for people who might want to fix it (currently I am not sure I have the bandwidth for doing myself).;;;","21/Oct/18 21:18;petertoth;[~mgaido], I submitted a PR based on your analysis.;;;","21/Oct/18 21:19;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/22789;;;","21/Oct/18 21:20;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/22789;;;","24/Jan/19 21:31;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/23642;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
binaryFiles broken for small files,SPARK-25753,13192089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,17/Oct/18 01:01,04/Oct/19 20:51,13/Jul/23 08:48,22/Oct/18 13:54,2.4.4,3.0.0,,,,,,,,,,,,,,,,2.4.5,3.0.0,,,Input/Output,,,,0,,,,,"_{{StreamFileInputFormat}}_ and {{_WholeTextFileInputFormat_(https://issues.apache.org/jira/browse/SPARK-24610)}} have the same problem: for small sized files, the computed maxSplitSize by `_{{StreamFileInputFormat}}_ `  is way smaller than the default or commonly used split size of 64/128M and spark throws an exception while trying to read them.

{{Exception info:}}

_{{Minimum split size pernode 5123456 cannot be larger than maximum split size 4194304 java.io.IOException: Minimum split size pernode 5123456 cannot be larger than maximum split size 4194304 at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java: 201) at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:52) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:254) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:252) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2138)}}_",,10110346,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24610,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 04 20:50:52 UTC 2019,,,,,,,,,,"0|i3za7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/18 01:25;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/22725;;;","04/Oct/19 20:50;dongjoon;This is merged to master via https://github.com/apache/spark/pull/22725 , and
This is backported to branch-2.4 via https://github.com/apache/spark/pull/26026 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker-image-tool.sh ignores errors from Docker,SPARK-25745,13191907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rvesse,rvesse,rvesse,16/Oct/18 12:48,17/May/20 18:23,13/Jul/23 08:48,19/Oct/18 22:04,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,,,,3.0.0,,,,Deploy,Kubernetes,Spark Core,,0,,,,,"In attempting to use the {{docker-image-tool.sh}} scripts to build some custom Dockerfiles I ran into issues with the scripts interaction with Docker.  Most notably if the Docker build/push fails the script continues blindly ignoring the errors.  This can either result in complete failure to build or lead to subtle bugs where images are built against different base images than expected.

Additionally while the Dockerfiles assume that Spark is first built locally the scripts fail to validate this which they could easily do by checking the expected JARs location.  This can also lead to failed Docker builds which could easily be avoided.",,apachespark,rvesse,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 19 22:04:24 UTC 2018,,,,,,,,,,"0|i3z93j:",9223372036854775807,,,,,liyinan926,,,,,,,,3.0.0,,,,,,,,,,"16/Oct/18 12:57;apachespark;User 'rvesse' has created a pull request for this issue:
https://github.com/apache/spark/pull/22748;;;","16/Oct/18 12:58;apachespark;User 'rvesse' has created a pull request for this issue:
https://github.com/apache/spark/pull/22748;;;","19/Oct/18 22:04;vanzin;Issue resolved by pull request 22748
[https://github.com/apache/spark/pull/22748];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long URLs are not rendered properly in web UI,SPARK-25741,13191865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,16/Oct/18 10:05,17/Oct/18 14:53,13/Jul/23 08:48,17/Oct/18 14:53,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Web UI,,,,0,,,,,"When the URL for description column in the table of job/stage page is long, WebUI doesn't render it properly.
",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 17 14:53:58 UTC 2018,,,,,,,,,,"0|i3z8u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/18 10:12;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22744;;;","16/Oct/18 10:12;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22744;;;","17/Oct/18 14:53;srowen;Issue resolved by pull request 22744
[https://github.com/apache/spark/pull/22744];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOAD DATA INPATH doesn't work if hdfs conf includes port,SPARK-25738,13191717,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,irashid,irashid,irashid,15/Oct/18 20:57,16/Oct/18 01:35,13/Jul/23 08:48,16/Oct/18 01:35,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"LOAD DATA INPATH throws {{java.net.URISyntaxException: Malformed IPv6 address at index 8}} if your hdfs conf includes a port for the namenode.

This is because the URI is passing in the value of the hdfs conf ""fs.defaultFS"" in for the host.  Note that variable is called {{authority}}, but the 4-arg URI constructor actually expects a host: https://docs.oracle.com/javase/7/docs/api/java/net/URI.html#URI(java.lang.String,%20java.lang.String,%20java.lang.String,%20java.lang.String)

{code}
val defaultFSConf = sparkSession.sessionState.newHadoopConf().get(""fs.defaultFS"")
...
val newUri = new URI(scheme, authority, pathUri.getPath, pathUri.getFragment)
{code}

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala#L386

This was introduced by SPARK-23425.

*Workaround*: specify a fully qualified path, eg. instead of 

{noformat}
LOAD DATA INPATH '/some/path/on/hdfs'
{noformat}

use

{noformat}
LOAD DATA INPATH 'hdfs://fizz.buzz.com:8020/some/path/on/hdfs'
{noformat}
",,apachespark,dongjoon,irashid,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23425,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 16 01:35:22 UTC 2018,,,,,,,,,,"0|i3z7xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/18 21:07;zsxwing;Marked as a blocker since this is a regression;;;","15/Oct/18 21:10;irashid;the fix is pretty trivial, I'm posting a pr now;;;","15/Oct/18 21:17;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22733;;;","16/Oct/18 01:35;vanzin;Issue resolved by pull request 22733
[https://github.com/apache/spark/pull/22733];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve start-thriftserver.sh: print clean usage and exit with code 1,SPARK-25735,13191599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,15/Oct/18 13:51,17/Oct/18 14:56,13/Jul/23 08:48,17/Oct/18 14:56,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,0,,,,,"Currently if we run 
sh start-thriftserver.sh -h

we get 

...
Thrift server options:
2018-10-15 21:45:39 INFO  HiveThriftServer2:54 - Starting SparkContext
2018-10-15 21:45:40 INFO  SparkContext:54 - Running Spark version 2.3.2
2018-10-15 21:45:40 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-10-15 21:45:40 ERROR SparkContext:91 - Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:367)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2493)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:934)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:925)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:925)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:48)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:79)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
2018-10-15 21:45:40 ERROR Utils:91 - Uncaught exception in thread main

After fix, the usage output is clean:
Thrift server options:
    --hiveconf <property=value>   Use value for given property

Also exit with code 1, to follow other scripts.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 17 14:56:33 UTC 2018,,,,,,,,,,"0|i3z77b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/18 13:57;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22727;;;","15/Oct/18 13:58;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22727;;;","17/Oct/18 14:56;srowen;Issue resolved by pull request 22727
[https://github.com/apache/spark/pull/22727];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes scheduler tries to read pod details that it just deleted,SPARK-25730,13191506,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mikekap,mikekap,mikekap,15/Oct/18 07:18,17/May/20 18:25,13/Jul/23 08:48,21/Oct/18 18:43,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,Spark Core,,,0,,,,,See [https://github.com/apache/spark/pull/22720/files] for the fix.,,apachespark,mikekap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 15 07:20:26 UTC 2018,,,,,,,,,,"0|i3z6mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/18 07:20;apachespark;User 'mikekap' has created a pull request for this issue:
https://github.com/apache/spark/pull/22720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
makeCopy failed in InMemoryRelation,SPARK-25727,13191420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,13/Oct/18 22:42,15/Oct/18 12:50,13/Jul/23 08:48,14/Oct/18 05:11,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"{code}
    val data = Seq(100).toDF(""count"").cache()
    data.queryExecution.optimizedPlan.toJSON
{code}

The above code can generate the following error:

{code}
assertion failed: InMemoryRelation fields: output, cacheBuilder, statsOfPlanToCache, outputOrdering, values: List(count#178), CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [value#176 AS count#178]
+- LocalTableScan [value#176]
,None), Statistics(sizeInBytes=12.0 B, hints=none)
java.lang.AssertionError: assertion failed: InMemoryRelation fields: output, cacheBuilder, statsOfPlanToCache, outputOrdering, values: List(count#178), CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [value#176 AS count#178]
+- LocalTableScan [value#176]
,None), Statistics(sizeInBytes=12.0 B, hints=none)
	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.sql.catalyst.trees.TreeNode.jsonFields(TreeNode.scala:611)
	at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$collectJsonValue$1(TreeNode.scala:599)
	at org.apache.spark.sql.catalyst.trees.TreeNode.jsonValue(TreeNode.scala:604)
	at org.apache.spark.sql.catalyst.trees.TreeNode.toJSON(TreeNode.scala:590)
{code}",,apachespark,dongjoon,smilegator,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 15 12:50:00 UTC 2018,,,,,,,,,,"0|i3z63j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/18 22:45;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22715;;;","13/Oct/18 22:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22715;;;","14/Oct/18 05:11;dongjoon;Issue resolved by pull request 22715
[https://github.com/apache/spark/pull/22715];;;","15/Oct/18 12:50;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: SaveIntoDataSourceCommandSuite.`simpleString is redacted`,SPARK-25726,13191418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,13/Oct/18 20:59,14/Oct/18 01:07,13/Jul/23 08:48,14/Oct/18 01:07,2.2.1,2.3.0,2.4.0,,,,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,SQL,Tests,,,0,,,,,"The test case fails because the used password string `123` matches `@41230802`. 

- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/97343/consoleFull
{code:java}
SaveIntoDataSourceCommandSuite:
- simpleString is redacted *** FAILED ***
""SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@41230802, Map(password -> *********(redacted), url -> *********(redacted), driver -> mydriver), ErrorIfExists
+- Range (0, 1, step=1, splits=Some(2))
"" contained ""123"" (SaveIntoDataSourceCommandSuite.scala:42){code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,SPARK-22479,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 14 01:07:07 UTC 2018,,,,,,,,,,"0|i3z633:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/18 21:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22716;;;","14/Oct/18 01:07;dongjoon;This is resolved via [https://github.com/apache/spark/pull/22716] .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null Handling in the Optimizer rule BooleanSimplification,SPARK-25714,13191052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,11/Oct/18 21:02,16/Oct/18 02:53,13/Jul/23 08:48,13/Oct/18 04:05,1.6.3,2.0.2,2.1.3,2.2.2,2.3.2,2.4.0,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,SQL,,,,0,correctness,,,,"{code}
scala> val df = Seq((""abc"", 1), (null, 3)).toDF(""col1"", ""col2"")
df: org.apache.spark.sql.DataFrame = [col1: string, col2: int]

scala> df.write.mode(""overwrite"").parquet(""/tmp/test1"")
                                                                                
scala> val df2 = spark.read.parquet(""/tmp/test1"");
df2: org.apache.spark.sql.DataFrame = [col1: string, col2: int]

scala> df2.filter(""col1 = 'abc' OR (col1 != 'abc' AND col2 == 3)"").show()
+----+----+
|col1|col2|
+----+----+
| abc|   1|
|null|   3|
+----+----+
{code}",,apachespark,smilegator,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 14 05:11:58 UTC 2018,,,,,,,,,,"0|i3z3uv:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"11/Oct/18 21:45;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22702;;;","11/Oct/18 21:45;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22702;;;","13/Oct/18 06:25;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22711;;;","14/Oct/18 03:57;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22718;;;","14/Oct/18 05:11;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22719;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range should report metrics correctly,SPARK-25710,13190937,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Oct/18 14:49,13/Oct/18 05:56,13/Jul/23 08:48,13/Oct/18 05:56,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 13 05:56:11 UTC 2018,,,,,,,,,,"0|i3z35b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/18 14:59;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22698;;;","11/Oct/18 15:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22698;;;","13/Oct/18 05:56;cloud_fan;Issue resolved by pull request 22698
[https://github.com/apache/spark/pull/22698];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HAVING without GROUP BY means global aggregate,SPARK-25708,13190882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Oct/18 11:51,05/Jan/21 23:51,13/Jul/23 08:48,12/Oct/18 07:26,2.0.2,2.1.3,2.2.3,2.3.4,2.4.0,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,correctness,release-notes,,,"According to the SQL standard, when a query contains `HAVING`, it indicates an aggregate operator. For more details please refer to https://blog.jooq.org/2014/12/04/do-you-really-understand-sqls-group-by-and-having-clauses/

However, in Spark SQL parser, we treat HAVING as a normal filter when there is no GROUP BY, which breaks SQL semantic and lead to wrong result. This PR fixes the parser.",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34012,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 09:46:26 UTC 2021,,,,,,,,,,"0|i3z2t3:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"11/Oct/18 12:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22696;;;","05/Jan/21 09:46;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31039;;;","05/Jan/21 09:46;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31039;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication of > 2GB block fails due to bad config default,SPARK-25704,13190706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,10/Oct/18 17:56,31/Oct/18 20:44,13/Jul/23 08:48,19/Oct/18 17:58,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"Replicating a block > 2GB currently fails because it tries to allocate a bytebuffer that is just a *bit* too large, due to a bad default config.  This [line|https://github.com/apache/spark/blob/cd40655965072051dfae65eabd979edff0e4d398/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L454]:

{code}
ChunkedByteBuffer.fromFile(tmpFile, conf.get(config.MEMORY_MAP_LIMIT_FOR_TESTS).toInt)
{code}

{{MEMORY_MAP_LIMIT_FOR_TESTS}} defaults to {{Integer.MAX_VALUE}}, but unfortunately that is just a tiny bit too big.  You'll see an exception like:

{noformat}
18/10/09 21:21:54 WARN server.TransportChannelHandler: Exception in connection from /172.31.118.153:53534
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
        at org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$8.apply(ChunkedByteBuffer.scala:199)
        at org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$8.apply(ChunkedByteBuffer.scala:199)
        at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
        at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
        at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2315)
        at org.apache.commons.io.IOUtils.copy(IOUtils.java:2270)
        at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2291)
        at org.apache.commons.io.IOUtils.copy(IOUtils.java:2246)
        at org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$fromFile$1.apply$mcI$sp(ChunkedByteBuffer.scala:201)
        at org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$fromFile$1.apply(ChunkedByteBuffer.scala:201)
        at org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$fromFile$1.apply(ChunkedByteBuffer.scala:201)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:202)
        at org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:184)
        at org.apache.spark.storage.BlockManager$$anon$1.onComplete(BlockManager.scala:454)
{noformat}

at least on my system, its just 2 bytes too big :(

{noformat}
> scala -J-Xmx4G
import java.nio.ByteBuffer
scala> ByteBuffer.allocate(Integer.MAX_VALUE)
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
  at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
  at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
  ... 30 elided

scala> ByteBuffer.allocate(Integer.MAX_VALUE - 1)
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
  at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
  at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
  ... 30 elided

scala> ByteBuffer.allocate(Integer.MAX_VALUE - 2)
res3: java.nio.ByteBuffer = java.nio.HeapByteBuffer[pos=0 lim=2147483645 cap=2147483645]
{noformat}

*Workaround*: Set to ""spark.storage.memoryMapLimitForTests"" something a bit smaller, eg. 2147483135 (that's Integer.MAX_VALUE - 512, just in case its a bit different on other systems).

This was introduced by SPARK-25422.  I'll file a PR shortly.",,apachespark,hzfeiwang,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25422,,SPARK-25827,,,SPARK-25904,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 19 17:58:21 UTC 2018,,,,,,,,,,"0|i3z1pz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/18 02:20;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22705;;;","12/Oct/18 02:21;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22705;;;","19/Oct/18 17:58;irashid;Resolved by pr https://github.com/apache/spark/pull/22705

Commit to master https://github.com/apache/spark/commit/43717dee570dc41d71f0b27b8939f6297a029a02

to branch-2.4
https://github.com/apache/spark/commit/1001d2314275c902da519725da266a23b537e33a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When zstd compression enabled in progress application is throwing Error in UI,SPARK-25697,13190571,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,abhishek.akg,abhishek.akg,10/Oct/18 07:11,29/Oct/18 00:57,13/Jul/23 08:48,12/Oct/18 18:00,2.3.2,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"# In spark-default.conf of Job History enable below parameter
spark.eventLog.compress=true
spark.io.compression.codec = org.apache.spark.io.ZStdCompressionCodec
 #  Restart Job History Services
 # Submit beeline jobs
 # Open Yarn Resource Page
 # Check for the running application in Yarn Resource Page it will list the application.
 # Open Job History Page 
 # Go and click Incomplete Application Link and click on the application

*Actual Result:*
UI display ""*Read error or truncated source*"" Error


*Expected Result:*
Job History should list the Jobs of the application on clicking the application ID.",,abhishek.akg,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/18 07:16;shahid;Screenshot from 2018-10-10 12-45-20.png;https://issues.apache.org/jira/secure/attachment/12943189/Screenshot+from+2018-10-10+12-45-20.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 12 18:00:14 UTC 2018,,,,,,,,,,"0|i3z0wf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/18 07:12;shahid;Thanks for reporting [~abhishek.akg]. I am working on it.;;;","10/Oct/18 18:36;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22689;;;","10/Oct/18 18:37;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22689;;;","12/Oct/18 18:00;srowen;Resolved by https://github.com/apache/spark/pull/22689;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
URL.setURLStreamHandlerFactory causing incompatible HttpURLConnection issue,SPARK-25694,13190500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ZhouJIANG,boyangwa,boyangwa,09/Oct/18 22:20,12/Dec/22 18:10,13/Jul/23 08:48,18/Nov/19 05:46,2.3.0,2.3.1,2.3.2,2.4.4,3.0.0,,,,,,,,,,,,,2.4.6,3.0.0,,,Spark Core,SQL,,,1,,,,,"URL.setURLStreamHandlerFactory() in SharedState causes URL.openConnection() returns FsUrlConnection object, which is not compatible with HttpURLConnection. This will cause exception when using some third party http library (e.g. scalaj.http).

The following code in Spark 2.3.0 introduced the issue: sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala:
{code}
object SharedState extends Logging  {   ...   
  URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory())   ...
}
{code}

Here is the example exception when using scalaj.http in Spark:
{code}
 StackTrace: scala.MatchError: org.apache.hadoop.fs.FsUrlConnection:[http://wwww.example.com|http://wwww.example.com/] (of class org.apache.hadoop.fs.FsUrlConnection)
 at scalaj.http.HttpRequest.scalaj$http$HttpRequest$$doConnection(Http.scala:343)
 at scalaj.http.HttpRequest.exec(Http.scala:335)
 at scalaj.http.HttpRequest.asString(Http.scala:455)
{code}
  
One option to fix the issue is to return null in URLStreamHandlerFactory.createURLStreamHandler when the protocol is http/https, so it will use the default behavior and be compatible with scalaj.http. Following is the code example:

{code}
class SparkUrlStreamHandlerFactory extends URLStreamHandlerFactory with Logging {

  private val fsUrlStreamHandlerFactory = new FsUrlStreamHandlerFactory()

  override def createURLStreamHandler(protocol: String): URLStreamHandler = {
    val handler = fsUrlStreamHandlerFactory.createURLStreamHandler(protocol)
    if (handler == null) {
      return null
    }

    if (protocol != null &&
      (protocol.equalsIgnoreCase(""http"")
      || protocol.equalsIgnoreCase(""https""))) {
      // return null to use system default URLStreamHandler
      null
    } else {
      handler
    }
  }
}
{code}

I would like to get some discussion here before submitting a pull request.
",,boyangwa,dbtsai,dongjoon,holmes.infra,howardatwork,toyboxman,ZhouJIANG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12868,HADOOP-14598,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 03:20:29 UTC 2020,,,,,,,,,,"0|i3z0h3:",9223372036854775807,,,,,felixcheung,,,,,,,,,,,,,,,,,,"12/Oct/18 01:42;gurwls223;Please avoid to set target versions which are usually reserved for committers.;;;","12/Oct/18 20:51;boyangwa;Got it, thanks [~hyukjin.kwon] for the suggestion!;;;","14/Dec/18 18:14;howardatwork;[~boyangwa], do you know of a workaround for this? I'm facing the same issue.;;;","14/Mar/19 01:16;toyboxman;[~howardatwork],

 

I have hit this problem as well, seems no workaround without change in httpclient.  I ever used scalaj.http, but replaced it httpcomponent

 ;;;","01/Nov/19 19:13;dongjoon;Although SPARK-12868 adds `setURLStreamHandlerFactory` for `ADD JARS` commands, this method can be called at most once in a given Java Virtual Machine. So, there is another issue with this.
- https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/net/URL.html#setURLStreamHandlerFactory(java.net.URLStreamHandlerFactory);;;","18/Nov/19 05:46;dbtsai;Issue resolved by pull request 26530
[https://github.com/apache/spark/pull/26530];;;","02/Mar/20 03:20;holmes.infra;i have a question than why FsUrlStreamHandlerFactory will deal with the http schema url. from the code , when protocal is http,method createURLStreamHandler in FsUrlStreamHandlerFactory will return null. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove static initialization of worker eventLoop handling chunk fetch requests within TransportContext,SPARK-25692,13190494,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sanket991,zsxwing,zsxwing,09/Oct/18 22:01,23/Apr/20 20:35,13/Jul/23 08:48,05/Feb/19 18:44,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,0,,,,,"Looks like the whole test suite is pretty flaky. See: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.6/5490/testReport/junit/org.apache.spark.network/ChunkFetchIntegrationSuite/history/

This may be a regression in 3.0 as this didn't happen in 2.4 branch.",,Dhruve Ashar,dongjoon,sanket991,tgraves,zsxwing,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,SPARK-31542,,,"22/Oct/18 20:14;sanket991;Screen Shot 2018-10-22 at 4.12.41 PM.png;https://issues.apache.org/jira/secure/attachment/12945099/Screen+Shot+2018-10-22+at+4.12.41+PM.png","01/Nov/18 17:17;zsxwing;Screen Shot 2018-11-01 at 10.17.16 AM.png;https://issues.apache.org/jira/secure/attachment/12946575/Screen+Shot+2018-11-01+at+10.17.16+AM.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 30 05:47:45 UTC 2019,,,,,,,,,,"0|i3z0fr:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"09/Oct/18 22:20;zsxwing;It may be caused by https://github.com/apache/spark/pull/22173;;;","19/Oct/18 19:31;tgraves;[~redsanket] can you please take a look at this;;;","22/Oct/18 13:34;sanket991;Sure [~tgraves] [~zsxwing] i am taking a look. Thanks for reporting.;;;","22/Oct/18 20:30;sanket991;[~zsxwing] I haven't been able to reproduce this, but the number of threads used for this tests are 2* number of cores or spark.shuffle.io.serverThreads. I think the test server seems to be down. Would appreciate any help to reproduce this.

!Screen Shot 2018-10-22 at 4.12.41 PM.png!;;;","24/Oct/18 15:19;sanket991;[https://github.com/apache/spark/pull/22628/files] went it after [https://github.com/apache/spark/pull/22173] not sure you are still seeing issues... I ran the tests in a loop for 100 times. Could not reproduce this.;;;","01/Nov/18 17:18;zsxwing;It's still flaky on Jenkins: [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/5554/testReport/junit/org.apache.spark.network/ChunkFetchIntegrationSuite/fetchBothChunks/history/]

 !Screen Shot 2018-11-01 at 10.17.16 AM.png! 

You may need to run the whole tests together. [https://github.com/apache/spark/pull/22173] added a global thread pool, so other tests may also impact this test suite.;;;","01/Nov/18 17:20;zsxwing;[~sanket991] You can download the unit test logs from https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/5554/artifact/common/network-common/target/ (It will be kept on Jenkins for several days);;;","05/Jan/19 15:59;dongjoon;Hi, [~zsxwing] and [~tgraves]. 

While looking other failures, I notice that this failure still happens frequently. 

The failure is always `fetchBothChunks`. `amp-jenkins-worker-05` machine might be related.

- [master 5856|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5856/] (amp-jenkins-worker-05)
- [master 5837|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5837/testReport] (amp-jenkins-worker-05)
- [master 5835|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5835/testReport] (amp-jenkins-worker-05)
- [master 5829|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5829/testReport] (amp-jenkins-worker-05)
- [master 5828|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5828/testReport] (amp-jenkins-worker-05)
- [master 5822|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5822/testReport] (amp-jenkins-worker-05)
- [master 5814|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5814/testReport] (amp-jenkins-worker-05)

- [SparkPullRequestBuilder 100784|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/100784/consoleFull] (amp-jenkins-worker-05)

- [SparkPullRequestBuilder 100785|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/100785/consoleFull] (amp-jenkins-worker-05)

- [SparkPullRequestBuilder 100787|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/100787/consoleFull] (amp-jenkins-worker-05)

- [SparkPullRequestBuilder 100788|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/100788/consoleFull] (amp-jenkins-worker-05);;;","12/Jan/19 01:25;dongjoon;This is resolved via https://github.com/apache/spark/pull/23522.;;;","19/Jan/19 19:07;dongjoon;Sorry, guys. I'm reopening this since it's observed again.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/5888/consoleFull  (amp-jenkins-worker-05)
{code}
[INFO] Running org.apache.spark.network.ChunkFetchIntegrationSuite
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 60.109 s <<< FAILURE! - in org.apache.spark.network.ChunkFetchIntegrationSuite
[ERROR] fetchBothChunks(org.apache.spark.network.ChunkFetchIntegrationSuite)  Time elapsed: 60.017 s  <<< FAILURE!
java.lang.AssertionError: Timeout getting response from the server
	at org.apache.spark.network.ChunkFetchIntegrationSuite.fetchChunks(ChunkFetchIntegrationSuite.java:176)
	at org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks(ChunkFetchIntegrationSuite.java:210)
{code};;;","28/Jan/19 22:16;sanket991;I had a few observations regarding this test suite...

When i run it on mac

$ sysctl hw.physicalcpu hw.logicalcpu
hw.physicalcpu: 4
hw.logicalcpu: 8 

I dont see the issue. This i think has got to do with io.serverThreads and the number of threads being used for handling chunk blocks and since there are multiple tests in the suite handling chunked blocks might require sufficient threads to handle the requests.

 

On a vm i was able to reproduce this consistently

-bash-4.1$ lscpu | grep -E '^Thread|^Core|^Socket|^CPU\('
CPU(s): 4
Thread(s) per core: 1
Core(s) per socket: 1
Socket(s): 4

The root cause might be why it is actually failing is due to like [~zsxwing] pointed out is due to [https://github.com/apache/spark/blob/c00186f90cfcc33492d760f874ead34f0e3da6ed/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java#L88|https://github.com/apache/spark/blob/c00186f90cfcc33492d760f874ead34f0e3da6ed/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java#L88.] sharing of worker threads.

When I remove the static I no longer see the test failure.

 

So do we really need it to be static?

I dont think this requires a global declaration as these threads are only required on the shuffle server end and on the client TransportContext initialization i.e the Client don't initialize these threads. I assume for Shuffle Server there would be only one TransportContext object. So, I think this is fine to be an instance variable and I see no harm. Will do some testing again and if everything is fine will put up the pr...

 ;;;","30/Jan/19 15:06;sanket991;Did some further digging

How to reproduce
./build/mvn test -Dtest=org.apache.spark.network.RequestTimeoutIntegrationSuite,org.apache.spark.network.ChunkFetchIntegrationSuite -DwildcardSuites=None test
furtherRequestsDelay Test within RequestTimeoutIntegrationSuite was holding onto worker references. The test does close the server context but since the threads are global and there is sleep of 60 secs to fetch a specific chunk within this test, it grabs on it and waits for the client to consume but however the test is testing for a request timeout and it times out after 10 secs, so the workers are just waiting there for the buffer to be consumed as per my understanding. I think we dont need this to be static as the server just initializes the TransportContext object once. I did some manual tests and it looks good;;;","30/Jan/19 15:25;sanket991;Created a pr [https://github.com/apache/spark/pull/23700] plz take a look thanks and let me know your thoughts;;;","05/Feb/19 18:44;srowen;Issue resolved by pull request 23700
[https://github.com/apache/spark/pull/23700];;;","30/Jun/19 05:47;dongjoon;I update the JIRA title according to the last patch which is the real fix of the underlying issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow running tests in Jenkins in enterprise Git repository,SPARK-25685,13190312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,09/Oct/18 08:22,12/Oct/18 17:41,13/Jul/23 08:48,12/Oct/18 17:41,2.3.2,,,,,,,,,,,,,,,,,3.0.0,,,,Build,Tests,,,0,,,,,Add some environment variables to allow regression testing in enterprise Jenkins instead of default Spark repository in GitHub.,,apachespark,cltlfcjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 12 17:41:45 UTC 2018,,,,,,,,,,"0|i3yzb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/18 08:31;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22678;;;","09/Oct/18 08:31;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22678;;;","12/Oct/18 17:41;srowen;Issue resolved by pull request 22678
[https://github.com/apache/spark/pull/22678];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker images generated from dev build and from dist tarball are different,SPARK-25682,13190227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,08/Oct/18 21:16,17/May/20 18:25,13/Jul/23 08:48,18/Oct/18 17:22,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,Spark Core,,,0,,,,,"There's at least one difference I noticed, because of this line:

{noformat}
COPY examples /opt/spark/examples
{noformat}

In a dev build, ""examples"" contains your usual source code and maven-style directories, whereas in the dist version, it's this:

{code}
cp ""$SPARK_HOME""/examples/target/scala*/jars/* ""$DISTDIR/examples/jars""
{code}

So the path to the actual jar files ends up being different depending on how you built the image.",,apachespark,liyinan926,rvesse,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 18 17:22:00 UTC 2018,,,,,,,,,,"0|i3yytr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/18 22:11;liyinan926;That looks like to me the only difference. {{bin}}, {{sbin}}, and {{data}} are also hard-coded but they appear to be the same between the source and a distribution. Are you working on a fix? ;;;","09/Oct/18 22:51;vanzin;I was busy with other things but I can send a patch later today.;;;","09/Oct/18 22:53;liyinan926;Cool, thanks!;;;","10/Oct/18 00:02;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22681;;;","10/Oct/18 00:03;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22681;;;","18/Oct/18 17:22;vanzin;Issue resolved by pull request 22681
[https://github.com/apache/spark/pull/22681];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL execution listener shouldn't happen on execution thread,SPARK-25680,13190155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,08/Oct/18 17:20,17/Oct/18 08:09,13/Jul/23 08:48,17/Oct/18 08:09,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 17 08:09:19 UTC 2018,,,,,,,,,,"0|i3yyen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/18 17:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22674;;;","08/Oct/18 17:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22674;;;","08/Oct/18 18:00;dongjoon;Hi, [~cloud_fan] . Is this targeting both Spark 3.0 and 2.4?;;;","17/Oct/18 08:09;cloud_fan;Issue resolved by pull request 22674
[https://github.com/apache/spark/pull/22674];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuring zstd compression in JDBC throwing IllegalArgumentException Exception,SPARK-25677,13190003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivusondur@gmail.com,abhishek.akg,abhishek.akg,08/Oct/18 05:47,12/Dec/22 18:10,13/Jul/23 08:48,08/Oct/18 07:46,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"To check the Event Log compression size with different compression technique mentioned in Spark Doc

Set below parameter in spark-default.conf of JDBC and JobHistory
 1. spark.eventLog.compress=true
 2. Enable spark.io.compression.codec = org.apache.spark.io.ZstdCompressionCodec
 3. Restart the JDBC and Job History Services
 4. Check the JDBC and Job History Logs
 Exception throws
 ava.lang.IllegalArgumentException: No short name for codec org.apache.spark.io.ZstdCompressionCodec.
 at org.apache.spark.io.CompressionCodec$$anonfun$getShortName$2.apply(CompressionCodec.scala:94)
 at org.apache.spark.io.CompressionCodec$$anonfun$getShortName$2.apply(CompressionCodec.scala:94)
 at scala.Option.getOrElse(Option.scala:121)
 at org.apache.spark.io.CompressionCodec$.getShortName(CompressionCodec.scala:94)
 at org.apache.spark.SparkContext$$anonfun$9.apply(SparkContext.scala:414)
 at org.apache.spark.SparkContext$$anonfun$9.apply(SparkContext.scala:414)
 at scala.Option.map(Option.scala:146)
 at org.apache.spark.SparkContext.<init>(SparkContext.scala:414)
 at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2507)
 at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:939)
 at",,abhishek.akg,apachespark,shivusondur@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 08 07:46:07 UTC 2018,,,,,,,,,,"0|i3yxgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/18 05:48;shivusondur@gmail.com;i am working on this issue;;;","08/Oct/18 06:14;apachespark;User 'shivusondur' has created a pull request for this issue:
https://github.com/apache/spark/pull/22669;;;","08/Oct/18 06:14;apachespark;User 'shivusondur' has created a pull request for this issue:
https://github.com/apache/spark/pull/22669;;;","08/Oct/18 07:46;gurwls223;Issue resolved by pull request 22669
[https://github.com/apache/spark/pull/22669];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark Job History] Job UI page does not show pagination with one page,SPARK-25675,13189999,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivusondur@gmail.com,abhishek.akg,abhishek.akg,08/Oct/18 04:26,23/Oct/18 16:21,13/Jul/23 08:48,21/Oct/18 18:46,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,Web UI,,,,0,,,,,"1. set spark.ui.retainedJobs= 10000 in spark-default conf of spark Job History
 2. Restart Job History
 3. Submit Beeline jobs for 10000
 4. Launch Job History UI Page
 5. Select JDBC Running Application ID from Incomplete Application Page
 6. Launch Jo Page
 7. Pagination Panel display based on page size as below
 ----------------------------------------------------------------------------------------------------------------------------
 Completed Jobs XXX
 Page: 1 2 3 ....................... XX Page: Jump to 1 show 100 items in a page
 -----------------------------------------------------------------------------------------------------------------------------
 8. Change the value in Jump to 1 show *XXX* items in page, that is display all completed Jobs in a single page

*Actual Result:*
 All completed Jobs will be display in a Page but no Pagination panel so that User can modify and set the number of Jobs in a page.

*Expected Result:*
 It should display the Pagination panel as below
 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 Page: 1                                                             1 Page: Jump to 1 show *XXX* items in a page
 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 Pagination of page size *1* because it is displaying total number of completed Jobs in a single Page.",,abhishek.akg,apachespark,shivusondur@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 16:21:39 UTC 2018,,,,,,,,,,"0|i3yxfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/18 04:30;shivusondur@gmail.com;I am working on this issue;;;","08/Oct/18 04:40;apachespark;User 'shivusondur' has created a pull request for this issue:
https://github.com/apache/spark/pull/22668;;;","08/Oct/18 04:41;apachespark;User 'shivusondur' has created a pull request for this issue:
https://github.com/apache/spark/pull/22668;;;","23/Oct/18 16:20;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22808;;;","23/Oct/18 16:21;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22808;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If the records are incremented by more than 1 at a time,the number of bytes might rarely ever get updated",SPARK-25674,13189985,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,08/Oct/18 02:17,12/Nov/21 00:28,13/Jul/23 08:48,11/Oct/18 21:25,2.3.0,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,SQL,,,,0,,,,,"If the records are incremented by more than 1 at a time,the number of bytes might rarely ever get updated in `FileScanRDD.scala`，because it might skip over the count that is an exact multiple of UPDATE_INPUT_METRICS_INTERVAL_RECORDS.",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37268,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 15 17:53:53 UTC 2018,,,,,,,,,,"0|i3yxcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/18 02:22;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/22594;;;","11/Oct/18 21:25;srowen;Issue resolved by pull request 22594
[https://github.com/apache/spark/pull/22594];;;","15/Oct/18 17:53;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22731;;;","15/Oct/18 17:53;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22731;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build external/spark-ganglia-lgpl in Jenkins Test ,SPARK-25671,13189908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,06/Oct/18 17:14,06/Oct/18 22:50,13/Jul/23 08:48,06/Oct/18 22:50,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,,,,0,,,,,We should build external/spark-ganglia-lgpl in Jenkins Test when the source code of external/spark-ganglia-lgpl is changed. ,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 06 17:16:32 UTC 2018,,,,,,,,,,"0|i3ywvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/18 17:16;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22658;;;","06/Oct/18 17:16;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22658;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check CSV header only when it exists,SPARK-25669,13189890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,06/Oct/18 12:59,12/Dec/22 18:11,13/Jul/23 08:48,09/Oct/18 06:38,2.4.0,,,,,,,,,,,,,,,,,2.4.0,3.0.0,,,SQL,,,,0,,,,,"Currently, Spark checks the header in CSV files to fields names in provided or inferred schema. The check is bypassed if the header doesn't exists and CSV content is read from files. In the case, when input CSV comes as dataset of strings, Spark always compares the first row to the user specified or inferred schema. For example, parsing the following dataset:
{code:scala}
val input = Seq(""1,2"").toDS()
spark.read.option(""enforceSchema"", false).csv(input)
{code}
throws the exception:
{code:java}
java.lang.IllegalArgumentException: CSV header does not conform to the schema.
 Header: 1, 2
 Schema: _c0, _c1
Expected: _c0 but found: 1   
{code}

Need to prevent comparison of the first row (if it is not a header) to specific or inferred schema.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27873,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 09 06:38:11 UTC 2018,,,,,,,,,,"0|i3ywrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/18 13:11;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22656;;;","06/Oct/18 13:12;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22656;;;","09/Oct/18 06:38;gurwls223;Issue resolved by pull request 22656
[https://github.com/apache/spark/pull/22656];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible to use the backward slash as the CSV fields delimiter ,SPARK-25660,13189876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,06/Oct/18 10:03,12/Oct/18 19:05,13/Jul/23 08:48,12/Oct/18 19:05,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"If fields in CSV input are delimited by *'\'*, for example:
{code}
123\4\5\1\Q\\P\P\2321213\1\\\P\\F
{code}
reading it by the code:
{code:python}
df = spark.read.format('csv').option(""header"",""false"").options(delimiter='\\').load(""file:///file.csv"")
{code}
causes the exception:
{code}
String index out of range: 1
java.lang.StringIndexOutOfBoundsException: String index out of range: 1
	at java.lang.String.charAt(String.java:658)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.toChar(CSVUtils.scala:101)
	at org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:86)
	at org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:41)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:488)
{code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 06 10:16:56 UTC 2018,,,,,,,,,,"0|i3ywov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/18 10:16;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22654;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Pspark-ganglia-lgpl to the scala style check,SPARK-25655,13189816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,05/Oct/18 20:43,12/Dec/22 18:10,13/Jul/23 08:48,06/Oct/18 06:26,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Build,,,,0,,,,,"Our lint failed due to the following errors:
{code}
[INFO] --- scalastyle-maven-plugin:1.0.0:check (default) @ spark-ganglia-lgpl_2.11 ---
error file=/home/jenkins/workspace/spark-master-maven-snapshots/spark/external/spark-ganglia-lgpl/src/main/scala/org/apache/spark/metrics/sink/GangliaSink.scala message=
      Are you sure that you want to use toUpperCase or toLowerCase without the root locale? In most cases, you
      should use toUpperCase(Locale.ROOT) or toLowerCase(Locale.ROOT) instead.
      If you must use toUpperCase or toLowerCase without the root locale, wrap the code block with
      // scalastyle:off caselocale
      .toUpperCase
      .toLowerCase
      // scalastyle:on caselocale
     line=67 column=49
error file=/home/jenkins/workspace/spark-master-maven-snapshots/spark/external/spark-ganglia-lgpl/src/main/scala/org/apache/spark/metrics/sink/GangliaSink.scala message=
      Are you sure that you want to use toUpperCase or toLowerCase without the root locale? In most cases, you
      should use toUpperCase(Locale.ROOT) or toLowerCase(Locale.ROOT) instead.
      If you must use toUpperCase or toLowerCase without the root locale, wrap the code block with
      // scalastyle:off caselocale
      .toUpperCase
      .toLowerCase
      // scalastyle:on caselocale
     line=71 column=32
Saving to outputFile=/home/jenkins/workspace/spark-master-maven-snapshots/spark/external/spark-ganglia-lgpl/target/scalastyle-output.xml
{code}

See https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-lint/8890/
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 06 06:26:35 UTC 2018,,,,,,,,,,"0|i3ywbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/18 20:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22647;;;","05/Oct/18 20:47;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22647;;;","06/Oct/18 06:26;gurwls223;Issue resolved by pull request 22647
[https://github.com/apache/spark/pull/22647];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker-image-tool.sh doesn't work on developer build,SPARK-25646,13189599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,04/Oct/18 23:32,17/May/20 18:26,13/Jul/23 08:48,06/Oct/18 04:19,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"The image references ""kubernetes/tests"" which only exists in the tarball generated by make-distribution.sh. It should be parameterized like other paths that can change between dev build / dist.",,apachespark,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 06 04:19:53 UTC 2018,,,,,,,,,,"0|i3yuzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/18 23:37;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22634;;;","04/Oct/18 23:38;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22634;;;","06/Oct/18 04:19;dongjoon;This is resolved via https://github.com/apache/spark/pull/22634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix java foreachBatch API,SPARK-25644,13189548,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,04/Oct/18 20:14,06/Oct/18 04:11,13/Jul/23 08:48,05/Oct/18 17:59,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,,,,,"The java foreachBatch API in DataStreamWriter should accept java.lang.Long rather scala.Long. It's better to fix the new API before the release gets out, so I marked this ticket as a blocker.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 06 04:11:40 UTC 2018,,,,,,,,,,"0|i3yunz:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"04/Oct/18 20:17;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22633;;;","06/Oct/18 04:11;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22649;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-submit swallows the failure reason when there is an error connecting to master,SPARK-25636,13189307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,devaraj,devaraj,devaraj,03/Oct/18 21:22,29/Oct/18 00:58,13/Jul/23 08:48,10/Oct/18 16:29,2.3.2,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"{code:xml}
[apache-spark]$ ./bin/spark-submit --verbose --master spark://
....
Error: Exception thrown in awaitResult:
Run with --help for usage help or --verbose for debug output
{code}

When the spark submit cannot connect to master, there is no error shown. I think it should display the cause for the problem.
",,apachespark,devaraj,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 10 16:29:38 UTC 2018,,,,,,,,,,"0|i3yt6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/18 21:35;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/22623;;;","10/Oct/18 16:29;vanzin;Issue resolved by pull request 22623
[https://github.com/apache/spark/pull/22623];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkPlan.getByteArrayRdd should not consume the input when not necessary,SPARK-25602,13189150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Oct/18 12:29,04/Oct/18 12:17,13/Jul/23 08:48,04/Oct/18 12:17,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 04 12:17:26 UTC 2018,,,,,,,,,,"0|i3ys7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/18 13:54;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22621;;;","03/Oct/18 13:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22621;;;","04/Oct/18 12:17;cloud_fan;Issue resolved by pull request 22621
[https://github.com/apache/spark/pull/22621];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make use of TypeCoercion.findTightestCommonType while inferring CSV schema,SPARK-25600,13189097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,03/Oct/18 08:25,12/Dec/22 18:10,13/Jul/23 08:48,06/Oct/18 06:50,2.3.2,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,Current the CSV's infer schema code inlines {{TypeCoercion.findTightestCommonType}}. This is a minor refactor to make use of the common type coercion code when applicable. This way we can take advantage of any improvement to the base method.,,apachespark,dkbiswal,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 06 06:50:40 UTC 2018,,,,,,,,,,"0|i3yrvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/18 08:28;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22619;;;","06/Oct/18 06:50;gurwls223;Issue resolved by pull request 22619
[https://github.com/apache/spark/pull/22619];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark Accumulators with multiple PythonUDFs,SPARK-25591,13188754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,AbdealiJK,AbdealiJK,02/Oct/18 04:46,12/Dec/22 18:10,13/Jul/23 08:48,08/Oct/18 07:39,2.1.3,2.2.2,2.3.2,,,,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,PySpark,,,,0,correctness,,,,"When having multiple Python UDFs - the last Python UDF's accumulator is the only accumulator that gets updated.


{code:python}
import pyspark
from pyspark.sql import SparkSession, Row
from pyspark.sql import functions as F
from pyspark.sql import types as T

from pyspark import AccumulatorParam

spark = SparkSession.builder.getOrCreate()
spark.sparkContext.setLogLevel(""ERROR"")
test_accum = spark.sparkContext.accumulator(0.0)

SHUFFLE = False

def main(data):
    print("">>> Check0"", test_accum.value)
    def test(x):
        global test_accum
        test_accum += 1.0
        return x

    print("">>> Check1"", test_accum.value)

    def test2(x):
        global test_accum
        test_accum += 100.0
        return x

    print("">>> Check2"", test_accum.value)
    func_udf = F.udf(test, T.DoubleType())
    print("">>> Check3"", test_accum.value)
    func_udf2 = F.udf(test2, T.DoubleType())
    print("">>> Check4"", test_accum.value)

    data = data.withColumn(""out1"", func_udf(data[""a""]))
    if SHUFFLE:
        data = data.repartition(2)
    print("">>> Check5"", test_accum.value)
    data = data.withColumn(""out2"", func_udf2(data[""b""]))
    if SHUFFLE:
        data = data.repartition(2)
    print("">>> Check6"", test_accum.value)

    data.show()  # ACTION
    print("">>> Check7"", test_accum.value)
    return data


df = spark.createDataFrame([
    [1.0, 2.0]
], schema=T.StructType([T.StructField(field_name, T.DoubleType(), True) for field_name in [""a"", ""b""]]))

df2 = main(df)
{code}


{code:python}
######## Output 1 - with SHUFFLE=False
...
# >>> Check7 100.0


######## Output 2 - with SHUFFLE=True
...
# >>> Check7 101.0
{code}

Basically looks like:
 - Accumulator works only for last UDF before a shuffle-like operation",,AbdealiJK,apachespark,bryanc,dongjoon,smilegator,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 01:35:23 UTC 2019,,,,,,,,,,"0|i3yps7:",9223372036854775807,,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,,,,,,,"04/Oct/18 23:50;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22635;;;","04/Oct/18 23:51;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22635;;;","08/Oct/18 07:39;gurwls223;Fixed in https://github.com/apache/spark/pull/22635;;;","08/Oct/18 21:25;smilegator;RC3 is not out yet. Thus, RC3 will include the fix.;;;","03/Jan/19 01:01;dongjoon;Hi, [~viirya], [~hyukjin.kwon].

This is only in branch-2.4. Can we backport this to older branches like branch-2.3 and branch-2.2?

cc [~AbdealiJK];;;","03/Jan/19 01:28;viirya;This is bug fixing, so I think it makes sense to backport this to branch-2.3 and 2.2 if needed.;;;","03/Jan/19 01:29;viirya;I can make backport PRs if you need. [~dongjoon];;;","03/Jan/19 01:33;dongjoon;Thank you for confirming, [~viirya]. Yes. Please make two PRs for them.;;;","03/Jan/19 01:35;gurwls223;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toString method of GeneralizedLinearRegressionTrainingSummary runs in infinite loop throwing StackOverflowError,SPARK-25586,13188703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ankur.gupta,ankur.gupta,ankur.gupta,01/Oct/18 21:05,03/Oct/18 23:20,13/Jul/23 08:48,03/Oct/18 23:19,2.3.0,,,,,,,,,,,,,,,,,3.0.0,,,,MLlib,Spark Core,,,0,,,,,"After the change in SPARK-25118, which enables spark-shell to run with default log level, test_glr_summary started failing with StackOverflow error.

Cause: ClosureCleaner calls logDebug on various objects and when it is called for GeneralizedLinearRegressionTrainingSummary, it starts a spark job which runs into infinite loop and fails with the below exception.

{code}
======================================================================
ERROR: test_glr_summary (pyspark.ml.tests.TrainingSummaryTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/ml/tests.py"", line 1809, in test_glr_summary
    self.assertTrue(isinstance(s.aic, float))
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/ml/regression.py"", line 1781, in aic
    return self._call_java(""aic"")
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/ml/wrapper.py"", line 55, in _call_java
    return _java2py(sc, m(*java_args))
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
Py4JJavaError: An error occurred while calling o31639.aic.
: java.lang.StackOverflowError
	at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
	at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242)
	at java.io.File.exists(File.java:819)
	at sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1245)
	at sun.misc.URLClassPath$FileLoader.findResource(URLClassPath.java:1212)
	at sun.misc.URLClassPath.findResource(URLClassPath.java:188)
	at java.net.URLClassLoader$2.run(URLClassLoader.java:569)
	at java.net.URLClassLoader$2.run(URLClassLoader.java:567)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findResource(URLClassLoader.java:566)
	at java.lang.ClassLoader.getResource(ClassLoader.java:1093)
	at java.net.URLClassLoader.getResourceAsStream(URLClassLoader.java:232)
	at java.lang.Class.getResourceAsStream(Class.java:2223)
	at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:43)
	at org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses(ClosureCleaner.scala:87)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:269)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2342)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:864)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:863)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:364)
	at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:863)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:613)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3038)
	at org.apache.spark.sql.Dataset.rdd(Dataset.scala:3036)
	at org.apache.spark.ml.regression.GeneralizedLinearRegressionSummary.nullDeviance$lzycompute(GeneralizedLinearRegression.scala:1342)
	at org.apache.spark.ml.regression.GeneralizedLinearRegressionSummary.nullDeviance(GeneralizedLinearRegression.scala:1315)
	at org.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary.toString(GeneralizedLinearRegression.scala:1556)
	at java.lang.String.valueOf(String.java:2994)
	at java.lang.StringBuilder.append(StringBuilder.java:131)
	at scala.StringContext.standardInterpolator(StringContext.scala:125)
	at scala.StringContext.s(StringContext.scala:95)
	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12$$anonfun$apply$6.apply(ClosureCleaner.scala:289)
	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12$$anonfun$apply$6.apply(ClosureCleaner.scala:289)
{code}",,ankur.gupta,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 03 23:20:15 UTC 2018,,,,,,,,,,"0|i3ypgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/18 21:16;apachespark;User 'ankuriitg' has created a pull request for this issue:
https://github.com/apache/spark/pull/22604;;;","01/Oct/18 22:56;srowen;This is not a bug; SPARK-25118 is not committed. This is an improvement that might work around a problem in the proposed implementation of that issue.;;;","02/Oct/18 18:52;apachespark;User 'ankuriitg' has created a pull request for this issue:
https://github.com/apache/spark/pull/22616;;;","02/Oct/18 18:52;apachespark;User 'ankuriitg' has created a pull request for this issue:
https://github.com/apache/spark/pull/22616;;;","03/Oct/18 23:19;vanzin;Issue resolved by pull request 22616
[https://github.com/apache/spark/pull/22616];;;","03/Oct/18 23:20;vanzin;bq. This is not a bug

Actually it's a bug if you set your log level to DEBUG and happen to be using that class... regardless of the other change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use quoted attribute names if needed in pushed ORC predicates,SPARK-25579,13188444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,01/Oct/18 04:32,12/Dec/22 18:10,13/Jul/23 08:48,16/Oct/18 12:31,2.4.0,,,,,,,,,,,,,,,,,2.4.0,3.0.0,,,SQL,,,,0,,,,,"This issue aims to fix an ORC performance regression at Spark 2.4.0 RCs from Spark 2.3.2. For column names with `.`, the pushed predicates are ignored.

*Test Data*
{code:java}
scala> val df = spark.range(Int.MaxValue).sample(0.2).toDF(""col.with.dot"")
scala> df.write.mode(""overwrite"").orc(""/tmp/orc"")
{code}
*Spark 2.3.2*
{code:java}
scala> spark.sql(""set spark.sql.orc.impl=native"")
scala> spark.sql(""set spark.sql.orc.filterPushdown=true"")
scala> spark.time(spark.read.orc(""/tmp/orc"").where(""`col.with.dot` < 10"").show)
+------------+
|col.with.dot|
+------------+
|           1|
|           8|
+------------+

Time taken: 1486 ms

scala> spark.time(spark.read.orc(""/tmp/orc"").where(""`col.with.dot` < 10"").show)
+------------+
|col.with.dot|
+------------+
|           1|
|           8|
+------------+

Time taken: 163 ms
{code}
*Spark 2.4.0 RC2*
{code:java}
scala> spark.time(spark.read.orc(""/tmp/orc"").where(""`col.with.dot` < 10"").show)
+------------+
|col.with.dot|
+------------+
|           1|
|           8|
+------------+

Time taken: 4087 ms

scala> spark.time(spark.read.orc(""/tmp/orc"").where(""`col.with.dot` < 10"").show)
+------------+
|col.with.dot|
+------------+
|           1|
|           8|
+------------+

Time taken: 1998 ms
{code}",,apachespark,dongjoon,Steven Rand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 16 12:31:10 UTC 2018,,,,,,,,,,"0|i3ynvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/18 04:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22597;;;","01/Oct/18 04:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22597;;;","16/Oct/18 12:31;gurwls223;Issue resolved by pull request 22597
[https://github.com/apache/spark/pull/22597];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update to Scala 2.12.7,SPARK-25578,13188423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,30/Sep/18 20:53,04/Dec/18 13:54,13/Jul/23 08:48,02/Oct/18 02:35,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,Spark Core,SQL,,0,,,,,"We should use Scala 2.12.7 over 2.12.6 now, to pick up this fix. We ought to be able to back out a workaround in Spark if so.

[https://github.com/scala/scala/releases/tag/v2.12.7]

[https://github.com/scala/scala/pull/7156] ",,apachespark,dongjoon,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26266,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 02 02:35:44 UTC 2018,,,,,,,,,,"0|i3ynqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/18 13:02;srowen;[~sadhen] I got this failure from {{QueryTest}} with Scala 2.12.7, after removing what I believed was your workaround for this Scala issue:
{code:java}
  == Results ==

  !== Correct Answer - 1 ==             == Spark Answer - 1 ==

  !struct<array(100, 100):array<int>>   struct<percentile_approx(100.0, array(0.9, 0.9), 10000):array<double>>

  ![WrappedArray(100, 100)]             [WrappedArray(100.0, 100.0)] (QueryTest.scala:163){code}
Is that something we expected to be fixed, or maybe I misunderstood?

It passes with 2.12.7, without undoing your change.;;;","01/Oct/18 14:46;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22600;;;","01/Oct/18 19:06;dongjoon;[~srowen]. Could you update the `Type` and `Priority` because we don't allow `Minor` and `Improvement` JIRA lands on `branch-2.4`?;;;","01/Oct/18 19:45;srowen;OK. I'm proposing it for 2.4.0 mostly because it _might_ be a bug fix, given [~sadhen]'s comment. As far as I can tell it's not breaking 2.4, but, according to the Scala release the change was to support 2.4. Maybe that overstated things, but might be less confusing to get this one in if we have another RC.;;;","02/Oct/18 02:35;srowen;Issue resolved by pull request 22600
[https://github.com/apache/spark/pull/22600];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace 2.3.1 with 2.3.2 in HiveExternalCatalogVersionsSuite,SPARK-25570,13188263,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,29/Sep/18 00:10,12/Dec/22 18:10,13/Jul/23 08:48,29/Sep/18 03:45,2.3.3,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,SQL,Tests,,,0,,,,,"This issue aims to prevent test slowdowns at HiveExternalCatalogVersionsSuite by using the latest Spark 2.3.2 because the Apache mirror will remove the old Spark 2.3.1 eventually. HiveExternalCatalogVersionsSuite will not fail because SPARK-24813 implements a fallback logic, but it causes many trials in all builds over `branch-2.3/branch-2.4/master`. We had better fix this issue.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 29 03:45:11 UTC 2018,,,,,,,,,,"0|i3ymr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/18 00:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22587;;;","29/Sep/18 00:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22587;;;","29/Sep/18 03:45;gurwls223;Issue resolved by pull request 22587
[https://github.com/apache/spark/pull/22587];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Continue to update the remaining accumulators when failing to update one accumulator,SPARK-25568,13188241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,28/Sep/18 20:51,30/Sep/18 01:23,13/Jul/23 08:48,30/Sep/18 01:23,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,Spark Core,,,,0,,,,,"Currently when failing to update an accumulator, DAGScheduler.updateAccumulators will skip the remaining accumulators. We should try to update the remaining accumulators if possible so that they can still report correct values.
",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 28 20:55:59 UTC 2018,,,,,,,,,,"0|i3ymm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/18 20:55;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark Job History] Table listing in SQL Tab not display Sort Icon,SPARK-25567,13188134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,abhishek.akg,abhishek.akg,28/Sep/18 11:15,02/Aug/19 02:42,13/Jul/23 08:48,12/Oct/18 17:37,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,Web UI,,,,0,,,,,"1. spark.sql.ui.retainedExecutions = 20000
2. Run Beeline Jobs
3. Open SQL Tab will list SQL Queries in table
4. ID column header does not display Sort Icon compare to other UI Tabs like Job Id in  Jobs
5. Id user clicks the Column Header Sorting is happening. 
Expected Result:
User should be provided with Sort Icon like other UI tab.",,abhishek.akg,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28599,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 12 17:37:38 UTC 2018,,,,,,,,,,"0|i3ylzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/18 11:16;shahid;Thanks. I will raise a PR;;;","05/Oct/18 17:48;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22645;;;","05/Oct/18 17:48;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22645;;;","05/Oct/18 17:48;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22645;;;","12/Oct/18 17:37;srowen;Issue resolved by pull request 22645
[https://github.com/apache/spark/pull/22645];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDDInfo uses SparkEnv before it may have been initialized,SPARK-25546,13187685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,26/Sep/18 18:10,27/Sep/18 16:27,13/Jul/23 08:48,27/Sep/18 16:27,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,Tests,,,0,,,,,"This code:

{code}
private[spark] object RDDInfo {
  private val callsiteLongForm = SparkEnv.get.conf.get(EVENT_LOG_CALLSITE_LONG_FORM)
{code}

Has two problems:
- it keeps that value across different SparkEnv instances. So e.g. if you have two tests that rely on different values for that config, one of them will break.
- it assumes tests always initialize a SparkEnv. e.g. if you run ""core/testOnly *.AppStatusListenerSuite"", it will fail because {{SparkEnv.get}} returns null.
",,apachespark,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23820,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 16:27:43 UTC 2018,,,,,,,,,,"0|i3yj7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 18:20;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22558;;;","27/Sep/18 16:27;dongjoon;Issue resolved by pull request 22558
[https://github.com/apache/spark/pull/22558];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Confusing log messages at DEBUG level, in K8s mode.",SPARK-25543,13187553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,prashant,prashant,prashant,26/Sep/18 09:55,17/May/20 18:26,13/Jul/23 08:48,30/Sep/18 21:29,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"Steps to reproduce.

Start spark shell by providing a K8s master. Then turn the debug log on, 
{code}
scala> sc.setLogLevel(""DEBUG"")
{code}

{code}
sc.setLogLevel(""DEBUG"")

scala> 2018-09-26 09:33:54 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:33:55 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:33:56 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:33:56 DEBUG ExecutorPodsPollingSnapshotSource:58 - Resynchronizing full executor pod state from Kubernetes.
2018-09-26 09:33:57 DEBUG ExecutorPodsAllocator:58 - Currently have 1 running executors and 0 pending executors. Map() executors have been requested but are pending appearance in the cluster.
2018-09-26 09:33:57 DEBUG ExecutorPodsAllocator:58 - Current number of running executors is equal to the number of requested executors. Not scaling up further.
2018-09-26 09:33:57 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:33:58 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:33:59 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:00 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:01 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:02 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:03 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:04 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:05 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from Spark that were either found to be deleted or non-existent in the cluster.
2018-09-26 09:34:06 DEBUG ExecutorPodsLifecycleManager:58 - Removed executors with ids  from ...
{code}

The fix is easy, first check if there are any removed executors, before producing the log message.",,apachespark,dongjoon,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 30 21:29:53 UTC 2018,,,,,,,,,,"0|i3yief:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/18 07:07;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/22565;;;","30/Sep/18 21:29;dongjoon;Issue resolved by pull request 22565
[https://github.com/apache/spark/pull/22565];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: OpenHashMapSuite,SPARK-25542,13187514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,dongjoon,dongjoon,26/Sep/18 06:43,24/Oct/18 18:12,13/Jul/23 08:48,28/Sep/18 21:33,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,Tests,,,0,,,,,"- [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/96585/testReport/org.apache.spark.util.collection/OpenHashMapSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/] (Sep 25, 2018 5:52:56 PM)
{code:java}
org.apache.spark.util.collection.OpenHashMapSuite.(It is not a test it is a sbt.testing.SuiteSelector)

Failing for the past 1 build (Since #96585 )
Took 0 ms.
Error Message
sbt.ForkMain$ForkError: java.lang.OutOfMemoryError: Java heap space
Stacktrace
sbt.ForkMain$ForkError: sbt.ForkMain$ForkError: java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
	at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
	at org.apache.spark.util.collection.OpenHashMap$$anonfun$1.apply$mcVI$sp(OpenHashMap.scala:159)
	at org.apache.spark.util.collection.OpenHashSet.rehash(OpenHashSet.scala:234)
	at org.apache.spark.util.collection.OpenHashSet.rehashIfNeeded(OpenHashSet.scala:171)
	at org.apache.spark.util.collection.OpenHashMap$mcI$sp.update$mcI$sp(OpenHashMap.scala:86)
	at org.apache.spark.util.collection.OpenHashMapSuite$$anonfun$17$$anonfun$apply$4.apply$mcVI$sp(OpenHashMapSuite.scala:192)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.collection.OpenHashMapSuite$$anonfun$17.apply(OpenHashMapSuite.scala:191)
	at org.apache.spark.util.collection.OpenHashMapSuite$$anonfun$17.apply(OpenHashMapSuite.scala:188)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
{code}",,apachespark,dongjoon,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 28 21:35:09 UTC 2018,,,,,,,,,,"0|i3yi5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/18 10:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22569;;;","27/Sep/18 10:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22569;;;","28/Sep/18 21:33;dongjoon;Resolved via https://github.com/apache/spark/pull/22569;;;","28/Sep/18 21:35;dongjoon;I marked this as 2.4.1 because we are in the middle of RC2 vote. cc [~cloud_fan];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CaseInsensitiveMap should be serializable after '-' operator,SPARK-25541,13187513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,26/Sep/18 06:38,02/Oct/18 15:51,13/Jul/23 08:48,26/Sep/18 11:42,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 02:58:50 UTC 2018,,,,,,,,,,"0|i3yi5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 06:49;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22553;;;","27/Sep/18 02:58;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22562;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make HiveContext in PySpark behave as the same as Scala.,SPARK-25540,13187512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,26/Sep/18 06:32,02/Oct/18 15:51,13/Jul/23 08:48,27/Sep/18 01:52,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,SQL,,,0,,,,,"In Scala, {{HiveContext}} sets a config {{spark.sql.catalogImplementation}} of the given {{SparkContext}} and then passes to {{SparkSession.builder}}.
The {{HiveContext}} in PySpark should behave as the same as it in Scala.",,apachespark,cloud_fan,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 01:52:40 UTC 2018,,,,,,,,,,"0|i3yi5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 06:47;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22552;;;","26/Sep/18 06:48;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22552;;;","27/Sep/18 01:52;cloud_fan;Issue resolved by pull request 22552
[https://github.com/apache/spark/pull/22552];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect row counts after distinct(),SPARK-25538,13187505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,Steven Rand,Steven Rand,26/Sep/18 05:53,20/Aug/20 15:39,13/Jul/23 08:48,03/Oct/18 14:29,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,1,correctness,,,,"It appears that {{df.distinct.count}} can return incorrect values after SPARK-23713. It's possible that other operations are affected as well; {{distinct}} just happens to be the one that we noticed. I believe that this issue was introduced by SPARK-23713 because I can't reproduce it until that commit, and I've been able to reproduce it after that commit as well as with {{tags/v2.4.0-rc1}}. 

Below are example spark-shell sessions to illustrate the problem. Unfortunately the data used in these examples can't be uploaded to this Jira ticket. I'll try to create test data which also reproduces the issue, and will upload that if I'm able to do so.

Example from Spark 2.3.1, which behaves correctly:

{code}
scala> val df = spark.read.parquet(""hdfs:///data"")
df: org.apache.spark.sql.DataFrame = [<redacted>]

scala> df.count
res0: Long = 123

scala> df.distinct.count
res1: Long = 115
{code}

Example from Spark 2.4.0-rc1, which returns different output:

{code}
scala> val df = spark.read.parquet(""hdfs:///data"")
df: org.apache.spark.sql.DataFrame = [<redacted>]

scala> df.count
res0: Long = 123

scala> df.distinct.count
res1: Long = 116

scala> df.sort(""col_0"").distinct.count
res2: Long = 123

scala> df.withColumnRenamed(""col_0"", ""newName"").distinct.count
res3: Long = 115
{code}",Reproduced on a Centos7 VM and from source in Intellij on OS X.,apachespark,cloud_fan,dongjoon,kiszk,maropu,mgaido,smilegator,Steven Rand,tgraves,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23713,,,,,,"30/Sep/18 05:17;Steven Rand;SPARK-25538-repro.tgz;https://issues.apache.org/jira/secure/attachment/12941861/SPARK-25538-repro.tgz",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 03 23:47:15 UTC 2018,,,,,,,,,,"0|i3yi3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 07:44;mgaido;Please do not use Blocker and Critical when reporting issues as they are reserved for committer. Though, I agree this should be a blocker for 2.4.0 as it is a correctness issue. cc [~cloud_fan];;;","26/Sep/18 08:06;cloud_fan;Hi [~Steven Rand], is it possible to upload the data file? We need to be able to reproduce it in order to debug it, thanks! I'll set it as a blocker once I confirm this is a correctness issue.;;;","26/Sep/18 08:58;Steven Rand;Hi [~cloud_fan], I'm still trying to create a scrubbed version of the file. It's proving to be difficult since mutating the original DataFrame often prevents the issue from reproducing (e.g., the example in the description where calling {{withColumnRenamed}} before {{distinct.count}} leads to the correct result being printed).;;;","26/Sep/18 13:12;cloud_fan;cc [~kiszk] as well;;;","26/Sep/18 19:04;kiszk;Hi [~Steven Rand], would it be possible to share the schema of this DataFrame?
;;;","27/Sep/18 00:17;Steven Rand;[~kiszk], yes, the schema is:

 
{code}
scala> spark.read.parquet(""hdfs:///data"").printSchema
root
 |-- col_0: string (nullable = true)
 |-- col_1: timestamp (nullable = true)
 |-- col_2: string (nullable = true)
 |-- col_3: timestamp (nullable = true)
 |-- col_4: string (nullable = true)
 |-- col_5: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_6: string (nullable = true)
 |-- col_7: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_8: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_9: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_10: string (nullable = true)
 |-- col_11: timestamp (nullable = true)
 |-- col_12: integer (nullable = true)
 |-- col_13: boolean (nullable = true)
 |-- col_14: decimal(38,18) (nullable = true)
 |-- col_15: long (nullable = true)
 |-- col_16: string (nullable = true)
 |-- col_17: integer (nullable = true)
 |-- col_18: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_19: string (nullable = true)
 |-- col_20: string (nullable = true)
 |-- col_21: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_22: string (nullable = true)
 |-- col_23: array (nullable = true)
 |    |-- element: timestamp (containsNull = true)
 |-- col_24: string (nullable = true)
 |-- col_25: string (nullable = true)
 |-- col_26: string (nullable = true)
 |-- col_27: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_28: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_29: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_30: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_31: decimal(38,18) (nullable = true)
 |-- col_32: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_33: string (nullable = true)
 |-- col_34: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_35: decimal(38,18) (nullable = true)
 |-- col_36: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_37: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_38: decimal(38,18) (nullable = true)
 |-- col_39: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_40: string (nullable = true)
 |-- col_41: string (nullable = true)
 |-- col_42: string (nullable = true)
 |-- col_43: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_44: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_45: string (nullable = true)
 |-- col_46: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_47: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_48: string (nullable = true)
 |-- col_49: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_50: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_51: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- col_52: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
 |-- col_53: string (nullable = true)
 |-- col_54: decimal(38,18) (nullable = true)
 |-- col_55: decimal(38,18) (nullable = true)
 |-- col_56: decimal(38,18) (nullable = true)
 |-- col_57: array (nullable = true)
 |    |-- element: decimal(38,18) (containsNull = true)
{code};;;","28/Sep/18 09:29;kiszk;Thank for upload a schema. While I looked at the schema, I am still not sure about the reason of this problem.
I would appreciate it if you could find a good input data that can reproduce a problem.;;;","29/Sep/18 02:26;Steven Rand;[~kiszk] that makes sense, I'll try to do so. The issue I've been having so far is that when I run the UDF I've written to change the data (while preserving number of duplicate rows), the resulting DataFrame doesn't reproduce the issue.;;;","30/Sep/18 05:56;Steven Rand;[~kiszk] I've uploaded a tarball containing parquet files that reproduce the issue but don't contain any of the values in the original dataset. Specifically, some columns have been dropped, all strings have been changed to ""test_string"", all values in col_50 have been changed to 0.0043, and the values in col_14 have all been mapped from their original values to values between 0.001 and 0.0044.

This new DataFrame still reproduces issues similar to those in the description:
{code:java}
scala> df.distinct.count
res3: Long = 64

scala> df.sort(""col_0"").distinct.count
res4: Long = 73

scala> df.withColumnRenamed(""col_0"", ""new"").distinct.count
res5: Long = 63
{code}
I get those inconsistent/wrong results on {{2.4.0-rc2}} and if I check out commit {{a7c19d9c21d59fd0109a7078c80b33d3da03fafd}}, which is SPARK-23713. If I check out the commit immediately before, which is {{fe2b7a4568d65a62da6e6eb00fff05f248b4332c}}, then all three commands return 63.

cc [~cloud_fan] – IMO this should block the 2.4.0 release.;;;","01/Oct/18 01:52;kiszk;Thank you. I will check it tonight in Japan.;;;","01/Oct/18 14:41;mgaido;I was able to reproduce also using limit instead of sort:
{code}
scala> df.limit(80).distinct.count
res83: Long = 72

scala> df.distinct.count
res84: Long = 64

scala> df.limit(20).distinct.count
res88: Long = 20

scala> df.limit(20).distinct.collect.distinct.length
res89: Int = 17
{code};;;","01/Oct/18 17:13;kiszk;This test case does not print {{63}} using master branch.

{code}
  test(""test2"") {
    val df = spark.read.parquet(""file:///SPARK-25538-repro"")
    val c1 = df.distinct.count
    val c2 = df.sort(""col_0"").distinct.count
    val c3 = df.withColumnRenamed(""col_0"", ""new"").distinct.count
    val c0 = df.count
    print(s""c1=$c1, c2=$c2, c3=$c3, c0=$c0\n"")
  }

c1=64, c2=73, c3=64, c0=123
{code};;;","01/Oct/18 18:18;dongjoon;[~mgaido]'s PR, https://github.com/apache/spark/pull/22602, fixes the decimal issue and looks reasonable.;;;","01/Oct/18 19:45;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22602;;;","01/Oct/18 19:46;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22602;;;","03/Oct/18 14:29;dongjoon;Issue resolved by pull request 22602
[https://github.com/apache/spark/pull/22602];;;","03/Oct/18 23:47;Steven Rand;Thanks all!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executorSource.METRIC read wrong record in Executor.scala Line444,SPARK-25536,13187501,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,Unkrible,Unkrible,26/Sep/18 03:51,27/Sep/18 04:25,13/Jul/23 08:48,27/Sep/18 04:23,2.3.0,2.3.1,2.3.2,,,,,,,,,,,,,,,2.3.3,2.4.0,,,Spark Core,,,,0,,,,,,,apachespark,dongjoon,shahid,Unkrible,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22190,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 04:25:16 UTC 2018,,,,,,,,,,"0|i3yi2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 05:30;shahid;Thanks. I will raise a pr;;;","26/Sep/18 07:16;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22555;;;","27/Sep/18 04:25;dongjoon;Issue resolved by pull request 22555
[https://github.com/apache/spark/pull/22555];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Work around bad error checking in commons-crypto,SPARK-25535,13187471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,26/Sep/18 00:01,27/Apr/19 06:33,13/Jul/23 08:48,09/Oct/18 14:28,2.2.2,,,,,,,,,,,,,,,,,2.4.3,3.0.0,,,Spark Core,,,,0,,,,,"The commons-crypto library used for encryption can get confused when certain errors happen; that can lead to crashes since the Java side thinks the ciphers are still valid while the native side has already cleaned up the ciphers.

We can work around that in Spark by doing some error checking at a higher level.",,apachespark,dongjoon,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 09 14:28:05 UTC 2018,,,,,,,,,,"0|i3yhw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 00:13;vanzin;I filed CRYPTO-141 for the commons-crypto fix. But in some parts of Spark we can avoid that issue, so let's do that to avoid requiring a new release of that library.;;;","26/Sep/18 17:29;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22557;;;","09/Oct/18 14:28;irashid;Issue resolved by pull request 22557
[https://github.com/apache/spark/pull/22557];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make `SQLHelper` trait,SPARK-25534,13187443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Sep/18 21:30,03/Oct/18 11:24,13/Jul/23 08:48,26/Sep/18 06:04,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Currently, Spark has 7 `withTempPath` and 6 `withSQLConf` functions. This PR aims to remove duplicated and inconsistent code and reduce them to the following meaningful implementations.

*withTempPath*
- `SQLHelper.withTempPath`: The one which was used in `SQLTestUtils`.

*withSQLConf*
- `SQLHelper.withSQLConf`: The one which was used in `PlanTest`.
- `ExecutorSideSQLConfSuite.withSQLConf`: The one which doesn't throw `AnalysisException` on StaticConf changes.
- `SQLTestUtils.withSQLConf`: The one which overrides intentionally to change the active session.
{code}
  protected override def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {
    SparkSession.setActiveSession(spark)
    super.withSQLConf(pairs: _*)(f)
  }
{code}",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 06:04:34 UTC 2018,,,,,,,,,,"0|i3yhpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 21:33;dongjoon;I'll make a PR soon.;;;","25/Sep/18 21:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22548;;;","26/Sep/18 06:04;dongjoon;Issue resolved by pull request 22548
[https://github.com/apache/spark/pull/22548];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Inconsistent message for Completed Jobs in the  JobUI, when there are failed jobs, compared to spark2.2",SPARK-25533,13187419,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,shahid,shahid,25/Sep/18 19:19,27/Sep/18 17:38,13/Jul/23 08:48,27/Sep/18 17:38,2.3.1,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,Web UI,,,,0,,,,,"Test steps:

 1) bin/spark-shell
{code:java}
sc.parallelize(1 to 5, 5).collect()
sc.parallelize(1 to 5, 2).map{ x => throw new RuntimeException(""Fail Job"")}.collect()
{code}
*Output in spark - 2.3.1:*

!Screenshot from 2018-09-26 00-42-00.png!

*Output in spark - 2.2.1:*

!Screenshot from 2018-09-26 00-46-35.png!",,apachespark,shahid,vanzin,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 19:21;shahid;Screenshot from 2018-09-26 00-42-00.png;https://issues.apache.org/jira/secure/attachment/12941273/Screenshot+from+2018-09-26+00-42-00.png","25/Sep/18 19:21;shahid;Screenshot from 2018-09-26 00-46-35.png;https://issues.apache.org/jira/secure/attachment/12941272/Screenshot+from+2018-09-26+00-46-35.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 18:32:19 UTC 2018,,,,,,,,,,"0|i3yhkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 19:20;shahid;I am working on it.;;;","25/Sep/18 22:01;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22549;;;","26/Sep/18 18:32;vanzin;This is merged to master. I'll backport it to 2.4 and 2.3 after I fix an unrelated issue that I ran into during testing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not update conf for existing SparkContext in SparkSession.getOrCreate.,SPARK-25525,13187268,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,25/Sep/18 11:30,12/Dec/22 18:11,13/Jul/23 08:48,27/Sep/18 04:37,2.3.0,2.4.0,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,SQL,,,0,,,,,"In SPARK-20946, we modified {{SparkSession.getOrCreate}} to not update conf for existing {{SparkContext}} because {{SparkContext}} is shared by all sessions.
We should not update it in PySpark side as well.",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 10 06:45:29 UTC 2018,,,,,,,,,,"0|i3ygnb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 11:38;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22545;;;","27/Sep/18 04:37;gurwls223;Issue resolved by pull request 22545
[https://github.com/apache/spark/pull/22545];;;","10/Oct/18 06:44;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22682;;;","10/Oct/18 06:44;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22682;;;","10/Oct/18 06:45;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22682;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Improve type promotion for input arguments of elementAt function,SPARK-25522,13187211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,25/Sep/18 08:30,27/Sep/18 11:50,13/Jul/23 08:48,27/Sep/18 07:05,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In ElementAt, when first argument is MapType, we should coerce the key type and the second argument based on findTightestCommonType. This is not happening currently.

Also, when the first argument is ArrayType, the second argument should be an integer type or a smaller integral type that can be safely casted to an integer type. Currently we may do an unsafe cast.
{code:java}
spark-sql> select element_at(array(1,2), 1.24);

1{code}
{code:java}
spark-sql> select element_at(map(1,""one"", 2, ""two""), 2.2);

two{code}",,apachespark,cloud_fan,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 07:05:19 UTC 2018,,,,,,,,,,"0|i3ygan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 08:59;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22544;;;","27/Sep/18 07:05;cloud_fan;Issue resolved by pull request 22544
[https://github.com/apache/spark/pull/22544];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job id showing null when insert into command Job is finished.,SPARK-25521,13187192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,S71955,Bjangir,Bjangir,25/Sep/18 06:27,05/Oct/18 08:54,13/Jul/23 08:48,05/Oct/18 08:54,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,SQL,,,0,,,,,"scala> spark.sql(""create table x1(name string,age int) stored as parquet"")
 scala> spark.sql(""insert into x1 select 'a',29"")
 check logs
 2018-08-19 12:45:36 INFO TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 874 ms on localhost (executor
 driver) (1/1)
 2018-08-19 12:45:36 INFO TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool
 2018-08-19 12:45:36 INFO DAGScheduler:54 - ResultStage 0 (sql at <console>:24) finished in 1.131 s
 2018-08-19 12:45:36 INFO DAGScheduler:54 - Job 0 finished: sql at <console>:24, took 1.233329 s
 2018-08-19 12:45:36 INFO FileFormatWriter:54 - Job {color:#d04437}null{color} committed.
 2018-08-19 12:45:36 INFO FileFormatWriter:54 - Finished processing stats for job null.
 res4: org.apache.spark.sql.DataFrame = []

 

  !image-2018-09-25-12-01-31-871.png!

 

 ",,apachespark,Bjangir,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 06:29;Bjangir;image-2018-09-25-12-01-31-871.png;https://issues.apache.org/jira/secure/attachment/12941173/image-2018-09-25-12-01-31-871.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 17:59:22 UTC 2018,,,,,,,,,,"0|i3yg6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/18 06:35;S71955;[~Bjangir] I could see the jobcontext doesn't have jobID when the flow hits FileFormatWriter.scala in the insert flow. Moreover this issue is happening in insert flow. I will check into this issue more and raise a PR for handling the same. Thanks for reporting.;;;","27/Sep/18 17:59;apachespark;User 'sujith71955' has created a pull request for this issue:
https://github.com/apache/spark/pull/22572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayRemove function may return incorrect result when right expression is implicitly downcasted.,SPARK-25519,13187135,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,24/Sep/18 21:17,25/Sep/18 04:06,13/Jul/23 08:48,25/Sep/18 04:06,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In ArrayRemove, we currently cast the right hand side expression to match the element type of the left hand side Array. This may result in down casting and may return wrong result or questionable result.

{code}
spark-sql> select array_remove(array(1,2,3), 1.23D)
         > ;
[2,3]
Time taken: 2.57 seconds, Fetched 1 row(s)
spark-sql> select array_remove(array(1,2,3), 'foo')
         > ;
NULL
{code}

We should safely coerce both left and right hand side expressions.
",,apachespark,cloud_fan,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 25 04:06:50 UTC 2018,,,,,,,,,,"0|i3yftr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/18 21:42;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22542;;;","25/Sep/18 04:06;cloud_fan;Issue resolved by pull request 22542
[https://github.com/apache/spark/pull/22542];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SHS V2 cannot enabled in Windows, because POSIX permissions is not support.",SPARK-25509,13186664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,trjianjianjiao,trjianjianjiao,trjianjianjiao,21/Sep/18 17:13,26/Sep/18 15:40,13/Jul/23 08:48,26/Sep/18 15:40,2.3.1,2.4.0,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,Spark Core,,,,0,,,,,"SHS V2 cannot enabled in Windoes, because windows doesn't support POSIX permission. 

with exception: java.lang.UnsupportedOperationException: 'posix:permissions' not supported as initial attribute.

test case fails in windows without this fix. 
 org.apache.spark.deploy.history.HistoryServerDiskManagerSuite test(""leasing space"")

 

PR: https://github.com/apache/spark/pull/22520

 ",,apachespark,elgoiri,trjianjianjiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 15:40:31 UTC 2018,,,,,,,,,,"0|i3ycxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/18 17:16;apachespark;User 'jianjianjiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22520;;;","21/Sep/18 17:17;apachespark;User 'jianjianjiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22520;;;","26/Sep/18 15:40;srowen;Issue resolved by pull request 22520
[https://github.com/apache/spark/pull/22520];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The output order of grouping columns in Pivot is different from the input order,SPARK-25505,13186632,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,21/Sep/18 14:27,28/Sep/18 11:00,13/Jul/23 08:48,28/Sep/18 07:10,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"For example,
{code}
SELECT * FROM (
  SELECT course, earnings, ""a"" as a, ""z"" as z, ""b"" as b, ""y"" as y, ""c"" as c, ""x"" as x, ""d"" as d, ""w"" as w
  FROM courseSales
)
PIVOT (
  sum(earnings)
  FOR course IN ('dotNET', 'Java')
)
{code}
The output columns should be ""a, z, b, y, c, x, d, w, ..."" but now it is ""a, b, c, d, w, x, y, z, ...""",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 28 11:00:58 UTC 2018,,,,,,,,,,"0|i3ycqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/18 15:41;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/22519;;;","21/Sep/18 15:42;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/22519;;;","28/Sep/18 11:00;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22582;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark Job History] Total task message in stage page is ambiguous,SPARK-25503,13186586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,abhishek.akg,abhishek.akg,21/Sep/18 10:03,25/Sep/18 03:05,13/Jul/23 08:48,25/Sep/18 03:05,2.3.1,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,Web UI,,,,0,,,,,"*Steps:*
 1. Spark installed and running properly.
 2. spark.ui.retainedTask=100000 ( it is default value )
 3.Launch Spark shell ./spark-shell --master yarn
 4. Create a spark-shell application with a single job and 500000 task
 val rdd = sc.parallelize(1 to 500000, 500000)
 rdd.count
 5. Launch Job History Page and go to spark-shell application created above under Incomplete Task
 6. Right click and got to Job page of the application and from there click and launch Stage Page
 7. Launch the Stage Id page for the specific Stage Id for the above created job
 8. Scroll down and check for the task msg above Pagination Panel
 It Displays *Task( 100000, Showing 500000)*

*Actual Result:*
 It displayed Task( 100000, Showing 500000)

*Expected Result:*
 Since retainedTask=100000 and it should show 100000 task
 So message should be Task( 500000, Showing 100000)
  ",,abhishek.akg,apachespark,dongjoon,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24414,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 25 03:05:58 UTC 2018,,,,,,,,,,"0|i3ycgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/18 10:04;shahid;I am working on it. will raise a PR soon;;;","22/Sep/18 13:58;shahid;I have raised the PR https://github.com/apache/spark/pull/22525;;;","22/Sep/18 13:58;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22525;;;","22/Sep/18 13:59;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22525;;;","25/Sep/18 03:05;dongjoon;Issue resolved by pull request 22525
[https://github.com/apache/spark/pull/22525];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark Job History] Empty Page when page number exceeds the reatinedTask size ,SPARK-25502,13186584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,abhishek.akg,abhishek.akg,21/Sep/18 09:51,26/Sep/18 10:55,13/Jul/23 08:48,24/Sep/18 21:21,2.3.1,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,Web UI,,,,0,,,,,"*Steps:*
1. Spark installed and running properly.
2. spark.ui.retainedTask=100000 ( it is default value )
3.Launch Spark shell ./spark-shell --master yarn
4. Create a spark-shell application with a single job and 500000 task
val rdd = sc.parallelize(1 to 500000, 500000)
rdd.count
5. Launch Job History Page and go to spark-shell application created above under Incomplete Task
6. Right click and got to Job page of the application and from there click and launch Stage Page
7. Launch the Stage Id page for the specific Stage Id for the above created job
8. Scroll down and check for the task completion Summary
It Displays pagination panel showing *5000 Pages Jump to 1 Show 100 items in a page* and Go button
9. Replace 1 with 2333 page number
*Actual Result:*
2 Pagination Panel displayed
*Expected Result:*
Pagination Panel should not display 5000 pages as retainedTask value is 100000 and it should display 1000 page only because each page holding 100 tasks

",,abhishek.akg,apachespark,shahid,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 10:55:38 UTC 2018,,,,,,,,,,"0|i3ycg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/18 09:52;shahid;I am working on it. Will raise a PR.;;;","22/Sep/18 14:34;shahid;I have raised PR https://github.com/apache/spark/pull/22526;;;","22/Sep/18 14:34;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22526;;;","24/Sep/18 21:21;vanzin;Issue resolved by pull request 22526
[https://github.com/apache/spark/pull/22526];;;","25/Sep/18 00:42;shahid;Hi [~vanzin], Could you please change the Assignee to my name, as [~abhishek.akg] has only reported the issue. Thanks.;;;","26/Sep/18 10:46;toopt4;related https://jira.apache.org/jira/browse/SPARK-16859 ?;;;","26/Sep/18 10:55;shahid;[~toopt4] No. please refer the PR, to see the fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SQLQueryTestSuite failures when the interpreter mode enabled,SPARK-25498,13186540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,21/Sep/18 05:08,04/Dec/18 00:46,13/Jul/23 08:48,03/Dec/18 16:06,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 04 00:46:33 UTC 2018,,,,,,,,,,"0|i3yc6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/18 05:10;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/22512;;;","03/Dec/18 16:06;cloud_fan;Issue resolved by pull request 22512
[https://github.com/apache/spark/pull/22512];;;","04/Dec/18 00:46;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/23212;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchedData.reset doesn't reset _nextOffsetInFetchedData and _offsetAfterPoll,SPARK-25495,13186508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,21/Sep/18 00:14,25/Sep/18 18:43,13/Jul/23 08:48,25/Sep/18 18:43,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,correctness,,,,FetchedData.reset doesn't reset _nextOffsetInFetchedData and _offsetAfterPoll and causes inconsistent cached data and may make Kafka connector return wrong results.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 00:23:28 UTC 2018,,,,,,,,,,"0|i3ybzb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"21/Sep/18 00:23;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22507;;;","21/Sep/18 00:23;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22507;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CRLF Line Separators don't work in multiline CSVs,SPARK-25493,13186457,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,justin.uang,justin.uang,justin.uang,20/Sep/18 20:53,12/Dec/22 18:10,13/Jul/23 08:48,19/Oct/18 03:15,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"CSVs with windows style crlf (carriage return line feed) don't work in multiline mode. They work fine in single line mode because the line separation is done by Hadoop, which can handle all the different types of line separators. In multiline mode, the Univocity parser is used to also handle splitting of records.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25506,SPARK-26040,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 19 03:15:32 UTC 2018,,,,,,,,,,"0|i3ybnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/18 20:55;apachespark;User 'justinuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22503;;;","09/Oct/18 17:50;apachespark;User 'justinuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22680;;;","19/Oct/18 03:15;gurwls223;Issue resolved by pull request 22503
[https://github.com/apache/spark/pull/22503];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fast equal can not check reuseSubquery when apply rule about ReuseSubquery,SPARK-25482,13186308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,hongnv110,hongnv110,20/Sep/18 11:08,13/Nov/18 17:55,13/Jul/23 08:48,13/Nov/18 17:53,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,Optimizer,SQL,,,0,,,,,"the datasource filter should be push down, so there will be two subquery on this query

select count(1) from lineitem where i_partkey > (select max(p_partkey)-1000 from part)


One of the subquery will apply a rule about ReuseSubquery and generate a new plan, but the new plan will be ignored by method fastEquals() and subquery will be executed two times.",,apachespark,cloud_fan,hongnv110,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 17:53:57 UTC 2018,,,,,,,,,,"0|i3yaqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/18 13:28;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22518;;;","13/Nov/18 17:53;cloud_fan;Issue resolved by pull request 22518
[https://github.com/apache/spark/pull/22518];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the docs for spark.sql.statistics.fallBackToHdfs,SPARK-25474,13186235,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,Ayush007,Ayush007,20/Sep/18 04:34,28/Feb/20 06:49,13/Jul/23 08:48,28/Aug/19 11:17,2.3.1,2.4.3,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"*Description :* Size in bytes of the query is coming in EB in case of parquet datasource. this would impact the performance , since join queries would always go as Sort Merge Join.

*Precondition :* spark.sql.statistics.fallBackToHdfs = true

Steps:
{code:java}
0: jdbc:hive2://10.xx:23040/default> create table t1110 (a int, b string) using parquet PARTITIONED BY (b) ;
+---------+--+
| Result |
+---------+--+
+---------+--+

0: jdbc:hive2://10.1xx:23040/default> insert into t1110 values (2,'b');
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.1xx:23040/default> insert into t1110 values (1,'a');
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.xx.xx:23040/default> select * from t1110;
+----+----+--+
| a | b |
+----+----+--+
| 1 | a |
| 2 | b |
+----+----+--+

{code}
*{color:#d04437}Cost of the query shows sizeInBytes in EB{color}*
{code:java}
 explain cost select * from t1110;



| == Optimized Logical Plan ==
Relation[a#23,b#24] parquet, Statistics(sizeInBytes=8.0 EB, hints=none)

== Physical Plan ==
*(1) FileScan parquet open.t1110[a#23,b#24] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://hacluster/user/sparkhive/warehouse/open.db/t1110], PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int> |
{code}
*{color:#d04437}This would lead to Sort Merge Join in case of join query{color}*
{code:java}
0: jdbc:hive2://10.xx.xx:23040/default> create table t110 (a int, b string) using parquet PARTITIONED BY (b) ;
+---------+--+
| Result |
+---------+--+
+---------+--+

0: jdbc:hive2://10.xx.xx:23040/default> insert into t110 values (1,'a');
+---------+--+
| Result |
+---------+--+
+---------+--+

 explain select * from t1110 t1 join t110 t2 on t1.a=t2.a;

| == Physical Plan ==
*(5) SortMergeJoin [a#23], [a#55], Inner
:- *(2) Sort [a#23 ASC NULLS FIRST], false, 0
: +- Exchange hashpartitioning(a#23, 200)
: +- *(1) Project [a#23, b#24]
: +- *(1) Filter isnotnull(a#23)
: +- *(1) FileScan parquet open.t1110[a#23,b#24] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://hacluster/user/sparkhive/warehouse/open.db/t1110], PartitionCount: 2, PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struct<a:int>
+- *(4) Sort [a#55 ASC NULLS FIRST], false, 0
+- Exchange hashpartitioning(a#55, 200)
+- *(3) Project [a#55, b#56]
+- *(3) Filter isnotnull(a#55)
+- *(3) FileScan parquet open.t110[a#55,b#56] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://hacluster/user/sparkhive/warehouse/open.db/t110], PartitionCount: 1, PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struct<a:int> |


{code}","Spark 2.3.1
Hadoop 2.7.2",abhishek.akg,apachespark,Ayush007,cloud_fan,dongjoon,maropu,sandeep.katta2007,shahid,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 11:17:04 UTC 2019,,,,,,,,,,"0|i3yaav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/18 05:32;sandeep.katta2007;what is the behavior for spark.sql.statistics.fallBackToHdfs = false  ?;;;","20/Sep/18 05:37;Ayush007;It behaves the same even if fall back to hdfs is enabled.

 ;;;","20/Sep/18 13:45;shahid;Thanks [~Ayush007], [~sandeep.katta2007], I am working on it. I will raise a PR :);;;","20/Sep/18 19:11;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22502;;;","20/Sep/18 19:11;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22502;;;","28/Jul/19 22:43;dongjoon;This is resolved via https://github.com/apache/spark/pull/22502.;;;","16/Aug/19 02:56;yumwang;More benchmark result about https://github.com/apache/spark/pull/24715:
{noformat}
scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 169
Get size fall back to HDFS: 706507984, time: 228
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 161
Get size fall back to HDFS: 706507984, time: 209
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 136
Get size fall back to HDFS: 706507984, time: 188
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 149
Get size fall back to HDFS: 706507984, time: 164
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 147
Get size fall back to HDFS: 706507984, time: 184
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 140
Get size fall back to HDFS: 706507984, time: 458
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 115
Get size fall back to HDFS: 706507984, time: 375
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 142
Get size fall back to HDFS: 706507984, time: 202
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 145
Get size fall back to HDFS: 706507984, time: 206
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 385
Get size fall back to HDFS: 706507984, time: 213
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 142
Get size fall back to HDFS: 706507984, time: 194
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 159
Get size fall back to HDFS: 706507984, time: 187
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 136
Get size fall back to HDFS: 706507984, time: 188
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 180
Get size fall back to HDFS: 706507984, time: 196
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 122
Get size fall back to HDFS: 706507984, time: 176
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 179
Get size fall back to HDFS: 706507984, time: 187
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 251
Get size fall back to HDFS: 706507984, time: 192
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 132
Get size fall back to HDFS: 706507984, time: 175
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 154
Get size fall back to HDFS: 706507984, time: 188
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 142
Get size fall back to HDFS: 706507984, time: 186
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 199
Get size fall back to HDFS: 706507984, time: 256
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 220
Get size fall back to HDFS: 706507984, time: 264
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 153
Get size fall back to HDFS: 706507984, time: 300
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 157
Get size fall back to HDFS: 706507984, time: 402
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 119
Get size fall back to HDFS: 706507984, time: 220
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 126
Get size fall back to HDFS: 706507984, time: 189
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 154
Get size fall back to HDFS: 706507984, time: 213
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 132
Get size fall back to HDFS: 706507984, time: 197
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 266
Get size fall back to HDFS: 706507984, time: 220
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 135
Get size fall back to HDFS: 706507984, time: 281
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 139
Get size fall back to HDFS: 706507984, time: 204
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 112
Get size fall back to HDFS: 706507984, time: 359
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 164
Get size fall back to HDFS: 706507984, time: 363
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 139
Get size fall back to HDFS: 706507984, time: 202
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 140
Get size fall back to HDFS: 706507984, time: 195
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 137
Get size fall back to HDFS: 706507984, time: 192
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 145
Get size fall back to HDFS: 706507984, time: 201
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 111
Get size fall back to HDFS: 706507984, time: 267
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


scala> spark.sql(""explain cost select * from test_non_partition_300000"").show
Get size from relation: 706507984, time: 126
Get size fall back to HDFS: 706507984, time: 226
+--------------------+
|                plan|
+--------------------+
|== Optimized Logi...|
+--------------------+


{noformat}

Enable {{spark.sql.statistics.fallBackToHdfs}} for partition tables:
{noformat}
scala> spark.sql(""set spark.sql.statistics.fallBackToHdfs"")
res1: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> spark.sql(""explain cost select * from test_partition_10000"").show(false)
Get size from relation: 9223372036854775807, time: 0
Get size fall back to HDFS: 1036799332, time: 47
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|== Optimized Logical Plan ==
Relation[id#15L,c3#16L,c2#17L] parquet, Statistics(sizeInBytes=988.8 MiB)

== Physical Plan ==
*(1) ColumnarToRow
+- FileScan parquet zeta_spark_dss.test_partition_10000[id#15L,c3#16L,c2#17L] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex[hdfs://hercules/sys/edw/zeta_spark_test_suite/dss/test_partition_10000], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,c3:bigint>

|
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

{noformat};;;","18/Aug/19 14:56;dongjoon;This is reverted from `branch-2.3` and `branch-2.4` to prevent VOTE failures. In `master` branch, we need to use the test cases.;;;","28/Aug/19 11:17;cloud_fan;Issue resolved by pull request 24715
[https://github.com/apache/spark/pull/24715];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark ForeachWriter test fails on Python 3.6 and macOS High Serria,SPARK-25473,13186223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,20/Sep/18 03:00,12/Dec/22 18:10,13/Jul/23 08:48,23/Sep/18 03:15,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,Structured Streaming,,,0,,,,,"
{code}
PYSPARK_PYTHON=python3.6 SPARK_TESTING=1 ./bin/pyspark pyspark.sql.tests SQLTests
{code}

{code}
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py:766: ResourceWarning: subprocess 27563 is still running
  ResourceWarning, source=self)
[Stage 0:>                                                          (0 + 1) / 1]objc[27586]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called.
objc[27586]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
ERROR

======================================================================
ERROR: test_streaming_foreach_with_simple_function (pyspark.sql.tests.SQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/.../spark/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/.../spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o54.processAllAvailable.
: org.apache.spark.sql.streaming.StreamingQueryException: Writing job aborted.
=== Streaming Query ===
Identifier: [id = f508d634-407c-4232-806b-70e54b055c42, runId = 08d1435b-5358-4fb6-b167-811584a3163e]
Current Committed Offsets: {}
Current Available Offsets: {FileStreamSource[file:/var/folders/71/484zt4z10ks1vydt03bhp6hr0000gp/T/tmpolebys1s]: {""logOffset"":0}}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
FileStreamSource[file:/var/folders/71/484zt4z10ks1vydt03bhp6hr0000gp/T/tmpolebys1s]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
Caused by: org.apache.spark.SparkException: Writing job aborted.
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:91)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:294)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2783)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2783)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2783)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$18.apply(MicroBatchExecution.scala:538)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:533)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:323)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:532)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:195)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:163)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:163)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:323)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:163)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:157)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
	... 1 more
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:485)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:474)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:570)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:404)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)
	at org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.commit(ForeachWriteSupportProvider.scala:130)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:125)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:114)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1381)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:144)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:65)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$8.apply(Executor.scala:372)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:378)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:577)
	... 19 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1882)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1870)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1869)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2103)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2052)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2041)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2073)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:63)
	... 35 more
Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:485)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:474)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:570)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:404)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)
	at org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.commit(ForeachWriteSupportProvider.scala:130)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:125)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:114)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1381)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:144)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:65)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$8.apply(Executor.scala:372)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:378)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:577)
	... 19 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/.../spark/python/pyspark/sql/tests.py"", line 1977, in test_streaming_foreach_with_simple_function
    tester.run_streaming_query_on_writer(foreach_func, 2)
  File ""/.../spark/python/pyspark/sql/tests.py"", line 1924, in run_streaming_query_on_writer
    sq.processAllAvailable()
  File ""/.../spark/python/pyspark/sql/streaming.py"", line 146, in processAllAvailable
    return self._jsq.processAllAvailable()
  File ""/.../spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/...d/spark/python/pyspark/sql/utils.py"", line 75, in deco
    raise StreamingQueryException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.StreamingQueryException: 'Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = f508d634-407c-4232-806b-70e54b055c42, runId = 08d1435b-5358-4fb6-b167-811584a3163e]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/var/folders/71/484zt4z10ks1vydt03bhp6hr0000gp/T/tmpolebys1s]: {""logOffset"":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFileStreamSource[file:/var/folders/71/484zt4z10ks1vydt03bhp6hr0000gp/T/tmpolebys1s]'

----------------------------------------------------------------------
Ran 1 test in 8.268s

FAILED (errors=1)
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 23 03:15:24 UTC 2018,,,,,,,,,,"0|i3ya87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/18 18:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/22480;;;","23/Sep/18 03:15;gurwls223;Issue resolved by pull request 22480
[https://github.com/apache/spark/pull/22480];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Structured Streaming query.stop() doesn't always stop gracefully,SPARK-25472,13186176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,19/Sep/18 21:43,02/Oct/18 15:55,13/Jul/23 08:48,20/Sep/18 22:47,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Structured Streaming,,,,0,,,,,We can have race conditions where the cancelling of Spark jobs will throw a SparkException when stopping a streaming query. This SparkException specifies that the job was cancelled. We can use this error message to swallow the error.,,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 20 22:48:36 UTC 2018,,,,,,,,,,"0|i3y9xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/18 22:47;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/22478;;;","20/Sep/18 22:47;brkyvz;Resolved by https://github.com/apache/spark/pull/22478;;;","20/Sep/18 22:48;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/22478;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix tests for Python 3.6 with Pandas 0.23+,SPARK-25471,13186173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,19/Sep/18 21:33,12/Dec/22 18:10,13/Jul/23 08:48,20/Sep/18 01:30,2.4.0,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,PySpark,Tests,,,0,,,,,"Running pyspark tests causes at least 1 error when using Python 3.6 and Pandas 0.23 or higher.  This is because the Pandas DataFrame constructor can create columns in the defined order, where earlier versions might be in alphabetical order.  This leads to the following failure:
{noformat}
======================================================================
ERROR: test_create_dataframe_from_pandas_with_timestamp (pyspark.sql.tests.SQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/bryan/git/spark/python/pyspark/sql/tests.py"", line 3275, in test_create_dataframe_from_pandas_with_timestamp
    df = self.spark.createDataFrame(pdf, schema=""d date, ts timestamp"")
  File ""/home/bryan/git/spark/python/pyspark/sql/session.py"", line 748, in createDataFrame
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/home/bryan/git/spark/python/pyspark/sql/session.py"", line 413, in _createFromLocal
    data = list(data)
  File ""/home/bryan/git/spark/python/pyspark/sql/session.py"", line 730, in prepare
    verify_func(obj)
  File ""/home/bryan/git/spark/python/pyspark/sql/types.py"", line 1389, in verify
    verify_value(obj)
  File ""/home/bryan/git/spark/python/pyspark/sql/types.py"", line 1370, in verify_struct
    verifier(v)
  File ""/home/bryan/git/spark/python/pyspark/sql/types.py"", line 1389, in verify
    verify_value(obj)
  File ""/home/bryan/git/spark/python/pyspark/sql/types.py"", line 1383, in verify_default
    verify_acceptable_types(obj)
  File ""/home/bryan/git/spark/python/pyspark/sql/types.py"", line 1278, in verify_acceptable_types
    % (dataType, obj, type(obj))))
TypeError: field ts: TimestampType can not accept object datetime.date(2018, 9, 19) in type <class 'datetime.date'>
{noformat}",,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 20 01:30:54 UTC 2018,,,,,,,,,,"0|i3y9xb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/18 01:30;gurwls223;Issue resolved by pull request 22477
[https://github.com/apache/spark/pull/22477];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark Pandas UDF outputs incorrect results when input columns contain None,SPARK-25461,13185969,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,xiangcy,xiangcy,19/Sep/18 04:49,12/Dec/22 18:10,13/Jul/23 08:48,07/Oct/18 15:28,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,,,,0,,,,,"The following PySpark script uses a simple pandas UDF to calculate a column given column 'A'. When column 'A' contains None, the results look incorrect.

Script: 

 
{code:java}
import pandas as pd
import random
import pyspark
from pyspark.sql.functions import col, lit, pandas_udf

values = [None] * 30000 + [1.0] * 170000 + [2.0] * 6000000
random.shuffle(values)
pdf = pd.DataFrame({'A': values})
df = spark.createDataFrame(pdf)

@pandas_udf(returnType=pyspark.sql.types.BooleanType())
def gt_2(column):
    return (column >= 2).where(column.notnull())

calculated_df = (df.select(['A'])
    .withColumn('potential_bad_col', gt_2('A'))
)

calculated_df = calculated_df.withColumn('correct_col', (col(""A"") >= lit(2)) | (col(""A"").isNull()))

calculated_df.show()
{code}
 

Output:
{code:java}
+---+-----------------+-----------+
| A|potential_bad_col|correct_col|
+---+-----------------+-----------+
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|1.0| false| false|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
|2.0| false| true|
+---+-----------------+-----------+
only showing top 20 rows
{code}
This problem disappears when the number of rows is small or when the input column does not contain None.","I reproduced this issue by running pyspark locally on mac:

Spark version: 2.3.1 pre-built with Hadoop 2.7

Python library versions: pyarrow==0.10.0, pandas==0.20.2",apachespark,bryanc,csaid,mstewart141,viirya,xiangcy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ARROW-3428,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 08 19:44:55 UTC 2018,,,,,,,,,,"0|i3y8o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/18 18:08;mstewart141;[~hyukjin.kwon] I have also recently observed this issue – it seems potentially rather serious. This is quite different than the behaviorr of the pandas function against a (pandas) df, which works exactly as expected.;;;","26/Sep/18 08:42;viirya;{{.where(column.notnull())}} returns pandas dataframe in float64 type, not bool, if there are None values. And looks like float values are interpreted as false, if the pandas udf's return type is BooleanType:
{code:java}
values = [1.0] * 5 + [2.0] * 5
pdf = pd.DataFrame({'A': values})
df = self.spark.createDataFrame(pdf)
@pandas_udf(returnType=BooleanType())
def to_boolean(column):
    return column
df.select(['A']).withColumn('to_boolean', to_boolean('A')).show()

+---+----------+                                            
|  A|to_boolean|       
+---+----------+                                                      
|1.0|     false|                                                             
|1.0|     false|                                                      
|1.0|     false|                  
|1.0|     false|                                                                                              
|1.0|     false|                                                                                                                                       
|2.0|     false|                                                                                 
|2.0|     false|                              
|2.0|     false|                                                                  
|2.0|     false|                                            
|2.0|     false|                                                      
+---+----------+      
{code}
So that's said the above {{gt_2}} pandas udf returns false for both 1.0 and 2.0 in your input data.;;;","27/Sep/18 07:00;viirya;That's said I'm not sure if this can be called a bug now. [~hyukjin.kwon], WDYT?;;;","27/Sep/18 18:00;csaid;For what it's worth, converting all floats to False goes against my expectations. ;;;","02/Oct/18 05:26;viirya;I've looked more at this. We don't really check if pandas.Series's type matches with pre-defined return type. For this case, seems the conversion is not correct.

I was trying to add some check and throw exception when mismatching is detected. But looks like we leverage such behavior in current codebase. For example, there is a test {{test_vectorized_udf_null_short}}:
{code}
data = [(None,), (2,), (3,), (4,)]
schema = StructType().add(""short"", ShortType())
df = self.spark.createDataFrame(data, schema)
short_f = pandas_udf(lambda x: x, ShortType())
res = df.select(short_f(col('short')))
self.assertEquals(df.collect(), res.collect())
{code}
The Pandas.Series is of float64 but we define return type as ShortType. In this case, it works well. So seems to disallow such conversion is not feasible. For now, I think we can print some warning message if such mismatching is detected.

cc [~hyukjin.kwon] What do you think about this idea?;;;","02/Oct/18 08:27;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22610;;;","02/Oct/18 08:27;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22610;;;","02/Oct/18 09:39;gurwls223;[~viirya], I am sorry I missed this. I have been busy this month. Will take a look for PR.;;;","02/Oct/18 09:41;viirya;[~hyukjin.kwon] Thanks and no problem at all! You can take a look when you have time. :);;;","02/Oct/18 16:05;bryanc;Thanks for looking into this [~viirya]! You are right that the above udf returns a float64 instead of boolean. I'm not sure what the expected cast from float to bool should be, but it does seem like pyarrow might be doing something wrong here. I'll look into it some more and raise an issue there if so.;;;","02/Oct/18 16:54;mstewart141;Thanks all; to me the largest issue with this behavior is the silent failure — there is a relatively sane workaround to the issue but the silent failure is deeply unnerving. Especially as the same code runs in pandas proper with no hint of the issue. Even raising some runtime error would be a huge win from my perspective! ;;;","03/Oct/18 23:42;bryanc;I filed ARROW-3428, which deals with the incorrect cast from float to bool;;;","04/Oct/18 00:36;xiangcy;Hi all, thanks for looking into the issue! As a follow up, I noticed that there were similar issues with casting to float as well. Just reusing my example and changing the return type to be FloatType: 

Script: 

 
{code:java}
import pandas as pd
import random
import pyspark
from pyspark.sql.functions import col, lit, pandas_udf

values = [None] * 30000 + [1.0] * 170000 + [2.0] * 6000000
random.shuffle(values)
pdf = pd.DataFrame({'A': values})
df = spark.createDataFrame(pdf)

@pandas_udf(returnType=pyspark.sql.types.FloatType())
def gt_2(column):
return (column >= 2).where(column.notnull())

calculated_df = (df.select(['A'])
.withColumn('potential_bad_col', gt_2('A'))
)

calculated_df = calculated_df.withColumn('correct_col', (col(""A"") >= lit(2)) | (col(""A"").isNull()))

calculated_df.filter(col(""A"") == 2).show(30)
{code}
 

Output:
{code:java}
+---+-----------------+-----------+
| A|potential_bad_col|correct_col|
+---+-----------------+-----------+
|2.0| 1.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 1.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 1.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 1.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
|2.0| 0.0| true|
+---+-----------------+-----------+{code}
 ;;;","05/Oct/18 19:23;bryanc;Thanks [~xiangcy]! Converting from bool to other numbers is related to the other problem, but was slightly different.  The fix for this is also included in ARROW-3428;;;","07/Oct/18 15:28;gurwls223;Fixed in https://github.com/apache/spark/pull/22610;;;","08/Oct/18 19:44;bryanc;Just wanted to add that the resolution here added a note for the user to verify the return type and data is correct.  The actual fix for correct data when converting to/from booleans is on the Arrow side and won't be available until Spark upgrades pyarrow to version 0.12.0+.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stages page doesn't show the right number of the total tasks,SPARK-25451,13185739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,zuo.tingbing9,zuo.tingbing9,18/Sep/18 08:44,26/Nov/18 22:20,13/Jul/23 08:48,26/Nov/18 21:19,2.3.1,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,Web UI,,,,0,,,,," 

See the attached pic.

  !mshot.png!

The executor 1 has 7 tasks, but in the Stages Page the total tasks of executor is 6.

 

to reproduce this simply start a shell:
{code:java}
$SPARK_HOME/bin/spark-shell --executor-cores 1 --executor-memory 1g --total-executor-cores 2 --master spark://localhost.localdomain:7077{code}
Run job as fellows:
{code:java}
sc.parallelize(1 to 10000, 3).map{ x => throw new RuntimeException(""Bad executor"")}.collect() {code}
 

Go to the stages page and you will see the Total Tasks  is not right in
{code:java}
Aggregated Metrics by Executor{code}
table. 

 ",,apachespark,devaraj,shahid,XuanYuan,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/18 08:45;zuo.tingbing9;mshot.png;https://issues.apache.org/jira/secure/attachment/12940161/mshot.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 26 22:20:02 UTC 2018,,,,,,,,,,"0|i3y793:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/18 09:26;yumwang;Please avoid to set the {{Target Version/s}} which is usually reserved for committers.;;;","18/Sep/18 10:10;zuo.tingbing9;yes, thanks [~yumwang];;;","14/Nov/18 21:07;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23038;;;","14/Nov/18 21:08;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/23038;;;","26/Nov/18 22:20;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PushProjectThroughUnion rule uses the same exprId for project expressions in each Union child, causing mistakes in constant propagation",SPARK-25450,13185698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,18/Sep/18 02:52,20/Sep/18 17:03,13/Jul/23 08:48,20/Sep/18 17:03,2.3.1,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,SQL,,,,0,,,,,"The problem was cause by the PushProjectThroughUnion rule, which, when creating new Project for each child of Union, uses the same exprId for expressions of the same position. This is wrong because, for each child of Union, the expressions are all independent, and it can lead to a wrong result if other rules like FoldablePropagation kicks in, taking two different expressions as the same.
",,maropu,maryannxue,smilegator,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 18 05:42:09 UTC 2018,,,,,,,,,,"0|i3y6zz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/18 05:42;smilegator;https://github.com/apache/spark/pull/22447;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix issues when building docs with release scripts in docker,SPARK-25443,13185410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Sep/18 04:50,18/Sep/18 02:11,13/Jul/23 08:48,18/Sep/18 02:11,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Project Infra,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 18 02:11:40 UTC 2018,,,,,,,,,,"0|i3y587:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/18 12:13;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22438;;;","17/Sep/18 12:14;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22438;;;","18/Sep/18 02:11;cloud_fan;Issue resolved by pull request 22438
[https://github.com/apache/spark/pull/22438];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPCHQuerySuite customer.c_nationkey should be bigint instead of string,SPARK-25439,13185309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,npoggi,npoggi,npoggi,15/Sep/18 14:38,16/Sep/18 03:08,13/Jul/23 08:48,16/Sep/18 03:08,2.3.0,2.3.1,2.4.0,,,,,,,,,,,,,,,2.4.0,,,,SQL,Tests,,,0,benchmark,easy-fix,test,," 

The [TPCHQuerySuite|https://github.com/apache/spark/blob/be454a7cef1cb5c76fb22589fc3a55c1bf519cf4/sql/core/src/test/scala/org/apache/spark/sql/TPCHQuerySuite.scala#L72] currently has {{string}} for the {{customer.c_nationkey}} column, while it should be bigint according to [the spec|http://www.tpc.org/TPC_Documents_Current_Versions/pdf/tpc-h_v2.17.3.pdf] (identifier type) and matching the {{nation}} table. 

Note: this update would make previousTPCH results not comparable for queries using the {{customer}} table

 ",,npoggi,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 15 15:55:33 UTC 2018,,,,,,,,,,"0|i3y4lr:",9223372036854775807,,,,,"LI,Xiao",,,,,,,,,,,,,,,,,,"15/Sep/18 15:55;npoggi;Created the [PR with the patch|https://github.com/apache/spark/pull/22430].

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FilterPushdownBenchmark to use the same memory assumption,SPARK-25438,13185294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Sep/18 09:07,16/Sep/18 01:03,13/Jul/23 08:48,16/Sep/18 00:49,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,Tests,,,0,,,,,"This issue aims to fix three things in `FilterPushdownBenchmark`.

1. Use the same memory assumption. 
The following configurations are used in ORC and Parquet.

*Memory buffer for writing*
- parquet.block.size (default: 128MB)
- orc.stripe.size (default: 64MB)

*Compression chunk size*
- parquet.page.size (default: 1MB)
- orc.compress.size (default: 256KB)

SPARK-24692 used 1MB, the default value of `parquet.page.size`, for `parquet.block.size` and `orc.stripe.size`. But, it missed to match `orc.compress.size`. So, the current benchmark shows the result from ORC with 256KB memory for compression and Parquet with 1MB. To compare correctly, we need to be consistent.

2. Dictionary encoding should not be enforced for all cases.
SPARK-24206 enforced dictionary encoding for all test cases. This issue recovers the ORC behavior in general and enforces dictionary encoding only for `prepareStringDictTable`.

3. Generate test result on AWS r3.xlarge.
We do not 
SPARK-24206 generates the result on AWS in order to reproduce and compare easily. This issue also aims to update the result on the same machine again in the same reason. Specifically, AWS r3.xlarge with Instance Store is used.",,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,SPARK-24692,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 16 00:49:24 UTC 2018,,,,,,,,,,"0|i3y4if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/18 00:49;dongjoon;Issue resolved by pull request 22427
[https://github.com/apache/spark/pull/22427];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix function examples and unify the format of the example results.,SPARK-25431,13185095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,14/Sep/18 09:26,12/Dec/22 18:10,13/Jul/23 08:48,17/Sep/18 12:41,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,There are some mistakes in examples of newly added functions. Also the format of the example results are not unified. We should fix and unify them.,,apachespark,dongjoon,ueshin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 17 12:41:17 UTC 2018,,,,,,,,,,"0|i3y3a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/18 13:44;viirya;Don't know why the PR link is not attached automatically.

The PR for this issue is at: https://github.com/apache/spark/pull/22421;;;","16/Sep/18 00:43;dongjoon;I reopened this since it's reverted now. We can resolve this back with the new commit.;;;","17/Sep/18 07:30;ueshin;Seems like the automatic link didn't work again.

The new PR is at: https://github.com/apache/spark/pull/22437;;;","17/Sep/18 12:04;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22437;;;","17/Sep/18 12:41;gurwls223;Issue resolved by pull request 22437
[https://github.com/apache/spark/pull/22437];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add BloomFilter creation test cases,SPARK-25427,13185063,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Sep/18 05:10,17/Sep/18 11:36,13/Jul/23 08:48,17/Sep/18 11:36,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.4.0,,,,SQL,Tests,,,0,,,,,Spark supports BloomFilter creation for ORC files. This issue aims to add test coverages to prevent regressions like SPARK-12417,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,SPARK-12417,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 17 11:36:30 UTC 2018,,,,,,,,,,"0|i3y333:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/18 11:36;cloud_fan;Issue resolved by pull request 22418
[https://github.com/apache/spark/pull/22418];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extra options must overwrite sessions options,SPARK-25425,13184998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,13/Sep/18 20:15,26/Sep/18 06:37,13/Jul/23 08:48,16/Sep/18 00:38,2.3.0,2.3.1,2.4.0,,,,,,,,,,,,,,,2.3.3,2.4.0,,,SQL,,,,0,,,,,"In load() and save() methods of DataSource V2, extra options are overwritten by session options:
* https://github.com/apache/spark/blob/c9cb393dc414ae98093c1541d09fa3c8663ce276/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L244-L245
* https://github.com/apache/spark/blob/c9cb393dc414ae98093c1541d09fa3c8663ce276/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L205

but implementation must be opposite - more specific extra options set via *.option(...)* must overwrite more common session options
",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 16:20:35 UTC 2018,,,,,,,,,,"0|i3y2on:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/18 00:52;dongjoon;This is resolved via https://github.com/apache/spark/pull/22413 at master branch. And, we are waiting for two PRs against branch-2.4 and 2.3.;;;","21/Sep/18 16:20;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22489;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The metadata of DataSource table should not include Hive-generated storage properties.,SPARK-25418,13184787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,13/Sep/18 05:21,14/Sep/18 05:23,13/Jul/23 08:48,14/Sep/18 05:23,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"When Hive support enabled, Hive catalog puts extra storage properties into table metadata even for DataSource tables, but we should not have them.",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 13 05:28:57 UTC 2018,,,,,,,,,,"0|i3y1en:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/18 05:28;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22410;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayContains function may return incorrect result when right expression is implicitly down casted,SPARK-25417,13184736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,12/Sep/18 22:09,07/Jan/20 00:07,13/Jul/23 08:48,20/Sep/18 12:36,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In ArrayContains, we currently cast the right hand side expression to match the element type of the left hand side Array. This may result in down casting and may return wrong result or questionable result.

Example :

{code:java}
spark-sql> select array_position(array(1), 1.34);
true


{code}
 
{code:java}
spark-sql> select array_position(array(1), 'foo');
null
{code}",,apachespark,cloud_fan,dkbiswal,maropu,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 16:58:58 UTC 2018,,,,,,,,,,"0|i3y13b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/18 22:28;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22408;;;","20/Sep/18 12:36;cloud_fan;Issue resolved by pull request 22408
[https://github.com/apache/spark/pull/22408];;;","21/Sep/18 16:58;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22448;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayPosition function may return incorrect result when right expression is implicitly downcasted.,SPARK-25416,13184727,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,12/Sep/18 21:13,25/Sep/18 08:30,13/Jul/23 08:48,24/Sep/18 13:45,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In ArrayPosition, we currently cast the right hand side expression to match the element type of the left hand side Array. This may result in down casting and may return wrong result or questionable result.

Example :
spark-sql> select array_position(array(1), 1.34);
1

spark-sql> select array_position(array(1), 'foo');
null

We should safely coerce both left and right hand side expressions.
",,apachespark,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25519,SPARK-25522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 12 22:30:57 UTC 2018,,,,,,,,,,"0|i3y11b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/18 22:30;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22407;;;","12/Sep/18 22:30;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22407;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark throws a `ParquetDecodingException` when attempting to read a field from a complex type in certain cases of schema merging,SPARK-25407,13184394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michael,michael,michael,11/Sep/18 15:59,12/Dec/22 18:10,13/Jul/23 08:48,08/Apr/19 13:29,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"Spark supports merging schemata across table partitions in which one partition is missing a subfield that's present in another. However, attempting to select that missing field with a query that includes a partition pruning predicate that filters out the partitions that include that field results in a `ParquetDecodingException` when attempting to get the query results.

This bug is specifically exercised by the failing (but ignored) test case [https://github.com/apache/spark/blob/f2d35427eedeacceb6edb8a51974a7e8bbb94bc2/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala#L125-L131].",,apachespark,Daimon,dongjoon,michael,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25879,,,,,SPARK-31116,,,,,,,,,,,SPARK-31536,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 08 13:29:13 UTC 2019,,,,,,,,,,"0|i3xyzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/18 16:00;michael;I have a code-complete patch for this bug, but I want to add some code comments before submitting it.;;;","29/Oct/18 19:18;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/22880;;;","29/Oct/18 19:19;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/22880;;;","08/Apr/19 13:29;gurwls223;Fixed in https://github.com/apache/spark/pull/24307;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect usage of withSQLConf method in Parquet schema pruning test suite masks failing tests,SPARK-25406,13184379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michael,michael,michael,11/Sep/18 14:56,21/Sep/18 07:13,13/Jul/23 08:48,13/Sep/18 17:09,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In {{ParquetSchemaPruning.scala}}, we use the helper method {{withSQLConf}} to set configuration settings within the scope of a test. However, the way we use that method is incorrect, and as a result the desired configuration settings are not propagated to the test cases. This masks some test case failures.",,apachespark,dbtsai,michael,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 13 17:09:27 UTC 2018,,,,,,,,,,"0|i3xyw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/18 15:19;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/22394;;;","13/Sep/18 17:09;dbtsai;Issue resolved by pull request 22394
[https://github.com/apache/spark/pull/22394];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Broadcast join is changing to sort merge join , after spark-beeline session restarts.",SPARK-25403,13184275,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,Ayush007,Ayush007,11/Sep/18 09:17,07/Jan/20 03:43,13/Jul/23 08:48,07/Jan/20 03:43,2.2.1,2.3.1,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"*Issue 1*: {color:#ff0000}Broadcast join is changing to sort merge join , after spark-beeline session restarts{color}.

*Precondition* : The JDBC/Thrift Server is continuously running. 

*Steps:*

 
{code:java}
0: jdbc:hive2://10.18.18.214:23040/default> use x1;
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.18.18.214:23040/default> create table cv (a int, b string) stored as parquet;
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.18.18.214:23040/default> create table c (a int, b string) stored as parquet;
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.18.18.214:23040/default> insert into table c values (1,'a');
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.18.18.214:23040/default> insert into table cv values (1,'a');
+---------+--+
| Result |
+---------+--+
+---------+--+
0: jdbc:hive2://10.18.18.214:23040/default> select * from c , cv where c.a = cv.
+----+----+----+----+--+
| a | b | a | b |
+----+----+----+----+--+
| 1 | a | 1 | a |
+----+----+----+----+--+

{code}
 
{code:java}
Before Restarting the session (spark-beeline)
{code}
*{color:#d04437}explain select * from c , cv where c.a = cv.a;{color}*
|== Physical Plan ==
 *(2) {color:#d04437}BroadcastHashJoin{color}[a#3284|#3284], [a#3286|#3286], Inner, BuildRight
 :- *(2) Project [a#3284, b#3285|#3284, b#3285]
 : +- *(2) Filter isnotnull(a#3284)
 : +- *(2) FileScan parquet x1.c[a#3284,b#3285|#3284,b#3285] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hacluster/user/sparkhive/warehouse/x1.db/c], PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struct<a:int,b:string>
 +- {color:#d04437}BroadcastExchange{color} HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
 +- *(1) Project [a#3286, b#3287|#3286, b#3287]
 +- *(1) Filter isnotnull(a#3286)
 +- *(1) FileScan parquet x1.cv[a#3286,b#3287|#3286,b#3287] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hacluster/user/sparkhive/warehouse/x1.db/cv], PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struct<a:int,b:string>|
{code:java}
After Session Restarts (spark-beeline)
{code}
{color:#d04437} *explain select * from c , cv where c.a = cv.a;*{color}
|== Physical Plan == *(5) *{color:#d04437}SortMergeJoin{color}* [a#3312|#3312], [a#3314|#3314], Inner :- *(2) Sort [a#3312 ASC NULLS FIRST|#3312 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(a#3312, 200) : +- *(1) Project [a#3312, b#3313|#3312, b#3313] : +- *(1) Filter isnotnull(a#3312) : +- *(1) FileScan parquet x1.c[a#3312,b#3313|#3312,b#3313] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hacluster/user/sparkhive/warehouse/x1.db/c], PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struct<a:int,b:string> +- *(4) Sort [a#3314 ASC NULLS FIRST|#3314 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(a#3314, 200) +- *(3) Project [a#3314, b#3315|#3314, b#3315] +- *(3) Filter isnotnull(a#3314) +- *(3) FileScan parquet x1.cv[a#3314,b#3315|#3314,b#3315] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hacluster/user/sparkhive/warehouse/x1.db/cv], PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struct<a:int,b:string>|

+_*Note: JDBC Server is continuously running at the time of session restart i.e. Application is not restarting. The driver remains the same.*_+","Spark 2.3.1

Hadoop 2.7.2",ajithshetty,apachespark,Ayush007,cloud_fan,sandeep.katta2007,toopt4,yucai,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 03:43:09 UTC 2020,,,,,,,,,,"0|i3xy9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/18 05:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22721;;;","15/Oct/18 05:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22721;;;","15/Oct/18 06:01;yumwang;The previous {{BroadcastHashJoin}} is incorrect. You should set {{spark.sql.statistics.size.autoUpdate.enabled=true}} or {{spark.sql.statistics.fallBackToHdfs=true}}.;;;","15/Feb/19 21:22;toopt4;gentle ping;;;","07/Jan/20 03:43;cloud_fan;Issue resolved by pull request 22721
[https://github.com/apache/spark/pull/22721];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null handling in BooleanSimplification,SPARK-25402,13184242,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,11/Sep/18 06:57,13/Sep/18 13:41,13/Jul/23 08:48,13/Sep/18 02:01,2.2.2,2.3.1,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,SQL,,,,0,,,,,"SPARK-20350 introduced a bug BooleanSimplification for null handling. For example, the following case returns a wrong answer. 

{code}
    val schema = StructType.fromDDL(""a boolean, b int"")
    val rows = Seq(Row(null, 1))

    val rdd = sparkContext.parallelize(rows)
    val df = spark.createDataFrame(rdd, schema)

    checkAnswer(df.where(""(NOT a) OR a""), Seq.empty)
{code}",,apachespark,dongjoon,maropu,smilegator,Steven Rand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20350,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 12 17:59:01 UTC 2018,,,,,,,,,,"0|i3xy2f:",9223372036854775807,,,,,,,,,,,,,2.2.3,2.3.2,,,,,,,,,"11/Sep/18 07:08;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22390;;;","12/Sep/18 17:58;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22403;;;","12/Sep/18 17:59;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22403;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reusing execution threads from continuous processing for microbatch streaming can result in correctness issues,SPARK-25399,13184157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mukulmurthy,mukulmurthy,mukulmurthy,10/Sep/18 21:21,21/Sep/18 07:14,13/Jul/23 08:48,11/Sep/18 22:53,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,correctness,,,,"Continuous processing sets some thread local variables that, when read by a thread running a microbatch stream, may result in incorrect or no previous state being read and resulting in wrong answers. This was caught by a job running the StreamSuite tests, and only repros occasionally when the same threads are used.

The issue is in StateStoreRDD.compute - when we compute currentVersion, we read from a thread local variable which is set by continuous processing threads. If this value is set, we then think we're on the wrong state version.

I imagine very few people, if any, would run into this bug, because you'd have to use continuous processing and then microbatch processing in the same cluster. However, it can result in silent correctness issues, and it would be very difficult for someone to tell if they were impacted by this or not.",,apachespark,iamhumanbeing,mukulmurthy,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 22:53:50 UTC 2018,,,,,,,,,,"0|i3xxjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/18 21:26;mukulmurthy;cc [~joseph.torres] and [~tdas];;;","10/Sep/18 23:58;apachespark;User 'mukulmurthy' has created a pull request for this issue:
https://github.com/apache/spark/pull/22386;;;","11/Sep/18 22:53;tdas;Issue resolved by pull request 22386
[https://github.com/apache/spark/pull/22386];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor bugs from comparing unrelated types,SPARK-25398,13184149,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,10/Sep/18 19:57,11/Sep/18 19:46,13/Jul/23 08:48,11/Sep/18 19:46,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Mesos,Spark Core,YARN,,0,,,,,"I noticed a potential issue from Scala inspections, like this clause in LiveEntity.scala around line 586:
{code:java}
 (!acc.metadata.isDefined ||
  acc.metadata.get != Some(AccumulatorContext.SQL_ACCUM_IDENTIFIER)){code}
The issue is that acc.metadata is Option[String], so can't equal Some[String]. This just meant to be:
{code:java}
 acc.metadata != Some(AccumulatorContext.SQL_ACCUM_IDENTIFIER){code}
This may or may not actually cause a bug, but seems worth fixing. And then there are a number of other ones like this, mostly in tests, that might likewise mask real assertion problems.

Many are, interestingly, flagging items like this on a Seq[String]:
{code:java}
.filter(_.getFoo.equals(""foo"")){code}
It complains that Any => Any is compared to String. Either it's wrong, or somehow, this is parsed as (_.getFoo).equals(""foo"")). In any event, easy enough to write this more clearly as:
{code:java}
.filter(_.getFoo == ""foo""){code}
And so on.",,apachespark,maropu,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 19:46:33 UTC 2018,,,,,,,,,,"0|i3xxi7:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"10/Sep/18 20:07;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22384;;;","10/Sep/18 20:08;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22384;;;","11/Sep/18 19:46;srowen;Issue resolved by pull request 22384
[https://github.com/apache/spark/pull/22384];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT OVERWRITE DIRECTORY STORED AS should prevent duplicate fields,SPARK-25389,13183934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Sep/18 05:15,11/Sep/18 15:59,13/Jul/23 08:48,11/Sep/18 15:59,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Like `INSERT OVERWRITE DIRECTORY USING` syntax, `INSERT OVERWRITE DIRECTORY STORED AS` should not generate files with duplicate fields because Spark cannot read those files.

*INSERT OVERWRITE DIRECTORY USING*
{code}
scala> sql(""INSERT OVERWRITE DIRECTORY 'file:///tmp/parquet' USING parquet SELECT 'id', 'id2' id"")
... ERROR InsertIntoDataSourceDirCommand: Failed to write to directory ...
org.apache.spark.sql.AnalysisException: Found duplicate column(s) when inserting into file:/tmp/parquet: `id`;
{code}

*INSERT OVERWRITE DIRECTORY STORED AS*
{code}
scala> sql(""INSERT OVERWRITE DIRECTORY 'file:///tmp/parquet' STORED AS parquet SELECT 'id', 'id2' id"")

scala> spark.read.parquet(""/tmp/parquet"").show
18/09/09 22:09:57 WARN DataSource: Found duplicate column(s) in the data schema and the partition schema: `id`;
{code}",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 15:59:21 UTC 2018,,,,,,,,,,"0|i3xw7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/18 05:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22378;;;","11/Sep/18 15:59;dongjoon;Issue resolved by pull request 22378
[https://github.com/apache/spark/pull/22378];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
checkEvaluation may miss incorrect nullable of DataType in the result,SPARK-25388,13183920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,10/Sep/18 01:47,12/Oct/18 03:15,13/Jul/23 08:48,12/Oct/18 03:15,3.0.0,,,,,,,,,,,,,,,,,3.0.0,,,,Tests,,,,0,,,,,Current {{checkEvalution}} may miss incorrect nullable of {{DataType}} in {{checkEvaluationWithUnsafeProjection}}.,,apachespark,cloud_fan,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 12 03:15:33 UTC 2018,,,,,,,,,,"0|i3xw4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/18 01:55;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22375;;;","12/Oct/18 03:15;cloud_fan;Issue resolved by pull request 22375
[https://github.com/apache/spark/pull/22375];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Malformed CSV causes NPE,SPARK-25387,13183894,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,09/Sep/18 15:40,13/Sep/18 01:53,13/Jul/23 08:48,13/Sep/18 01:53,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Loading a malformed CSV files or a dataset can cause NullPointerException, for example the code:
{code:scala}
val schema = StructType(StructField(""a"", IntegerType) :: Nil)
val input = spark.createDataset(Seq(""\u0000\u0000\u0001234""))
spark.read.schema(schema).csv(input).collect()
{code} 
crashes with the exception:
{code:java}
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(UnivocityParser.scala:219)
	at org.apache.spark.sql.execution.datasources.csv.UnivocityParser.parse(UnivocityParser.scala:210)
	at org.apache.spark.sql.DataFrameReader$$anonfun$11$$anonfun$12.apply(DataFrameReader.scala:523)
	at org.apache.spark.sql.DataFrameReader$$anonfun$11$$anonfun$12.apply(DataFrameReader.scala:523)
	at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:68)
{code}

If schema is not specified, the following exception is thrown:
{code:java}
java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.IndexedSeqOptimized$class.zipWithIndex(IndexedSeqOptimized.scala:99)
	at scala.collection.mutable.ArrayOps$ofRef.zipWithIndex(ArrayOps.scala:186)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.makeSafeHeader(CSVDataSource.scala:109)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:247)
{code}",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 13 01:53:41 UTC 2018,,,,,,,,,,"0|i3xvz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/18 15:53;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22374;;;","09/Sep/18 15:53;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22374;;;","13/Sep/18 01:53;cloud_fan;Issue resolved by pull request 22374
[https://github.com/apache/spark/pull/22374];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Yarn-specific configs in regards to keytab login for SparkSubmit,SPARK-25372,13183737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ifilonenko,ifilonenko,ifilonenko,07/Sep/18 21:19,17/May/20 18:14,13/Jul/23 08:48,27/Sep/18 00:25,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,Spark Core,YARN,,0,release-notes,,,,"{{SparkSubmit}} already logs in the user if a keytab is provided, the only issue is that it uses the existing configs which have ""yarn"" in their name. As such, we should use a common name for the principal and keytab configs, and deprecate the YARN-specific ones.
cc [~vanzin]",,apachespark,ifilonenko,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 14 02:22:17 UTC 2018,,,,,,,,,,"0|i3xv07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/18 22:33;apachespark;User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22362;;;","07/Sep/18 22:33;apachespark;User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22362;;;","27/Sep/18 00:25;vanzin;Issue resolved by pull request 22362
[https://github.com/apache/spark/pull/22362];;;","14/Oct/18 02:22;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22717;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector Assembler with no input columns leads to opaque error,SPARK-25371,13183693,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mgaido,Odessa,Odessa,07/Sep/18 17:41,12/Sep/18 12:35,13/Jul/23 08:48,12/Sep/18 12:35,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,ML,MLlib,,,0,,,,,"When `VectorAssembler ` is given an empty array as its inputColumns it throws an opaque error. In versions less than 2.3 `VectorAssembler` it simply appends a column containing empty vectors. 

 
{code:java}
val inputCols = Array()
val outputCols = Array(""A"")
val vectorAssembler = new VectorAssembler()
.setInputCols(inputCols)
.setOutputCol(outputCols)

vectorAssmbler.fit(data).transform(df)

{code}
In versions 2.3 > this throws the exception below
{code:java}
org.apache.spark.sql.AnalysisException: cannot resolve 'named_struct()' due to data type mismatch: input to function named_struct requires at least one argument;;
{code}
Whereas in versions less than 2.3 it just adds a column containing an empty vector.

I'm not certain if this is an intentional choice or an actual bug. If this is a bug, the `VectorAssembler` should be modified to append an empty vector column if it detects no inputCols.

 

If it is a design decision it would be nice to throw a human readable exception explicitly stating inputColumns must not be empty. The current error is somewhat opaque.",,apachespark,Odessa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 08:11:59 UTC 2018,,,,,,,,,,"0|i3xuqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/18 13:13;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22373;;;","09/Sep/18 13:14;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22373;;;","11/Sep/18 08:11;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22391;;;","11/Sep/18 08:11;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22391;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect constraint inference returns wrong result,SPARK-25368,13183617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yumwang,lev,lev,07/Sep/18 12:06,09/Sep/18 22:49,13/Jul/23 08:48,09/Sep/18 16:10,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Optimizer,SQL,,,0,correctness,,,,"there is a breaking change in spark 2.3 (I checked on 2.3.1 and 2.3.2-rc5)

the following code recreates the problem
 (it's a bit convoluted examples, I tried to simplify it as much as possible from my code)
{code:java}
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

import spark.implicits._

case class Data(a: Option[Int],b: String,c: Option[String],d: String)

val df1 = spark.createDataFrame(Seq(
   Data(Some(1), ""1"", None, ""1""),
   Data(None, ""2"", Some(""2""), ""2"")
))

val df2 = df1
.where( $""a"".isNotNull)
.withColumn(""e"", lit(null).cast(""string""))

val columns = df2.columns.map(c => col(c))

val df3 = df1
.select(
  $""c"",
  $""b"" as ""e""
  )
  .withColumn(""a"", lit(null).cast(""int""))
  .withColumn(""b"", lit(null).cast(""string""))
  .withColumn(""d"", lit(null).cast(""string""))
  .select(columns :_*)

val df4 =
  df2.union(df3)
  .withColumn(""e"", last(col(""e""), ignoreNulls = true).over(Window.partitionBy($""c"").orderBy($""d"")))
  .filter($""a"".isNotNull)

df4.show

{code}
 

notice that the last statement in for df4 is to filter rows where a is null

in spark 2.2.1, the above code prints:
{code:java}
+---+---+----+---+---+ 
| a| b| c| d| e|
 +---+---+----+---+---+ 
| 1| 1|null| 1| 1| 
+---+---+----+---+---+
{code}
in spark 2.3.x, it prints: 
{code:java}
+----+----+----+----+---+ 
| a| b| c| d| e| 
+----+----+----+----+---+ 
|null|null|null|null| 1| 
| 1| 1|null| 1| 1| 
|null|null| 2|null| 2|
 +----+----+----+----+---+
{code}
 the column a still contains null values

 

attached are the plans.

in the parsed logical plan, the filter for isnotnull('a), is on top,
 but in the optimized logical plan, it is pushed down",,apachespark,lev,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/18 12:09;lev;plan.txt;https://issues.apache.org/jira/secure/attachment/12938810/plan.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 09 04:30:06 UTC 2018,,,,,,,,,,"0|i3xu9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/18 10:48;yumwang;Can you try master, You provided test case can passed since https://github.com/apache/spark/pull/22205.;;;","08/Sep/18 12:09;lev;[~yumwang] yes, the code worked as expected on the master branch
how do you know that the linked PR is the one that fixes the problem?

Since on version 2.3, it is possible to get incorrect result,
I'm wondering would it make sense to backport that change into the 2.3 branch;;;","08/Sep/18 15:15;yumwang;I think the root cause is [https://github.com/apache/spark/pull/20155.] I will fix it soon.;;;","09/Sep/18 04:29;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22368;;;","09/Sep/18 04:30;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22368;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema pruning doesn't work if nested column is used in where clause,SPARK-25363,13183553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,07/Sep/18 05:04,21/Sep/18 07:14,13/Jul/23 08:48,12/Sep/18 17:44,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Schema pruning doesn't work if nested column is used in where clause.

For example,
{code}
sql(""select name.first from contacts where name.first = 'David'"")

== Physical Plan ==
*(1) Project [name#19.first AS first#40]
+- *(1) Filter (isnotnull(name#19) && (name#19.first = David))
   +- *(1) FileScan parquet [name#19] Batched: false, Format: Parquet, PartitionFilters: [], 
    PushedFilters: [IsNotNull(name)], ReadSchema: struct<name:struct<first:string,middle:string,last:string>>
{code}

In above query plan, the scan node reads the entire schema of `name` column.

This issue is reported by:
https://github.com/apache/spark/pull/21320#issuecomment-419290197",,apachespark,dbtsai,maropu,Tagar,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 12 17:44:21 UTC 2018,,,,,,,,,,"0|i3xtvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/18 05:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22357;;;","07/Sep/18 05:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22357;;;","12/Sep/18 17:44;dbtsai;Issue resolved by pull request 22357
[https://github.com/apache/spark/pull/22357];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metadata to SparkPlanInfo to dump more information like file path to event log,SPARK-25357,13183404,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,06/Sep/18 14:03,13/Sep/18 02:03,13/Jul/23 08:48,13/Sep/18 01:59,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,"Field {{metadata}} removed from {{SparkPlanInfo}} in SPARK-17701. Corresponding, this field was also removed from event {{SparkListenerSQLExecutionStart}} in Spark event log. If we want to analyze event log to get some fields which wider than 100 (e.g the Location or ReadSchema of FileScan), they are abbreviated in {{simpleString}} of SparkPlanInfo JSON or {{physicalPlanDescription}} JSON.

Before 2.3, the fragment of SparkListenerSQLExecutionStart in event log (It contains the metadata field):
{quote}Location: InMemoryFileIndex[hdfs://hercules/sys/edw/prs_idm/idm_cbt_am_t/cbt/cbt_acct_prfl_info/snapshot/dt..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<snpsht_start_dt:date,snpsht_end_dt:date,am_ntlogin_name:string,am_first_name:string,am_las..."",""children"":[],""metadata"":{""Location"":""InMemoryFileIndex[hdfs://hercules/sys/edw/prs_idm/idm_cbt_am_t/cbt/cbt_acct_prfl_info/snapshot/dt=20180904]"",""ReadSchema"":""struct<snpsht_start_dt:date,snpsht_end_dt:date,am_ntlogin_name:string,am_first_name:string,am_last_name:string,isg_name:string,acct_isg_stat_desc:string,prmry_user_slctd_id:string,prmry_orcl_id:bigint,acct_cmpny_bsns_lcns_num:string,acct_slctd_id:string,acct_orcl_id:bigint,acct_cmpny_name:string,acct_cmpny_region_txt:string,acct_cmpny_prvnc_txt:string,acct_cmpny_addr_txt:string,acct_type_seg:string,p4_acct_ind:tinyint,i320_acct_ind:tinyint,i463_acct_ind:tinyint,i319_acct_ind:tinyint,acct_cntry:string,acct_stat:string,acct_club_ind:string,acct_src_bd_name:string,acct_prmry_bsns_vrtcl_desc:string,acct_minor_bsns_vrtcl_desc:string,acct_src_desc:string,acct_pre_ams_id:bigint,src_last_mdfd_dt:date,src_last_mdfd_tm:string,CRE_DATE:date,CRE_USER:string,UPD_DATE:timestamp,UPD_USER:string>""
{quote}

So I add this field back to SparkPlanInfo class. Then it will log out the meta data to event log. Intact information in event log is very useful for offline job analysis.",,apachespark,cloud_fan,cltlfcjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 13 01:59:36 UTC 2018,,,,,,,,,,"0|i3xsyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/18 14:09;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22353;;;","13/Sep/18 01:59;cloud_fan;Issue resolved by pull request 22353
[https://github.com/apache/spark/pull/22353];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perform ordered global limit when limit number is bigger than topKSortFallbackThreshold,SPARK-25352,13183292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,06/Sep/18 04:42,05/Aug/19 05:17,13/Jul/23 08:48,12/Sep/18 14:56,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"We have optimization on global limit to evenly distribute limit rows across all partitions. This optimization doesn't work for ordered results.

For a query ending with sort + limit, in most cases it is performed by `TakeOrderedAndProjectExec`.

But if limit number is bigger than `SQLConf.TOP_K_SORT_FALLBACK_THRESHOLD`, global limit will be used. At this moment, we need to do ordered global limit.

",,apachespark,cloud_fan,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 13 05:11:20 UTC 2018,,,,,,,,,,"0|i3xs9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/18 04:52;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22344;;;","12/Sep/18 14:56;cloud_fan;Issue resolved by pull request 22344
[https://github.com/apache/spark/pull/22344];;;","13/Sep/18 05:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22409;;;","13/Sep/18 05:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22409;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Permission issue after upgrade hadoop version to 2.7.7,SPARK-25330,13182749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,04/Sep/18 08:09,25/Jan/20 06:54,13/Jul/23 08:48,25/Jan/20 06:51,2.3.2,2.4.0,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Build,,,,0,,,,,"How to reproduce:
{code:java}
# build spark
./dev/make-distribution.sh --name SPARK-25330 --tgz  -Phadoop-2.7 -Phive -Phive-thriftserver -Pyarn

tar -zxf spark-2.4.0-SNAPSHOT-bin-SPARK-25330.tgz && cd spark-2.4.0-SNAPSHOT-bin-SPARK-25330
export HADOOP_PROXY_USER=user_a
bin/spark-sql

export HADOOP_PROXY_USER=user_b
bin/spark-sql{code}
 
{noformat}
Exception in thread ""main"" java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=user_b, access=EXECUTE, inode=""/tmp/hive-$%7Buser.name%7D/user_b/668748f2-f6c5-4325-a797-fd0a7ee7f4d4"":user_b:hadoop:drwx------
at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)
at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)
at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190){noformat}",,apachespark,brahmareddy,eyang,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25015,,,,,,,,,,,,HADOOP-15722,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 04:42:20 UTC 2018,,,,,,,,,,"0|i3xp2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/18 08:44;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22327;;;","06/Sep/18 00:48;eyang;[~yumwang] Does Hadoop 2.7.5 works?  It might help us to isolate the release that started the regression to isolate the number of JIRAs that Hadoop team needs to go through.  Thanks;;;","06/Sep/18 01:07;yumwang;No. The issue occurred in this commit: [apache/hadoop@{{feb886f}}|https://github.com/apache/hadoop/commit/feb886f2093ea5da0cd09c69bd1360a335335c86].;;;","06/Sep/18 01:08;yumwang;I try to build Hadoop 2.7.7 with [{{Configuration.getRestrictParserDefault(Object resource)}}|https://github.com/apache/hadoop/blob/release-2.7.7-RC0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L236] = true and false.
 It succeeded when {{Configuration.getRestrictParserDefault(Object resource)=false}}, but failed when {{Configuration.getRestrictParserDefault(Object resource)=true}}.;;;","06/Sep/18 12:54;brahmareddy;[~yumwang] is it possible to share debug logs..? or did you notice when *{{getRestrictParserDefault(Object resource)}}* returns *true*. Or do you've script or testcase to reproduce this..want to dig more.
{quote}The issue occurred in this commit: [apache/hadoop@{{feb886f}}|https://github.com/apache/hadoop/commit/feb886f2093ea5da0cd09c69bd1360a335335c86].
{quote}
FYI..There is another commit on top of this which will skip the proxy user check when ugi isn't initialized.[04219e55c8983f88573b10205dbca5411e744b35|https://github.com/apache/hadoop/commit/04219e55c8983f88573b10205dbca5411e744b35]

 

Some more inputs will be helpful.;;;","06/Sep/18 15:14;yumwang;[~brahmareddy] Sorry. I didn't have script because we need a kerberos env to reproduce it. I try to change {{getRestrictParserDefault}} to
{code:java}
    private static boolean getRestrictParserDefault(Object resource) {
      if (resource instanceof String || !UserGroupInformation.isInitialized()) {
        System.err.println(""resource instanceof String."");
        return false;
      } else {
        UserGroupInformation user;
        try {
          System.err.println(""resource: "" + resource);
          System.err.println(""UserGroupInformation.isInitialized(): "" + UserGroupInformation.isInitialized());
          user = UserGroupInformation.getCurrentUser();
          System.err.println(""UserGroupInformation.getCurrentUser(): "" + UserGroupInformation.getCurrentUser().toString());
          System.err.println(""user.getRealUser(): "" + UserGroupInformation.getCurrentUser().getRealUser());
          System.err.println(""user.getRealUser().isFromKeytab(): "" + UserGroupInformation.getCurrentUser().isFromKeytab());
          System.err.println(""user.getRealUser().hasKerberosCredentials(): "" + UserGroupInformation.getCurrentUser().hasKerberosCredentials());
          System.err.println(""user.getLoginUser().isFromKeytab(): "" + UserGroupInformation.getLoginUser().isFromKeytab());
          System.err.println(""user.getLoginUser().getRealUser(): "" + UserGroupInformation.getLoginUser().getRealUser());
          System.err.println(""user.getLoginUser().hasKerberosCredentials(): "" + UserGroupInformation.getLoginUser().hasKerberosCredentials());
          System.err.println(""user.getLoginUser().isFromKeytab(): "" + UserGroupInformation.getLoginUser().isFromKeytab());
        } catch (IOException e) {
          throw new RuntimeException(""Unable to determine current user"", e);
        }
        return user.getRealUser() != null;
      }
    }
{code}
The output is:
{noformat}
...
resource: org.apache.hadoop.hive.conf.LoopingByteArrayInputStream@62e6b5c8
UserGroupInformation.isInitialized(): true
UserGroupInformation.getCurrentUser(): user_b (auth:PROXY) via admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser().isFromKeytab(): false
user.getRealUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
user.getLoginUser().getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getLoginUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
resource: file:/apache/hive/conf/hive-site.xml
UserGroupInformation.isInitialized(): true
UserGroupInformation.getCurrentUser(): user_b (auth:PROXY) via admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser().isFromKeytab(): false
user.getRealUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
user.getLoginUser().getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getLoginUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
18/09/06 07:49:29 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
18/09/06 07:49:29 WARN HiveConf: HiveConf of name hive.metastore.local does not exist
18/09/06 07:49:29 WARN HiveConf: HiveConf of name hive.metastore.ds.retry.attempts does not exist
18/09/06 07:49:29 WARN HiveConf: HiveConf of name hive.metastore.ds.retry.interval does not exist
18/09/06 07:49:29 WARN HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
18/09/06 07:49:29 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/09/06 07:49:30 INFO ObjectStore: ObjectStore, initialize called
18/09/06 07:49:30 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/09/06 07:49:30 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
resource: org.apache.hadoop.hive.conf.LoopingByteArrayInputStream@ae7950d
UserGroupInformation.isInitialized(): true
UserGroupInformation.getCurrentUser(): user_b (auth:PROXY) via admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser().isFromKeytab(): false
user.getRealUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
user.getLoginUser().getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getLoginUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
resource: file:/apache/hive/conf/hive-site.xml
UserGroupInformation.isInitialized(): true
UserGroupInformation.getCurrentUser(): user_b (auth:PROXY) via admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser().isFromKeytab(): false
user.getRealUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
user.getLoginUser().getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getLoginUser().hasKerberosCredentials(): false
user.getLoginUser().isFromKeytab(): false
......
18/09/06 07:23:08 INFO deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name
18/09/06 07:23:09 INFO SessionState: Created HDFS directory: /tmp/hive-${user.name}/user_b
18/09/06 07:23:09 INFO SessionState: Created local directory: /tmp/136f93db-8924-4366-a795-49960acde4d0_resources
Exception in thread ""main"" java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=user_b, access=EXECUTE, inode=""/tmp/hive-$%7Buser.name%7D/user_b/136f93db-8924-4366-a795-49960acde4d0"":user_a:hadoop:drwx------
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
{noformat}
The correct output should be:
{noformat}
......
18/09/06 08:12:04 INFO SessionState: Created local directory: /tmp/799640f8-3d34-4cb7-90fe-5368c22881d5_resources
18/09/06 08:12:04 INFO SessionState: Created HDFS directory: /tmp/hive-admin/user_b/799640f8-3d34-4cb7-90fe-5368c22881d5
18/09/06 08:12:04 INFO SessionState: Created local directory: /tmp/admin/799640f8-3d34-4cb7-90fe-5368c22881d5
18/09/06 08:12:04 INFO SessionState: Created HDFS directory: /tmp/hive-admin/user_b/799640f8-3d34-4cb7-90fe-5368c22881d5/_tmp_space.db
......
{noformat};;;","06/Sep/18 15:20;srowen;[~yumwang] does this affect basically anyone using spark-sql with a proxy user? does it affect just the command line? I'm trying to get a sense of the scope of the impact. Would a normal Spark SQL job work?;;;","06/Sep/18 16:35;eyang;{quote}
user.getRealUser(): admin@KERBEROS.MYCOM.COM (auth:KERBEROS)
user.getRealUser().isFromKeytab(): false
user.getRealUser().hasKerberosCredentials(): false
{quote}

If I am reading this correctly, the RealUser must be from either a keytab or hasKerberosCredentials.  Both can not be false, otherwise, it is a security breach to Kerberos that RealUser was not authorized by KDC.  [~daryn] [~jlowe] thoughts?;;;","07/Sep/18 02:22;yumwang;[~srowen] It affects (Spark enable Hive support). {{spark-sql}}, {{spark-shell}} and {{spark-submit are all possible to using Hive with a proxy user.}}

The StackTrace:
{noformat}
Caused by: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=user_b, access=EXECUTE, inode=""/tmp/hive-$%7Buser.name%7D/user_b/2fa7b2a0-e8fb-4f26-ba96-61c52f0c64ee"":user_a:hadoop:drwx------
   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)
   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)
   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
   at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
   at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:108)
   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3933)
   at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1109)
   at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:851)
   at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
   at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
   at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
   at java.security.AccessController.doPrivileged(Native Method)
   at javax.security.auth.Subject.doAs(Subject.java:422)
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
   at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

  at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
  at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:183)
  at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:272)
  at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)
  at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)
  at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
  at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)
  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
  ... 79 more{noformat};;;","07/Sep/18 02:33;srowen;For clarity, you mean none of those things work with a proxy user? the example above suggests spark-sql does not work with a proxy user, but it works otherwise?;;;","07/Sep/18 03:06;yumwang;It affects Spark enable Hive support with a proxy user.;;;","07/Sep/18 04:42;srowen;Issue resolved by pull request 22327
[https://github.com/apache/spark/pull/22327];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryBlock performance regression,SPARK-25317,13182699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,cloud_fan,cloud_fan,04/Sep/18 00:23,07/Sep/18 19:15,13/Jul/23 08:48,06/Sep/18 07:29,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"eThere is a performance regression when calculating hash code for UTF8String:
{code:java}
  test(""hashing"") {
    import org.apache.spark.unsafe.hash.Murmur3_x86_32
    import org.apache.spark.unsafe.types.UTF8String
    val hasher = new Murmur3_x86_32(0)
    val str = UTF8String.fromString(""b"" * 10001)
    val numIter = 100000
    val start = System.nanoTime
    for (i <- 0 until numIter) {
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
      Murmur3_x86_32.hashUTF8String(str, 0)
    }
    val duration = (System.nanoTime() - start) / 1000 / numIter
    println(s""duration $duration us"")
  }
{code}
To run this test in 2.3, we need to add
{code:java}
public static int hashUTF8String(UTF8String str, int seed) {
    return hashUnsafeBytes(str.getBaseObject(), str.getBaseOffset(), str.numBytes(), seed);
  }
{code}
to `Murmur3_x86_32`

In my laptop, the result for master vs 2.3 is: 120 us vs 40 us",,apachespark,cloud_fan,dongjoon,kabhwan,kiszk,maropu,mgaido,npoggi,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 19:15:06 UTC 2018,,,,,,,,,,"0|i3xorj:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"04/Sep/18 00:23;cloud_fan;cc [~kiszk]  [~rednaxelafx];;;","04/Sep/18 00:38;kiszk;Let me run this on 2.3 and master.
One question. This benchmark does not have an warm up loop. In other words, this benchmark may include execution time on an interpreter, too. Is this behavior intentional?;;;","04/Sep/18 01:05;kabhwan;Why not running test with JMH, applying warmup and iteration? Not sure it can be applied to scala test, but the Java test code should be simple if these Spark classes are aware of interop.;;;","04/Sep/18 03:10;kiszk;I confirmed this performance difference even after adding warmup. Let me investigate furthermore.;;;","04/Sep/18 15:10;mgaido;I think I have a fix for this. I can submit a PR if you want, but I am still not sure about the root cause of the regression. My best guess is that there are more than one reason and the perf improvement happens iff all the reasons are fixed, which is rather strange to me.;;;","05/Sep/18 08:33;kiszk;When I have been investigating this issue, I realized that # of Javabyte code size in a method can change performance. I guess that this issue is related to method inlining. However, I have not found the root cause yet.

[~mgaido] Would it be possible to submit a PR to fix this issue if possible?;;;","05/Sep/18 08:45;mgaido;[~kiszk] sure, we can investigate further in the PR the root cause. Thanks.;;;","05/Sep/18 08:56;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22338;;;","05/Sep/18 08:57;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22338;;;","06/Sep/18 07:29;cloud_fan;Issue resolved by pull request 22338
[https://github.com/apache/spark/pull/22338];;;","07/Sep/18 19:15;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22361;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Invalid PythonUDF - requires attributes from more than one child - in ""on"" join condition",SPARK-25314,13182604,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,bahchis,bahchis,03/Sep/18 10:55,06/Nov/18 12:26,13/Jul/23 08:48,27/Sep/18 07:15,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,PySpark,SQL,,,0,,,,,"This is another variation of the SPARK-19728 which was tagged as resolved. So I base the example on it:

 
{code:java}
from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType
df1 = sc.parallelize([(1, ), (2, )]).toDF([""col_a""])
df2 = sc.parallelize([(2, ), (3, )]).toDF([""col_b""])
pred = udf(lambda x, y: x == y, BooleanType())
df1.join(df2, pred(df1.col_a, df2.col_b)).show()
{code}
 

This throws:
{quote}java.lang.RuntimeException: Invalid PythonUDF <lambda>(col_a#132L, col_b#135L), requires attributes from more than one child.
 at scala.sys.package$.error(package.scala:27)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:182)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:181)
 at scala.collection.immutable.Stream.foreach(Stream.scala:594)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(ExtractPythonUDFs.scala:181)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:118)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:114)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:114)
 at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:94)
 at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:87)
 at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:87)
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
 at scala.collection.immutable.List.foldLeft(List.scala:84)
 at org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:87)
 at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)
 at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)
 at org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1.apply(CacheManager.scala:100)
 at org.apache.spark.sql.execution.CacheManager.writeLock(CacheManager.scala:67)
 at org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:91)
 at org.apache.spark.sql.Dataset.persist(Dataset.scala:2902)
 at org.apache.spark.sql.Dataset.cache(Dataset.scala:2912)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
 at py4j.Gateway.invoke(Gateway.java:282)
 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
 at py4j.commands.CallCommand.execute(CallCommand.java:79)
 at py4j.GatewayConnection.run(GatewayConnection.java:238)
 at java.lang.Thread.run(Thread.java:748)
{quote}",,apachespark,bahchis,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25949,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 07:15:46 UTC 2018,,,,,,,,,,"0|i3xo6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/18 06:53;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/22326;;;","27/Sep/18 07:15;cloud_fan;Issue resolved by pull request 22326
[https://github.com/apache/spark/pull/22326];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix regression in FileFormatWriter output schema,SPARK-25313,13182573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,03/Sep/18 07:12,11/Sep/18 02:11,13/Jul/23 08:48,06/Sep/18 02:39,2.4.0,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,"In the follow example:

        val location = ""/tmp/t""
        val df = spark.range(10).toDF(""id"")
        df.write.format(""parquet"").saveAsTable(""tbl"")
        spark.sql(""CREATE VIEW view1 AS SELECT id FROM tbl"")
        spark.sql(s""CREATE TABLE tbl2(ID long) USING parquet location $location"")
        spark.sql(""INSERT OVERWRITE TABLE tbl2 SELECT ID FROM view1"")
        println(spark.read.parquet(location).schema)
        spark.table(""tbl2"").show()

The output column name in schema will be id instead of ID, thus the last query shows nothing from tbl2.
By enabling the debug message we can see that the output naming is changed from ID to id, and then the outputColumns in InsertIntoHadoopFsRelationCommand is changed in RemoveRedundantAliases.

To guarantee correctness, we should change the output columns from `Seq[Attribute]` to `Seq[String]` to avoid its names being replaced by optimizer.",,apachespark,cloud_fan,Gengliang.Wang,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25135,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 02:11:46 UTC 2018,,,,,,,,,,"0|i3xnzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/18 07:19;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22320;;;","06/Sep/18 02:39;cloud_fan;Issue resolved by pull request 22320
[https://github.com/apache/spark/pull/22320];;;","06/Sep/18 06:44;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22346;;;","07/Sep/18 07:45;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22359;;;","11/Sep/18 02:11;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22387;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArraysOverlap may throw a CompileException,SPARK-25310,13182536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,02/Sep/18 15:49,04/Sep/18 05:00,13/Jul/23 08:48,04/Sep/18 05:00,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Invoking {{ArraysOverlap}} function with non-nullable array type throws the following error in the code generation phase.

{code:java}
Code generation of arrays_overlap([1,2,3], [4,5,3]) failed:
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 56, Column 11: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 56, Column 11: Expression ""isNull_0"" is not an rvalue
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 56, Column 11: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 56, Column 11: Expression ""isNull_0"" is not an rvalue
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1305)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:143)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:48)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1260)
{code}",,apachespark,kiszk,maropu,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 04 05:00:52 UTC 2018,,,,,,,,,,"0|i3xnrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/18 17:30;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22317;;;","04/Sep/18 05:00;ueshin;Issue resolved by pull request 22317
https://github.com/apache/spark/pull/22317;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayContains function may return a error in the code generation phase.,SPARK-25308,13182518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,02/Sep/18 05:28,04/Sep/18 04:30,13/Jul/23 08:48,04/Sep/18 04:30,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Invoking ArrayContains function with non nullable array type throws the following error in the code generation phase.

{code}
Code generation of array_contains([1,2,3], 1) failed:
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 11: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 11: Expression ""isNull_0"" is not an rvalue
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 11: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 11: Expression ""isNull_0"" is not an rvalue
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1305)
{code}",,apachespark,dkbiswal,maropu,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 04 04:30:44 UTC 2018,,,,,,,,,,"0|i3xnnr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/18 05:48;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22315;;;","04/Sep/18 04:30;ueshin;Issue resolved by pull request 22315
https://github.com/apache/spark/pull/22315;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArraySort function may return a error in the code generation phase.,SPARK-25307,13182515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,02/Sep/18 04:02,04/Sep/18 04:40,13/Jul/23 08:48,04/Sep/18 04:40,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Sorting array of booleans (not nullable) returns a compilation error in the code generation phase. Below is the compilation error :
{code:java}
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 23: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 23: No applicable constructor/method found for actual parameters ""boolean[]""; candidates are: ""public static void java.util.Arrays.sort(long[])"", ""public static void java.util.Arrays.sort(long[], int, int)"", ""public static void java.util.Arrays.sort(byte[], int, int)"", ""public static void java.util.Arrays.sort(float[])"", ""public static void java.util.Arrays.sort(float[], int, int)"", ""public static void java.util.Arrays.sort(char[])"", ""public static void java.util.Arrays.sort(char[], int, int)"", ""public static void java.util.Arrays.sort(short[], int, int)"", ""public static void java.util.Arrays.sort(short[])"", ""public static void java.util.Arrays.sort(byte[])"", ""public static void java.util.Arrays.sort(java.lang.Object[], int, int, java.util.Comparator)"", ""public static void java.util.Arrays.sort(java.lang.Object[], java.util.Comparator)"", ""public static void java.util.Arrays.sort(int[])"", ""public static void java.util.Arrays.sort(java.lang.Object[], int, int)"", ""public static void java.util.Arrays.sort(java.lang.Object[])"", ""public static void java.util.Arrays.sort(double[])"", ""public static void java.util.Arrays.sort(double[], int, int)"", ""public static void java.util.Arrays.sort(int[], int, int)""
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1305)

 {code}",,apachespark,dkbiswal,maropu,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 04 04:40:52 UTC 2018,,,,,,,,,,"0|i3xnn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/18 04:08;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22314;;;","04/Sep/18 04:40;ueshin;Issue resolved by pull request 22314
https://github.com/apache/spark/pull/22314;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid skewed filter trees to speed up `createFilter` in ORC,SPARK-25306,13182510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,dongjoon,dongjoon,01/Sep/18 22:21,05/Sep/18 05:17,13/Jul/23 08:48,05/Sep/18 02:24,1.6.3,2.0.2,2.1.3,2.2.2,2.3.1,2.4.0,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In both ORC data sources, createFilter function has exponential time complexity due to its skewed filter tree generation. This PR aims to improve it by using new buildTree function.

*REPRODUCE*
{code}
// Create and read 1 row table with 1000 columns
sql(""set spark.sql.orc.filterPushdown=true"")
val selectExpr = (1 to 1000).map(i => s""id c$i"")
spark.range(1).selectExpr(selectExpr: _*).write.mode(""overwrite"").orc(""/tmp/orc"")
print(s""With 0 filters, "")
spark.time(spark.read.orc(""/tmp/orc"").count)

// Increase the number of filters
(20 to 30).foreach { width =>
  val whereExpr = (1 to width).map(i => s""c$i is not null"").mkString("" and "")
  print(s""With $width filters, "")
  spark.time(spark.read.orc(""/tmp/orc"").where(whereExpr).count)
}
{code}

*RESULT*
{code}
With 0 filters, Time taken: 653 ms                                              
With 20 filters, Time taken: 962 ms
With 21 filters, Time taken: 1282 ms
With 22 filters, Time taken: 1982 ms
With 23 filters, Time taken: 3855 ms
With 24 filters, Time taken: 6719 ms
With 25 filters, Time taken: 12669 ms
With 26 filters, Time taken: 25032 ms
With 27 filters, Time taken: 49585 ms
With 28 filters, Time taken: 98980 ms    // over 1 min 38 seconds
With 29 filters, Time taken: 198368 ms   // over 3 mins
With 30 filters, Time taken: 393744 ms   // over 6 mins
{code}",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 05 05:17:29 UTC 2018,,,,,,,,,,"0|i3xnlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/18 22:36;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22313;;;","05/Sep/18 02:24;cloud_fan;Issue resolved by pull request 22313
[https://github.com/apache/spark/pull/22313];;;","05/Sep/18 05:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22336;;;","05/Sep/18 05:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22336;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pod names conflicts in client mode, if previous submission was not a clean shutdown.",SPARK-25295,13182291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,prashant,prashant,31/Aug/18 06:17,17/May/20 18:26,13/Jul/23 08:48,13/Sep/18 05:08,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"If the previous job was killed somehow, by disconnecting the client. It leaves behind the executor pods named spark-exec-#, which cause naming conflicts and failures for the next job submission.

io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://<ip>:6443/api/v1/namespaces/default/pods. Message: pods ""spark-exec-4"" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=spark-exec-4, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods ""spark-exec-4"" already exists, metadata=ListMeta(resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).
",,apachespark,dongjoon,liyinan926,prashant,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25148,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 08 15:57:39 UTC 2018,,,,,,,,,,"0|i3xm9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/18 15:29;skonto;guys I started working on a short fix.;;;","12/Sep/18 19:58;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/22405;;;","08/Oct/18 15:57;dongjoon;Hi, [~liyinan926] .

Please update the assignee together when you resolve the issue. :D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flakiness of tests in terms of executor memory (SecretsTestSuite),SPARK-25291,13182282,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ifilonenko,ifilonenko,ifilonenko,31/Aug/18 04:31,17/May/20 18:23,13/Jul/23 08:48,18/Sep/18 18:49,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"SecretsTestSuite shows flakiness in terms of correct setting of executor memory: 

Run SparkPi with env and mount secrets. *** FAILED ***
 ""[884]Mi"" did not equal ""[1408]Mi"" (KubernetesSuite.scala:272)

When ran with default settings ",,apachespark,felixcheung,ifilonenko,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 17 14:40:04 UTC 2018,,,,,,,,,,"0|i3xm7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/18 00:32;skonto;[~ifilonenko] I can have a look a bit weird, but kind of expected as these tests are the only ones that use fabric8io client to connect to the running pod. There is no good way to do this right now.

I will try to debug it.;;;","13/Sep/18 00:52;ifilonenko;[~skonto] the problem stems from the executors not being created in time for the reading of logs. As such, the tests fail. As such, it is required that we block on executor creation via a Watcher and only read the logs when executors are up. In essence:

 val executorPods = kubernetesTestComponents.kubernetesClient
 .pods()
 .withLabel(""spark-app-locator"", appLocator)
 .withLabel(""spark-role"", ""executor"")
 .list()
 .getItems
 executorPods.asScala.foreach { pod =>
 executorPodChecker(pod)
 }

runs after the spark-submit command and it takes an arbitrary period of time for the executors to get spun up. If the k8s client which is reading the executor logs returns 0 pods it won't check over the executor pods. As such, this flakiness occurs when the executor pod isn't always checking the executor pods that are made. In terms of the flakiness for the PySpark tests it seems that the executor pods are setting: .set(""spark.executor.memory"", ""500m"") in the SparkConf and as such are expecting 884 instead of the 1408. So that error seems to be related to the PySpark test framework. ;;;","17/Sep/18 14:40;apachespark;User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22415;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChiSqSelector max on empty collection,SPARK-25289,13182277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mabeaulieu,mabeaulieu,31/Aug/18 02:57,01/Sep/18 13:42,13/Jul/23 08:48,01/Sep/18 13:42,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,MLlib,,,,0,,,,,"In org.apache.spark.mllib.feature.ChiSqSelector.fit, there is a max taken on a possibly empty collection.

I am using Spark 2.3.1.

Here is an example to reproduce.
{code:java}
import org.apache.spark.mllib.feature.ChiSqSelector
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)
implicit val spark = sqlContext.sparkSession

val labeledPoints = (0 to 1).map(n => {
  val v = Vectors.dense((1 to 3).map(_ => n * 1.0).toArray)
  LabeledPoint(n.toDouble, v)
})
val rdd = sc.parallelize(labeledPoints)
val selector = new ChiSqSelector().setSelectorType(""fdr"").setFdr(0.05)
selector.fit(rdd){code}
Here is the stack trace:
{code:java}
java.lang.UnsupportedOperationException: empty.max
at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)
at scala.collection.mutable.ArrayOps$ofInt.max(ArrayOps.scala:234)
at org.apache.spark.mllib.feature.ChiSqSelector.fit(ChiSqSelector.scala:280)
{code}
Looking at line 280 in ChiSqSelector, it's pretty obvious how the collection can be empty. A simple non empty validation should do the trick.",,apachespark,mabeaulieu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 01 13:42:13 UTC 2018,,,,,,,,,,"0|i3xm6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/18 10:57;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22303;;;","01/Sep/18 13:42;srowen;Issue resolved by pull request 22303
[https://github.com/apache/spark/pull/22303];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka transaction tests are flaky,SPARK-25288,13182244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,30/Aug/18 21:43,31/Aug/18 06:23,13/Jul/23 08:48,31/Aug/18 06:23,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Tests,,,,0,,,,,"http://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.kafka010.KafkaRelationSuite&test_name=read+Kafka+transactional+messages%3A+read_committed
http://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceSuite&test_name=read+Kafka+transactional+messages%3A+read_committed
http://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceSuite&test_name=read+Kafka+transactional+messages%3A+read_committed",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 30 21:48:04 UTC 2018,,,,,,,,,,"0|i3xlz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/18 21:48;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22293;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A deadlock in UnionRDD,SPARK-25283,13182110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,30/Aug/18 12:26,08/Oct/18 15:59,13/Jul/23 08:48,31/Aug/18 21:38,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"The PR https://github.com/apache/spark/pull/21913 replaced Scala parallel collections in UnionRDD by new parmap function. This changes cause a deadlock in the partitions method. The code demonstrates the problem:
{code:scala}
    val wide = 20
    def unionRDD(num: Int): UnionRDD[Int] = {
      val rdds = (0 until num).map(_ => sc.parallelize(1 to 10, 1))
      new UnionRDD(sc, rdds)
    }
    val level0 = (0 until wide).map { _ =>
      val level1 = (0 until wide).map(_ => unionRDD(wide))
      new UnionRDD(sc, level1)
    }
    val rdd = new UnionRDD(sc, level0)

    rdd.partitions.length
{code}
",,apachespark,maxgekk,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25286,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 31 21:38:16 UTC 2018,,,,,,,,,,"0|i3xl5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/18 12:35;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22283;;;","31/Aug/18 21:38;maxgekk;It is fixed by the PR: https://github.com/apache/spark/pull/22292;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of output rows metric of union of views is multiplied by their occurrences,SPARK-25278,13182055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,jlaskowski,jlaskowski,30/Aug/18 07:32,27/Dec/18 13:10,13/Jul/23 08:48,10/Sep/18 11:43,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"When you use a view in a union multiple times (self-union), the {{number of output rows}} metric seems to be the correct {{number of output rows}} multiplied by the occurrences of the view, e.g.
{code:java}
scala> spark.version
res0: String = 2.3.1

val name = ""demo_view""
sql(s""CREATE OR REPLACE VIEW $name AS VALUES 1,2"")
assert(spark.catalog.tableExists(name))

val view = spark.table(name)

assert(view.count == 2)

view.union(view).show // gives 4 for every view (as a LocalTableScan), but should be 2
view.union(view).union(view).show // gives 6{code}
I think it's because {{View}} logical operator is a {{MultiInstanceRelation}} (and think other {{MultiInstanceRelations}} may also be affected).",,apachespark,cloud_fan,jlaskowski,maropu,mgaido,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/18 07:32;jlaskowski;union-2-views.png;https://issues.apache.org/jira/secure/attachment/12937724/union-2-views.png","30/Aug/18 07:32;jlaskowski;union-3-views.png;https://issues.apache.org/jira/secure/attachment/12937725/union-3-views.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 10 12:26:23 UTC 2018,,,,,,,,,,"0|i3xktb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/18 14:38;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22284;;;","10/Sep/18 11:43;cloud_fan;Issue resolved by pull request 22284
[https://github.com/apache/spark/pull/22284];;;","10/Sep/18 12:26;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22380;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN applicationMaster metrics should not register static and JVM metrics,SPARK-25277,13182053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lucacanali,lucacanali,lucacanali,30/Aug/18 07:20,17/May/20 18:13,13/Jul/23 08:48,13/Dec/18 00:18,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,,,,,,,,,,,2.4.5,3.0.0,,,Spark Core,YARN,,,0,,,,,"YARN applicationMaster metrics registration introduced in SPARK-24594 causes further registration of static metrics (Codegenerator and HiveExternalCatalog) and of JVM metrics, which I believe do not belong in this context.",,apachespark,githubbot,lucacanali,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24594,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 13 00:21:57 UTC 2018,,,,,,,,,,"0|i3xksv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/18 07:22;lucacanali;This looks like an unintended side effect of using the start method of [[MetricsSystem]].
A possible solution is to introduce startNoRegisterSources to avoid these additional registrations of static sources and of JVM sources in the case of YARN applicationMaster metrics (this could be useful for other metrics that may be added in the future).;;;","30/Aug/18 07:28;apachespark;User 'LucaCanali' has created a pull request for this issue:
https://github.com/apache/spark/pull/22279;;;","10/Dec/18 23:58;githubbot;vanzin commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446020969
 
 
   ok to test

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Dec/18 23:59;githubbot;AmplabJenkins removed a comment on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-417219160
 
 
   Can one of the admins verify this patch?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 00:02;githubbot;SparkQA commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446021730
 
 
   **[Test build #99939 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99939/testReport)** for PR 22279 at commit [`a990758`](https://github.com/apache/spark/commit/a99075873fa1519fe07344eb90d5c8af02b9d7e5).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 00:02;githubbot;AmplabJenkins commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446021727
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 00:02;githubbot;AmplabJenkins commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446021735
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5944/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 00:03;githubbot;AmplabJenkins removed a comment on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446021727
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 00:03;githubbot;AmplabJenkins removed a comment on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446021735
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5944/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:20;githubbot;SparkQA commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446067213
 
 
   **[Test build #99939 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99939/testReport)** for PR 22279 at commit [`a990758`](https://github.com/apache/spark/commit/a99075873fa1519fe07344eb90d5c8af02b9d7e5).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds no public classes.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:21;githubbot;SparkQA removed a comment on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446021730
 
 
   **[Test build #99939 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99939/testReport)** for PR 22279 at commit [`a990758`](https://github.com/apache/spark/commit/a99075873fa1519fe07344eb90d5c8af02b9d7e5).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:22;githubbot;AmplabJenkins commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446067495
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:22;githubbot;AmplabJenkins commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446067496
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99939/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:23;githubbot;AmplabJenkins removed a comment on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446067495
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 04:23;githubbot;AmplabJenkins removed a comment on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446067496
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99939/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 07:40;githubbot;LucaCanali commented on issue #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#issuecomment-446102201
 
 
   Thanks @vanzin for looking at this.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 18:57;githubbot;vanzin commented on a change in pull request #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279#discussion_r240746967
 
 

 ##########
 File path: core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
 ##########
 @@ -103,6 +103,14 @@ private[spark] class MetricsSystem private (
     sinks.foreach(_.start)
   }
 
+  // Same as start but this method only registers sinks
 
 Review comment:
   Just to close the loop here: I like the extra flag to `start` instead of the separate method.
   
   As for AM vs. driver, the way I understand things, in cluster mode, there will be two `MetricSystem` instances - one for the AM and one for the driver, so you shouldn't lose any driver metrics.
   
   JVM metrics for the client-mode AM can be interesting, but that's generally not a source of problems that I've noticed, so we can probably punt on it for now.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Dec/18 00:18;vanzin;Issue resolved by pull request 22279
[https://github.com/apache/spark/pull/22279];;;","13/Dec/18 00:21;githubbot;asfgit closed pull request #22279: [SPARK-25277][YARN] YARN applicationMaster metrics should not register static metrics
URL: https://github.com/apache/spark/pull/22279
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala b/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
index 3457a2632277d..1894033bbbf14 100644
--- a/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
+++ b/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala
@@ -94,11 +94,13 @@ private[spark] class MetricsSystem private (
 
   metricsConfig.initialize()
 
-  def start() {
+  def start(registerStaticSources: Boolean = true) {
     require(!running, ""Attempting to start a MetricsSystem that is already running"")
     running = true
-    StaticSources.allSources.foreach(registerSource)
-    registerSources()
+    if (registerStaticSources) {
+      StaticSources.allSources.foreach(registerSource)
+      registerSources()
+    }
     registerSinks()
     sinks.foreach(_.start)
   }
diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala
index 55ed114f8500f..5be67d31cf6ae 100644
--- a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala
@@ -450,7 +450,8 @@ private[spark] class ApplicationMaster(args: ApplicationMasterArguments) extends
     val ms = MetricsSystem.createMetricsSystem(""applicationMaster"", sparkConf, securityMgr)
     val prefix = _sparkConf.get(YARN_METRICS_NAMESPACE).getOrElse(appId)
     ms.registerSource(new ApplicationMasterSource(prefix, allocator))
-    ms.start()
+    // do not register static sources in this case as per SPARK-25277
+    ms.start(false)
     metricsSystem = Some(ms)
     reporterThread = launchReporterThread()
   }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating parquet table with all the column null throws exception,SPARK-25271,13181915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,shivusondur@gmail.com,shivusondur@gmail.com,29/Aug/18 13:08,12/Dec/22 18:10,13/Jul/23 08:48,20/Dec/18 02:50,2.3.1,,,,,,,,,,,,,,,,,2.4.8,3.0.0,,,SQL,,,,1,,,,,"
{code:java}
 1)cat /data/parquet.dat

1$abc2$pqr:3$xyz
null{code}
 


{code:java}
2)spark.sql(""create table vp_reader_temp (projects map<int, string>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY ':' MAP KEYS TERMINATED BY '$'"")
{code}

{code:java}
3)spark.sql(""
LOAD DATA LOCAL INPATH '/data/parquet.dat' INTO TABLE vp_reader_temp"")
{code}

{code:java}
4)spark.sql(""create table vp_reader STORED AS PARQUET as select * from vp_reader_temp"")
{code}


*Result :* Throwing exception (Working fine with spark 2.2.1)

{code:java}
java.lang.RuntimeException: Parquet record is malformed: empty fields are illegal, the field should be ommited completely instead
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:64)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:59)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:31)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:180)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:46)
	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:112)
	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:125)
	at org.apache.spark.sql.hive.execution.HiveOutputWriter.write(HiveFileFormat.scala:149)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:406)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:283)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:281)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1438)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:286)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:349)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endField(MessageColumnIO.java:320)
	at org.apache.parquet.io.RecordConsumerLoggingWrapper.endField(RecordConsumerLoggingWrapper.java:165)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeMap(DataWritableWriter.java:241)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeValue(DataWritableWriter.java:116)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeGroupFields(DataWritableWriter.java:89)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:60)
	... 21 more
{code}
",,apachespark,bomeng,cloud_fan,Daimon,dongjoon,githubbot,maropu,Mr.黄,S71955,shahid,shivusondur@gmail.com,viirya,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29295,,,,,,,,,SPARK-26437,,,,"07/Sep/18 03:42;shivusondur@gmail.com;image-2018-09-07-09-12-34-944.png;https://issues.apache.org/jira/secure/attachment/12938756/image-2018-09-07-09-12-34-944.png","07/Sep/18 03:59;shivusondur@gmail.com;image-2018-09-07-09-29-33-370.png;https://issues.apache.org/jira/secure/attachment/12938757/image-2018-09-07-09-29-33-370.png","07/Sep/18 03:59;shivusondur@gmail.com;image-2018-09-07-09-29-52-899.png;https://issues.apache.org/jira/secure/attachment/12938758/image-2018-09-07-09-29-52-899.png","07/Sep/18 04:02;shivusondur@gmail.com;image-2018-09-07-09-32-43-892.png;https://issues.apache.org/jira/secure/attachment/12938760/image-2018-09-07-09-32-43-892.png","07/Sep/18 04:03;shivusondur@gmail.com;image-2018-09-07-09-33-03-095.png;https://issues.apache.org/jira/secure/attachment/12938761/image-2018-09-07-09-33-03-095.png",,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 18 03:11:52 UTC 2022,,,,,,,,,,"0|i3xjyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/18 13:10;shivusondur@gmail.com;cc [~cloud_fan];;;","30/Aug/18 00:07;viirya;I think this is a known issue on Hive and Parquet, some context can be found at https://issues.apache.org/jira/browse/HIVE-11625.

It can be reproduced by:
{code:java}
sql(""create table vp_reader STORED AS PARQUET as select map() as a"")
18/08/30 00:07:15 ERROR DataWritableWriter: Parquet record is malformed: empty fields are illegal, the field should be ommited completely instead
parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead                                          
...
{code}
If you don't store it as Parquet format, it can work:
{code:java}
sql(""create table vp_reader STORED AS ORC as select map() as a"")
sql(""select * from vp_reader"").show
+---+
|  a|
+---+
| []|
+---+
{code};;;","30/Aug/18 07:17;shivusondur@gmail.com; [~viirya], It works fine in Spark 2.2.1 version. Below is the details
{code:java}
c:\spark-2.2.1-bin-hadoop2.7>bin\spark-shell
 Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
 Setting default log level to ""WARN"".
 To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
 Spark context Web UI available at http://localhost:4040
 Spark context available as 'sc' (master = local[*], app id = local-1535611823064).
 Spark session available as 'spark'.
 Welcome to
 ____ __
 / _/_ ___ ____/ /_
 \ \/ _ \/ _ `/ __/ '/
 /__/ ./_,// //_\ version 2.2.1
 /_/
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
 Type in expressions to have them evaluated.
 Type :help for more information.
scala> spark.sql(""create table vp_reader_temp (projects map<int, string>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY ':' MAP KEYS TERMINATED BY '$'"")
 res0: org.apache.spark.sql.DataFrame = []
scala> spark.sql(""LOAD DATA LOCAL INPATH 'parquetReader' INTO TABLE vp_reader_temp"")
 res1: org.apache.spark.sql.DataFrame = []
scala> spark.sql(""create table vp_reader STORED AS PARQUET as select * from vp_reader_temp"")
 res2: org.apache.spark.sql.DataFrame = []
scala> spark.sql(""select * from vp_reader"").collect
 res3: Array[org.apache.spark.sql.Row] = Array([Map(1 -> abc, 2 -> pqr, 3 -> xyz)], [Map()])
scala>
{code}
 ;;;","04/Sep/18 14:25;shivusondur@gmail.com;After further analyzing the issue i got following details

In  SingleDirectoryWriteTask private class(org.apache.spark.sql.execution.datasources.FileFormatWriter File) , currentWriter is  initialized with different outputWriter in spark-2.2.1 and spar-2.3.1, as shown below. 


{code:java}
Spark-2.3.1= currentWriter is initilized with ""HiveOutputWriter""
Spark-2.2.1= currentWriter is initilized with ""ParquetOutputWriter""
{code}


So ParquetOutputWriter may be handling the null/empty values.;;;","04/Sep/18 14:41;S71955;[~cloud_fan] [~sowen]  Will this cause a compatibility problem compare to older version, If user has  null record ,then he is getting an exception with the current version where as the older version of spark(2.2.1)  wont throw any exception.

I think the Output writers has been updated in the below PR

[https://github.com/apache/spark/pull/20521];;;","04/Sep/18 14:41;S71955;cc [~hyukjin.kwon];;;","06/Sep/18 08:21;gurwls223;will take a look later but mind if I ask to elaborate

{quote}
I think the Output writers has been updated in the below PR

https://github.com/apache/spark/pull/20521
{quote}

? Sounds rather a corner case but still a regression.;;;","07/Sep/18 04:13;shivusondur@gmail.com;As [~S71955] told, The Behaviour changed from the [https://github.com/apache/spark/pull/20521]

While debugging ""spark.sql(""create table vp_reader STORED AS PARQUET as select * from vp_reader_temp"")""    I found following  details.
 
{code:java}
In Spark-2.2.1 It is using the ""InsertIntoTable""(org.apache.spark.sql.hive.execution.createHiveTableAsSelectCommand.run()) which will use ParquetFileFormat as shown below snaps{code}

1 Figure: It uses  ""InsertIntoTable"" for plan generation

!image-2018-09-07-09-29-33-370.png!       

2 Figure: It is using the ""ParquetFileFormat""  as fileformat

!image-2018-09-07-09-29-52-899.png! 

{code:java}
But in Spark-2.3.1, It is using the ""InsertIntoHiveTable"", (org.apache.spark.sql.hive.execution.createHiveTableAsSelectCommand.run()) which will use HiveFileFormat as shown below snap's{code}
  
3 Figure: It uses ""InsertIntoHiveTable"" for plan generation
!image-2018-09-07-09-32-43-892.png! 


4 Figure: It is using the ""HiveFileFormat"" as fileformat
!image-2018-09-07-09-33-03-095.png!

cc [~hyukjin.kwon] Let me know any further clarification

 ;;;","12/Sep/18 03:37;viirya;Yeah, looks like after some changes, this kind of queries now uses Hive's record writer. So it inherits the issue in Hive.;;;","21/Sep/18 06:27;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22514;;;","11/Dec/18 08:11;githubbot;viirya commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446109671
 
 
   Synced with master.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:12;githubbot;SparkQA commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446109785
 
 
   **[Test build #99958 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)** for PR 22514 at commit [`ef52536`](https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:17;githubbot;AmplabJenkins commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446111230
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:17;githubbot;AmplabJenkins commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446111236
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5961/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:18;githubbot;AmplabJenkins removed a comment on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446111230
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:18;githubbot;AmplabJenkins removed a comment on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446111236
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5961/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:27;githubbot;cloud-fan commented on a change in pull request #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#discussion_r240510001
 
 

 ##########
 File path: sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
 ##########
 @@ -97,9 +77,118 @@ case class CreateHiveTableAsSelectCommand(
     Seq.empty[Row]
   }
 
+  // Returns `DataWritingCommand` used to write data when the table exists.
+  def writingCommandForExistingTable(
+    catalog: SessionCatalog,
+    tableDesc: CatalogTable): DataWritingCommand
+
+  // Returns `DataWritingCommand` used to write data when the table doesn't exist.
+  def writingCommandForNewTable(
+    catalog: SessionCatalog,
+    tableDesc: CatalogTable): DataWritingCommand
+
   override def argString: String = {
     s""[Database:${tableDesc.database}, "" +
     s""TableName: ${tableDesc.identifier.table}, "" +
     s""InsertIntoHiveTable]""
   }
 }
+
+/**
+ * Create table and insert the query result into it.
+ *
+ * @param tableDesc the table description, which may contain serde, storage handler etc.
+ * @param query the query whose result will be insert into the new relation
+ * @param mode SaveMode
+ */
+case class CreateHiveTableAsSelectCommand(
+    tableDesc: CatalogTable,
+    query: LogicalPlan,
+    outputColumnNames: Seq[String],
+    mode: SaveMode)
+  extends CreateHiveTableAsSelectBase {
+
+  override def writingCommandForExistingTable(
+      catalog: SessionCatalog,
+      tableDesc: CatalogTable): DataWritingCommand = {
+    // For CTAS, there is no static partition values to insert.
+    val partition = tableDesc.partitionColumnNames.map(_ -> None).toMap
+    InsertIntoHiveTable(
+      tableDesc,
+      partition,
+      query,
+      overwrite = false,
 
 Review comment:
   so the only difference is this parameter?
   
   Now I feel maybe it's better to define only one `getWritingCommand` in `CreateHiveTableAsSelectBase`.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:31;githubbot;cloud-fan commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446114955
 
 
   To be safe, let's add a `HiveUtils.CONVERT_METASTORE_CTAS` with default value true in this PR. It's also a good practice to have fine-grained optimization flags. I think migration guide is not needed here.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:34;githubbot;viirya commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446115800
 
 
   I see, we have discussed before. Is it good to add it here or a follow-up?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:35;githubbot;viirya commented on a change in pull request #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#discussion_r240512341
 
 

 ##########
 File path: sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
 ##########
 @@ -97,9 +77,118 @@ case class CreateHiveTableAsSelectCommand(
     Seq.empty[Row]
   }
 
+  // Returns `DataWritingCommand` used to write data when the table exists.
+  def writingCommandForExistingTable(
+    catalog: SessionCatalog,
+    tableDesc: CatalogTable): DataWritingCommand
+
+  // Returns `DataWritingCommand` used to write data when the table doesn't exist.
+  def writingCommandForNewTable(
+    catalog: SessionCatalog,
+    tableDesc: CatalogTable): DataWritingCommand
+
   override def argString: String = {
     s""[Database:${tableDesc.database}, "" +
     s""TableName: ${tableDesc.identifier.table}, "" +
     s""InsertIntoHiveTable]""
   }
 }
+
+/**
+ * Create table and insert the query result into it.
+ *
+ * @param tableDesc the table description, which may contain serde, storage handler etc.
+ * @param query the query whose result will be insert into the new relation
+ * @param mode SaveMode
+ */
+case class CreateHiveTableAsSelectCommand(
+    tableDesc: CatalogTable,
+    query: LogicalPlan,
+    outputColumnNames: Seq[String],
+    mode: SaveMode)
+  extends CreateHiveTableAsSelectBase {
+
+  override def writingCommandForExistingTable(
+      catalog: SessionCatalog,
+      tableDesc: CatalogTable): DataWritingCommand = {
+    // For CTAS, there is no static partition values to insert.
+    val partition = tableDesc.partitionColumnNames.map(_ -> None).toMap
+    InsertIntoHiveTable(
+      tableDesc,
+      partition,
+      query,
+      overwrite = false,
 
 Review comment:
   ha, ok.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 08:40;githubbot;cloud-fan commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446117432
 
 
   Seems like a trivial change, let's do it in this PR.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:12;githubbot;SparkQA commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446181366
 
 
   **[Test build #99958 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)** for PR 22514 at commit [`ef52536`](https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217).
    * This patch passes all tests.
    * This patch merges cleanly.
    * This patch adds the following public classes _(experimental)_:
     * `sealed trait SingleValueExecutorMetricType extends ExecutorMetricType `
     * `class GBTClassifierParams(GBTParams, HasVarianceImpurity):`
     * `class GBTClassifier(JavaEstimator, HasFeaturesCol, HasLabelCol, HasPredictionCol,`
     * `class HasDistanceMeasure(Params):`
     * `class HasValidationIndicatorCol(Params):`
     * `class HasVarianceImpurity(Params):`
     * `class TreeRegressorParams(HasVarianceImpurity):`
     * `class GBTParams(TreeEnsembleParams, HasMaxIter, HasStepSize, HasValidationIndicatorCol):`
     * `class GBTRegressorParams(GBTParams, TreeRegressorParams):`
     * `class GBTRegressor(JavaEstimator, HasFeaturesCol, HasLabelCol, HasPredictionCol,`
     * `class ArrowCollectSerializer(Serializer):`
     * `class CSVInferSchema(val options: CSVOptions) extends Serializable `
     * `class InterpretedSafeProjection(expressions: Seq[Expression]) extends Projection `
     * `sealed trait DateTimeFormatter `
     * `class Iso8601DateTimeFormatter(`
     * `class LegacyDateTimeFormatter(`
     * `class LegacyFallbackDateTimeFormatter(`
     * `sealed trait DateFormatter `
     * `class Iso8601DateFormatter(`
     * `class LegacyDateFormatter(`
     * `class LegacyFallbackDateFormatter(`
     * `case class ArrowEvalPython(`
     * `case class BatchEvalPython(`

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:13;githubbot;SparkQA removed a comment on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446109785
 
 
   **[Test build #99958 has started](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)** for PR 22514 at commit [`ef52536`](https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:14;githubbot;AmplabJenkins commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446181832
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:14;githubbot;AmplabJenkins commented on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446181838
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99958/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:15;githubbot;AmplabJenkins removed a comment on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446181832
 
 
   Merged build finished. Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Dec/18 12:15;githubbot;AmplabJenkins removed a comment on issue #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514#issuecomment-446181838
 
 
   Test PASSed.
   Refer to this link for build results (access rights to CI server needed): 
   https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99958/
   Test PASSed.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Dec/18 02:50;cloud_fan;Issue resolved by pull request 22514
[https://github.com/apache/spark/pull/22514];;;","20/Dec/18 02:50;githubbot;asfgit closed pull request #22514: [SPARK-25271][SQL] Hive ctas commands should use data source if it is convertible
URL: https://github.com/apache/spark/pull/22514
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala
index e1faecedd20ed..096481f68275d 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala
@@ -820,6 +820,14 @@ object DDLUtils {
     table.provider.isDefined && table.provider.get.toLowerCase(Locale.ROOT) != HIVE_PROVIDER
   }
 
+  def readHiveTable(table: CatalogTable): HiveTableRelation = {
+    HiveTableRelation(
+      table,
+      // Hive table columns are always nullable.
+      table.dataSchema.asNullable.toAttributes,
+      table.partitionSchema.asNullable.toAttributes)
+  }
+
   /**
    * Throws a standard error for actions that require partitionProvider = hive.
    */
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
index b304e2da6e1cf..b5cf8c9515bfb 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
@@ -244,27 +244,19 @@ class FindDataSourceTable(sparkSession: SparkSession) extends Rule[LogicalPlan]
     })
   }
 
-  private def readHiveTable(table: CatalogTable): LogicalPlan = {
-    HiveTableRelation(
-      table,
-      // Hive table columns are always nullable.
-      table.dataSchema.asNullable.toAttributes,
-      table.partitionSchema.asNullable.toAttributes)
-  }
-
   override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {
     case i @ InsertIntoTable(UnresolvedCatalogRelation(tableMeta), _, _, _, _)
         if DDLUtils.isDatasourceTable(tableMeta) =>
       i.copy(table = readDataSourceTable(tableMeta))
 
     case i @ InsertIntoTable(UnresolvedCatalogRelation(tableMeta), _, _, _, _) =>
-      i.copy(table = readHiveTable(tableMeta))
+      i.copy(table = DDLUtils.readHiveTable(tableMeta))
 
     case UnresolvedCatalogRelation(tableMeta) if DDLUtils.isDatasourceTable(tableMeta) =>
       readDataSourceTable(tableMeta)
 
     case UnresolvedCatalogRelation(tableMeta) =>
-      readHiveTable(tableMeta)
+      DDLUtils.readHiveTable(tableMeta)
   }
 }
 
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
index 5823548a8063c..03f4b8d83e353 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
@@ -17,6 +17,8 @@
 
 package org.apache.spark.sql.hive
 
+import java.util.Locale
+
 import scala.util.control.NonFatal
 
 import com.google.common.util.concurrent.Striped
@@ -29,6 +31,8 @@ import org.apache.spark.sql.catalyst.{QualifiedTableName, TableIdentifier}
 import org.apache.spark.sql.catalyst.catalog._
 import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.execution.datasources._
+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetOptions}
+import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.internal.SQLConf.HiveCaseSensitiveInferenceMode._
 import org.apache.spark.sql.types._
 
@@ -113,7 +117,44 @@ private[hive] class HiveMetastoreCatalog(sparkSession: SparkSession) extends Log
     }
   }
 
-  def convertToLogicalRelation(
+  // Return true for Apache ORC and Hive ORC-related configuration names.
+  // Note that Spark doesn't support configurations like `hive.merge.orcfile.stripe.level`.
+  private def isOrcProperty(key: String) =
+    key.startsWith(""orc."") || key.contains("".orc."")
+
+  private def isParquetProperty(key: String) =
+    key.startsWith(""parquet."") || key.contains("".parquet."")
+
+  def convert(relation: HiveTableRelation): LogicalRelation = {
+    val serde = relation.tableMeta.storage.serde.getOrElse("""").toLowerCase(Locale.ROOT)
+
+    // Consider table and storage properties. For properties existing in both sides, storage
+    // properties will supersede table properties.
+    if (serde.contains(""parquet"")) {
+      val options = relation.tableMeta.properties.filterKeys(isParquetProperty) ++
+        relation.tableMeta.storage.properties + (ParquetOptions.MERGE_SCHEMA ->
+        SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING).toString)
+        convertToLogicalRelation(relation, options, classOf[ParquetFileFormat], ""parquet"")
+    } else {
+      val options = relation.tableMeta.properties.filterKeys(isOrcProperty) ++
+        relation.tableMeta.storage.properties
+      if (SQLConf.get.getConf(SQLConf.ORC_IMPLEMENTATION) == ""native"") {
+        convertToLogicalRelation(
+          relation,
+          options,
+          classOf[org.apache.spark.sql.execution.datasources.orc.OrcFileFormat],
+          ""orc"")
+      } else {
+        convertToLogicalRelation(
+          relation,
+          options,
+          classOf[org.apache.spark.sql.hive.orc.OrcFileFormat],
+          ""orc"")
+      }
+    }
+  }
+
+  private def convertToLogicalRelation(
       relation: HiveTableRelation,
       options: Map[String, String],
       fileFormatClass: Class[_ <: FileFormat],
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala
index 07ee105404311..8a5ab188a949f 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala
@@ -31,8 +31,7 @@ import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoDir, InsertIntoTab
 import org.apache.spark.sql.catalyst.rules.Rule
 import org.apache.spark.sql.execution._
 import org.apache.spark.sql.execution.command.{CreateTableCommand, DDLUtils}
-import org.apache.spark.sql.execution.datasources.{CreateTable, LogicalRelation}
-import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetOptions}
+import org.apache.spark.sql.execution.datasources.CreateTable
 import org.apache.spark.sql.hive.execution._
 import org.apache.spark.sql.internal.{HiveSerDe, SQLConf}
 
@@ -181,49 +180,17 @@ case class RelationConversions(
     conf: SQLConf,
     sessionCatalog: HiveSessionCatalog) extends Rule[LogicalPlan] {
   private def isConvertible(relation: HiveTableRelation): Boolean = {
-    val serde = relation.tableMeta.storage.serde.getOrElse("""").toLowerCase(Locale.ROOT)
-    serde.contains(""parquet"") && conf.getConf(HiveUtils.CONVERT_METASTORE_PARQUET) ||
-      serde.contains(""orc"") && conf.getConf(HiveUtils.CONVERT_METASTORE_ORC)
+    isConvertible(relation.tableMeta)
   }
 
-  // Return true for Apache ORC and Hive ORC-related configuration names.
-  // Note that Spark doesn't support configurations like `hive.merge.orcfile.stripe.level`.
-  private def isOrcProperty(key: String) =
-    key.startsWith(""orc."") || key.contains("".orc."")
-
-  private def isParquetProperty(key: String) =
-    key.startsWith(""parquet."") || key.contains("".parquet."")
-
-  private def convert(relation: HiveTableRelation): LogicalRelation = {
-    val serde = relation.tableMeta.storage.serde.getOrElse("""").toLowerCase(Locale.ROOT)
-
-    // Consider table and storage properties. For properties existing in both sides, storage
-    // properties will supersede table properties.
-    if (serde.contains(""parquet"")) {
-      val options = relation.tableMeta.properties.filterKeys(isParquetProperty) ++
-        relation.tableMeta.storage.properties + (ParquetOptions.MERGE_SCHEMA ->
-        conf.getConf(HiveUtils.CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING).toString)
-      sessionCatalog.metastoreCatalog
-        .convertToLogicalRelation(relation, options, classOf[ParquetFileFormat], ""parquet"")
-    } else {
-      val options = relation.tableMeta.properties.filterKeys(isOrcProperty) ++
-        relation.tableMeta.storage.properties
-      if (conf.getConf(SQLConf.ORC_IMPLEMENTATION) == ""native"") {
-        sessionCatalog.metastoreCatalog.convertToLogicalRelation(
-          relation,
-          options,
-          classOf[org.apache.spark.sql.execution.datasources.orc.OrcFileFormat],
-          ""orc"")
-      } else {
-        sessionCatalog.metastoreCatalog.convertToLogicalRelation(
-          relation,
-          options,
-          classOf[org.apache.spark.sql.hive.orc.OrcFileFormat],
-          ""orc"")
-      }
-    }
+  private def isConvertible(tableMeta: CatalogTable): Boolean = {
+    val serde = tableMeta.storage.serde.getOrElse("""").toLowerCase(Locale.ROOT)
+    serde.contains(""parquet"") && SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_PARQUET) ||
+      serde.contains(""orc"") && SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_ORC)
   }
 
+  private val metastoreCatalog = sessionCatalog.metastoreCatalog
+
   override def apply(plan: LogicalPlan): LogicalPlan = {
     plan resolveOperators {
       // Write path
@@ -231,12 +198,21 @@ case class RelationConversions(
         // Inserting into partitioned table is not supported in Parquet/Orc data source (yet).
           if query.resolved && DDLUtils.isHiveTable(r.tableMeta) &&
             !r.isPartitioned && isConvertible(r) =>
-        InsertIntoTable(convert(r), partition, query, overwrite, ifPartitionNotExists)
+        InsertIntoTable(metastoreCatalog.convert(r), partition,
+          query, overwrite, ifPartitionNotExists)
 
       // Read path
       case relation: HiveTableRelation
           if DDLUtils.isHiveTable(relation.tableMeta) && isConvertible(relation) =>
-        convert(relation)
+        metastoreCatalog.convert(relation)
+
+      // CTAS
+      case CreateTable(tableDesc, mode, Some(query))
+          if DDLUtils.isHiveTable(tableDesc) && tableDesc.partitionColumnNames.isEmpty &&
+            isConvertible(tableDesc) && SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_CTAS) =>
+        DDLUtils.checkDataColNames(tableDesc)
+        OptimizedCreateHiveTableAsSelectCommand(
+          tableDesc, query, query.output.map(_.name), mode)
     }
   }
 }
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala
index 66067704195dd..b60d4c71f5941 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala
@@ -110,6 +110,14 @@ private[spark] object HiveUtils extends Logging {
     .booleanConf
     .createWithDefault(true)
 
+  val CONVERT_METASTORE_CTAS = buildConf(""spark.sql.hive.convertMetastoreCtas"")
+    .doc(""When set to true,  Spark will try to use built-in data source writer "" +
+      ""instead of Hive serde in CTAS. This flag is effective only if "" +
+      ""`spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is "" +
+      ""enabled respectively for Parquet and ORC formats"")
+    .booleanConf
+    .createWithDefault(true)
+
   val HIVE_METASTORE_SHARED_PREFIXES = buildConf(""spark.sql.hive.metastore.sharedPrefixes"")
     .doc(""A comma separated list of class prefixes that should be loaded using the classloader "" +
       ""that is shared between Spark SQL and a specific version of Hive. An example of classes "" +
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
index fd1e931ee0c7a..608f21e726259 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
@@ -20,32 +20,26 @@ package org.apache.spark.sql.hive.execution
 import scala.util.control.NonFatal
 
 import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}
-import org.apache.spark.sql.catalyst.catalog.CatalogTable
-import org.apache.spark.sql.catalyst.expressions.Attribute
+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, SessionCatalog}
 import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
 import org.apache.spark.sql.execution.SparkPlan
-import org.apache.spark.sql.execution.command.DataWritingCommand
+import org.apache.spark.sql.execution.command.{DataWritingCommand, DDLUtils}
+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, InsertIntoHadoopFsRelationCommand, LogicalRelation}
+import org.apache.spark.sql.hive.HiveSessionCatalog
 
+trait CreateHiveTableAsSelectBase extends DataWritingCommand {
+  val tableDesc: CatalogTable
+  val query: LogicalPlan
+  val outputColumnNames: Seq[String]
+  val mode: SaveMode
 
-/**
- * Create table and insert the query result into it.
- *
- * @param tableDesc the Table Describe, which may contain serde, storage handler etc.
- * @param query the query whose result will be insert into the new relation
- * @param mode SaveMode
- */
-case class CreateHiveTableAsSelectCommand(
-    tableDesc: CatalogTable,
-    query: LogicalPlan,
-    outputColumnNames: Seq[String],
-    mode: SaveMode)
-  extends DataWritingCommand {
-
-  private val tableIdentifier = tableDesc.identifier
+  protected val tableIdentifier = tableDesc.identifier
 
   override def run(sparkSession: SparkSession, child: SparkPlan): Seq[Row] = {
     val catalog = sparkSession.sessionState.catalog
-    if (catalog.tableExists(tableIdentifier)) {
+    val tableExists = catalog.tableExists(tableIdentifier)
+
+    if (tableExists) {
       assert(mode != SaveMode.Overwrite,
         s""Expect the table $tableIdentifier has been dropped when the save mode is Overwrite"")
 
@@ -57,15 +51,8 @@ case class CreateHiveTableAsSelectCommand(
         return Seq.empty
       }
 
-      // For CTAS, there is no static partition values to insert.
-      val partition = tableDesc.partitionColumnNames.map(_ -> None).toMap
-      InsertIntoHiveTable(
-        tableDesc,
-        partition,
-        query,
-        overwrite = false,
-        ifPartitionNotExists = false,
-        outputColumnNames = outputColumnNames).run(sparkSession, child)
+      val command = getWritingCommand(catalog, tableDesc, tableExists = true)
+      command.run(sparkSession, child)
     } else {
       // TODO ideally, we should get the output data ready first and then
       // add the relation into catalog, just in case of failure occurs while data
@@ -77,15 +64,8 @@ case class CreateHiveTableAsSelectCommand(
       try {
         // Read back the metadata of the table which was created just now.
         val createdTableMeta = catalog.getTableMetadata(tableDesc.identifier)
-        // For CTAS, there is no static partition values to insert.
-        val partition = createdTableMeta.partitionColumnNames.map(_ -> None).toMap
-        InsertIntoHiveTable(
-          createdTableMeta,
-          partition,
-          query,
-          overwrite = true,
-          ifPartitionNotExists = false,
-          outputColumnNames = outputColumnNames).run(sparkSession, child)
+        val command = getWritingCommand(catalog, createdTableMeta, tableExists = false)
+        command.run(sparkSession, child)
       } catch {
         case NonFatal(e) =>
           // drop the created table.
@@ -97,9 +77,89 @@ case class CreateHiveTableAsSelectCommand(
     Seq.empty[Row]
   }
 
+  // Returns `DataWritingCommand` which actually writes data into the table.
+  def getWritingCommand(
+    catalog: SessionCatalog,
+    tableDesc: CatalogTable,
+    tableExists: Boolean): DataWritingCommand
+
   override def argString: String = {
     s""[Database:${tableDesc.database}, "" +
     s""TableName: ${tableDesc.identifier.table}, "" +
     s""InsertIntoHiveTable]""
   }
 }
+
+/**
+ * Create table and insert the query result into it.
+ *
+ * @param tableDesc the table description, which may contain serde, storage handler etc.
+ * @param query the query whose result will be insert into the new relation
+ * @param mode SaveMode
+ */
+case class CreateHiveTableAsSelectCommand(
+    tableDesc: CatalogTable,
+    query: LogicalPlan,
+    outputColumnNames: Seq[String],
+    mode: SaveMode)
+  extends CreateHiveTableAsSelectBase {
+
+  override def getWritingCommand(
+      catalog: SessionCatalog,
+      tableDesc: CatalogTable,
+      tableExists: Boolean): DataWritingCommand = {
+    // For CTAS, there is no static partition values to insert.
+    val partition = tableDesc.partitionColumnNames.map(_ -> None).toMap
+    InsertIntoHiveTable(
+      tableDesc,
+      partition,
+      query,
+      overwrite = if (tableExists) false else true,
+      ifPartitionNotExists = false,
+      outputColumnNames = outputColumnNames)
+  }
+}
+
+/**
+ * Create table and insert the query result into it. This creates Hive table but inserts
+ * the query result into it by using data source.
+ *
+ * @param tableDesc the table description, which may contain serde, storage handler etc.
+ * @param query the query whose result will be insert into the new relation
+ * @param mode SaveMode
+ */
+case class OptimizedCreateHiveTableAsSelectCommand(
+    tableDesc: CatalogTable,
+    query: LogicalPlan,
+    outputColumnNames: Seq[String],
+    mode: SaveMode)
+  extends CreateHiveTableAsSelectBase {
+
+  override def getWritingCommand(
+      catalog: SessionCatalog,
+      tableDesc: CatalogTable,
+      tableExists: Boolean): DataWritingCommand = {
+    val metastoreCatalog = catalog.asInstanceOf[HiveSessionCatalog].metastoreCatalog
+    val hiveTable = DDLUtils.readHiveTable(tableDesc)
+
+    val hadoopRelation = metastoreCatalog.convert(hiveTable) match {
+      case LogicalRelation(t: HadoopFsRelation, _, _, _) => t
+      case _ => throw new AnalysisException(s""$tableIdentifier should be converted to "" +
+        ""HadoopFsRelation."")
+    }
+
+    InsertIntoHadoopFsRelationCommand(
+      hadoopRelation.location.rootPaths.head,
+      Map.empty, // We don't support to convert partitioned table.
+      false,
+      Seq.empty, // We don't support to convert partitioned table.
+      hadoopRelation.bucketSpec,
+      hadoopRelation.fileFormat,
+      hadoopRelation.options,
+      query,
+      if (tableExists) mode else SaveMode.Overwrite,
+      Some(tableDesc),
+      Some(hadoopRelation.location),
+      query.output.map(_.name))
+  }
+}
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala
index e5c9df05d5674..470c6a342b4dd 100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala
@@ -92,4 +92,18 @@ class HiveParquetSuite extends QueryTest with ParquetTest with TestHiveSingleton
       }
     }
   }
+
+  test(""SPARK-25271: write empty map into hive parquet table"") {
+    import testImplicits._
+
+    Seq(Map(1 -> ""a""), Map.empty[Int, String]).toDF(""m"").createOrReplaceTempView(""p"")
+    withTempView(""p"") {
+      val targetTable = ""targetTable""
+      withTable(targetTable) {
+        sql(s""CREATE TABLE $targetTable STORED AS PARQUET AS SELECT m FROM p"")
+        checkAnswer(sql(s""SELECT m FROM $targetTable""),
+          Row(Map(1 -> ""a"")) :: Row(Map.empty[Int, String]) :: Nil)
+      }
+    }
+  }
 }
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala
index fab2a27cdef17..6acf44606cbbe 100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala
@@ -2276,6 +2276,46 @@ class SQLQuerySuite extends QueryTest with SQLTestUtils with TestHiveSingleton {
     }
   }
 
+  test(""SPARK-25271: Hive ctas commands should use data source if it is convertible"") {
+    withTempView(""p"") {
+      Seq(1, 2, 3).toDF(""id"").createOrReplaceTempView(""p"")
+
+      Seq(""orc"", ""parquet"").foreach { format =>
+        Seq(true, false).foreach { isConverted =>
+          withSQLConf(
+            HiveUtils.CONVERT_METASTORE_ORC.key -> s""$isConverted"",
+            HiveUtils.CONVERT_METASTORE_PARQUET.key -> s""$isConverted"") {
+            Seq(true, false).foreach { isConvertedCtas =>
+              withSQLConf(HiveUtils.CONVERT_METASTORE_CTAS.key -> s""$isConvertedCtas"") {
+
+                val targetTable = ""targetTable""
+                withTable(targetTable) {
+                  val df = sql(s""CREATE TABLE $targetTable STORED AS $format AS SELECT id FROM p"")
+                  checkAnswer(sql(s""SELECT id FROM $targetTable""),
+                    Row(1) :: Row(2) :: Row(3) :: Nil)
+
+                  val ctasDSCommand = df.queryExecution.analyzed.collect {
+                    case _: OptimizedCreateHiveTableAsSelectCommand => true
+                  }.headOption
+                  val ctasCommand = df.queryExecution.analyzed.collect {
+                    case _: CreateHiveTableAsSelectCommand => true
+                  }.headOption
+
+                  if (isConverted && isConvertedCtas) {
+                    assert(ctasDSCommand.nonEmpty)
+                    assert(ctasCommand.isEmpty)
+                  } else {
+                    assert(ctasDSCommand.isEmpty)
+                    assert(ctasCommand.nonEmpty)
+                  }
+                }
+              }
+            }
+          }
+        }
+      }
+    }
+  }
 
   test(""SPARK-26181 hasMinMaxStats method of ColumnStatsMap is not correct"") {
     withSQLConf(SQLConf.CBO_ENABLED.key -> ""true"") {


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Oct/20 16:29;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30017;;;","18/Feb/22 02:58;Mr.黄;[~viirya] [~apachespark] Hello, I did not have this problem in Spark2.4.0-CDH6.3.2 version, but this problem was repeated in Spark2.4.3 version, I do not understand why the lower version succeeded and the higher version failed, I would like to ask whether the fix of this bug does not support the 2.4.3 version? The following is my version information and error message:
{code:java}
spark version: 2.4.0-cdh6.3.2
hive version: 2.1.1-cdh.6.3.2
scala> spark.sql(""create table test STORED AS PARQUET as select map() as a"")
scala> sql(""select * from test"").show
+---+                                                                           
|  a|
+---+
| []|
+---+

-----------------------------------------------------------------------------------------------------------------
spark version: 2.4.3
hive version: 3.1.2
scala> spark.sql(""create table test STORED AS PARQUET as select map() as a"")

Caused by: org.apache.spark.SparkException: Task failed while writing rows.
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:121)
  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Parquet record is malformed: empty fields are illegal, the field should be ommited completely instead
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:64)
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:59)
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:31)
  at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:121)
  at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:123)
  at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:42)
  at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:111)
  at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:124)
  at org.apache.spark.sql.hive.execution.HiveOutputWriter.write(HiveFileFormat.scala:149)
  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:137)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:245)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
  ... 10 more
Caused by: parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
  at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endField(MessageColumnIO.java:244)
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeMap(DataWritableWriter.java:241)
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeValue(DataWritableWriter.java:116)
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeGroupFields(DataWritableWriter.java:89)
  at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:60)
  ... 23 more
{code}
 ;;;","18/Feb/22 03:11;viirya;Based on this JIRA, we only have this fix since 2.4.8.

I guess 2.4.0-cdh6.3.2 may backport the fix as this is a distribution maintained by the vendor, I don't know about the detail.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
runParallelPersonalizedPageRank throws serialization Exception,SPARK-25268,13181786,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shahid,bago.amirbekian,bago.amirbekian,29/Aug/18 04:51,21/Sep/18 07:14,13/Jul/23 08:48,06/Sep/18 16:53,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,GraphX,,,,0,,,,,"A recent change to PageRank introduced a bug in the ParallelPersonalizedPageRank implementation. The change prevents serialization of a Map which needs to be broadcast to all workers. The issue is in this line here: [https://github.com/apache/spark/blob/6c5cb85856235efd464b109558896f81ae2c4c75/graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala#L201]

Because graphx units tests are run in local mode, the Serialization issue is not caught.

 
{code:java}
[info] - Star example parallel personalized PageRank *** FAILED *** (2 seconds, 160 milliseconds)
[info] java.io.NotSerializableException: scala.collection.immutable.MapLike$$anon$2
[info] Serialization stack:
[info] - object not serializable (class: scala.collection.immutable.MapLike$$anon$2, value: Map(1 -> SparseVector(3)((0,1.0)), 2 -> SparseVector(3)((1,1.0)), 3 -> SparseVector(3)((2,1.0))))
[info] at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
[info] at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)
[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)
[info] at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1348)
[info] at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:292)
[info] at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:127)
[info] at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)
[info] at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
[info] at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
[info] at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)
[info] at org.apache.spark.graphx.lib.PageRank$.runParallelPersonalizedPageRank(PageRank.scala:205)
[info] at org.apache.spark.graphx.lib.GraphXHelpers$.runParallelPersonalizedPageRank(GraphXHelpers.scala:31)
[info] at org.graphframes.lib.ParallelPersonalizedPageRank$.run(ParallelPersonalizedPageRank.scala:115)
[info] at org.graphframes.lib.ParallelPersonalizedPageRank.run(ParallelPersonalizedPageRank.scala:84)
[info] at org.graphframes.lib.ParallelPersonalizedPageRankSuite$$anonfun$2.apply$mcV$sp(ParallelPersonalizedPageRankSuite.scala:62)
[info] at org.graphframes.lib.ParallelPersonalizedPageRankSuite$$anonfun$2.apply(ParallelPersonalizedPageRankSuite.scala:51)
[info] at org.graphframes.lib.ParallelPersonalizedPageRankSuite$$anonfun$2.apply(ParallelPersonalizedPageRankSuite.scala:51)
[info] at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info] at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] at org.scalatest.Transformer.apply(Transformer.scala:22)
[info] at org.scalatest.Transformer.apply(Transformer.scala:20)
[info] at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info] at org.graphframes.SparkFunSuite.withFixture(SparkFunSuite.scala:40)
[info] at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info] at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info] at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info] at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info] at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info] at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info] at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info] at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info] at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info] at scala.collection.immutable.List.foreach(List.scala:383)
[info] at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info] at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info] at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info] at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info] at org.scalatest.Suite$class.run(Suite.scala:1424)
[info] at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info] at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info] at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info] at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info] at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info] at org.graphframes.lib.ParallelPersonalizedPageRankSuite.org$scalatest$BeforeAndAfterAll$$super$run(ParallelPersonalizedPageRankSuite.scala:27)
[info] at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
[info] at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
[info] at org.graphframes.lib.ParallelPersonalizedPageRankSuite.run(ParallelPersonalizedPageRankSuite.scala:27)
[info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
[info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
[info] at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info] at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info] at java.lang.Thread.run(Thread.java:745){code}",,apachespark,bago.amirbekian,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25149,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 06 16:53:42 UTC 2018,,,,,,,,,,"0|i3xj5r:",9223372036854775807,,,,,josephkb,,,,,,,,2.4.0,,,,,,,,,,"29/Aug/18 18:16;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/22271;;;","06/Sep/18 16:53;josephkb;Issue resolved by pull request 22271
[https://github.com/apache/spark/pull/22271];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix memory leak in Barrier Execution Mode,SPARK-25266,13181674,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sarutak,sarutak,28/Aug/18 17:44,02/Sep/18 23:06,13/Jul/23 08:48,29/Aug/18 14:13,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Scheduler,Spark Core,,,0,,,,,"BarrierCoordinator uses Timer and TimerTask. `TimerTask#cancel()` is invoked in ContextBarrierState#cancelTimerTask but `Timer#purge()` is never invoked.

Once a TimerTask is scheduled, the reference to it is not released until `Timer#purge()` is invoked even though `TimerTask#cancel()` is invoked.

 ",,apachespark,mengxr,riza,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25265,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-24374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 29 14:13:30 UTC 2018,,,,,,,,,,"0|i3xigv:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"28/Aug/18 17:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/22258;;;","29/Aug/18 14:13;mengxr;Issue resolved by pull request 22258
[https://github.com/apache/spark/pull/22258];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix comma-delineated arguments passed into PythonRunner and RRunner,SPARK-25264,13181661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ifilonenko,ifilonenko,28/Aug/18 16:52,17/May/20 18:25,13/Jul/23 08:48,31/Aug/18 22:48,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,PySpark,Spark Core,,0,,,,,"The arguments passed into the PythonRunner and RRunner are comma-delineated. 

Because the Runners do a arg.slice(2,...) This means that the delineation in the entrypoint needs to be a space, as it would be expected by the Runner arguments. 

This issue was logged here: [https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/273]",,apachespark,ifilonenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 28 17:08:06 UTC 2018,,,,,,,,,,"0|i3xidz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/18 17:08;apachespark;User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22257;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition with tasks running when new attempt for same stage is created leads to other task in the next attempt running on the same partition id retry multiple times,SPARK-25250,13181392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pgandhi,pgandhi,pgandhi,27/Aug/18 15:03,14/Jul/22 18:31,13/Jul/23 08:48,16/Apr/19 07:03,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,Scheduler,Spark Core,,,1,,,,,"We recently had a scenario where a race condition occurred when a task from previous stage attempt just finished before new attempt for the same stage was created due to fetch failure, so the new task created in the second attempt on the same partition id was retrying multiple times due to TaskCommitDenied Exception without realizing that the task in earlier attempt was already successful.  

For example, consider a task with partition id 9000 and index 9000 running in stage 4.0. We see a fetch failure so thus, we spawn a new stage attempt 4.1. Just within this timespan, the above task completes successfully, thus, marking the partition id 9000 as complete for 4.0. However, as stage 4.1 has not yet been created, the taskset info for that stage is not available to the TaskScheduler so, naturally, the partition id 9000 has not been marked completed for 4.1. Stage 4.1 now spawns task with index 2000 on the same partition id 9000. This task fails due to CommitDeniedException and since, it does not see the corresponding partition id as been marked successful, it keeps retrying multiple times until the job finally succeeds. It doesn't cause any job failures because the DAG scheduler is tracking the partitions separate from the task set managers.

 

Steps to Reproduce:
 # Run any large job involving shuffle operation.
 # When the ShuffleMap stage finishes and the ResultStage begins running, cause this stage to throw a fetch failure exception(Try deleting certain shuffle files on any host).
 # Observe the task attempt numbers for the next stage attempt. Please note that this issue is an intermittent one, so it might not happen all the time.",,apachespark,cloud_fan,devaraj,Dhruve Ashar,dongjoon,irashid,mdeady,pgandhi,rajeshhadoop,riza,roczei,tgraves,viirya,XuanYuan,,,,,,,,,,,,,,,,,SPARK-26634,,,,,,SPARK-24622,,,SPARK-23433,SPARK-27065,,,,,,,,,"14/Jul/22 18:25;dongjoon;Screen Shot 2022-07-14 at 11.24.58 AM.png;https://issues.apache.org/jira/secure/attachment/13046778/Screen+Shot+2022-07-14+at+11.24.58+AM.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 18:30:15 UTC 2022,,,,,,,,,,"0|i3xgqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/18 22:07;tgraves;We are hitting a race condition here between the taskSetManager and the DAGScheduler.  

There is code in the TaskSetManager that is supposed to mark all task attempts for a partition completed when any of them succeed, but in this case the second attempt has been finished. There is also code that only starts tasks in the second attempt that have not yet finished, but again there is a race here between when the taskSetManager sends the message that the task has ended and when it starts a new stage attempt.

 In the example given above stage 4.0 has fetch failed, it reran the map stage, task 9000 for partition 9000 in stage 4.0 finishes and sends a taskEnded messages to DAGSCheduler, before the DAGScheduler processed that task finished, it calculates the tasks needed for stage 4.1 which included the task for partition 9000, so it runs a task for partition 9000 but it always just fails with commitDenied and continues to rerun that task.

when task 9000 for stage 4.0 finished the taskSetManager calls into sched.markPartitionCompletedInAllTaskSets but since the 4.1 stage attempt hadn't been created yet it didn't mark the task 2000 for that partition as completed and since the DAGScheduler hadn't processed the taskEnd event for it, when it started stage 4.1 it started a task when it didn't need to.  We need to figure out how to handle the race.;;;","23/Sep/18 22:50;pgandhi;Currently working on this issue, will file a pull request soon. Thank you.;;;","23/Oct/18 14:37;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/22806;;;","23/Oct/18 14:37;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/22806;;;","22/Feb/19 09:59;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/23871;;;","22/Feb/19 14:25;pgandhi;[~Ngone51] I understand that you had a proposal and we were actively discussing on various solutions in the PR #22806 , but however, I have been working on that PR tirelessly for a few months and we still have an ongoing discussion going on there. Any specific reasons as to why did you create your own PR for the same issue? WDYT [~irashid] [~cloud_fan] ?;;;","07/Mar/19 14:22;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/24006;;;","07/Mar/19 14:22;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/24006;;;","07/Mar/19 14:25;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/24007;;;","07/Mar/19 14:26;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/24007;;;","14/Apr/19 09:04;cloud_fan;The commit has been reverted.;;;","15/Apr/19 15:11;tgraves;[~cloud_fan] can you please add details as to where and why this was reverted?

 

This went into multiple branches 2.4.1 has already been released so I don't necessarily agree with a revert here.  I would prefer to see another bug since its been released already.;;;","15/Apr/19 19:28;irashid;I agree about opening a new jira.

Wenchen discussed reverting it here: https://github.com/apache/spark/pull/24359
I agree we made a major mistake in that fix. I don't care too much about how the commits look in git, I am fine with having a revert followed by a different fix, rather than rolling it into one change.

;;;","16/Apr/19 07:03;cloud_fan;good points! I'm creating another ticket for the issue, and leave this as resolved.;;;","11/Dec/19 17:47;mdeady;[~cloud_fan] Has another ticket been opened for this? If so can you share link please? I am interested to know the plans for releasing fix for this bug.;;;","16/Dec/19 13:47;cloud_fan;It's https://issues.apache.org/jira/browse/SPARK-27474;;;","14/Jul/22 18:30;dongjoon;To sync with the comment and commit history, I removed 2.4.1 and 2.3.4 from the Fix Version of this JIRA.

Please note that SPARK-27474 is also fixed at Spark 3.0.0+.

 

!Screen Shot 2022-07-14 at 11.24.58 AM.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A deadlock in ALTER TABLE RECOVER PARTITIONS,SPARK-25240,13181126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maxgekk,maxgekk,maxgekk,25/Aug/18 13:36,28/Aug/18 18:29,13/Jul/23 08:48,28/Aug/18 18:29,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Recover Partitions in ALTER TABLE is performed in recursive way by calling the scanPartitions() method. scanPartitions() lists files sequentially or in parallel if the [condition|https://github.com/apache/spark/blob/131ca146ed390cd0109cd6e8c95b61e418507080/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala#L685] is true:
{code:scala}
partitionNames.length > 1 && statuses.length > threshold || partitionNames.length > 2
{code}
Parallel listening is executed on [the fixed thread pool|https://github.com/apache/spark/blob/131ca146ed390cd0109cd6e8c95b61e418507080/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala#L622] which can have 8 threads in total. Dead lock occurs when all 8 cores have been already occupied and recursive call of scanPartitions() submits new parallel file listening.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 25 13:49:05 UTC 2018,,,,,,,,,,"0|i3xf3b:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"25/Aug/18 13:49;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22233;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileScanRdd's inputMetrics is wrong  when select the datasource table with limit,SPARK-25237,13181109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,doozer,doozer,25/Aug/18 06:54,26/Dec/19 08:04,13/Jul/23 08:48,07/Sep/18 04:45,2.2.2,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In FileScanRdd, we will update inputMetrics's bytesRead using updateBytesRead  every 1000 rows or when close the iterator.

but when close the iterator,  we will invoke updateBytesReadWithFileSize to increase the inputMetrics's bytesRead with file's length.

this will result in the inputMetrics's bytesRead is wrong when run the query with limit such as select * from table limit 1.

because we do not support for Hadoop 2.5 and earlier now, we always get the bytesRead from  Hadoop FileSystem statistics other than files's length.

 ",,apachespark,doozer,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 04:45:54 UTC 2018,,,,,,,,,,"0|i3xezj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/18 06:57;apachespark;User 'dujunling' has created a pull request for this issue:
https://github.com/apache/spark/pull/22232;;;","04/Sep/18 02:07;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/22324;;;","04/Sep/18 02:07;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/22324;;;","07/Sep/18 04:45;srowen;Issue resolved by pull request 22324
[https://github.com/apache/spark/pull/22324];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running a Large Job with Speculation On Causes Executor Heartbeats to Time Out on Driver,SPARK-25231,13180993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pgandhi,pgandhi,pgandhi,24/Aug/18 15:14,05/Sep/18 21:11,13/Jul/23 08:48,05/Sep/18 21:11,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Scheduler,Spark Core,,,0,,,,,"Running a large Spark job with speculation turned on was causing executor heartbeats to time out on the driver end after sometime and eventually, after hitting the max number of executor failures, the job would fail. ",,apachespark,nadavw,pgandhi,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 15:34:06 UTC 2018,,,,,,,,,,"0|i3xe9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/18 15:34;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/22221;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DEPLOY] Consistent trailing whitespace treatment of conf values,SPARK-25221,13180863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jira.shegalov,jira.shegalov,jira.shegalov,24/Aug/18 02:44,11/Sep/18 16:29,13/Jul/23 08:48,11/Sep/18 16:29,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Deploy,,,,0,,,,,When you use a custom line delimiter {{spark.hadoop.textinputformat.record.delimiter}} that has a leading or a trailing whitespace character it's only possible when specified via  {{--conf}} . Our pipeline consists of a highly customized generated jobs. Storing all the config in a properities file is not only better for readability but even necessary to avoid dealing with {{ARGS_MAX}} on different OS. Spark should uniformly avoid trimming conf values in both cases. ,,apachespark,jerryshao,jira.shegalov,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 16:29:07 UTC 2018,,,,,,,,,,"0|i3xdgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/18 03:15;apachespark;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/22213;;;","27/Aug/18 02:22;jerryshao;I'm going to remove the target version, I don't think it is a critical/blocker issue, committers will set the proper fix version when merged.;;;","11/Sep/18 16:29;vanzin;Issue resolved by pull request 22213
[https://github.com/apache/spark/pull/22213];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential resource leaks in TransportServer and SocketAuthHelper,SPARK-25218,13180819,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,23/Aug/18 20:13,28/Aug/18 15:36,13/Jul/23 08:48,28/Aug/18 15:36,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,They don't release the resources for all types of errors.,,apachespark,riza,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 20:16:05 UTC 2018,,,,,,,,,,"0|i3xd73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/18 20:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22210;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka v2 source may return duplicated records when `failOnDataLoss` is `false`,SPARK-25214,13180779,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,23/Aug/18 17:25,24/Aug/18 21:45,13/Jul/23 08:48,24/Aug/18 20:58,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,correctness,,,,"When there are missing offsets, Kafka v2 source may return duplicated records when failOnDataLoss=false because it doesn't skip missing offsets.

",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 21:45:05 UTC 2018,,,,,,,,,,"0|i3xcy7:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"23/Aug/18 18:18;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22207;;;","24/Aug/18 21:45;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22230;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong records are returned when Hive metastore schema and parquet schema are in different letter cases,SPARK-25206,13180652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,yucai,yucai,23/Aug/18 07:40,12/Dec/22 18:11,13/Jul/23 08:48,31/Oct/18 16:22,2.2.0,2.2.2,2.3.1,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,correctness,known_issue,Parquet,,"In current Spark 2.3.1, below query returns wrong data silently.
{code:java}
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")

scala> sql(""select * from t where id > 0"").show
+---+
| ID|
+---+
+---+

{code}
 

*Root Cause*

After deep dive, it has two issues, both are related to different letter cases between Hive metastore schema and parquet schema.

1. Wrong column is pushdown.

Spark pushdowns FilterApi.gt(intColumn(""{color:#ff0000}ID{color}""), 0: Integer) into parquet, but {color:#ff0000}ID{color} does not exist in /tmp/data (parquet is case sensitive, it has {color:#ff0000}id{color} actually).
So no records are returned.

Since SPARK-24716, Spark uses Parquet schema instead of Hive metastore schema to do the pushdown, perfect for this issue.

2. Spark SQL returns NULL for a column whose Hive metastore schema and Parquet schema are in different letter cases, even spark.sql.caseSensitive set to false.

SPARK-25132 addressed this issue already.

 

The biggest difference is, in Spark 2.1, user will get Exception for the same query:
{code:java}
Caused by: java.lang.IllegalArgumentException: Column [ID] was not found in schema!{code}
So they will know the issue and fix the query.

But in Spark 2.3, user will get the wrong results sliently.

 

To make the above query work, we need both SPARK-25132 and -SPARK-24716.-

 

[~yumwang] , [~cloud_fan], [~smilegator], any thoughts? Should we backport it?",,bomeng,cloud_fan,dongjoon,jerryshao,maropu,rajeshhadoop,rezasafi,smilegator,vanzin,yucai,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25207,SPARK-24716,SPARK-25132,,,,,,,,"24/Aug/18 10:05;yucai;image-2018-08-24-18-05-23-485.png;https://issues.apache.org/jira/secure/attachment/12936980/image-2018-08-24-18-05-23-485.png","24/Aug/18 14:33;yucai;image-2018-08-24-22-33-03-231.png;https://issues.apache.org/jira/secure/attachment/12937016/image-2018-08-24-22-33-03-231.png","24/Aug/18 14:34;yucai;image-2018-08-24-22-34-11-539.png;https://issues.apache.org/jira/secure/attachment/12937017/image-2018-08-24-22-34-11-539.png","24/Aug/18 14:46;yucai;image-2018-08-24-22-46-05-346.png;https://issues.apache.org/jira/secure/attachment/12937018/image-2018-08-24-22-46-05-346.png","25/Aug/18 01:54;yucai;image-2018-08-25-09-54-53-219.png;https://issues.apache.org/jira/secure/attachment/12937104/image-2018-08-25-09-54-53-219.png","25/Aug/18 02:04;yucai;image-2018-08-25-10-04-21-901.png;https://issues.apache.org/jira/secure/attachment/12937107/image-2018-08-25-10-04-21-901.png","24/Aug/18 14:25;yucai;pr22183.png;https://issues.apache.org/jira/secure/attachment/12937015/pr22183.png",,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 31 16:22:42 UTC 2018,,,,,,,,,,"0|i3xc67:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"23/Aug/18 08:41;cloud_fan;SGTM. We should backport it with a regression test, and also add the test to the master branch.;;;","23/Aug/18 09:18;yumwang;OK, I'll backport it.;;;","23/Aug/18 16:32;vanzin;Updating to blocker given recent discussions about correctness bugs.;;;","24/Aug/18 14:47;yucai;[~cloud_fan] , we need both [https://github.com/apache/spark/pull/21696] and [https://github.com/apache/spark/pull/22183] for this bug.

 

*With only* [https://github.com/apache/spark/pull/21696], no records are returned.
{code:java}
rm -rf /tmp/data /tmp/data_csv
./bin/spark-shell
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")
sql(""select * from t where id > 0"").write.csv(""/tmp/data_csv"")

scala> spark.read.csv(""/tmp/data_csv"")
res4: org.apache.spark.sql.DataFrame = []{code}
*Root Cause*: No filter is pushed, but ""ID"" is selected from parquet file, which has no this field, so 10 null records are returned from parquet scan, and then they are filtered by ""ID"" > 0 in FilterExec, finally, 0 records are returned. See:

!image-2018-08-24-22-46-05-346.png!

*With both* [https://github.com/apache/spark/pull/21696] and [https://github.com/apache/spark/pull/22183]
{code:java}
rm -rf /tmp/data /tmp/data_csv
./bin/spark-shell
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")
sql(""select * from t where id > 0"").write.csv(""/tmp/data_csv"")

scala> spark.read.csv(""/tmp/data_csv"").show
+---+
|_c0|
+---+
| 2|
| 3|
| 4|
| 7|
| 8|
| 9|
| 5|
| 6|
| 1|
+---+{code};;;","24/Aug/18 23:44;dongjoon;+1 for fixing this. However, actually, it isn't an issue with predicate pushdowns. Please see `where id is null`. It's because filters become false due to `null` values. If you see `null` without predicate pushdown, the empty result with predicate pushdown is a correct result.
{code}
scala> sql(""select * from p1 where id > 0"").show
+---+
| ID|
+---+
+---+
scala> sql(""select * from p1 where id is null"").show
+----+
|  ID|
+----+
|null|
|null|
|null|
|null|
|null|
|null|
|null|
|null|
|null|
|null|
+----+
{code}

And, are we going to backport this to `branch-2.2`, too?;;;","25/Aug/18 02:14;yucai;[~dongjoon], I still think this bug is related to pushdown, but unfortunately, there are two issues actually, which make it quite confusing. Let me explain:


1. The wrong column name is pushdown into parquet. Spark pushdowns ""ID > 0"", but parquet file has ""id"" in its schema instead of ""ID"", so 0 record is returned in {color:#FF0000}*PARQUET SCAN stage.*{color}

{color:#FF0000}*Attention*{color}: not because of the filter, no record from *{color:#FF0000}parquet scan{color}* in this case.

We can confirm this in Spark's chart, ""number of output rows"" in Scan is 0.
{code:java}
rm -rf /tmp/data /tmp/data_csv
./bin/spark-shell
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")
scala> sql(""select * from t where id > 0"").show
+---+
| ID|
+---+
+---+
{code}
 

!image-2018-08-25-10-04-21-901.png!

That's why we need backport [https://github.com/apache/spark/pull/21696].

With it, Spark will pushdown correct filter ""id > 0"" into parquet.

2. Unfortunately, with only [https://github.com/apache/spark/pull/21696], it is not enough, because of https://issues.apache.org/jira/browse/SPARK-25132, we still need backport [https://github.com/apache/spark/pull/22183]. 

 

Does it make sense to you?;;;","25/Aug/18 02:21;yucai;Link to SPARK-25132, this bug needs two PRs backport.;;;","25/Aug/18 02:26;yucai;[~dongjoon] , the reason you see `null` without predicate pushdown, it is because of https://issues.apache.org/jira/browse/SPARK-25132. It is one of the issues of this bug.
{code:java}
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")
scala> sql(""select * from t"").show
+----+
| ID|
+----+
|null|
|null|
|null|
|null|
|null|
|null|
|null|
|null|
|null|
|null|
+----+{code};;;","25/Aug/18 02:46;dongjoon;Yes. That's my point. This is a simple duplication of SPARK-25132 because this is not related to the configuration of predicate pushdown .;;;","25/Aug/18 02:52;dongjoon;If this is only reporting SPARK-25132, we had better close this as `DUPLCATE`.
I added `2.2.0` as `Affected Versions` into SPARK-25132.;;;","25/Aug/18 02:58;yucai;Not a simple duplication.

Backport -SPARK-25132-, but without -SPARK-24716-, still buggy.

 

See my test.

*{color:#FF0000}Attention{color}*: backport SPARK-25132 only.
{code:java}
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")

scala> sql(""select * from t where id > 0"").show
+---+
| ID|
+---+
+---+


scala> sql(""set spark.sql.parquet.filterPushdown"").show
+--------------------+-----+
| key|value|
+--------------------+-----+
|spark.sql.parquet...| true|
+--------------------+-----+


scala> sql(""set spark.sql.parquet.filterPushdown=false"").show
+--------------------+-----+
| key|value|
+--------------------+-----+
|spark.sql.parquet...|false|
+--------------------+-----+


scala> sql(""select * from t where id > 0"").show
+---+
| ID|
+---+
| 7|
| 8|
| 9|
| 2|
| 3|
| 4|
| 5|
| 6|
| 1|
+---+


{code}
 ;;;","25/Aug/18 03:00;dongjoon;Let me put this way. Parquet returns `null` for all unknown columns. This is a *Parquet feature* to handle unknown columns. Usually, it's for missing columns, but it happens for mismatched columns like this due to case-insensitive. Pushdown or not is not a problem here. As you pointed above, Parquet returns null columns. This is not about filters at all.

bq. Attention: not because of the filter, no record from parquet scan in this case.;;;","25/Aug/18 03:32;yucai;[~dongjoon] , correct me if I am wrong.
{code:java}
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")
sql(""select * from t where id > 0"").show{code}
Based on 2.3.1,

Backport SPARK-25132, Spark will show nothing if pushdown enabled and show ""1 to 9"" if pushdown disabled.

Backport -SPARK-25132- + SPARK-24716, Spark will pushdown nothing and it shows ""1 to 9"".

-SPARK-25132- +  +-SPARK-24716-+  + SPARK-25207, Spark will pushdown ""id > 0"" correctly and shows ""1 to 9"".;;;","25/Aug/18 04:13;dongjoon;[~yucai]. First of all, I know your intention and support both backports (one bug + one improvement). Let me rephrase your words and add some.

1. Vanilla Spark 2.2.0 ~ 2.3.1 always returns NULL for Parquet mismatched columns due to case sensitivity. It's not related to filters at all.
2. Spark 2.4 can handle this by SPARK-24716(Improvement, 2018-07-04) and SPARK-25132(Bug, 2018-08-21). So, we need both of them in `branch-2.3`.

I'm saying (1) like you. Is (1) wrong? Does Spark 2.2.0 ~ 2.3.1 have some inconsistency between `on` and `off` for parquet predicate pushdowns?

The reason why SPARK-25132 is not complete to your situations is simply that it's based on `master` branch. It depends on lots of improvements only in `master` branch. Now now, we only have one (SPARK-24716). But, for some other corner cases, we may require more `master` patches.;;;","25/Aug/18 04:34;yucai;{quote} # Vanilla Spark 2.2.0 ~ 2.3.1 always returns NULL for Parquet mismatched columns due to case sensitivity. It's not related to filters at all.{quote}
Agree, it has nothing to do with filter, actually, the issue exists since 2.0.
{quote}The reason why SPARK-25132 is not complete to your situations is simply that it's based on `master` branch. It depends on lots of improvements only in `master` branch.
{quote}
This part might need to clarify.

-SPARK-25132- has no dependence, I can backport it alone, but data issue still exists if I only backport it.

This time the root cause is filter pushdown.;;;","25/Aug/18 04:44;yucai;[~dongjoon] , thanks a lot for so many explanations, if we both agree to backport  -SPARK-25132- + -SPARK-24716.-

We can go ahead :).

But master's parquet filter pushdown is still buggy in case-insensitive mode, I have summited the PR in SPARK-25207.

Kindly help review.;;;","26/Aug/18 13:04;gurwls223;Please fix the JIRA title to reflect more precisely rather then just wrong results since this one is a blocker and better be clarified.;;;","26/Aug/18 20:38;dongjoon;Hi, [~yucai], [~cloud_fan], [~smilegator], [~hyukjin.kwon].

In Spark 2.4, we are still trying to fix long-lasting Parquet case-sensitivity issues (Spark 2.1.x raises Exceptions and Spark 2.2.x is the same with Spark 2.3.x).
Unfortunately, this effort is incomplete and unstable even in Spark 2.4 because we have unmerged one (SPARK-25207) and we may have more future unknown patches.
In this case, we had better consider any backporting to `branch-2.3` after Spark 2.4 becomes stable first. We may land them together, not one by one.
How do you think about this? Are the current three Spark-2.4-only Parquet patches(SPARK-25132, SPARK-24716, SPARK-25207) considered as a complete set of patches for this?;;;","26/Aug/18 22:03;smilegator;Currently, we do not have a good test coverage when the physical schema and logical schema use difference cases. Thus, any new change could introduce new behavior changes or bugs. Thus, the first step is to add the tests first. [~yucai] Could you help this effort?

Merging Parquet filter refactoring is kind of breaking our backport rule. Maybe we do not need to claim we support this scenario before Spark 2.4?;;;","27/Aug/18 01:18;cloud_fan;I'm fine to mark it as a known correctness bug in Spark 2.2, 2.3, shall we put it in the release notes of Spark 2.3.2? cc [~jerryshao];;;","27/Aug/18 01:30;yucai;[~smilegator] , sure, I will add tests.

 

If we don't backport SPARK-25132 and SPARK-24716, user will have below issue.
{code:java}
spark.range(10).write.parquet(""/tmp/data"")
sql(""DROP TABLE t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/data'"")

scala> sql(""select * from t where id > 0"").show
+---+
| ID|
+---+
+---+
{code}
 

The biggest difference is, in Spark 2.1, they will get Exception:
{code:java}
Caused by: java.lang.IllegalArgumentException: Column [ID] was not found in schema!{code}
So will they know the issue and fix the query.

But in Spark 2.3, they will get the wrong results sliently and might be ignored?

 

Could it be risky for the user?

 ;;;","27/Aug/18 01:36;yucai;I am OK with ""known correctness bug in 2.3"" way, just raise some concern in my previous post.;;;","27/Aug/18 02:11;gurwls223;[~yucai], mind fixing the JIRA title?;;;","27/Aug/18 02:26;yucai;[~dongjoon], because of the below root cause
{quote}Spark pushdowns FilterApi.gt(intColumn(""ID""), 0: Integer) into parquet, but ID does not exist in /tmp/data (parquet is case sensitive, it has id actually).
{quote}
I changed the title to emphasize wrong column is pushdown: ""id"" should be pushdown instead of ""ID"".

Feel free to let me know if you have any concern.

This issue exists in 2.3 only, master is different.;;;","27/Aug/18 15:38;smilegator;Silently ignoring it is bad. We should issue an exception like what we did in the previous version? What do you think? ;;;","28/Aug/18 13:16;yucai;[~smilegator] , 2.1's exception is from parquet.
{code:java}
java.lang.IllegalArgumentException: Column [ID] was not found in schema!
at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.getColumnDescriptor(SchemaCompatibilityValidator.java:181)
at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:169)
at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:151)
at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:91)
at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:58)
at org.apache.parquet.filter2.predicate.Operators$NotEq.accept(Operators.java:194)
at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:121)
{code}
2.1 uses parquet 1.8.1, while 2.3 uses parquet 1.8.3, it is behavior change in parquet.

See:

https://issues.apache.org/jira/browse/PARQUET-389

[https://github.com/apache/parquet-mr/commit/2282c22c5b252859b459cc2474350fbaf2a588e9]

 ;;;","28/Aug/18 17:05;yucai; Do you want to simulate an Exception in Spark? 

Backporting SPARK-25132 and SPARK-24716 is to fix a bug for our data source table, it could be more meaningful.;;;","31/Aug/18 01:40;jerryshao;What is the status of this JIRA, are we going to backport, or just mark as a known issue?

[~yucai] [~cloud_fan] [~smilegator];;;","31/Aug/18 01:48;yucai;Based on our discussion in [https://github.com/apache/spark/pull/22184#issuecomment-416840509],

seems like [~cloud_fan] prefers not backport, need his confirmation.;;;","31/Aug/18 01:52;jerryshao;I see, if it is not going to be merged, let's close this JIRA and add to the release note.;;;","31/Aug/18 04:25;cloud_fan;It turns out we need to backport 3 non-trivial PRs to entirely fix the problem, which is risky. Let's close this JIRA if the problem has been resolved in master.;;;","31/Aug/18 04:38;yucai;Not backport to 2.3 as per [~cloud_fan]'s summary, closed.;;;","31/Aug/18 09:12;dongjoon;Thank you all for the decision.;;;","31/Oct/18 16:22;cloud_fan;marked it as resolved in 2.4.0, since the test is already in 2.4.0 and passes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in spark.network.crypto.keyFactoryIteration,SPARK-25205,13180608,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,irashid,irashid,irashid,23/Aug/18 03:09,12/Dec/22 18:10,13/Jul/23 08:48,24/Aug/18 01:32,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Spark Core,,,,0,,,,,"I happened to notice this typo ""spark.networy.crypto.keyFactoryIteration"".  probably nobody ever uses this conf, but still should be fixed.",,apachespark,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 01:32:05 UTC 2018,,,,,,,,,,"0|i3xbwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/18 03:13;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22195;;;","24/Aug/18 01:32;gurwls223;Issue resolved by pull request 22195
[https://github.com/apache/spark/pull/22195];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rate source test is flaky,SPARK-25204,13180569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joseph.torres,joseph.torres,joseph.torres,22/Aug/18 22:12,21/Sep/18 05:48,13/Jul/23 08:48,23/Aug/18 19:14,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,,,,,"We try to restart a manually clocked rate stream in a test. This is inherently race-prone, because the stream will go backwards in time (and throw an assertion failure) if the clock can't be incremented before it tries to schedule the first batch.",,apachespark,joseph.torres,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 19:14:58 UTC 2018,,,,,,,,,,"0|i3xbnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 22:15;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/22191;;;","23/Aug/18 19:14;tdas;Issue resolved by pull request 22191
[https://github.com/apache/spark/pull/22191];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark HiveServer2 registers shutdown hook with JVM, not ShutdownHookManager; race conditions can arise",SPARK-25183,13180351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,22/Aug/18 00:56,31/Aug/18 06:46,13/Jul/23 08:48,31/Aug/18 06:46,2.2.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Spark's HiveServer2 registers a shutdown hook with the JVM {{Runtime.addShutdownHook()}} which can happen in parallel with the ShutdownHookManager sequence of spark & Hadoop, which operate the shutdowns in an ordered sequence.

This has some risks

* FS shutdown before rename of logs completes, SPARK-6933
* Delays of rename on object stores may block FS close operation, which, on clusters with timeouts hooks (HADOOP-12950) of FileSystem.closeAll() can force a kill of that shutdown hook and other problems.

General outcome: logs aren't present.

Proposed fix: 

* register hook with {{org.apache.spark.util.ShutdownHookManager}}
* HADOOP-15679 to make shutdown wait time configurable, so O(data) renames don't trigger timeouts.",,apachespark,jerryshao,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-12950,HADOOP-15679,SPARK-6933,SPARK-6014,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 31 06:46:07 UTC 2018,,,,,,,,,,"0|i3xabj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 17:27;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/22186;;;","31/Aug/18 06:46;jerryshao;Issue resolved by pull request 22186
[https://github.com/apache/spark/pull/22186];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Block Manager master and slave thread pools are unbounded,SPARK-25181,13180327,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mukulmurthy,mukulmurthy,mukulmurthy,21/Aug/18 23:03,22/Aug/18 17:36,13/Jul/23 08:48,22/Aug/18 17:36,2.3.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"Currently, BlockManagerMasterEndpoint and BlockManagerSlaveEndpoint both have thread pools with unbounded numbers of threads. In certain cases, this can lead to driver OOM errors. We should add an upper bound on the number of threads in these thread pools; this should not break any existing behavior because they still have queues of size Integer.MAX_VALUE.",,apachespark,mukulmurthy,riza,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25182,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 23:22:05 UTC 2018,,,,,,,,,,"0|i3xa67:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"21/Aug/18 23:22;apachespark;User 'mukulmurthy' has created a pull request for this issue:
https://github.com/apache/spark/pull/22176;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kryo fails to serialize a parametrised type hierarchy,SPARK-25176,13180222,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,m.pryahin,m.pryahin,21/Aug/18 14:16,12/Dec/22 18:10,13/Jul/23 08:48,05/Sep/18 22:50,2.2.2,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,5,,,,,"I'm using the latest spark version spark-core_2.11:2.3.1 which 
transitively depends on com.esotericsoftware:kryo-shaded:3.0.3 via the
com.twitter:chill_2.11:0.8.0 dependency. This exact version of kryo serializer contains an issue [1,2] which results in throwing ClassCastExceptions when serialising parameterised type hierarchy.
This issue has been fixed in kryo version 4.0.0 [3]. It would be great to have this update in Spark as well. Could you please upgrade the version of com.twitter:chill_2.11 dependency from 0.8.0 up to 0.9.2?
You can find a simple test to reproduce the issue [4].

[1] https://github.com/EsotericSoftware/kryo/issues/384
[2] https://github.com/EsotericSoftware/kryo/issues/377
[3] https://github.com/EsotericSoftware/kryo/releases/tag/kryo-parent-4.0.0
[4] https://github.com/mpryahin/kryo-parametrized-type-inheritance",,apachespark,dongjoon,kent2171,m.pryahin,riza,szhemzhitsky,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 05 22:50:16 UTC 2018,,,,,,,,,,"0|i3x9j3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/18 03:29;gurwls223;(please avoid to set Critical+ which is usually reserved for committers);;;","02/Sep/18 18:48;dongjoon;Sorry, [~m.pryahin]. I overlooked the example in [4]. I deleted my previous comment.;;;","04/Sep/18 12:15;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22179;;;","05/Sep/18 22:50;srowen;Issue resolved by pull request 22179
[https://github.com/apache/spark/pull/22179];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Field resolution should fail if there's ambiguity for ORC native reader,SPARK-25175,13180172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,seancxmao,seancxmao,seancxmao,21/Aug/18 09:23,10/Sep/18 02:26,13/Jul/23 08:48,10/Sep/18 02:24,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"SPARK-25132 adds support for case-insensitive field resolution when reading from Parquet files. We found ORC files have similar issues, but not identical to Parquet. Spark has two OrcFileFormat.
 * Since SPARK-2883, Spark supports ORC inside sql/hive module with Hive dependency. This hive OrcFileFormat always do case-insensitive field resolution regardless of case sensitivity mode. When there is ambiguity, hive OrcFileFormat always returns the first matched field, rather than failing the reading operation.
 * SPARK-20682 adds a new ORC data source inside sql/core. This native OrcFileFormat supports case-insensitive field resolution, however it cannot handle duplicate fields.

Besides data source tables, hive serde tables also have issues. If ORC data file has more fields than table schema, we just can't read hive serde tables. If ORC data file does not have more fields, hive serde tables always do field resolution by ordinal, rather than by name.

Both ORC data source hive impl and hive serde table rely on the hive orc InputFormat/SerDe to read table. I'm not sure whether we can change underlying hive classes to make all orc read behaviors consistent.

This ticket aims to make read behavior of ORC data source native impl consistent with Parquet data source.",,apachespark,dongjoon,seancxmao,yucai,yumwang,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,SPARK-25132,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 10 02:24:52 UTC 2018,,,,,,,,,,"0|i3x97z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/18 09:28;seancxmao;This is followup of SPARK-25132. I'm working on this.;;;","25/Aug/18 03:10;dongjoon;[~seancxmao]. I know you are working, but could you give us some failure examples?;;;","26/Aug/18 20:08;dongjoon;[~seancxmao]. If there is no example, we can not help you. In that case, we usually close this as `Cannot Reproduce`.;;;","27/Aug/18 03:08;yucai;I pinged [~seancxmao] offline, he will give more details.;;;","27/Aug/18 03:10;dongjoon;I followed the same direction given by SPARK-25132, but I cannot reproduce this in Spark 2.3.1.

{code}
scala> spark.version
res8: String = 2.3.1

scala> spark.range(5).toDF.write.mode(""overwrite"").format(""orc"").saveAsTable(""t3"")

scala> sql(""create table t4 (`ID` BIGINT) USING orc LOCATION '/Users/dongjoon/spark-release/spark-2.3.1-bin-hadoop2.7/spark-warehouse/t3'"")

scala> sql(""select * from t3"").show
+---+
| id|
+---+
|  2|
|  3|
|  4|
|  1|
|  0|
+---+

scala> sql(""select * from t4"").show
+---+
| ID|
+---+
|  2|
|  3|
|  4|
|  1|
|  0|
+---+
{code}

Please reopen this with a reproducible example. Thanks, [~seancxmao].;;;","27/Aug/18 03:11;dongjoon;Thanks, [~yucai]. I'm highly interested in this case. I'll wait for his reopening. :);;;","27/Aug/18 04:58;seancxmao;Investigation about ORC tables with duplicate fields (c and C), thus also data file has more fields than table schema.
{code:java}
val data = spark.range(5).selectExpr(""id as a"", ""id * 2 as B"", ""id * 3 as c"", ""id * 4 as C"")
spark.conf.set(""spark.sql.caseSensitive"", true)
data.write.format(""orc"").mode(""overwrite"").save(""/user/hive/warehouse/orc_data"")

$> hive --orcfiledump /user/hive/warehouse/orc_data/part-00001-9716d241-9ad9-4d56-8de3-7bc482067614-c000.snappy.orc
Structure for /user/hive/warehouse/orc_data/part-00001-9716d241-9ad9-4d56-8de3-7bc482067614-c000.snappy.orc
Type: struct<a:bigint,B:bigint,c:bigint,C:bigint>

CREATE TABLE orc_data_source_lower (a LONG, b LONG, c LONG) USING orc LOCATION '/user/hive/warehouse/orc_data'
CREATE TABLE orc_data_source_upper (A LONG, B LONG, C LONG) USING orc LOCATION '/user/hive/warehouse/orc_data'
CREATE TABLE orc_hive_serde_lower (a LONG, b LONG, c LONG) STORED AS orc LOCATION '/user/hive/warehouse/orc_data'
CREATE TABLE orc_hive_serde_upper (A LONG, B LONG, C LONG) STORED AS orc LOCATION '/user/hive/warehouse/orc_data'

DESC EXTENDED orc_data_source_lower;
DESC EXTENDED orc_data_source_upper;
DESC EXTENDED orc_hive_serde_lower;
DESC EXTENDED orc_hive_serde_upper;

spark.conf.set(""spark.sql.hive.convertMetastoreOrc"", false)
{code}
 
||no.||caseSensitive||table columns||select column||orc column
 (select via data source table, hive impl)||orc column
 (select via data source table, native impl)||orc column
 (select via hive serde table)||
|1|true|a, b, c|a|a |a|IndexOutOfBoundsException |
|2| | |b|B |null|IndexOutOfBoundsException |
|3| | |c|c |c|IndexOutOfBoundsException |
|4| | |A|AnalysisException|AnalysisException|AnalysisException|
|5| | |B|AnalysisException|AnalysisException|AnalysisException|
|6| | |C|AnalysisException|AnalysisException|AnalysisException|
|7| |A, B, C|a|AnalysisException |AnalysisException|AnalysisException|
|8| | |b|AnalysisException |AnalysisException|AnalysisException |
|9| | |c|AnalysisException |AnalysisException|AnalysisException |
|10| | |A|a |null|IndexOutOfBoundsException |
|11| | |B|B |B|IndexOutOfBoundsException |
|12| | |C|c |C|IndexOutOfBoundsException |
|13|false|a, b, c|a|a |a|IndexOutOfBoundsException |
|14| | |b|B |B|IndexOutOfBoundsException |
|15| | |c|c |c|IndexOutOfBoundsException |
|16| | |A|a |a|IndexOutOfBoundsException |
|17| | |B|B |B|IndexOutOfBoundsException |
|18| | |C|c |c|IndexOutOfBoundsException |
|19| |A, B, C|a|a |a|IndexOutOfBoundsException |
|20| | |b|B |B|IndexOutOfBoundsException |
|21| | |c|c |c|IndexOutOfBoundsException |
|22| | |A|a |a|IndexOutOfBoundsException |
|23| | |B|B |B|IndexOutOfBoundsException |
|24| | |C|c |c|IndexOutOfBoundsException |

Followup tests that use ORC files with no duplicate fields (only a,B).
{code:java}
val data = spark.range(5).selectExpr(""id as a"", ""id * 2 as B"")
spark.conf.set(""spark.sql.caseSensitive"", true)
data.write.format(""orc"").mode(""overwrite"").save(""/user/hive/warehouse/orc_data_nodup"")

$> hive --orcfiledump /user/hive/warehouse/orc_data_nodup//user/hive/warehouse/orc_data_nodup/part-00001-4befd318-9ed5-4d77-b51b-09848d71d9cd-c000.snappy.orc
Structure for /user/hive/warehouse/orc_data_nodup/part-00001-4befd318-9ed5-4d77-b51b-09848d71d9cd-c000.snappy.orc
Type: struct<a:bigint,B:bigint>

CREATE TABLE orc_nodup_hive_serde_lower (a LONG, b LONG) STORED AS orc LOCATION '/user/hive/warehouse/orc_data_nodup'
CREATE TABLE orc_nodup_hive_serde_upper (A LONG, B LONG) STORED AS orc LOCATION '/user/hive/warehouse/orc_data_nodup'

DESC EXTENDED orc_nodup_hive_serde_lower;
DESC EXTENDED orc_nodup_hive_serde_upper;

spark.conf.set(""spark.sql.hive.convertMetastoreOrc"", false)
{code}
||no.||caseSensitive||table columns||select column||orc column
 (select via hive serde table)||
|1|true|a, b|a|a|
|2| | |b|B|
|4| | |A|AnalysisException|
|5| | |B|AnalysisException|
|7| |A, B|a|AnalysisException|
|8| | |b|AnalysisException |
|10| | |A|a|
|11| | |B|B|
|13|false|a, b|a|a|
|14| | |b|B|
|16| | |A|a|
|17| | |B|B|
|19| |A, B|a|a|
|20| | |b|B|
|22| | |A|a|
|23| | |B|B|

Tests show that for hive serde table field resolution is by ordinal, not by name.

{code}
spark.conf.set(""spark.sql.caseSensitive"", true)
spark.conf.set(""spark.sql.hive.convertMetastoreOrc"", false)
val data = spark.range(1).selectExpr(""id + 1 as x"", ""id + 2 as y"", ""id + 3 as z"")
data.write.format(""orc"").mode(""overwrite"").save(""/user/hive/warehouse/orc_data_xyz"")
sql(""CREATE TABLE orc_table_ABC (A LONG, B LONG, C LONG) STORED AS orc LOCATION '/user/hive/warehouse/orc_data_xyz'"")
sql(""select B from orc_table_ABC"").show
+---+
|  B|
+---+
|  2|
+---+
{code}
;;;","27/Aug/18 05:22;seancxmao;[~dongjoon] [~yucai] Here is a brief summary. We can see that
 * The data source tables with hive impl always return a,B,c, no matter whether spark.sql.caseSensitive is set to true or false and no matter metastore table schema is in lower case or upper case. They always do case-insensitive field resolution, and if there is ambiguity they return the first matched one. Given ORC file schema is (a,B,c,C)
 ** Is it better to return null in scenario 2 and 10? 
 ** Is it better to return C in scenario 12?
 ** Is it better to fail due to ambiguity in scenario 15, 18, 21, 24, rather than always return lower case one?

 * The data source tables with native impl, compared to hive impl, handle scenario 2, 10, 12 in a more reasonable way. However, they handles ambiguity in the same way as hive impl, which is not consistent with Parquet data source.
 * The hive serde tables always throw IndexOutOfBoundsException at runtime when ORC file schema has more fields than table schema. If ORC schema does NOT have more fields, hive serde tables do field resolution by ordinal rather than by name.
 * Since in case-sensitive mode analysis should fail if a column name in query and metastore schema are in different cases, all AnalysisException(s) are reasonable.

Stacktrace of IndexOutOfBoundsException:
{code:java}
java.lang.IndexOutOfBoundsException: toIndex = 4
	at java.util.ArrayList.subListRangeCheck(ArrayList.java:1004)
	at java.util.ArrayList.subList(ArrayList.java:996)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.getSchemaOnRead(RecordReaderFactory.java:161)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.createTreeReader(RecordReaderFactory.java:66)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:202)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1215)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1113)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:257)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:256)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
{code}
 ;;;","27/Aug/18 05:32;seancxmao;Also here is similar investigation I did for parquet tables. Just for your information: [https://github.com/apache/spark/pull/22184/files#r212405373];;;","27/Aug/18 20:54;dongjoon;Okay, thank you for the details, [~seancxmao]. 

BTW, for me, the higher purpose is JIRA is the reverse of this JIRA title. What you are aiming is to support `Case Sensitivity` in Spark, right?

In general, `spark.sql.caseSensitive=true` has been `false` by default since Spark 2.0.0. And, by default, Spark prevents you from generating such data. (not only ORC.)
{code}
scala> data.write.format(""orc"").mode(""overwrite"").save(""/tmp/orc"")
org.apache.spark.sql.AnalysisException: Found duplicate column(s) when inserting into file:/tmp/orc: `c`;
{code}

Thus, your use case is specifically *generating* data with `spark.sql.caseSensitive=true` and reading data with `spark.sql.caseSensitive=false` (without problems).
Do you really have a *case-sensitive* data(schema)? Could you elaborate more why you hit this situation?;;;","28/Aug/18 16:20;seancxmao;After a deep dive into ORC file read paths (data source native, data source hive, hive serde), I realized that this is a little complicated. I'm not sure whether it's technically possible to make all three read paths consistent with respect to case sensitivity, because we rely on hive InputFormat/SerDe which we might not be able to change.

Please also see [~cloud_fan]'s comment on Parquet: [https://github.com/apache/spark/pull/22184/files#r212849852]

So I changed the title of this Jira to reduce the scope. This ticket aims to make ORC data source native impl consistent with Parquet data source. The gap is that field resolution should fail if there is ambiguity in case-insensitive mode when reading from ORC. Does it make sense?

As for duplicate fields with different letter cases, we don't have real use cases. It's just for testing purpose.

 ;;;","28/Aug/18 16:29;seancxmao;[~cloud_fan] Does it make sense?;;;","29/Aug/18 06:46;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22262;;;","10/Sep/18 02:24;dongjoon;Issue resolved by pull request 22262
[https://github.com/apache/spark/pull/22262];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationMaster suspends when unregistering itself from RM with extreme large diagnostic message,SPARK-25174,13180159,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,21/Aug/18 08:54,17/May/20 18:14,13/Jul/23 08:48,24/Aug/18 20:44,2.1.1,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,YARN,,,0,,,,,"We recently ran into SPARK-18016 which has been fixed in v2.3.0. This JIRA is not about the issue in SPARK-18016 but the side-effect which it brings. When SPARK-18016 occurs, ApplicationMaster fails unregistering itself because the exception contains extreme large error information.

{code:java}
ERROR yarn.ApplicationMaster: User class threw exception: java.lang.RuntimeException: Error while decoding: java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.janino.JaninoRuntimeException: Constant pool has grown past JVM limit of 0xFFFF
/* 001 */ public java.lang.Object generate(Object[] references) {
....

/* 395656 */       mutableRow.update(0, value);
/* 395657 */     }
/* 395658 */
/* 395659 */     return mutableRow;
/* 395660 */   }
/* 395661 */ }
{code}

The above codegen text is included in the final message for AM to wave goodbye to RM, while it ends up crashing the rm's ZKRMStateStore for YARN-6125 not covering the unregisterApplicationMaster's message truncation. We also create an Jira on YARN Side https://issues.apache.org/jira/browse/YARN-8691 

Although SPARK-18016 fixed already, there are maybe other uncaught exceptions will cause this problem. I guess that we should limit the error message's size sent to RM while unregistering AM .",,apachespark,Qin Yao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-8691,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 20:44:58 UTC 2018,,,,,,,,,,"0|i3x953:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 02:25;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/22180;;;","24/Aug/18 20:44;vanzin;Issue resolved by pull request 22180
[https://github.com/apache/spark/pull/22180];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor fixes for R sql tests (tests that fail in development environment),SPARK-25167,13180120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,21/Aug/18 04:40,12/Dec/22 18:10,13/Jul/23 08:48,23/Aug/18 02:57,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SparkR,,,,0,,,,,"A few SQL tests for R are failing development environment (Mac). 

*  The catalog api tests assumes catalog artifacts named ""foo"" to be non existent. I think name such as foo and bar are common and developers use it frequently. 
*  One test assumes that we only have one database in the system. I had more than one and it caused the test to fail. I have changed that check.",,apachespark,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 29 21:16:04 UTC 2018,,,,,,,,,,"0|i3x8wf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/18 04:41;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22161;;;","23/Aug/18 02:57;gurwls223;Issue resolved by pull request 22161
[https://github.com/apache/spark/pull/22161];;;","29/Aug/18 21:16;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22274;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet reader builds entire list of columns once for each column,SPARK-25164,13180047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,20/Aug/18 19:34,16/Oct/18 16:32,13/Jul/23 08:48,23/Aug/18 06:53,2.4.0,,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,SQL,,,,0,,,,,"{{VectorizedParquetRecordReader.initializeInternal}} loops through each column, and for each column it calls
{noformat}
requestedSchema.getColumns().get(i)
{noformat}
However, {{MessageType.getColumns}} will build the entire column list from getPaths(0).
{noformat}
  public List<ColumnDescriptor> getColumns() {
    List<String[]> paths = this.getPaths(0);
    List<ColumnDescriptor> columns = new ArrayList<ColumnDescriptor>(paths.size());
    for (String[] path : paths) {
      // TODO: optimize this                                                                                                                    
      PrimitiveType primitiveType = getType(path).asPrimitiveType();
      columns.add(new ColumnDescriptor(
                      path,
                      primitiveType,
                      getMaxRepetitionLevel(path),
                      getMaxDefinitionLevel(path)));
    }
    return columns;
  }
{noformat}
This means that for each parquet file, this routine indirectly iterates colCount*colCount times.

This is actually not particularly noticeable unless you have:
 - many parquet files
 - many columns

To verify that this is an issue, I created a 1 million record parquet table with 6000 columns of type double and 67 files (so initializeInternal is called 67 times). I ran the following query:
{noformat}
sql(""select * from 6000_1m_double where id1 = 1"").collect
{noformat}
I used Spark from the master branch. I had 8 executor threads. The filter returns only a few thousand records. The query ran (on average) for 6.4 minutes.

Then I cached the column list at the top of {{initializeInternal}} as follows:
{noformat}
List<ColumnDescriptor> columnCache = requestedSchema.getColumns();
{noformat}
Then I changed {{initializeInternal}} to use {{columnCache}} rather than {{requestedSchema.getColumns()}}.

With the column cache variable, the same query runs in 5 minutes. So with my simple query, you save %22 of time by not rebuilding the column list for each column.

You get additional savings with a paths cache variable, now saving 34% in total on the above query.",,apachespark,bersprockets,cloud_fan,Tagar,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24316,,,SPARK-25643,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 05 16:26:30 UTC 2018,,,,,,,,,,"0|i3x8gn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/18 23:29;viirya;This looks easy and good to have. [~bersprockets] Do you want to submit a PR for this?;;;","21/Aug/18 23:46;bersprockets;[~viirya] Sure. I will try to get something up by tonight or tomorrow morning (Pacific Time).;;;","22/Aug/18 18:30;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/22188;;;","23/Aug/18 06:53;cloud_fan;Issue resolved by pull request 22188
[https://github.com/apache/spark/pull/22188];;;","11/Sep/18 18:58;Tagar;Thanks [~bersprockets]

Very good find ! Thanks.

As described in SPARK-24316, ""even simple queries of fetching 70k rows takes 20 minutes"". 

This PR-22188 gives 21-44% improvement, reducing total runtime to 11-16 minutes.

It seems *reading 70k rows for over 10 minutes* with multiple executors is still quite slow. 

Do you think there might be other issue? So it seems time complexity of reading parquet files is O(num_columns * num_parquet_files)?
 Is there is any way to optimize this further?

Thanks.

 ;;;","11/Sep/18 22:27;bersprockets;Thanks [~Tagar] for the feedback. I assume the 44% improvement was for a table with lots of file splits.

I have thoughts on this issue, but fetching 70k rows from a wide table that has itself only 70k rows should be somewhat fast. With a table like that, I can fetch 70k rows on my laptop in under a minute.

Fetching 70k rows from a table that has, say, 10m million rows, can be pretty poky, even with this fix.

I have theories (or partially informed speculation) about why this is. Here is the gist:

Let's say
 * your projection includes every column of a wide table (i.e., {{select *}})
 * you are filtering away most rows from a large table (e.g., {{select * from table where id1 = 1}}, which would fetch, say, only 0.2% of the rows)
 * matching rows are sprinkled fairly evenly throughout the table

In this case, Spark ends up reading (potentially) every data page from the parquet file, and realizing each wide row in memory, just to pass the row to Spark's filter operator so it can be (most likely) discarded.

This is true even when Spark pushes down the filter to the parquet reader.

This is because the matching rows are sprinkled evenly throughout the table, so (potentially) every data page for column id1 has at least one entry where value = 1. When a page has even a single matching entry, Spark realizes all the rows associated with that page.

Realizing very wide rows in memory seems to be somewhat expensive, according to my profiling. I am not sure yet what part of realizing the rows in memory is expensive.

If the matching rows tend to be clumped together in one part of the table (say, the table is sorted on id1), most data pages will not contain matching rows. Spark can skip reading most data pages, and therefore avoid realizing most rows in memory. In that case, the query will be much faster: In a test I just ran, my query on the sorted table was 3 times faster. The filter push down code seems to be intended for cases like this.

{{select * from table limit nnnn}} is also slow, although not as slow as filtering the same number of records, but I have not looked into why.;;;","13/Sep/18 20:19;Tagar;Hi [~bersprockets]

Thanks a lot for the detailed response.

I totally see with what you're saying.

That's interesting that Spark realizing all rows even though where filter has a predicate for just one column.

I am thinking if it's feasible to lazily realize list of columns in select-clause only after filtering is complete?

It seems could be a huge performance improvement for wider tables like this.

In other words, if Spark would realize list of columns specified in where clause first, and only after filtering 
realize rest of columns needed for select-clause.

Thoughts? 

Thank you!
Ruslan

 ;;;","19/Sep/18 20:03;bersprockets;{quote}I am thinking if it's feasible to lazily realize list of columns in select-clause only after filtering is complete?
{quote}
Assuming my analysis is correct, that would be the fix (or at least, the only one I can think of at the moment).;;;","04/Oct/18 20:14;bersprockets;[~Tagar] I've opened SPARK-25643 to keep track of the wide row issue discussed above.;;;","05/Oct/18 16:26;Tagar;Thank you [~bersprockets] - SPARK-25643 would be a huge improvement for wider datasets,
but will also be helpful for querying performance on normal dataframes too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.util.collection.ExternalAppendOnlyMapSuite.spilling with compression,SPARK-25163,13180028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,zsxwing,zsxwing,20/Aug/18 17:58,22/Aug/18 21:17,13/Jul/23 08:48,22/Aug/18 21:17,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Tests,,,,0,,,,,"I saw it failed multiple times on Jenkins:

https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/4813/testReport/junit/org.apache.spark.util.collection/ExternalAppendOnlyMapSuite/spilling_with_compression/",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 22 02:46:03 UTC 2018,,,,,,,,,,"0|i3x8cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 02:46;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22181;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix several bugs in failure handling of barrier execution mode,SPARK-25161,13180013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,20/Aug/18 17:23,21/Aug/18 15:25,13/Jul/23 08:48,21/Aug/18 15:25,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"Fix several bugs in failure handling of barrier execution mode:
* Mark TaskSet for a barrier stage as zombie when a task attempt fails;
* Multiple barrier task failures from a single barrier stage should not trigger multiple stage retries;
* Barrier task failure from a previous failed stage attempt should not trigger stage retry;
* Fail the job when a task from a barrier ResultStage failed.",,apachespark,jiangxb1987,mengxr,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-24374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 15:25:19 UTC 2018,,,,,,,,,,"0|i3x89b:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"20/Aug/18 17:28;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/22158;;;","21/Aug/18 15:25;mengxr;Issue resolved by pull request 22158
[https://github.com/apache/spark/pull/22158];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json schema inference should only trigger one job,SPARK-25159,13179882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Aug/18 07:28,04/Oct/19 09:55,13/Jul/23 08:48,22/Aug/18 05:21,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 02:26:35 UTC 2018,,,,,,,,,,"0|i3x7g7:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"20/Aug/18 08:41;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22152;;;","23/Oct/18 02:25;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22799;;;","23/Oct/18 02:25;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22799;;;","23/Oct/18 02:26;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22799;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor accidentally exit because ScriptTransformationWriterThread throws TaskKilledException.,SPARK-25158,13179868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,20/Aug/18 06:09,12/Feb/19 05:06,13/Jul/23 08:48,12/Feb/19 04:17,2.1.0,2.2.0,2.3.0,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"In production environment, user run Spark-Sql use transform features with config 'spark.speculation = true', sometimes job fails and we found many Executor Dead through `Executor Tab` of Spark ui and here are some relevant sample logs:

Driver Side  Log:
{code:java}
18/08/14 16:17:52 INFO TaskSetManager: Starting task 2909.1 in stage 2.0 (TID 3929, executor.330, executor 7, partition 2909, PROCESS_LOCAL, 6791 bytes)
18/08/14 16:17:53 INFO TaskSetManager: Killing attempt 1 for task 2909.1 in stage 2.0 (TID 3929) on executor.330 as the attempt 0 succeeded on executor.58
18/08/14 16:17:53 WARN TaskSetManager: Lost task 2909.1 in stage 2.0 (TID 3929, executor.330, executor 7): TaskKilled (killed intentionally)
18/08/14 16:17:53 INFO TaskSetManager: Task 2909.1 in stage 2.0 (TID 3929) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
{code}
 

Executor Side Log: 
{code:java}
18/08/14 16:17:52 INFO Executor: Running task 2909.1 in stage 2.0 (TID 3929)
18/08/14 16:17:53 INFO Executor: Executor is trying to kill task 2909.1 in stage 2.0 (TID 3929)
18/08/14 16:17:53 ERROR ScriptTransformationWriterThread:
18/08/14 16:17:53 ERROR Utils: Uncaught exception in thread Thread-ScriptTransformation-Feed
org.apache.spark.TaskKilledException
at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:295)
at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:573)
at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:161)
at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:148)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:380)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply$mcV$sp(ScriptTransformation.scala:289)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:278)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:278)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread.run(ScriptTransformation.scala:278)
18/08/14 16:17:53 INFO Executor: Executor killed task 2909.1 in stage 2.0 (TID 3929)
18/08/14 16:17:53 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Thread-ScriptTransformation-Feed,5,main]
org.apache.spark.TaskKilledException
at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:295)
at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:573)
at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:161)
at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:148)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:380)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply$mcV$sp(ScriptTransformation.scala:289)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:278)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:278)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread.run(ScriptTransformation.scala:278)
18/08/14 16:17:53 INFO DiskBlockManager: Shutdown hook called
{code}
 

 

Through analysis log of Task TID 3929, we found that the Task had completed the Task Kill process normally on the Driver side and Executor side, the discord was SparkUncaughtExceptionHandler captured and handled TaskKilledException, then Executor entering the Shutdown process. The following is the code part and analysis of the problem process ：

 
{code:java}
override def run(): Unit = Utils.logUncaughtExceptions {
...

// We can't use Utils.tryWithSafeFinally here because we also need a `catch` block, so
// let's use a variable to record whether the `finally` block was hit due to an exception
var threwException: Boolean = true
val len = inputSchema.length
try {
iter.map(outputProjection).foreach { row => // line 289
...
}
threwException = false
} catch {
case t: Throwable =>
// An error occurred while writing input, so kill the child process. According to the
// Javadoc this call will not throw an exception:
_exception = t
proc.destroy()
throw t
} finally {
...
}
}
{code}
 
 # TaskKill cmd will mark `interrupted=true` of TaskContext, forecach method in ScriptTransformationWriterThread trigger TaskKilledException throw and catch by ScriptTransformationWriterThread.
 # ScriptTransformationWriterThread catch TaskKilledException ,  assign it to `_exception` and re-throw it.
 # ScriptTransformation in TaskRuner found `ScriptTransformationWriterThread.exception.isDefined` is true and throw TaskKilledException, it will handle by `catch block` in TaskRunner to complete TaskKill, we can confirm the conclusion from the log.
 # TaskKilledException re-throw by ScriptTransformationWriterThread will catch by Utils.logUncaughtExceptions method, logUncaughtExceptions method will log and re-throw it again.
 # ScriptTransformationWriterThread is sub Thread of TaskRuner, belonging to main ThreadGroup, so TaskKilledException throw from ScriptTransformationWriterThread will captured by SparkUncaughtExceptionHandler which registered during Executor start rather than TaskRunner， and SparkUncaughtExceptionHandler will log the TaskKilledException then call System.exit (SparkExitCode.UNCAUGHT_EXCEPTION) to shutdown Executor

 

From the above analysis we can be sure that any Throwable is thrown during the ScriptTransformationWriterThread run will lead Executor into the shutdown process, which is not appropriate. Current solution is add case matches for TaskKilledException in ScriptTransformationWriterThread catch block, only log and assign TaskKilledException to `_exception`, no longer rethrow it. We re-run the user job with this change and set `spark.speculation = true`, the problem no longer reappears.

 

 ",,apachespark,cloud_fan,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/spark/pull/22149,,,,,,,,,,9223372036854775807,,,Tue Feb 12 04:17:17 UTC 2019,,,,,,,,,,"0|i3x7d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/18 06:13;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/22149;;;","12/Feb/19 04:17;cloud_fan;Issue resolved by pull request 22149
[https://github.com/apache/spark/pull/22149];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joining DataFrames derived from the same source yields confusing/incorrect results,SPARK-25150,13179651,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,nchammas,nchammas,17/Aug/18 18:06,14/Dec/21 21:28,13/Jul/23 08:48,14/Dec/21 21:28,2.3.1,2.4.3,,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,correctness,,,,"I have two DataFrames, A and B. From B, I have derived two additional DataFrames, B1 and B2. When joining A to B1 and B2, I'm getting a very confusing error:
{code:java}
Join condition is missing or trivial.
Either: use the CROSS JOIN syntax to allow cartesian products between these
relations, or: enable implicit cartesian products by setting the configuration
variable spark.sql.crossJoin.enabled=true;
{code}
Then, when I configure ""spark.sql.crossJoin.enabled=true"" as instructed, Spark appears to give me incorrect answers.

I am not sure if I am missing something obvious, or if there is some kind of bug here. The ""join condition is missing"" error is confusing and doesn't make sense to me, and the seemingly incorrect output is concerning.

I've attached a reproduction, along with the output I'm seeing with and without the implicit cross join enabled.

I realize the join I've written is not ""correct"" in the sense that it should be left outer join instead of an inner join (since some of the aggregates are not available for all states), but that doesn't explain Spark's behavior.",,apachespark,bsplosion,EeveeB,maropu,nchammas,petertoth,TomaszGaweda,xkrogen,,,,,,,,,,,,,,,,,,,,,,,SPARK-26231,,,,,,,,,SPARK-20804,SPARK-6459,,,,,,,,,"28/Sep/18 19:20;nchammas;expected-output.txt;https://issues.apache.org/jira/secure/attachment/12941728/expected-output.txt","17/Aug/18 18:06;nchammas;output-with-implicit-cross-join.txt;https://issues.apache.org/jira/secure/attachment/12936053/output-with-implicit-cross-join.txt","17/Aug/18 18:06;nchammas;output-without-implicit-cross-join.txt;https://issues.apache.org/jira/secure/attachment/12936054/output-without-implicit-cross-join.txt","17/Aug/18 18:06;nchammas;persons.csv;https://issues.apache.org/jira/secure/attachment/12936055/persons.csv","17/Aug/18 18:06;nchammas;states.csv;https://issues.apache.org/jira/secure/attachment/12936056/states.csv","17/Aug/18 18:06;nchammas;zombie-analysis.py;https://issues.apache.org/jira/secure/attachment/12936057/zombie-analysis.py",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 14 21:28:37 UTC 2021,,,,,,,,,,"0|i3x60v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/18 18:11;nchammas;I know there are a bunch of pending bug fixes in 2.3.2. I'm not sure if this is covered by any of them, and didn't have time to setup 2.3.2 to see if this problem is still present there. I will be away for some time and thought it best to report this now in case someone can pick it up and investigate further until I get back.

cc [~marmbrus].;;;","17/Aug/18 19:00;TomaszGaweda;[~nchammas] Maybe it's related to: [https://twitter.com/KurtFehlhauer/status/1030490707641790474]

It looks like Spark is resolving columns to the ones in it's lineage, but not always in current schema;;;","31/Aug/18 05:46;EeveeB;I'd love the chance to bug patch this.

I've included a simplified version of the python script which produces it, if you switch out the second join to the commented join it works as it should. Unable to embed resource: zombie-analysis.py of type application/octet-stream

What's happening is during the creation of the logical plan it's re-aliasing the right side of the join because the left and right refer to the same base column. When it does this it renames all the columns in the right side of the join to the new alias but not the column which is actually a part of the join.

Then because the join refers to the column which hasn't been updated it now refers to the left side of the join. So it does a cartesian join on itself and straps on the right side of the join on the end.

The part of the code which is doing the renaming is:
 [https://github.com/apache/spark/blob/v2.3.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala]
 It's using ResolveReferences.dedupRight which as it says just de duplicates the right side references from the left side (this might be a naive understanding of it).

Then if you just alias one of these columns it's fine. But that really shouldn't be required for the logical plan to be accurate.

 

 ;;;","31/Aug/18 05:58;EeveeB;Sorry my attachment doesn't want to stick, feel free to ask me to email it or explain to me how it works. Sorry!

 ;;;","03/Sep/18 02:07;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/22318;;;","03/Sep/18 05:26;petertoth;[~EeveeB], sorry, I have just noticed that you might have started working on a patch. I think I came to the same conclusion as you and submitted a PR, but I'm quite new to Spark so any comments are welcome.;;;","12/Sep/18 12:07;EeveeB;Hey Peter, don't stress it. I'm new to the community as well but I'm been a busy so all good :);;;","21/Sep/18 15:43;nchammas;Given that Spark appears to provide incorrect results when spark.sql.crossJoin.enabled is set to true, shall we mark this as a correctness issue?

[~petertoth] / [~EeveeB] - Would you agree with that characterization?;;;","28/Sep/18 18:32;nchammas;[~cloud_fan] / [~srowen] - Would you consider this issue (particularly the one expressed when spark.sql.crossJoin.enabled is set to true) to be a correctness bug? I think it is, but I'd like a committer to confirm and add the appropriate label if necessary.;;;","28/Sep/18 18:42;srowen;What's an example of expected vs actual results here that show the bug? is it simple to summarize?;;;","28/Sep/18 18:55;nchammas;The attachments on this ticket contain a complete reproduction. The comment towards the beginning of zombie-analysis.py points to the config that, when enabled, appears to yield incorrect results. (Without the config enabled we get a confusing/incorrect error, which is a second issue.)

The results with and without the config enabled are also attached here. I will add another attachment showing the expected results.

I believe some folks over on the linked PR provided a simpler reproduction of part of this issue, but I haven't taken a close look at it to see if it captures the same two issues (incorrect results + confusing/incorrect error).;;;","28/Sep/18 19:16;petertoth;[~nchammas], sorry for the late reply.

There is only one issue here. Please see zombie-analysis.py, it contains 2 joins and both joins define the condition explicitly, so setting spark.sql.crossJoin.enabled=true {color:#333333}should not have any effect.{color}

{color:#333333}The root cause of the error you see when spark.sql.crossJoin.enabled=false (default) and the incorrect results when spark.sql.crossJoin.enabled=true is the same, the join condition is handled incorrectly.{color}

{color:#333333}Please see my PR's description for further details: [https://github.com/apache/spark/pull/22318]{color}

 ;;;","28/Sep/18 19:30;nchammas;I've uploaded the expected output.

I realize that the reproduction I've attached to this ticket (zombie-analysis.py plus the related files), though complete and self-contained, is a bit verbose. If it's not helpful enough I will see if I can boil it down further.

[~petertoth] - I suggest you take another look at the output with cross joins enabled and compare it to what (I think) is the correct expected output. If I'm understanding things correctly, there are two issues: 1) the bad error when cross join is not enabled (there should be no error), and 2) the incorrect results when cross join _is_ enabled (the results I just uploaded).

Your PR doesn't appear to investigate or address the incorrect results issue, so I'm not sure if it would fix that too of if I am just mistaken about there being a second issue.;;;","28/Sep/18 19:36;nchammas;([~petertoth] - Seeing your comment edit now.) OK, so it seems the two problems I identified are accurate, but they have a common root cause. Thanks for confirming.

[~srowen] - Given Peter's confirmation that the results with cross join enabled are incorrect, I believe we should mark this as a correctness issue.;;;","28/Sep/18 19:54;srowen;Hm, I am not sure I understand the example yet – help me clarify here. We have three dataframes, really; states, humans, zombies:

 
{code:java}
State,Total Population,Total Area
RI,1200000,300000
MA,8000000,17000000
NH,3300000,9100000

+-----+-----+
|State|count|
+-----+-----+
|   RI|    2|
|   NH|    1|
+-----+-----+

+-----+-----+
|State|count|
+-----+-----+
|   RI|    1|
|   MA|    1|
+-----+-----+{code}
You join all three on state:
{code:java}
analysis = (
        states
        .join(
            total_humans,
            on=(states['State'] == total_humans['State'])
        )
        .join(
            total_zombies,
            on=(states['State'] == total_zombies['State'])
        )
        .orderBy(states['State'], ascending=True)
        .select(
            states['State'],
            states['Total Population'],
            total_humans['count'].alias('Total Humans'),
            total_zombies['count'].alias('Total Zombies'),
        )
    )
{code}
and you get
{code:java}
+-----+----------------+------------+-------------+
|State|Total Population|Total Humans|Total Zombies|
+-----+----------------+------------+-------------+
|   NH|         3300000|           1|            1|
|   NH|         3300000|           1|            1|
|   RI|         1200000|           2|            1|
|   RI|         1200000|           2|            1|
+-----+----------------+------------+-------------+{code}
But say you expect
{code:java}
+-----+----------------+------------+-------------+
|State|Total Population|Total Humans|Total Zombies|
+-----+----------------+------------+-------------+
|   RI|         1200000|           2|            1|
+-----+----------------+------------+-------------+{code}
 

First, this isn't a cross join right? the message says it thinks there is no join condition and wonders if you're really trying to do a cross join, but you're not, so enabling it isn't helping. If these were cross-joins, the output would be correct I think?

The second join joins on a column in {{states}}, but that is not a DataFrame used in that join. Is that the problem?

 ;;;","28/Sep/18 20:12;nchammas;Correct, this isn't a cross join. It's just a plain inner join.

In theory, whether cross joins are enabled or not should have no bearing on the result. However, what we're seeing is that without them enabled we get an incorrect error and with them enabled we get incorrect results.

If we were actually trying a cross join (i.e. no {{on=(...)}} condition specified) I think those results (with the 4 output rows) would still be incorrect since you'd expect NH's population to be combined with RI's stats in one of the output rows, but that's not the case. You'd also expect MA to show up in the output, too.

> The second join joins on a column in {{states}}, but that is not a DataFrame used in that join. Is that the problem?

Not sure what you mean here. Both joins join on {{states}}, which is the first DataFrame in the definition of {{analysis}}.

 ;;;","10/Oct/18 20:07;srowen;The second join joins ""states-joined-with-humans"" with ""zombies"", but the join condition references a column in dataframe ""states"", which isn't one of those two dataframes being joined. Obviously all of these tables have a column ""State"" but that's not quite what this code is specifying. I had thought that wasn't allowed or didn't work? Can you try breaking the join down into two statements and making sure the column references only refer to dataframes in each join?

If that condition is being ignored, then you end up with a full cross join, right? Spark seems to think so because it asks if that's what you're doing. And its answer is correct as if it were doing a cross join. It looks correct to me. You do see NH and RI zombie stats mixed with each other; that count is 1 in every case though. So you get double the rows as in the result of the first join, with 1 zombie each.

 ;;;","09/Apr/19 14:58;bsplosion;[~srowen], I ran into this situation yesterday as well, and I think there may be some miscommunication about expected behavior vs actual here.  Many people are accustomed to writing joins in a sequential manner in SQL; using the sample scenario here:

{code:SQL|borderstyle=solid}
SELECT 
a.State, 
a.`Total Population`,
b.count AS `Total Humans`,
c.count AS `Total Zombies`
FROM states AS a
JOIN total_humans AS b
ON a.state = b.state
JOIN total_zombies AS c
ON a.state = c.state
ORDER BY a.state ASC;
{code}

On virtually all ANSI SQL systems, this will result in the output which [~nchammas] mentions is expected.  However, it looks like Spark actually evaluates the chained joins by doing something like (states JOIN humans ON state) JOIN (states JOIN zombies ON state) ON (_no condition specified_).

Part of the problem is that even when you attempt to fix the states['State'] join, you get the ""trivially inferred"" warning with inappropriate output, as they share the same lineage and Spark optimizes past the intended logic:

{code:Python|borderstyle=solid}
states_with_humans = states \
        .join(
            total_humans,
            on=(states['State'] == total_humans['State'])
        )
analysis = states_with_humans \
        .join(
            total_zombies,
            on=(states_with_humans['State'] == total_zombies['State'])
        ) \
        .orderBy(states['State'], ascending=True) \
        .select(
            states_with_humans['State'],
            states_with_humans['Total Population'],
            states_with_humans['count'].alias('Total Humans'),
            total_zombies['count'].alias('Total Zombies'),
        )
    )
{code}

Is there something we're all missing here?  This seems to be a cookie-cutter example of a three-way join not functioning as expected without explicit aliasing.  Is there a reason this behavior is desirable?;;;","09/Apr/19 15:48;srowen;What happens on master, and what happens if you run the SQL query in your example -- is it different?
Your second example is unexpected to me, so I think there is probably an issue here somewhere, especially if ANSI SQL mandates a different behavior here (does it? I don't know);;;","19/Aug/19 19:37;nchammas;I haven't been able to boil down the reproduction further, but I'm updating this issue to confirm that it is still present as of Spark 2.4.3 and, particularly in the case where cross joins are enabled, it appears to be a correctness issue.

My original attachments still capture the problem. These are the inputs:
 * [^persons.csv]
 * [^states.csv]
 * [^zombie-analysis.py]

And here are the outputs:
 * [^expected-output.txt]
 * [^output-without-implicit-cross-join.txt]
 * [^output-with-implicit-cross-join.txt];;;","14/Dec/21 21:20;nchammas;I re-ran my test (described in the issue description + summarized in my comment just above) on Spark 3.2.0, and this issue appears to be resolved! Whether with cross joins enabled or disabled, I now get the correct results.

Obviously, I have no clue what change since Spark 2.4.3 (the last time I reran this test) was responsible for the fix.

But to be clear, in case anyone wants to reproduce my test:
 # Download all 6 files attached to this issue into a folder.
 # Then, from within that folder, run {{spark-submit zombie-analysis.py}} and inspect the output.
 # Then, enable cross joins (commented out on line 9), rerun the script, and reinspect the output.
 # Compare the final bit of output from both runs against {{{}expected-output.txt{}}}.;;;","14/Dec/21 21:28;nchammas;It looks like Spark 3.1.2 exhibits a different sort of broken behavior:
{code:java}
pyspark.sql.utils.AnalysisException: Column State#38 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(""a"").join(df.as(""b""), $""a.id"" > $""b.id"")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check. {code}
I don't think the join in {{zombie-analysis.py}} is ambiguous, and since this now works fine in Spark 3.2.0, that's what I'm going to mark as the ""Fix Version"" for this issue.

The fix must have made it in somewhere between Spark 3.1.2 and 3.2.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Personalized PageRank raises an error if vertexIDs are > MaxInt,SPARK-25149,13179648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,17/Aug/18 17:58,05/Sep/18 22:23,13/Jul/23 08:48,21/Aug/18 22:22,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,GraphX,,,,0,,,,,Looking at the implementation I think we don't actually need this check. The current implementation indexes the sparse vectors used and returned by the method are index by the _position_ of the vertexId in `sources` not by the vertex ID itself. We should remove this check and add a test to confirm the implementation works with large vertex IDs.,,apachespark,bago.amirbekian,dongjoon,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25268,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 22:22:27 UTC 2018,,,,,,,,,,"0|i3x607:",9223372036854775807,,,,,josephkb,,,,,,,,,,,,,,,,,,"17/Aug/18 23:54;apachespark;User 'MrBago' has created a pull request for this issue:
https://github.com/apache/spark/pull/22139;;;","20/Aug/18 02:34;dongjoon;Thank you for the contribution, [~bago.amirbekian]. `Fix Version` will be set later when this is fixed. (http://spark.apache.org/contributing.html 'JIRA' section);;;","21/Aug/18 22:22;josephkb;Issue resolved by pull request 22139
[https://github.com/apache/spark/pull/22139];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
distinct on Dataset leads to exception due to Managed memory leak detected  ,SPARK-25144,13179576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,Ayoub,Ayoub,17/Aug/18 12:47,12/Dec/22 18:10,13/Jul/23 08:48,21/Aug/18 01:09,2.0.2,2.1.3,2.2.2,2.3.1,2.3.2,,,,,,,,,,,,,2.2.3,2.3.2,,,SQL,,,,0,,,,,"The following code example: 
{code}
case class Foo(bar: Option[String])
val ds = List(Foo(Some(""bar""))).toDS
val result = ds.flatMap(_.bar).distinct
result.rdd.isEmpty
{code}
Produces the following stacktrace
{code}
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 42 in stage 7.0 failed 1 times, most recent failure: Lost task 42.0 in stage 7.0 (TID 125, localhost, executor driver): org.apache.spark.SparkException: Managed memory leak detected; size = 16777216 bytes, TID = 125
[info] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:358)
[info] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info] 	at java.lang.Thread.run(Thread.java:748)
[info] 
[info] Driver stacktrace:
[info]   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)
[info]   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
[info]   at scala.Option.foreach(Option.scala:257)
[info]   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)
[info]   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[info]   at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
[info]   at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1358)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
[info]   at org.apache.spark.rdd.RDD.take(RDD.scala:1331)
[info]   at org.apache.spark.rdd.RDD$$anonfun$isEmpty$1.apply$mcZ$sp(RDD.scala:1466)
[info]   at org.apache.spark.rdd.RDD$$anonfun$isEmpty$1.apply(RDD.scala:1466)
[info]   at org.apache.spark.rdd.RDD$$anonfun$isEmpty$1.apply(RDD.scala:1466)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
[info]   at org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1465)

{code}
The code example doesn't produce any error when `distinct` function is not called.",,apachespark,Ayoub,dongjoon,jerryshao,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 20 16:20:05 UTC 2018,,,,,,,,,,"0|i3x5kf:",9223372036854775807,,,,,,,,,,,,,2.2.3,2.3.2,,,,,,,,,"17/Aug/18 14:00;viirya;Hmm, can't reproduce this on master branch, so it is specific to 2.3.1?;;;","17/Aug/18 14:17;Ayoub;[~viirya] the same happens with spark 2.2.1 I assume that it was there since some time.
Maybe you have ""spark.unsafe.exceptionOnMemoryLeak"" set to false which produces a warning instead of throwing an exception ;;;","17/Aug/18 14:46;Ayoub;Somehow adding cache before calling ""isEmpty"" avoids the problem :/
{code}
case class Foo(bar: Option[String])
val ds = List(Foo(Some(""bar""))).toDS
val result = ds.flatMap(_.bar).distinct
result.cache
result.rdd.isEmpty

{code};;;","17/Aug/18 14:55;viirya;Have you tried on master branch? Tried with {{spark.unsafe.exceptionOnMemoryLeak}} as true, but can't reproduce it. I guess it is fixed in master branch?;;;","17/Aug/18 14:58;Ayoub;[~viirya] I haven't tried on master branch, is there any snapshot jar somewhere ?;;;","17/Aug/18 15:31;viirya;I'm not sure if there is, can you build it?;;;","20/Aug/18 03:13;gurwls223;{code}
scala> case class Foo(bar: Option[String])
defined class Foo

scala> val ds = List(Foo(Some(""bar""))).toDS
ds: org.apache.spark.sql.Dataset[Foo] = [bar: string]

scala> val result = ds.flatMap(_.bar).distinct
result: org.apache.spark.sql.Dataset[String] = [value: string]

scala> result.rdd.isEmpty
res0: Boolean = false
{code}

I tried in the master. i am unable to reproduce this. Resolving this. It would be nicer if we identify and see if we can backport. Please link this to the duplicate if it's identified.;;;","20/Aug/18 06:15;dongjoon;I'll reopen this since I can reproduce this in 2.1.3, 2.2.2, 2.3.2-RC5. I found the difference in master. I'll make a PR soon.;;;","20/Aug/18 06:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22150;;;","20/Aug/18 06:38;dongjoon;Ping [~jerryshao] since you are a release engineer.;;;","20/Aug/18 06:41;jerryshao;Does this exist in master branch?;;;","20/Aug/18 07:43;Ayoub;[~jerryshao] according the comments above this bug doesn't happen in Master but I haven't tried my self 

 

thanks for the PR [~dongjoon];;;","20/Aug/18 15:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22155;;;","20/Aug/18 16:20;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22156;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonRunner#WriterThread released block after TaskRunner finally block which  invoke BlockManager#releaseAllLocksForTask,SPARK-25139,13179513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,Deng FEI,Deng FEI,17/Aug/18 06:27,12/Dec/22 18:10,13/Jul/23 08:48,08/May/19 04:54,2.3.1,,,,,,,,,,,,,,,,,2.3.4,2.4.4,3.0.0,,Block Manager,Spark Core,,,0,,,,,"We run pyspark streaming on YARN, the executor will die caused by the error: the task released lock while finished, but the python writer haven't do real releasing lock.

Normally the task just double check the lock, but it ran wrong in front.

The executor trace log is below:
 18/08/17 13:52:20 Executor task launch worker for task 137 DEBUG BlockManager: Getting local block input-0-1534485138800 18/08/17 13:52:20 Executor task launch worker for task 137 TRACE BlockInfoManager: Task 137 trying to acquire read lock for input-0-1534485138800 18/08/17 13:52:20 Executor task launch worker for task 137 TRACE BlockInfoManager: Task 137 acquired read lock for input-0-1534485138800 18/08/17 13:52:20 Executor task launch worker for task 137 DEBUG BlockManager: Level for block input-0-1534485138800 is StorageLevel(disk, memory, 1 replicas) 18/08/17 13:52:20 Executor task launch worker for task 137 INFO BlockManager: Found block input-0-1534485138800 locally 18/08/17 13:52:20 Executor task launch worker for task 137 INFO PythonRunner: Times: total = 8, boot = 3, init = 5, finish = 0 18/08/17 13:52:20 stdout writer for python TRACE BlockInfoManager: Task 137 releasing lock for input-0-1534485138800 18/08/17 13:52:20 Executor task launch worker for task 137 INFO Executor: 1 block locks were not released by TID = 137: [input-0-1534485138800] 18/08/17 13:52:20 stdout writer for python ERROR Utils: Uncaught exception in thread stdout writer for python java.lang.AssertionError: assertion failed: Block input-0-1534485138800 is not locked for reading at scala.Predef$.assert(Predef.scala:170) at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:299) at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:769) at org.apache.spark.storage.BlockManager$$anonfun$1.apply$mcV$sp(BlockManager.scala:540) at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:44) at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:33) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213) at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407) at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991) at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170) 18/08/17 13:52:20 stdout writer for python ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[stdout writer for python,5,main]

 

I think shoud wait WriterThread after Task#run.",,Deng FEI,rezasafi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18406,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 22 02:05:54 UTC 2019,,,,,,,,,,"0|i3x56f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/May/19 04:54;gurwls223;Issue resolved by pull request 24542
[https://github.com/apache/spark/pull/24542];;;","21/May/19 18:53;rezasafi;Any reason that this bug fix wasn't merged to 2.3 line?;;;","22/May/19 01:19;gurwls223;I think we can. Please go ahead and open a backporting PR.;;;","22/May/19 02:05;rezasafi;Sure, I will send a pr soon. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumberFormatException` when starting spark-shell  from Mac terminal,SPARK-25137,13179500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,vinodkc,vinodkc,vinodkc,17/Aug/18 05:05,12/Dec/22 18:10,13/Jul/23 08:48,18/Aug/18 09:19,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Shell,,,,0,easyfix,,,,"NumberFormatException` when starting spark-shell from Mac terminal

./bin/spark-shell
 18/08/17 08:43:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
 Setting default log level to ""WARN"".
 To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
 Welcome to
 ____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /___/ .__/_,_/_/ /_/_\ version 2.4.0-SNAPSHOT
 /_/
 Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
 Type in expressions to have them evaluated.
 Type :help for more information.
 [ERROR] Failed to construct terminal; falling back to unsupported
 java.lang.NumberFormatException: For input string: ""0x100""
 at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
 at java.lang.Integer.parseInt(Integer.java:580)
 at java.lang.Integer.valueOf(Integer.java:766)
 at jline.internal.InfoCmp.parseInfoCmp(InfoCmp.java:59)
 at jline.UnixTerminal.parseInfoCmp(UnixTerminal.java:242)
 at jline.UnixTerminal.<init>(UnixTerminal.java:65)
 at jline.UnixTerminal.<init>(UnixTerminal.java:50)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at java.lang.Class.newInstance(Class.java:442)
 at jline.TerminalFactory.getFlavor(TerminalFactory.java:211)
 at jline.TerminalFactory.create(TerminalFactory.java:102)
 at jline.TerminalFactory.get(TerminalFactory.java:186)
 at jline.TerminalFactory.get(TerminalFactory.java:192)
 at jline.console.ConsoleReader.<init>(ConsoleReader.java:243)
 at jline.console.ConsoleReader.<init>(ConsoleReader.java:235)
 at jline.console.ConsoleReader.<init>(ConsoleReader.java:223)
 at scala.tools.nsc.interpreter.jline.JLineConsoleReader.<init>(JLineReader.scala:64)
 at scala.tools.nsc.interpreter.jline.InteractiveReader.<init>(JLineReader.scala:33)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$scala$tools$nsc$interpreter$ILoop$$instantiater$1$1.apply(ILoop.scala:858)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$scala$tools$nsc$interpreter$ILoop$$instantiater$1$1.apply(ILoop.scala:855)
 at scala.tools.nsc.interpreter.ILoop.scala$tools$nsc$interpreter$ILoop$$mkReader$1(ILoop.scala:862)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$22$$anonfun$apply$10.apply(ILoop.scala:873)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$22$$anonfun$apply$10.apply(ILoop.scala:873)
 at scala.util.Try$.apply(Try.scala:192)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$22.apply(ILoop.scala:873)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$22.apply(ILoop.scala:873)
 at scala.collection.immutable.Stream.map(Stream.scala:418)
 at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:873)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1$$anonfun$newReader$1$1.apply(ILoop.scala:893)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.newReader$1(ILoop.scala:893)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.scala$tools$nsc$interpreter$ILoop$$anonfun$$preLoop$1(ILoop.scala:897)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(ILoop.scala:964)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:990)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:891)
 at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:891)
 at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
 at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:891)
 at org.apache.spark.repl.Main$.doMain(Main.scala:78)
 at org.apache.spark.repl.Main$.main(Main.scala:58)
 at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:847)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:922)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
 scala> Spark context Web UI available at [http://192.168.0.199:4040|http://192.168.0.199:4040/]
 Spark context available as 'sc' (master = local[*], app id = local-1534475624109).
 Spark session available as 'spark'.
 scala> spark.version
 res0: String = 2.4.0-SNAPSHOT

This issue is fixed in jline  2.14.4

Jline issue: [https://github.com/jline/jline2/issues/281]

Spark needs to bump up the jline version","MacOS High Sirra Version 10.13.6

 ",apachespark,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 18 09:19:58 UTC 2018,,,,,,,,,,"0|i3x53j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/18 05:13;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/22130;;;","18/Aug/18 09:19;gurwls223;Issue resolved by pull request 22130
[https://github.com/apache/spark/pull/22130];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Csv column pruning with checking of headers throws incorrect error,SPARK-25134,13179386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koertkuipers,koert,koert,16/Aug/18 15:47,12/Dec/22 18:10,13/Jul/23 08:48,21/Aug/18 02:25,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"hello!
seems to me there is some interaction between csv column pruning and the checking of csv headers that is causing issues. for example this fails:
{code:scala}
Seq((""a"", ""b"")).toDF(""columnA"", ""columnB"").write
  .format(""csv"")
  .option(""header"", true)
  .save(dir)
spark.read
  .format(""csv"")
  .option(""header"", true)
  .option(""enforceSchema"", false)
  .load(dir)
  .select(""columnA"")
  .show
{code}

the error is:
{code:bash}
291.0 (TID 319, localhost, executor driver): java.lang.IllegalArgumentException: Number of column in CSV header is not equal to number of fields in the schema:
[info]  Header length: 1, schema size: 2
{code}

if i remove the project it works fine. if i disable column pruning it also works fine.",spark master branch at a791c29bd824adadfb2d85594bc8dad4424df936,apachespark,koert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23786,SPARK-24244,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 02:25:08 UTC 2018,,,,,,,,,,"0|i3x4e7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/18 15:59;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/22123;;;","21/Aug/18 02:25;gurwls223;Fixed in https://github.com/apache/spark/pull/22123;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case-insensitive field resolution when reading from Parquet,SPARK-25132,13179358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,seancxmao,seancxmao,seancxmao,16/Aug/18 13:48,12/Dec/22 18:10,13/Jul/23 08:48,21/Aug/18 02:35,2.2.0,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,Parquet,,,,"Spark SQL returns NULL for a column whose Hive metastore schema and Parquet schema are in different letter cases, regardless of spark.sql.caseSensitive set to true or false.

Here is a simple example to reproduce this issue:

scala> spark.range(5).toDF.write.mode(""overwrite"").saveAsTable(""t1"")

spark-sql> show create table t1;
CREATE TABLE `t1` (`id` BIGINT)
USING parquet
OPTIONS (
 `serialization.format` '1'
)

spark-sql> CREATE TABLE `t2` (`ID` BIGINT)
 > USING parquet
 > LOCATION 'hdfs://localhost/user/hive/warehouse/t1';

spark-sql> select * from t1;
0
1
2
3
4

spark-sql> select * from t2;
NULL
NULL
NULL
NULL
NULL

 ",,apachespark,rajeshhadoop,seancxmao,yucai,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25175,SPARK-25206,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 05 15:19:46 UTC 2018,,,,,,,,,,"0|i3x47z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/18 14:38;seancxmao;We found this PR ([https://github.com/apache/spark/pull/15799)] can solve this issue, but no idea why it is removed.;;;","16/Aug/18 15:09;yucai;If Spark allows data source case insensitive, query t2 should return number.
If Spark does not allow data source case insensitive, Spark should remind user with warning, return NULL may lead to the potential issue that is very difficult to find.;;;","16/Aug/18 22:43;yucai;[~cloud_fan] [~smilegator] [~budde] [~ekhliang], do you have any insight?;;;","17/Aug/18 03:20;seancxmao;I'm working on this.;;;","17/Aug/18 09:22;yumwang;Stackoverflow has been asked this question [Spark SQL returns null for a column in HIVE table while HIVE query returns non null values|https://stackoverflow.com/questions/50298909/spark-sql-returns-null-for-a-column-in-hive-table-while-hive-query-returns-non-n].;;;","19/Aug/18 16:31;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22142;;;","20/Aug/18 03:25;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22148;;;","21/Aug/18 02:35;gurwls223;Issue resolved by pull request 22148
[https://github.com/apache/spark/pull/22148];;;","22/Aug/18 05:28;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22183;;;","22/Aug/18 09:32;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22184;;;","05/Sep/18 15:13;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22343;;;","05/Sep/18 15:14;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22343;;;","05/Dec/18 15:18;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/23238;;;","05/Dec/18 15:19;apachespark;User 'seancxmao' has created a pull request for this issue:
https://github.com/apache/spark/pull/23238;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid creating OrcFile.Reader for all orc files,SPARK-25126,13179140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rfu,rfu,rfu,15/Aug/18 18:06,12/Dec/22 18:10,13/Jul/23 08:48,23/Aug/18 14:01,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Input/Output,,,,0,,,,,"We have a spark job that starts by reading orc files under an S3 directory and we noticed the job consumes a lot of memory when both the number of orc files and the size of the file are large. The memory bloat went away with the following workaround.

1) create a DataSet<Row> from a single orc file.

Dataset<Row> rowsForFirstFile = spark.read().format(""orc"").load(oneFile);

2) when creating DataSet<Row> from all files under the directory, use the schema from the previous DataSet.

Dataset<Row> rows = spark.read().schema(rowsForFirstFile.schema()).format(""orc"").load(path);

I believe the issue is due to the fact in order to infer the schema a FileReader is created for each orc file under the directory although only the first one is used. The FileReader creation loads the metadata of the orc file and the memory consumption is very high when there are many files under the directory.

The issue exists in both 2.0 and HEAD.

In 2.0, OrcFileOperator.readSchema is used.

[https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileOperator.scala#L95]

In HEAD, OrcUtils.readSchema is used.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala#L82

 

 ",,apachespark,erwaman,rfu,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 14:01:58 UTC 2018,,,,,,,,,,"0|i3x2vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/18 20:35;apachespark;User 'raofu' has created a pull request for this issue:
https://github.com/apache/spark/pull/22113;;;","17/Aug/18 23:36;rfu;Updated the bug description to accurately describe the issue. ;;;","20/Aug/18 17:13;apachespark;User 'raofu' has created a pull request for this issue:
https://github.com/apache/spark/pull/22157;;;","22/Aug/18 20:33;stevel@apache.org;+ [~dongjoon];;;","23/Aug/18 14:01;gurwls223;Issue resolved by pull request 22157
[https://github.com/apache/spark/pull/22157];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"VectorSizeHint.size is buggy, breaking streaming pipeline",SPARK-25124,13179038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,timhunter,timhunter,15/Aug/18 09:45,24/Aug/18 22:41,13/Jul/23 08:48,24/Aug/18 22:41,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,ML,,,,0,beginner,starter,,,"Currently, when using {{VectorSizeHint().setSize(3)}} in an ML pipeline, transforming a stream will return a nondescript exception about the stream not started. At core are the following bugs that {{setSize}} and {{getSize}} do not {{return}} values but {{None}}:

https://github.com/apache/spark/blob/master/python/pyspark/ml/feature.py#L3846

How to reproduce, using the example in the doc:

{code}
from pyspark.ml.linalg import Vectors
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import VectorAssembler, VectorSizeHint
data = [(Vectors.dense([1., 2., 3.]), 4.)]
df = spark.createDataFrame(data, [""vector"", ""float""])
sizeHint = VectorSizeHint(inputCol=""vector"", handleInvalid=""skip"").setSize(3) # Will fail
vecAssembler = VectorAssembler(inputCols=[""vector"", ""float""], outputCol=""assembled"")
pipeline = Pipeline(stages=[sizeHint, vecAssembler])
pipelineModel = pipeline.fit(df)
pipelineModel.transform(df).head().assembled
{code}",,apachespark,dongjoon,huaxingao,josephkb,timhunter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 22:41:59 UTC 2018,,,,,,,,,,"0|i3x29b:",9223372036854775807,,,,,josephkb,,,,,,,,2.3.2,2.4.0,,,,,,,,,"17/Aug/18 17:52;huaxingao;I will submit a PR very soon. ;;;","17/Aug/18 18:24;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22136;;;","23/Aug/18 23:23;josephkb;I merged https://github.com/apache/spark/pull/22136 into master for target 2.4.0.  I'll leave this open until we backport it to branch-2.3;;;","24/Aug/18 21:15;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22228;;;","24/Aug/18 22:41;josephkb;Issue resolved by pull request 22228
[https://github.com/apache/spark/pull/22228];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix the ""exit code 1"" error when terminating Kafka tests",SPARK-25116,13178912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,14/Aug/18 17:59,17/Aug/18 21:21,13/Jul/23 08:48,17/Aug/18 21:21,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Tests,,,,0,,,,,"SBT may report the following error when all Kafka tests are finished
{code}
sbt/sbt/0.13.17/test-interface-1.0.jar sbt.ForkMain 39359 failed with exit code 1
[error] (sql-kafka-0-10/test:test) sbt.TestsFailedException: Tests unsuccessful
{code}

This is because we are leaking a Kafka cluster.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 14 18:13:05 UTC 2018,,,,,,,,,,"0|i3x1hb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/18 18:13;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22106;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecordBinaryComparator may return wrong result when subtraction between two words is divisible by Integer.MAX_VALUE,SPARK-25114,13178868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,14/Aug/18 14:26,02/Mar/20 08:01,13/Jul/23 08:48,21/Aug/18 22:56,2.1.3,2.2.2,2.3.0,2.3.1,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,Spark Core,,,,0,correctness,,,,"It is possible for two objects to be unequal and yet we consider them as equal within RecordBinaryComparator, if the long values are separated by Int.MaxValue.",,apachespark,bersprockets,jerryshao,jiangxb1987,riza,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23207,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 23:27:11 UTC 2018,,,,,,,,,,"0|i3x17j:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"14/Aug/18 14:27;jiangxb1987;I created https://github.com/apache/spark/pull/22101 for this.;;;","14/Aug/18 14:28;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/22101;;;","14/Aug/18 17:46;smilegator;[~jerryshao] Another blocker for 2.3;;;","20/Aug/18 01:03;jerryshao;What's the ETA of this issue [~jiangxb1987]?;;;","21/Aug/18 06:41;jiangxb1987;The PR has been merged to master and 2.3;;;","21/Aug/18 07:14;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/22166;;;","21/Aug/18 22:51;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/22079;;;","21/Aug/18 22:56;smilegator;Let us update the fix version after the fix of 2.2 is merged;;;","23/Aug/18 23:27;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/22211;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using KryoSerializer and setting registrationRequired true can lead job failed,SPARK-25100,13178557,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,deshanxiao,deshanxiao,deshanxiao,13/Aug/18 10:37,10/Feb/22 01:10,13/Jul/23 08:48,15/Dec/19 01:15,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,1,,,,,"When spark.serializer is `org.apache.spark.serializer.KryoSerializer` and  `spark.kryo.registrationRequired` is true in SparkConf. I invoked  saveAsNewAPIHadoopDataset to store data in hdfs. The job will fail because the class TaskCommitMessage hasn't be registered.

 
{code:java}
java.lang.IllegalArgumentException: Class is not registered: org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage
Note: To register this class use: kryo.register(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage.class);
at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:488)
at com.twitter.chill.KryoBase.getRegistration(KryoBase.scala:52)
at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97)
at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517)
at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:622)
at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:347)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:393)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
{code}
 ",,apachespark,deshanxiao,dongjoon,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21569,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 15 01:15:56 UTC 2019,,,,,,,,,,"0|i3wzan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/18 14:55;apachespark;User 'deshanxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22093;;;","15/Dec/19 01:15;dongjoon;Issue resolved by pull request 26714
[https://github.com/apache/spark/pull/26714];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trim the string when cast stringToTimestamp and stringToDate,SPARK-25098,13178523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,a283791411,a283791411,13/Aug/18 07:23,22/Nov/19 11:55,13/Jul/23 08:48,07/Nov/18 05:30,2.3.0,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"UDF ‘Cast’ will return NULL when input string starts/ends with special character, but hive doesn't.

For examle, we get hour from a string ends with a blank :

hive：

```
hive> SELECT CAST(' 2018-08-13' AS DATE);//starts with a blank
OK 
2018-08-13

hive> SELECT HOUR('2018-08-13 17:20:07 );//ends with a blank
OK
17
```

spark-sql:

```
spark-sql> SELECT CAST(' 2018-08-13' AS DATE);//starts with a blank
NULL

spark-sql> SELECT HOUR('2018-08-13 17:20:07 );//ends with a blank
NULL
```

All of the following UDFs will be affected:
```
year
month
day
hour
minute
second
date_add
date_sub
```

",,a283791411,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28023,SPARK-30000,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 07 05:30:11 UTC 2018,,,,,,,,,,"0|i3wz33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/18 08:26;apachespark;User 'bingbai0912' has created a pull request for this issue:
https://github.com/apache/spark/pull/22089;;;","05/Nov/18 05:44;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22943;;;","05/Nov/18 05:44;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22943;;;","07/Nov/18 05:30;dongjoon;This is resolved via https://github.com/apache/spark/pull/22943;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loosen nullability if the cast is force-nullable.,SPARK-25096,13178503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,13/Aug/18 03:26,12/Dec/22 18:11,13/Jul/23 08:48,13/Aug/18 11:27,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"In type coercion for complex types, if the found type is force-nullable to cast, we should loosen the nullability to be able to cast. Also for map key type, we can't use the type.",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 13 11:27:58 UTC 2018,,,,,,,,,,"0|i3wyyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/18 03:32;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22086;;;","13/Aug/18 11:27;gurwls223;Issue resolved by pull request 22086
[https://github.com/apache/spark/pull/22086];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add RewriteExceptAll, RewriteIntersectAll and RewriteCorrelatedScalarSubquery in the list of nonExcludableRules",SPARK-25092,13178441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,11/Aug/18 18:51,15/Aug/18 07:02,13/Jul/23 08:48,12/Aug/18 05:51,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,Add RewriteExceptAll and RewriteIntersectAll in the list of nonExcludableRules as the rewrites are essential for the functioning of EXCEPT ALL and INTERSECT ALL feature.,,apachespark,dkbiswal,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 14 21:46:05 UTC 2018,,,,,,,,,,"0|i3wykv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/18 18:57;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22080;;;","14/Aug/18 21:46;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/22108;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassCastException when using a CrossValidator,SPARK-25090,13178401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,Mark53,Mark53,10/Aug/18 23:33,12/Dec/22 18:11,13/Jul/23 08:48,13/Aug/18 01:12,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,ML,,,,0,,,,,"When I fit a LogisticRegression on a dataset, everything works fine but when I fit a CrossValidator, I get this error:

py4j.protocol.Py4JJavaError: An error occurred while calling o1187.w.
: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double
 at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
 at org.apache.spark.ml.param.DoubleParam.w(params.scala:330)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
 at py4j.Gateway.invoke(Gateway.java:282)
 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
 at py4j.commands.CallCommand.execute(CallCommand.java:79)
 at py4j.GatewayConnection.run(GatewayConnection.java:238)
 at java.lang.Thread.run(Unknown Source)

Casting the target variable into double didn't solve the issue.

Here is the snippet:
lr = LogisticRegression(maxIter=10, labelCol=""class"", featuresCol=""features"", rawPredictionCol=""score"")
evaluator = BinaryClassificationEvaluator(rawPredictionCol=""score"", labelCol=""class"", metricName=""areaUnderROC"")
paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.05, 0.1, 0.5, 1]).build()

crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)

bestModel = crossval.fit(train)
 ","Windows 10 64-bits, pyspark 2.3.1 on Anaconda.",apachespark,Mark53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 13 01:12:39 UTC 2018,,,,,,,,,,"0|i3wybz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/18 10:18;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22076;;;","13/Aug/18 01:12;gurwls223;Issue resolved by pull request 22076
[https://github.com/apache/spark/pull/22076];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""distribute by"" on multiple columns may lead to codegen issue",SPARK-25084,13178190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yucai,yucai,yucai,10/Aug/18 05:19,13/Aug/18 00:51,13/Jul/23 08:48,11/Aug/18 13:39,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,"Test Query:
{code:java}
select * from store_sales distribute by (ss_sold_time_sk, ss_item_sk, ss_customer_sk, ss_cdemo_sk, ss_addr_sk, ss_promo_sk) limit 1;{code}
Exception:
{code:java}
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 131, Column 67: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 131, Column 67: One of ', )' expected instead of '['
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1435)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1497)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1494)
at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342){code}
Wrong Codegen:
{code:java}
/* 131 */ private int computeHashForStruct_1(InternalRow mutableStateArray[0], int value1) {
/* 132 */
/* 133 */
/* 134 */ if (!mutableStateArray[0].isNullAt(5)) {
/* 135 */
/* 136 */ final int element5 = mutableStateArray[0].getInt(5);
/* 137 */ value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(element5, value1);
/* 138 */
/* 139 */ }{code}
 ",,apachespark,cloud_fan,cltlfcjin,dongjoon,jerryshao,mgaido,praetp,rajeshhadoop,smilegator,yucai,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 11 15:13:10 UTC 2018,,,,,,,,,,"0|i3wx13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/18 05:27;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/22066;;;","10/Aug/18 05:35;yumwang;[~smilegator], [~jerryshao] I think It should be target 2.3.2.

 ;;;","10/Aug/18 05:38;jerryshao;I'm already preparing new RC4. If this is not a severe issue, I would not block the RC4 release.;;;","10/Aug/18 05:44;jerryshao;Is this a regression or just a bug existed in old version?;;;","10/Aug/18 05:50;yumwang;It's a regression.;;;","10/Aug/18 05:54;jerryshao;I see. Unfortunately I've cut the RC4, if it worth to include in 2.3.2, I will cut a new RC.;;;","10/Aug/18 06:12;smilegator;Let us mark it as a blocker. How about the master branch? Does it work?;;;","10/Aug/18 06:12;smilegator;Could you investigate which PR introduced this bug? What is the error message?;;;","10/Aug/18 06:20;yucai;It is a regression, when the generated codes size is more than 1024, newer Spark will split it into many functions, but the function definition is wrong, like below:
{code:java}
private int computeHashForStruct_0(InternalRow mutableStateArray[0], int value1) {
{code}
 

In the older version, like 2.1.0, it does not split function, so it has no this issue.

 ;;;","10/Aug/18 07:15;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22067;;;","10/Aug/18 07:18;cltlfcjin;I offer other fix way. https://github.com/apache/spark/pull/22067
It doesn't need ""input"" as a global variable (If distribute by random);;;","10/Aug/18 15:16;yucai;[~smilegator], [~jerryshao]
Thanks a lot for marking it blocker.
A lot of eBay's tables use ""distribute by"" or ""cluster by"", it is important for us to move to Spark 2.3.;;;","11/Aug/18 13:39;cloud_fan;Issue resolved by pull request 22066
[https://github.com/apache/spark/pull/22066];;;","11/Aug/18 15:13;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22077;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested spill in ShuffleExternalSorter may access a released memory page ,SPARK-25081,13178159,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,10/Aug/18 00:01,12/Aug/18 05:54,13/Jul/23 08:48,10/Aug/18 18:13,1.6.0,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.1.3,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.3.2,2.2.3,2.3.3,2.4.0,,Spark Core,,,,0,correctness,,,,"This issue is pretty similar to SPARK-21907. 
""allocateArray"" in [ShuffleInMemorySorter.reset|https://github.com/apache/spark/blob/9b8521e53e56a53b44c02366a99f8a8ee1307bbf/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java#L99] may trigger a spill and cause ShuffleInMemorySorter access the released `array`. Another task may get the same memory page from the pool. This will cause two tasks access the same memory page. When a task reads memory written by another task, many types of failures may happen. Here are some examples I  have seen:

- JVM crash. (This is easy to reproduce in a unit test as we fill newly allocated and deallocated memory with 0xa5 and 0x5a bytes which usually points to an invalid memory address)
- java.lang.IllegalArgumentException: Comparison method violates its general contract!
- java.lang.NullPointerException at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:384)
- java.lang.UnsupportedOperationException: Cannot grow BufferHolder by size -536870912 because the size after growing exceeds size limitation 2147483632",,apachespark,dongjoon,kiszk,riza,roczei,tgraves,toopt4,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21907,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 10 18:55:05 UTC 2018,,,,,,,,,,"0|i3wwu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/18 00:13;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22062;;;","10/Aug/18 13:02;tgraves;Does this ever result in the task reading the wrong data and succeeding?  so we essentially lost data or got the wrong results.;;;","10/Aug/18 17:45;zsxwing;That's possible. That's why I added the ""corrnectness"" label.;;;","10/Aug/18 18:32;tgraves;thanks, wanted to clarify since the description only mentioned exceptions/crashes. ;;;","10/Aug/18 18:55;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22072;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLConf should not be retrieved from a stopped SparkSession,SPARK-25076,13178010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,09/Aug/18 14:25,10/Aug/18 01:25,13/Jul/23 08:48,09/Aug/18 21:40,2.4.0,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24502,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 09 14:33:04 UTC 2018,,,,,,,,,,"0|i3wvx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/18 14:33;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22056;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark custom Row class can be given extra parameters,SPARK-25072,13177965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,XuanYuan,dutch_gecko,dutch_gecko,09/Aug/18 12:06,12/Dec/22 18:10,13/Jul/23 08:48,06/Sep/18 17:18,2.2.0,,,,,,,,,,,,,,,,,2.4.0,,,,PySpark,,,,0,,,,,"When a custom Row class is made in PySpark, it is possible to provide the constructor of this class with more parameters than there are columns. These extra parameters affect the value of the Row, but are not part of the {{repr}} or {{str}} output, making it hard to debug errors due to these ""invisible"" values. The hidden values can be accessed through integer-based indexing though.

Some examples:

{code:python}
In [69]: RowClass = Row(""column1"", ""column2"")

In [70]: RowClass(1, 2) == RowClass(1, 2)
Out[70]: True

In [71]: RowClass(1, 2) == RowClass(1, 2, 3)
Out[71]: False

In [75]: RowClass(1, 2, 3)
Out[75]: Row(column1=1, column2=2)

In [76]: RowClass(1, 2)
Out[76]: Row(column1=1, column2=2)

In [77]: RowClass(1, 2, 3).asDict()
Out[77]: {'column1': 1, 'column2': 2}

In [78]: RowClass(1, 2, 3)[2]
Out[78]: 3

In [79]: repr(RowClass(1, 2, 3))
Out[79]: 'Row(column1=1, column2=2)'

In [80]: str(RowClass(1, 2, 3))
Out[80]: 'Row(column1=1, column2=2)'
{code}","{noformat}
SPARK_MAJOR_VERSION is set to 2, using Spark2
Python 3.4.5 (default, Dec 11 2017, 16:57:19)
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/08/01 04:49:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/08/01 04:49:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
18/08/01 04:49:27 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/

Using Python version 3.4.5 (default, Dec 11 2017 16:57:19)
SparkSession available as 'spark'.
{noformat}

{{CentOS release 6.9 (Final)}}
{{Linux sandbox-hdp.hortonworks.com 4.14.0-1.el7.elrepo.x86_64 #1 SMP Sun Nov 12 20:21:04 EST 2017 x86_64 x86_64 x86_64 GNU/Linux}}
{noformat}openjdk version ""1.8.0_161""
OpenJDK Runtime Environment (build 1.8.0_161-b14)
OpenJDK 64-Bit Server VM (build 25.161-b14, mixed mode){noformat}",apachespark,bryanc,dongjoon,dutch_gecko,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 10 23:53:00 UTC 2018,,,,,,,,,,"0|i3wvn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/18 09:02;gurwls223;From a cursory look, we should add a check in

{code}
def _create_row(fields, values):
    row = Row(*values)
    row.__fields__ = fields
    return row
{code}

at {{type.spy}} although we should double check if that's going to break something else.;;;","18/Aug/18 08:35;XuanYuan;Interesting issue, but maybe this only in PySpark, Scala Row will not have this problem. I'll give a swiftly fix for this.;;;","18/Aug/18 08:43;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/22140;;;","06/Sep/18 17:18;bryanc;Issue resolved by pull request 22140
[https://github.com/apache/spark/pull/22140];;;","09/Sep/18 04:31;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/22369;;;","09/Sep/18 04:32;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/22369;;;","10/Sep/18 23:53;dongjoon;[~bryanc] and [~smilegator] Since this is reverted from `branch-2.3`, I removed the fixed version 2.3.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Block.isEmpty/nonEmpty to check whether the code is empty or not.,SPARK-25058,13177734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,08/Aug/18 17:07,09/Aug/18 05:07,13/Jul/23 08:48,09/Aug/18 05:07,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"We should use {{Block.isEmpty/nonEmpty}} instead of comparing with empty string to check whether the code is empty or not.

{noformat}
[error] [warn] /.../sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala:278: org.apache.spark.sql.catalyst.expressions.codegen.Block and String are unrelated: they will most likely always compare unequal
[error] [warn]       if (ev.code != """" && required.contains(attributes(i))) {
[error] [warn] 
[error] [warn] /.../sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala:323: org.apache.spark.sql.catalyst.expressions.codegen.Block and String are unrelated: they will most likely never compare equal
[error] [warn]          |  ${buildVars.filter(_.code == """").map(v => s""${v.isNull} = true;"").mkString(""\n"")}
[error] [warn] 
{noformat}
",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 09 05:07:19 UTC 2018,,,,,,,,,,"0|i3wu7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/18 17:12;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22041;;;","09/Aug/18 05:07;ueshin;Issue resolved by pull request 22041
https://github.com/apache/spark/pull/22041;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
where clause on dataset gives AnalysisException,SPARK-25051,13177489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,MIK1007,MIK1007,07/Aug/18 18:59,12/Dec/22 18:10,13/Jul/23 08:48,14/Aug/18 17:26,2.3.0,,,,,,,,,,,,,,,,,2.3.2,,,,SQL,,,,0,correctness,,,,"*schemas :*
df1
=> id ts
df2
=> id name country

*code:*

val df = df1.join(df2, Seq(""id""), ""left_outer"").where(df2(""id"").isNull)

*error*:

org.apache.spark.sql.AnalysisException:Resolved attribute(s) id#0 missing from xx#15,xx#9L,id#5,xx#6,xx#11,xx#14,xx#13,xx#12,xx#7,xx#16,xx#10,xx#8L in operator !Filter isnull(id#0). Attribute(s) with the same name appear in the operation: id. Please check if the right attribute(s) are used.;;

 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:41)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:289)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)
    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)
    at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
    at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
    at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
    at org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)
    at org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)
    at org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)
    at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3300)
    at org.apache.spark.sql.Dataset.filter(Dataset.scala:1458)
    at org.apache.spark.sql.Dataset.where(Dataset.scala:1486)

This works fine in spark 2.2.2",,apachespark,mgaido,MIK1007,smilegator,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24865,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 14 15:35:21 UTC 2018,,,,,,,,,,"0|i3wspb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/18 02:13;gurwls223;Can you post some codes for df1 and df2 as well?;;;","08/Aug/18 21:15;MIK1007;df1 and df2, both are reading from S3 files

df1 = spark.read.format(""csv"").option(""header"", ""false"").
                 option(""codec"", ""org.apache.hadoop.io.compress.GzipCodec"").
                 option(""sep"", ""\t"").schema(schema).load(datafile);;;","08/Aug/18 21:32;MIK1007;I have added a sample program which shows this issue:

Works fine with Spark 2.2.1 but not with 2.3.0
{code:java}
val testschema1 = StructType(
 Array(
 StructField(""id"", IntegerType, false),
 StructField(""name"", StringType, true)
 ))

val testschema2 = StructType(
 Array(
 StructField(""id"", IntegerType, false)
 ))

val someData1 = Seq(
 Row(1, ""one""),
 Row(2, ""two""),
 Row(3, ""three"")
)

val df1 = spark.createDataFrame(
 spark.sparkContext.parallelize(someData1),
 StructType(testschema1)
)

val someData2 = Seq(
 Row(2)
)

val df2 = spark.createDataFrame(
 spark.sparkContext.parallelize(someData2),
 StructType(testschema2)
)

val df = df1.join(df2, Seq(""id""), ""left_outer"").where(df2(""id"").isNull)
println(df.show()){code};;;","10/Aug/18 17:02;MIK1007;Any update on this issue ? thanks.;;;","12/Aug/18 03:38;yumwang;Can you verify it with Spark [2.3.2-rc4 |https://dist.apache.org/repos/dist/dev/spark/v2.3.2-rc4-bin/]?;;;","13/Aug/18 18:21;MIK1007;Thanks [~yumwang] , with 2.3.2-rc4, not getting the error now but the result is not correct (getting 0 records), 
 +-----+--+
|id|name|

+-----+--+

The sample program should return 2 records.
 +-----+---+
|id|name|
|1|one|
|3|three|

+-----+---+;;;","14/Aug/18 03:12;yumwang;Yes. The bug only exist in branch-2.3. I can reproduced by:
{code}
val df1 = spark.range(4).selectExpr(""id"", ""cast(id as string) as name"")
val df2 = spark.range(3).selectExpr(""id"")
df1.join(df2, Seq(""id""), ""left_outer"").where(df2(""id"").isNull).show
{code};;;","14/Aug/18 08:40;mgaido;cc [~jerryshao] shall we set it as a blocker for 2.3.2?;;;","14/Aug/18 09:24;mgaido;This was caused by the introduction of AnalysisBarrier. I will submit a PR for branch 2.3. On 2.4+ (current master) we don't have anymore this issue because AnalysisBarrier was removed. Anyway, this brings a question to me: shall we remove AnalysisBarrier from 2.3 line too? In the current situation, backporting any analyzer fix to 2.3 is going to be painful.
cc [~rxin] [~cloud_fan];;;","14/Aug/18 09:35;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22102;;;","14/Aug/18 15:35;smilegator;[~mgaido] This breaks the backport rule. We are unable to remove AnalysisBarrier from 2.3. AnalysisBarrier is a nightmare to many. Sorry for that. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Alter View  can excute sql  like ""ALTER VIEW ... AS INSERT INTO"" ",SPARK-25046,13177435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sddyljsx,sddyljsx,sddyljsx,07/Aug/18 15:18,07/Aug/18 21:54,13/Jul/23 08:48,07/Aug/18 21:54,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,pull-request-available,,,,"Alter View  can excute sql  like ""ALTER VIEW ... AS INSERT INTO"" . We should throw 

ParseException(s""Operation not allowed: $message"", ctx)  as Create View does.",,apachespark,sddyljsx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 07 15:31:05 UTC 2018,,,,,,,,,,"0|i3wsdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/18 15:31;apachespark;User 'sddyljsx' has created a pull request for this issue:
https://github.com/apache/spark/pull/22028;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
genjavadoc-plugin_0.10 is not found with sbt in scala-2.12,SPARK-25041,13177315,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,07/Aug/18 07:44,07/Aug/18 16:59,13/Jul/23 08:48,07/Aug/18 16:59,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,,,,0,,,,,"When the master is build with sbt in scala-2.12, the following error occurs:

{code}
[warn] 	module not found: com.typesafe.genjavadoc#genjavadoc-plugin_2.12.6;0.10
[warn] ==== public: tried
[warn]   https://repo1.maven.org/maven2/com/typesafe/genjavadoc/genjavadoc-plugin_2.12.6/0.10/genjavadoc-plugin_2.12.6-0.10.pom
[warn] ==== Maven2 Local: tried
[warn]   file:/gsa/jpngsa/home/i/s/ishizaki/.m2/repository/com/typesafe/genjavadoc/genjavadoc-plugin_2.12.6/0.10/genjavadoc-plugin_2.12.6-0.10.pom
[warn] ==== local: tried
[warn]   /gsa/jpngsa/home/i/s/ishizaki/.ivy2/local/com.typesafe.genjavadoc/genjavadoc-plugin_2.12.6/0.10/ivys/ivy.xml
[info] Resolving jline#jline;2.14.3 ...
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	::          UNRESOLVED DEPENDENCIES         ::
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	:: com.typesafe.genjavadoc#genjavadoc-plugin_2.12.6;0.10: not found
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 
[warn] 	Note: Unresolved dependencies path:
[warn] 		com.typesafe.genjavadoc:genjavadoc-plugin_2.12.6:0.10 (/home/ishizaki/Spark/PR/scala212/spark/project/SparkBuild.scala#L118)
[warn] 		  +- org.apache.spark:spark-tags_2.12:2.4.0-SNAPSHOT
sbt.ResolveException: unresolved dependency: com.typesafe.genjavadoc#genjavadoc-plugin_2.12.6;0.10: not found
	at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:320)
	at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:191)
	at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:168)
	at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:156)
	at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:156)
	at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:133)
	at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:57)
	at sbt.IvySbt$$anon$4.call(Ivy.scala:65)
	at xsbt.boot.Locks$GlobalLock.withChannel$1(Locks.scala:93)
	at xsbt.boot.Locks$GlobalLock.xsbt$boot$Locks$GlobalLock$$withChannelRetries$1(Locks.scala:78)
	at xsbt.boot.Locks$GlobalLock$$anonfun$withFileLock$1.apply(Locks.scala:97)
	at xsbt.boot.Using$.withResource(Using.scala:10)
	at xsbt.boot.Using$.apply(Using.scala:9)
	at xsbt.boot.Locks$GlobalLock.ignoringDeadlockAvoided(Locks.scala:58)
	at xsbt.boot.Locks$GlobalLock.withLock(Locks.scala:48)
	at xsbt.boot.Locks$.apply0(Locks.scala:31)
	at xsbt.boot.Locks$.apply(Locks.scala:28)
	at sbt.IvySbt.withDefaultLogger(Ivy.scala:65)
	at sbt.IvySbt.withIvy(Ivy.scala:128)
	at sbt.IvySbt.withIvy(Ivy.scala:125)
	at sbt.IvySbt$Module.withModule(Ivy.scala:156)
	at sbt.IvyActions$.updateEither(IvyActions.scala:168)
	at sbt.Classpaths$$anonfun$sbt$Classpaths$$work$1$1.apply(Defaults.scala:1555)
	at sbt.Classpaths$$anonfun$sbt$Classpaths$$work$1$1.apply(Defaults.scala:1551)
	at sbt.Classpaths$$anonfun$doWork$1$1$$anonfun$122.apply(Defaults.scala:1586)
	at sbt.Classpaths$$anonfun$doWork$1$1$$anonfun$122.apply(Defaults.scala:1584)
	at sbt.Tracked$$anonfun$lastOutput$1.apply(Tracked.scala:37)
	at sbt.Classpaths$$anonfun$doWork$1$1.apply(Defaults.scala:1589)
	at sbt.Classpaths$$anonfun$doWork$1$1.apply(Defaults.scala:1583)
	at sbt.Tracked$$anonfun$inputChanged$1.apply(Tracked.scala:60)
	at sbt.Classpaths$.cachedUpdate(Defaults.scala:1606)
	at sbt.Classpaths$$anonfun$updateTask$1.apply(Defaults.scala:1533)
	at sbt.Classpaths$$anonfun$updateTask$1.apply(Defaults.scala:1485)
	at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)
	at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)
	at sbt.std.Transform$$anon$4.work(System.scala:63)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)
	at sbt.Execute.work(Execute.scala:237)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[error] (tags/*:update) sbt.ResolveException: unresolved dependency: com.typesafe.genjavadoc#genjavadoc-plugin_2.12.6;0.10: not found
{code}",,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 07 16:59:08 UTC 2018,,,,,,,,,,"0|i3wrmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/18 08:05;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22020;;;","07/Aug/18 16:59;srowen;Issue resolved by pull request 22020
[https://github.com/apache/spark/pull/22020];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty string should be disallowed for data types except for string and binary types in JSON,SPARK-25040,13177297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,gurwls223,,07/Aug/18 05:19,12/Dec/22 18:11,13/Jul/23 08:48,23/Oct/18 05:44,2.2.0,2.4.0,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"The issue itself seems to be a behaviour change between 1.6 and 2.x for treating empty string as null or not in double and float.

{code}
{""a"":""a1"",""int"":1,""other"":4.4}
{""a"":""a2"",""int"":"""",""other"":""""}
{code}

code ：

{code}
val config = new SparkConf().setMaster(""local[5]"").setAppName(""test"")
val sc = SparkContext.getOrCreate(config)
val sql = new SQLContext(sc)

val file_path = this.getClass.getClassLoader.getResource(""Sanity4.json"").getFile
val df = sql.read.schema(null).json(file_path)
df.show(30)
{code}

then in spark 1.6, result is
{code}
+---+----+-----+
| a| int|other|
+---+----+-----+
| a1| 1| 4.4|
| a2|null| null|
+---+----+-----+
{code}

{code}
root
|-- a: string (nullable = true)
|-- int: long (nullable = true)
|-- other: double (nullable = true)
{code}

but in spark 2.2, result is

{code}
+----+----+-----+
| a| int|other|
+----+----+-----+
| a1| 1| 4.4|
|null|null| null|
+----+----+-----+
{code}

{code}
root
|-- a: string (nullable = true)
|-- int: long (nullable = true)
|-- other: double (nullable = true)
{code}

Another easy reproducer:

{code}
spark.read.schema(""a DOUBLE, b FLOAT"")
      .option(""mode"", ""FAILFAST"").json(Seq(""""""{""a"":"""", ""b"": """"}"""""", """"""{""a"": 1.1, ""b"": 1.1}"""""").toDS)
{code}",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 06:15:58 UTC 2018,,,,,,,,,,"0|i3wrin:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"07/Aug/18 05:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/22019;;;","17/Aug/18 01:58;gurwls223;We will fix this by disallowing empty string as nulls in all data types as discussed in the PR. It targets 3.0.0.;;;","21/Oct/18 13:02;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22787;;;","21/Oct/18 13:03;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22787;;;","23/Oct/18 05:44;gurwls223;Issue resolved by pull request 22787
[https://github.com/apache/spark/pull/22787];;;","23/Oct/18 06:15;viirya;The JIRA title is not correct now. I changed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala 2.12 issues: Compilation error with sbt,SPARK-25036,13177186,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,06/Aug/18 18:55,12/Dec/22 18:10,13/Jul/23 08:48,10/Sep/18 13:42,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"When compiling with sbt, the following errors occur:

There are -two- three types:
1. {{ExprValue.isNull}} is compared with unexpected type.
2. {{match may not be exhaustive}} is detected at {{match}}
3. discarding unmoored doc comment

The first one is more serious since it may also generate incorrect code in Spark 2.3.
{code:java}
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/ValueInterval.scala:63: match may not be exhaustive.
[error] It would fail on the following inputs: (NumericValueInterval(_, _), _), (_, NumericValueInterval(_, _)), (_, _)
[error] [warn]   def isIntersected(r1: ValueInterval, r2: ValueInterval): Boolean = (r1, r2) match {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/ValueInterval.scala:79: match may not be exhaustive.
[error] It would fail on the following inputs: (NumericValueInterval(_, _), _), (_, NumericValueInterval(_, _)), (_, _)
[error] [warn]     (r1, r2) match {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala:67: match may not be exhaustive.
[error] It would fail on the following inputs: (ArrayType(_, _), _), (_, ArrayData()), (_, _)
[error] [warn]     (endpointsExpression.dataType, endpointsExpression.eval()) match {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala:470: match may not be exhaustive.
[error] It would fail on the following inputs: NewFunctionSpec(_, None, Some(_)), NewFunctionSpec(_, Some(_), None)
[error] [warn]     newFunction match {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala:94: org.apache.spark.sql.catalyst.expressions.codegen.ExprValue and String are unrelated: they will most likely always compare unequal
[error] [warn]         if (eval.isNull != ""true"") {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala:126: org.apache.spark.sql.catalyst.expressions.codegen.ExprValue and String are unrelated: they will most likely never compare equal
[error] [warn]              if (eval.isNull == ""true"") {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala:133: org.apache.spark.sql.catalyst.expressions.codegen.ExprValue and String are unrelated: they will most likely never compare equal
[error] [warn]             if (eval.isNull == ""true"") {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala:709: match may not be exhaustive.
[error] It would fail on the following input: Schema((x: org.apache.spark.sql.types.DataType forSome x not in org.apache.spark.sql.types.StructType), _)
[error] [warn]   def attributesFor[T: TypeTag]: Seq[Attribute] = schemaFor[T] match {
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection.scala:90: org.apache.spark.sql.catalyst.expressions.codegen.ExprValue and String are unrelated: they will most likely never compare equal
[error] [warn]       if (inputs.map(_.isNull).forall(_ == ""false"")) {
[error] [warn] 
{code}


{code:java}
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala:410: discarding unmoored doc comment
[error] [warn]     /**
[error] [warn] 
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala:441: discarding unmoored doc comment
[error] [warn]     /**
[error] [warn] 
...
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:440: discarding unmoored doc comment
[error] [warn]     /**
[error] [warn] 
{code}",,apachespark,cloud_fan,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 02:13:54 UTC 2018,,,,,,,,,,"0|i3wqtz:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"06/Aug/18 19:02;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22012;;;","06/Aug/18 19:28;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22014;;;","08/Aug/18 06:46;gurwls223;Issue resolved by pull request 22014
[https://github.com/apache/spark/pull/22014];;;","08/Aug/18 17:00;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22039;;;","09/Aug/18 16:47;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22058;;;","09/Aug/18 17:04;kiszk;Another type of compilation error is found. Added the log to the description;;;","09/Aug/18 17:19;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22059;;;","06/Sep/18 13:32;cloud_fan;Have we resolved all the problems for this ticket?;;;","07/Sep/18 02:13;gurwls223;There are already too many warnings and I assume it's difficult to spot warnings specific to Scala 2.12 given my resent look (https://github.com/apache/spark/pull/21975). Am I correct? I would rather leave this resolved, and fix them in a batch later in a separate ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bump Apache commons.{httpclient, httpcore}",SPARK-25033,13177121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fokko,fokko,fokko,06/Aug/18 14:15,12/Dec/22 18:11,13/Jul/23 08:48,13/Aug/18 01:14,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"I would like to bump the versions to make it up to date with my other dependencies, in my case Stocator.",,apachespark,fokko,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 13 01:14:48 UTC 2018,,,,,,,,,,"0|i3wqfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/18 14:18;apachespark;User 'Fokko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22007;;;","13/Aug/18 01:14;gurwls223;Issue resolved by pull request 22007
[https://github.com/apache/spark/pull/22007];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The schema of MapType can not be printed correctly,SPARK-25031,13177076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,invkrh,invkrh,invkrh,06/Aug/18 11:21,16/Aug/18 23:10,13/Jul/23 08:48,16/Aug/18 23:10,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,easyfix,,,,"Something wrong with the function `buildFormattedString` in `MapType`

 
{code:java}
import spark.implicits._
case class Key(a: Int)
case class Value(b: Int)
Seq(
  (1, Map(Key(1) -> Value(2))), 
  (2, Map(Key(1) -> Value(2)))
).toDF(""id"", ""dict"").printSchema
{code}
The result is:
{code:java}
root
|-- id: integer (nullable = false)
|-- dict: map (nullable = true)
| |-- key: struct
| |-- value: struct (valueContainsNull = true)
| | |-- a: integer (nullable = false)
| | |-- b: integer (nullable = false)
{code}
 The expected is
{code:java}
root
|-- id: integer (nullable = false)
|-- dict: map (nullable = true)
| |-- key: struct
| | |-- a: integer (nullable = false)
| |-- value: struct (valueContainsNull = true)
| | |-- b: integer (nullable = false)
{code}",,apachespark,invkrh,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 16 13:42:10 UTC 2018,,,,,,,,,,"0|i3wq5j:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"06/Aug/18 11:35;apachespark;User 'invkrh' has created a pull request for this issue:
https://github.com/apache/spark/pull/22006;;;","08/Aug/18 14:41;mgaido;[~smilegator] shall this be resolved as https://github.com/apache/spark/pull/22006 was merged? Thanks.;;;","16/Aug/18 13:42;mgaido;^ kindly ping [~smilegator];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnalyzePartitionCommand failed with NPE if value is null,SPARK-25028,13176949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,igreenfi,igreenfi,05/Aug/18 07:25,12/Dec/22 18:10,13/Jul/23 08:48,14/Aug/18 00:29,2.4.0,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Spark Core,,,,0,,,,,"on line 143: val partitionColumnValues = partitionColumns.indices.map(r.get(_).toString)

If the value is NULL the code will fail with NPE

*sample:*

{code:scala}
val df = List((1, null , ""first""), (2, null , ""second"")).toDF(""index"", ""name"", ""value"").withColumn(""name"", $""name"".cast(""string""))
df.write.partitionBy(""name"").saveAsTable(""df13"")
spark.sql(""ANALYZE TABLE df13 PARTITION (name) COMPUTE STATISTICS"")
{code}

output:

2018-08-08 09:25:43 WARN  BaseSessionStateBuilder$$anon$1:66 - Max iterations (2) reached for batch Resolution
java.lang.NullPointerException
  at org.apache.spark.sql.execution.command.AnalyzePartitionCommand$$anonfun$calculateRowCountsPerPartition$1$$anonfun$8.apply(AnalyzePartitionCommand.scala:143)
  at org.apache.spark.sql.execution.command.AnalyzePartitionCommand$$anonfun$calculateRowCountsPerPartition$1$$anonfun$8.apply(AnalyzePartitionCommand.scala:143)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.Range.foreach(Range.scala:160)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.execution.command.AnalyzePartitionCommand$$anonfun$calculateRowCountsPerPartition$1.apply(AnalyzePartitionCommand.scala:143)
  at org.apache.spark.sql.execution.command.AnalyzePartitionCommand$$anonfun$calculateRowCountsPerPartition$1.apply(AnalyzePartitionCommand.scala:142)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.execution.command.AnalyzePartitionCommand.calculateRowCountsPerPartition(AnalyzePartitionCommand.scala:142)
  at org.apache.spark.sql.execution.command.AnalyzePartitionCommand.run(AnalyzePartitionCommand.scala:104)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:190)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)
  ... 49 elided",,achuth17,apachespark,cloud_fan,igreenfi,maropu,mgaido,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 14 00:29:48 UTC 2018,,,,,,,,,,"0|i3wpdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/18 01:29;achuth17;[~igreenfi], Can you share a test case to reproduce this? ;;;","07/Aug/18 01:59;gurwls223;Please reopen if there are reproduce steps updated, or anyone is able to explain / reproduce.;;;","08/Aug/18 09:27;igreenfi;Add sample code;;;","08/Aug/18 12:12;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22036;;;","14/Aug/18 00:29;cloud_fan;Issue resolved by pull request 22036
[https://github.com/apache/spark/pull/22036];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add spark.executor.pyspark.memory support to Kubernetes,SPARK-25021,13176791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ifilonenko,rdblue,rdblue,03/Aug/18 18:35,17/May/20 18:24,13/Jul/23 08:48,09/Sep/18 05:20,2.3.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,SPARK-25004 adds {{spark.executor.pyspark.memory}} to control the memory allocation for PySpark and updates YARN to add this memory to its container requests. Kubernetes should do something similar to account for the python memory allocation.,,apachespark,holden,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25004,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 10 03:21:57 UTC 2018,,,,,,,,,,"0|i3woe7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/18 04:47;apachespark;User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22298;;;","09/Sep/18 05:20;holden;Merged for 3 - open to the discussion around backporting. Thanks for doing this!;;;","10/Sep/18 03:21;apachespark;User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22376;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The published spark sql pom does not exclude the normal version of orc-core ,SPARK-25019,13176778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,yhuai,yhuai,03/Aug/18 18:06,06/Aug/18 19:01,13/Jul/23 08:48,06/Aug/18 19:01,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,SQL,,,0,,,,,"I noticed that [https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-sql_2.11/2.4.0-SNAPSHOT/spark-sql_2.11-2.4.0-20180803.100335-189.pom] does not exclude the normal version of orc-core. Comparing with [https://github.com/apache/spark/blob/92b48842b944a3e430472294cdc3c481bad6b804/sql/core/pom.xml#L108] and [https://github.com/apache/spark/blob/92b48842b944a3e430472294cdc3c481bad6b804/pom.xml#L1767,] we only exclude the normal version of orc-core in the parent pom. So, the problem is that if a developer depends on spark-sql-core directly, orc-core and orc-core-nohive will be in the dependency list. ",,apachespark,dongjoon,viirya,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 06 19:01:33 UTC 2018,,,,,,,,,,"0|i3wobb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"03/Aug/18 18:07;yhuai;[~dongjoon] can you help us fix this issue? Or there is a reason that the parent pom and sql/core/pom are not consistent?;;;","05/Aug/18 17:24;dongjoon;Sure, I'll take a look, [~yhuai] .;;;","05/Aug/18 20:39;dongjoon;I'll make a PR soon. There is no reason that the parent pom and sql/core/pom are different. While removing new dependency, the inheritance of depedency exclusion was broken before.;;;","05/Aug/18 22:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/22003;;;","06/Aug/18 19:01;yhuai;[https://github.com/apache/spark/pull/22003] has been merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add PrefixSpan to __all__ in fpm.py,SPARK-25011,13176625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,03/Aug/18 05:15,12/Dec/22 18:10,13/Jul/23 08:48,03/Aug/18 07:05,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,ML,,,,0,,,,,Add PrefixSpan to __all__ in fpm.py,,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 03 07:05:13 UTC 2018,,,,,,,,,,"0|i3wndb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/18 07:05;gurwls223;Fixed in https://github.com/apache/spark/pull/21981;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rand/Randn should produce different values for each execution in streaming query,SPARK-25010,13176589,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,02/Aug/18 23:27,07/Aug/18 15:22,13/Jul/23 08:48,07/Aug/18 06:29,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,Structured Streaming,,,0,,,,,"Like Uuid in SPARK-24896, Rand and Randn expressions now produce the same results for each execution in streaming query. It doesn't make too much sense for streaming queries. We should make them produce different results as Uuid.",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 07 15:22:05 UTC 2018,,,,,,,,,,"0|i3wn5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/18 23:44;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21980;;;","07/Aug/18 06:29;cloud_fan;Issue resolved by pull request 21980
[https://github.com/apache/spark/pull/21980];;;","07/Aug/18 15:22;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22027;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone Cluster mode application submit is not working,SPARK-25009,13176576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,devaraj,devaraj,devaraj,02/Aug/18 22:10,03/Aug/18 07:25,13/Jul/23 08:48,03/Aug/18 07:25,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"It is not showing any error while submitting but the app is not running and as well as not showing in the web UI.
",,apachespark,devaraj,irashid,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22941,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 03 01:44:42 UTC 2018,,,,,,,,,,"0|i3wn2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/18 22:15;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/21979;;;","03/Aug/18 01:44;irashid;[~devaraj.k] I don't think SPARK-22941 is in 2.3.1, so I changed the Affects Versions, please let me know if I'm mistaken.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Structured streaming doesn't support kafka transaction (creating empty offset with abort & markers),SPARK-25005,13176565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,qambard,qambard,02/Aug/18 20:47,03/Oct/18 22:26,13/Jul/23 08:48,28/Aug/18 15:38,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,,,,,"Structured streaming can't consume kafka transaction. 
We could try to apply SPARK-24720 (DStream) logic to Structured Streaming source",,apachespark,kristopherkane,qambard,Tagar,Teng Peng,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 03 22:26:48 UTC 2018,,,,,,,,,,"0|i3wmzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/18 17:52;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/22042;;;","03/Oct/18 21:37;qambard;How do you make difference between data loss or data missing when .pool() doesn't return any value [~zsxwing] ? Correct me if I'm wrong but you could lose data in this situation no ?

I think there is a third case here [https://github.com/zsxwing/spark/blob/ea804cfe840196519cc9444be9bedf03d10aa11a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaDataConsumer.scala#L474] which is : something went wrong, data is available in kafka but I failed to get it.
I've seen it happening when the max.pool size is big with big messages and the heap is getting full. Message exist but the jvm lags and the consumer timeout before getting the messages;;;","03/Oct/18 21:47;zsxwing;[~qambard] If `poll` returns and offset gets changed, it means Kafka consumer fetches something but all of messages are invisible so consumer return empty.

If `poll` returns but offset doesn't change, it means Kafka fetches nothing before timeout. In this case, we just throw ""TimeoutException"". Spark will retry the task or just fail the job. Large GC pause can cause timeout and the user should tune the configs to avoid this happening. We cannot do much in Spark.;;;","03/Oct/18 22:16;qambard;ok I see, great idea, and the consumer ensure us that the position won't be updated if pool returns an empty list for any reason?

Also if a partition is full with invisible messages due to transaction abort, we'll have to wait for the pool timeout everytime (at least that's what I see in my tests) It could hurt throughput, especially if we have to wait for each partition. Not sure how we could solve that...;;;","03/Oct/18 22:26;zsxwing;[~qambard] Not sure about your question. If Kafka consumers fetch nothing, it will not update the position.

And yes, if a partition is full with invisible messages, we have to wait for timeout. I don't see any API to avoid this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add spark.executor.pyspark.memory config to set resource.RLIMIT_AS,SPARK-25004,13176554,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,02/Aug/18 19:25,19/Mar/20 06:59,13/Jul/23 08:48,28/Aug/18 19:31,2.3.0,,,,,,,,,,,,,,,,,2.4.0,,,,PySpark,,,,1,,,,,"Some platforms support limiting Python's addressable memory space by limiting [{{resource.RLIMIT_AS}}|https://docs.python.org/3/library/resource.html#resource.RLIMIT_AS].

We've found that adding a limit is very useful when running in YARN because when Python doesn't know about memory constraints, it doesn't know when to garbage collect and will continue using memory when it doesn't need to. Adding a limit reduces PySpark memory consumption and avoids YARN killing containers because Python hasn't cleaned up memory.

This also improves error messages for users, allowing them to see when Python is allocating too much memory instead of YARN killing the container:

{code:lang=python}
  File ""build/bdist.linux-x86_64/egg/package/library.py"", line 265, in fe_engineer
    fe_eval_rec.update(f(src_rec_prep, mat_rec_prep))
  File ""build/bdist.linux-x86_64/egg/package/library.py"", line 163, in fe_comp
    comparisons = EvaluationUtils.leven_list_compare(src_rec_prep.get(item, []), mat_rec_prep.get(item, []))
  File ""build/bdist.linux-x86_64/egg/package/evaluationutils.py"", line 25, in leven_list_compare
    permutations = sorted(permutations, reverse=True)
  MemoryError
{code}",,apachespark,dongjoon,holden,irashid,ouyangxc.zte,rdblue,Steven Rand,Tagar,vanzin,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26743,SPARK-26080,SPARK-26679,,,,,,,,SPARK-25021,SPARK-25022,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 06:59:50 UTC 2020,,,,,,,,,,"0|i3wmxj:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"02/Aug/18 19:33;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21977;;;","28/Aug/18 19:31;vanzin;Issue resolved by pull request 21977
[https://github.com/apache/spark/pull/21977];;;","19/Mar/20 06:59;ouyangxc.zte;[~rdblue] This configuration can only control the worker.py process, and the maximum memory limit of the derived child process cannot be controlled. 

Worker(JVM) --> Executor–> python.demon–>python.demon , the last python demon process can not be controlled by this configuration.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark Does not use Spark Sql Extensions,SPARK-25003,13176532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rspitzer,rspitzer,rspitzer,02/Aug/18 18:00,12/Dec/22 18:11,13/Jul/23 08:48,18/Oct/18 04:29,2.2.2,2.3.1,,,,,,,,,,,,,,,,3.0.0,,,,PySpark,,,,1,,,,,"When creating a SparkSession here

[https://github.com/apache/spark/blob/v2.2.2/python/pyspark/sql/session.py#L216]
{code:python}
if jsparkSession is None:
  jsparkSession = self._jvm.SparkSession(self._jsc.sc())
self._jsparkSession = jsparkSession
{code}

I believe it ends up calling the constructor here
https://github.com/apache/spark/blob/v2.2.2/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L85-L87
{code:scala}
  private[sql] def this(sc: SparkContext) {
    this(sc, None, None, new SparkSessionExtensions)
  }
{code}

Which creates a new SparkSessionsExtensions object and does not pick up new extensions that could have been set in the config like the companion getOrCreate does.
https://github.com/apache/spark/blob/v2.2.2/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L928-L944
{code:scala}
//in getOrCreate
        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s""Cannot use $extensionConfClassName to configure session extensions."", e)
          }
        }
{code}

I think a quick fix would be to use the getOrCreate method from the companion object instead of calling the constructor from the SparkContext. Or we could fix this by ensuring that all constructors attempt to pick up custom extensions if they are set.",,apachespark,ksunitha,rspitzer,sebastian.estevez@datastax.com,shiyuhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25432,SUBMARINE-409,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 08 05:55:49 UTC 2022,,,,,,,,,,"0|i3wmsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/18 15:58;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/21988;;;","03/Aug/18 16:01;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/21989;;;","03/Aug/18 16:08;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/21990;;;","03/Aug/18 16:08;rspitzer;[~holden.karau] , Wrote up a PR for each branch target since I'm not sure what version you would think best for the update. Please let me know if you have any feedback or advice on how to get an automatic test in :);;;","07/Aug/18 13:04;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/21988;;;","07/Aug/18 13:04;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/21989;;;","18/Oct/18 04:29;gurwls223;Issue resolved by pull request 21990
[https://github.com/apache/spark/pull/21990];;;","01/Apr/19 18:52;ksunitha;Hi [~hyukjin.kwon], [~rspitzer],   The extension points functionality is not available from pyspark. Would it be possible to get this fix on the v2.4 branch. Are there any issues with back porting it?  Thank you so much. ;;;","01/Apr/19 19:47;rspitzer;There was no interest in putting in OSS 2.4 and 2.2, but I did do this backport for the Datastax Distribution of Spark 2.4 and I can report it is a relatively simple and straightforward process.;;;","02/Apr/19 17:52;ksunitha;Thanks [~rspitzer].  That is good to know. 

[~hyukjin.kwon],  Do you have any concerns if we open a PR to back port to v2.4?;;;","02/Apr/19 22:16;gurwls223;This session stuff logic is a bit convoluted and many session changes were made .. I wouldn't backport it from 3.0 to 2.x unless it's quite serious one.;;;","08/Jun/22 05:55;shiyuhang;`The extension points functionality is not available from pyspark` I believe it is a serious bug.

I wonder what scenarios would be considered serious and backport it to 2.x ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support MINUS ALL,SPARK-24997,13176330,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,02/Aug/18 06:50,03/Aug/18 05:45,13/Jul/23 08:48,03/Aug/18 05:45,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,MINUS is synonym for EXCEPT. We have added support for EXCEPT ALL. We need to enable support for MINUS ALL as well.,,apachespark,dkbiswal,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 06:51:05 UTC 2018,,,,,,,,,,"0|i3wlu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/18 06:51;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/21963;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Cached Consumer Leaking File Descriptors,SPARK-24987,13176073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Yuval.Itzchakov,Yuval.Itzchakov,Yuval.Itzchakov,01/Aug/18 10:15,24/May/19 04:13,13/Jul/23 08:48,04/Aug/18 19:46,2.2.2,2.3.0,2.3.1,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Structured Streaming,,,,1,,,,,"Setup:
 * Spark 2.3.1
 * Java 1.8.0 (112)
 * Standalone Cluster Manager
 * 3 Nodes, 1 Executor per node.

Spark 2.3.0 introduced a new mechanism for caching Kafka consumers (https://issues.apache.org/jira/browse/SPARK-23623?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel) via KafkaDataConsumer.acquire.

It seems that there are situations (I've been trying to debug it, haven't been able to find the root cause as of yet) where cached consumers remain ""in use"" throughout the life time of the task and are never released. This can be identified by the following line of the stack trace:

at org.apache.spark.sql.kafka010.KafkaDataConsumer$.acquire(KafkaDataConsumer.scala:460)

Which points to:
{code:java}
} else if (existingInternalConsumer.inUse) {
  // If consumer is already cached but is currently in use, then return a new consumer
  NonCachedKafkaDataConsumer(newInternalConsumer)
{code}
 Meaning the existing consumer created for that `TopicPartition` is still in use for some reason. The weird thing is that you can see this for very old tasks which have already finished successfully.

I've traced down this leak using file leak detector, attaching it to the running Executor JVM process. I've emitted the list of open file descriptors which [you can find here|https://gist.github.com/YuvalItzchakov/cdbdd7f67604557fccfbcce673c49e5d], and you can see that the majority of them are epoll FD used by Kafka Consumers, indicating that they aren't closing.

 Spark graph:
{code:java}
kafkaStream
  .load()
  .selectExpr(""CAST(key AS STRING)"", ""CAST(value AS STRING)"")
  .as[(String, String)]
  .flatMap {...}
  .groupByKey(...)
  .mapGroupsWithState(GroupStateTimeout.ProcessingTimeTimeout())(...)
  .foreach(...)
  .outputMode(OutputMode.Update)
  .option(""checkpointLocation"",
sparkConfiguration.properties.checkpointDirectory)
  .start()
  .awaitTermination(){code}","Spark 2.3.1

Java(TM) SE Runtime Environment (build 1.8.0_112-b15)
 Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)

 ",apachespark,koeninger,viirya,Yuval.Itzchakov,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27818,SPARK-25106,,,SPARK-23623,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 05 15:10:00 UTC 2018,,,,,,,,,,"0|i3wk9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/18 09:51;apachespark;User 'YuvalItzchakov' has created a pull request for this issue:
https://github.com/apache/spark/pull/21983;;;","04/Aug/18 07:32;apachespark;User 'YuvalItzchakov' has created a pull request for this issue:
https://github.com/apache/spark/pull/21997;;;","05/Aug/18 10:51;Yuval.Itzchakov;[~cody@koeninger.org] Is there any chance this will make it in time for 2.3.2? This is a critical fix for us (or more generally for all users of the Kafka source).;;;","05/Aug/18 15:10;koeninger;It was merged to branch-2.3

https://github.com/apache/spark/commits/branch-2.3

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShutdownHook timeout causes job to fail when succeeded when SparkContext stop() not called by user program,SPARK-24981,13175917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hthuynh2,hthuynh2,hthuynh2,31/Jul/18 18:21,04/Sep/20 01:45,13/Jul/23 08:48,06/Aug/18 14:02,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,"When user does not stop the SparkContext at the end of their program, ShutdownHookManger will stop the SparkContext. However, each shutdown hook is only given 10s to run, it will be interrupted and cancelled after that given time. In case stopping spark context takes longer than 10s, InterruptedException will be thrown, and the job will fail even though it succeeded before. An example of this is shown below.

I think there are a few ways to fix this, below are the 2 ways that I have now: 

1. After user program finished, we can check if user program stoped SparkContext or not. If user didn't stop the SparkContext, we can stop it before finishing the userThread. By doing so, SparkContext.stop() can take as much time as it needed.

2. We can just catch the InterruptedException thrown by ShutdownHookManger while we are stopping the SparkContext, and ignoring all the things that we haven't stopped inside the SparkContext. Since we are shutting down, I think it will be okay to ignore those things.

 
{code:java}
18/07/31 17:11:49 ERROR Utils: Uncaught exception in thread pool-4-thread-1
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1249)
	at java.lang.Thread.join(Thread.java:1323)
	at org.apache.spark.scheduler.AsyncEventQueue.stop(AsyncEventQueue.scala:136)
	at org.apache.spark.scheduler.LiveListenerBus$$anonfun$stop$1.apply(LiveListenerBus.scala:219)
	at org.apache.spark.scheduler.LiveListenerBus$$anonfun$stop$1.apply(LiveListenerBus.scala:219)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.LiveListenerBus.stop(LiveListenerBus.scala:219)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1922)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1360)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1921)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:573)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/07/31 17:11:49 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException
java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67)
{code}",,apachespark,hthuynh2,riza,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 04 01:45:25 UTC 2020,,,,,,,,,,"0|i3wjan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/18 18:42;apachespark;User 'hthuynh2' has created a pull request for this issue:
https://github.com/apache/spark/pull/21936;;;","03/Sep/20 13:28;victor-wong;`ERROR Utils: Uncaught exception in thread pool-4-thread-1` is logged by `org.apache.spark.util.Utils#tryLogNonFatalError`, I think it would not cause SparkContext#stop throwing exception. I wonder how this PR solves this problem.

```
  /** Executes the given block. Log non-fatal errors if any, and only throw fatal errors */
  def tryLogNonFatalError(block: => Unit) {
    try {
      block
    } catch {
      case NonFatal(t) =>
        logError(s""Uncaught exception in thread ${Thread.currentThread().getName}"", t)
    }
  }
```;;;","03/Sep/20 17:59;hthuynh2;As far as I know, InterruptedException is considered as a fatal exception. (Please correct me if I'm wrong);;;","04/Sep/20 01:45;victor-wong;My mistake:D, thanks for reply.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PivotFirst could not handle pivot columns of complex types,SPARK-24972,13175690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,30/Jul/18 22:22,31/Jul/18 06:44,13/Jul/23 08:48,31/Jul/18 06:44,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"{{PivotFirst}} did not handle complex types for pivot columns properly. And as a result, the pivot column could not be matched with any pivot value and it always returned empty result.",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 23:24:06 UTC 2018,,,,,,,,,,"0|i3whw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/18 23:24;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21926;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the precedence rule for set operations.,SPARK-24966,13175439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,29/Jul/18 23:49,12/Dec/22 18:11,13/Jul/23 08:48,03/Aug/18 05:05,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Currently the set operations INTERSECT, UNION and EXCEPT are assigned the same precedence. We need to change to make sure INTERSECT is given higher precedence than UNION and EXCEPT. UNION and EXCEPT should be evaluated in the order they appear in the query from left to right. 

Given this will result in a change in behavior, we need to keep it under a config.

Here is a reference :
https://docs.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql?view=sql-server-2017",,apachespark,dkbiswal,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 01 05:50:05 UTC 2018,,,,,,,,,,"0|i3wgcn:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"29/Jul/18 23:50;dkbiswal;I am working on a fix for this.;;;","30/Jul/18 08:21;gurwls223;Please avoid setting a target version which is usually reserved for committers.;;;","30/Jul/18 09:26;dkbiswal;[~hyukjin.kwon] Hmmn.. i thought Sean set the target version ? I remember setting only the affected version ?;;;","30/Jul/18 09:28;gurwls223;Ooops, I am sorry. my bad.;;;","01/Aug/18 05:50;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/21941;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests will fail if they run in a namespace not being the default,SPARK-24963,13175427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mccheah,skonto,skonto,29/Jul/18 20:30,17/May/20 18:23,13/Jul/23 08:48,30/Jul/18 18:42,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"Related discussion is here: [https://github.com/apache/spark/pull/21748#pullrequestreview-141048893]

If  spark-rbac.yaml is used when tests are used locally, client mode tests will fail.",,apachespark,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 22:16:06 UTC 2018,,,,,,,,,,"0|i3wg9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/18 18:41;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/21900;;;","30/Jul/18 22:16;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/21924;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decimal arithmetic can lead to wrong values using codegen,SPARK-24957,13175312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,dvogelbacher,dvogelbacher,28/Jul/18 04:26,02/Mar/20 07:54,13/Jul/23 08:48,30/Jul/18 13:02,2.0.2,2.1.3,2.2.2,2.3.1,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,SQL,,,,0,correctness,,,,"I noticed a bug when doing arithmetic on a dataframe containing decimal values with codegen enabled.
I tried to narrow it down on a small repro and got this (executed in spark-shell):
{noformat}
scala> val df = Seq(
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""11.9999999988"")),
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""11.9999999988"")),
     | (""a"", BigDecimal(""11.9999999988""))
     | ).toDF(""text"", ""number"")
df: org.apache.spark.sql.DataFrame = [text: string, number: decimal(38,18)]

scala> val df_grouped_1 = df.groupBy(df.col(""text"")).agg(functions.avg(df.col(""number"")).as(""number""))
df_grouped_1: org.apache.spark.sql.DataFrame = [text: string, number: decimal(38,22)]

scala> df_grouped_1.collect()
res0: Array[org.apache.spark.sql.Row] = Array([a,11.9999999994857142857143])

scala> val df_grouped_2 = df_grouped_1.groupBy(df_grouped_1.col(""text"")).agg(functions.sum(df_grouped_1.col(""number"")).as(""number""))
df_grouped_2: org.apache.spark.sql.DataFrame = [text: string, number: decimal(38,22)]

scala> df_grouped_2.collect()
res1: Array[org.apache.spark.sql.Row] = Array([a,1199999999948571.4285714285714285714286])

scala> val df_total_sum = df_grouped_1.agg(functions.sum(df_grouped_1.col(""number"")).as(""number""))
df_total_sum: org.apache.spark.sql.DataFrame = [number: decimal(38,22)]

scala> df_total_sum.collect()
res2: Array[org.apache.spark.sql.Row] = Array([11.9999999994857142857143])
{noformat}

The results of {{df_grouped_1}} and {{df_total_sum}} are correct, whereas the result of {{df_grouped_2}} is clearly incorrect (it is the value of the correct result times {{10^14}}).

When codegen is disabled all results are correct. ",,apachespark,cloud_fan,dvogelbacher,jerryshao,kiszk,maropu,maxgekk,mgaido,yumwang,,,,,,,,,,,,,,,,,,,,,,SPARK-25146,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 00:27:04 UTC 2018,,,,,,,,,,"0|i3wfkf:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"29/Jul/18 10:57;mgaido;I am not sure what you mean by ""When codegen is disabled all results are correct."". I checked and I was able to reproduce both with codegen enabled and with codegen disabled.

cc [~jerryshao] this doesn't seem a regression to me but it is a pretty serious bug, I am not sure whether we should include it in the next 2.3 version.
cc [~smilegator] [~cloud_fan] I think we should consider this a blocker for 2.4. What do you think? Thanks.;;;","29/Jul/18 11:09;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21910;;;","29/Jul/18 21:53;dvogelbacher;[~mgaido] thanks for putting up the PR!

I wasn't able to reproduce the incorrectness for the specific example I gave with wholestage codegen disabled:
{noformat}
scala> spark.conf.set(""spark.sql.codegen.wholeStage"", false)

scala> import org.apache.spark.sql.functions
import org.apache.spark.sql.functions

scala> val df = Seq(
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""11.9999999988"")),
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""12.0"")),
     | (""a"", BigDecimal(""11.9999999988"")),
     | (""a"", BigDecimal(""11.9999999988""))
     | ).toDF(""text"", ""number"")
df: org.apache.spark.sql.DataFrame = [text: string, number: decimal(38,18)]

scala> val df_grouped_1 = df.groupBy(df.col(""text"")).agg(functions.avg(df.col(""number"")).as(""number""))
df_grouped_1: org.apache.spark.sql.DataFrame = [text: string, number: decimal(38,22)]

scala> df_grouped_1.collect()
res1: Array[org.apache.spark.sql.Row] = Array([a,11.9999999994857142857143])

scala> val df_grouped_2 = df_grouped_1.groupBy(df_grouped_1.col(""text"")).agg(functions.sum(df_grouped_1.col(""number"")).as(""number""))
df_grouped_2: org.apache.spark.sql.DataFrame = [text: string, number: decimal(38,22)]

scala> df_grouped_2.collect()
res2: Array[org.apache.spark.sql.Row] = Array([a,11.9999999994857142857143])

scala> val df_total_sum = df_grouped_1.agg(functions.sum(df_grouped_1.col(""number"")).as(""number""))
df_total_sum: org.apache.spark.sql.DataFrame = [number: decimal(38,22)]

scala> df_total_sum.collect()
res3: Array[org.apache.spark.sql.Row] = Array([11.9999999994857142857143])
{noformat};;;","30/Jul/18 01:41;jerryshao;Not necessary to mark as blocker, we still have plenty of time for 2.4 release, I will mark the target version as 2.4.0.;;;","30/Jul/18 13:02;cloud_fan;Issue resolved by pull request 21910
[https://github.com/apache/spark/pull/21910];;;","01/Aug/18 19:28;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21949;;;","02/Aug/18 00:27;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21951;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scala DateTimeUtilsSuite daysToMillis and millisToDays fails w/java 8 181-b13,SPARK-24950,13175244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,d80tb7,shaneknapp,shaneknapp,27/Jul/18 18:20,23/Oct/19 22:28,13/Jul/23 08:48,09/Aug/18 22:42,2.1.3,2.2.2,2.3.1,2.4.0,,,,,,,,,,,,,,2.1.4,2.2.3,2.3.2,2.4.0,Build,Tests,,,1,,,,,"during my travails to port the spark builds to run on ubuntu 16.04LTS, i have encountered a strange and apparently java version-specific failure on *one* specific unit test.

the failure is here:

[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6-ubuntu-test/868/testReport/junit/org.apache.spark.sql.catalyst.util/DateTimeUtilsSuite/daysToMillis_and_millisToDays/]

the java version on this worker is:

sknapp@ubuntu-testing:~$ java -version
 java version ""1.8.0_181""
 Java(TM) SE Runtime Environment (build 1.8.0_181-b13)
 Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)

however, when i run this exact build on the other ubuntu workers, it passes.  they systems are set up (for the most part) identically except for the java version:

sknapp@amp-jenkins-staging-worker-02:~$ java -version
 java version ""1.8.0_171""
 Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
 Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)

there are some minor kernel and other package differences on these ubuntu workers, but nothing that (in my opinion) would affect this test.  i am willing to help investigate this, however.

the test also passes on the centos 6.9 workers, which have the following java version installed:

[sknapp@amp-jenkins-worker-05 ~]$ java -version
java version ""1.8.0_60""
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)my guess is that either:

sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala

or

sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala

is doing something wrong.  i am not a scala expert by any means, so i'd really like some help in trying to un-block the project to port the builds to ubuntu.",,apachespark,d80tb7,dongjoon,maropu,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29578,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 09 22:52:48 UTC 2018,,,,,,,,,,"0|i3wf5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/18 18:41;shaneknapp;one solution, of course, is to pin the java version on the upcoming ubuntu workers to one that passes this test, but things like this make build engineers like me die a little bit inside.;;;","27/Jul/18 20:03;srowen;It's pretty clear this is down to differences in how time zones are defined, as they change over time and the JDK incorporates updated versions of the standard definitions in each release. 

It looks like the difference between _171 and _181 is the difference between 2018c and 2018e in this table: http://www.oracle.com/technetwork/java/javase/tzdata-versions-138805.html

Nothing obviously relevant from Oracle's release notes. But I found this in the notes for 2018d:

[http://mm.icann.org/pipermail/tz-announce/2018-March/000049.html]

""Enderbury and Kiritimati skipped New Year's Eve 1994, not New Year's Day 1995.  (Thanks to Kerry Shetline.)""

So the answer is probably that the test has to be updated to reflect the fix to the timezone definition.

 

Of course, if the test changes, it also starts failing on older Java 8 versions! probably not worth it.

I'd suggest we resolve it by commenting this out with a note. There's no evidence this is a problem in Spark itself.;;;","27/Jul/18 20:44;shaneknapp;sgtm

i also dug through the java release notes WRT timezone changes and didn't find anything (which i forgot to mention).  sorry about that!  :)

i'll start by just commenting out the failing TZ (Pacific/Enderbury) and see if that works.

 ;;;","27/Jul/18 20:46;apachespark;User 'd80tb7' has created a pull request for this issue:
https://github.com/apache/spark/pull/21901;;;","27/Jul/18 20:48;d80tb7;Hi,

 

just to say that I looked at this and came to the same conclusion as Sean.  I've submitted a PR which excludes both New Years Eve and New Years day from the test- which should mean it will work on both old and new jvms.;;;","27/Jul/18 20:54;shaneknapp;testing this manually:

https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6-ubuntu-test/878/;;;","27/Jul/18 22:20;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/21903;;;","28/Jul/18 01:43;shaneknapp;the spark-master-test-sbt build failed on ubuntu, but the the improtant bit is that the DateTimeUtilsSuite tests passed!

back to unraveling R...  :\

thanks for the quick patch, [~d80tb7]!;;;","28/Jul/18 15:41;srowen;Issue resolved by pull request 21901
[https://github.com/apache/spark/pull/21901];;;","09/Aug/18 22:20;shaneknapp;[~srowen] [~d80tb7] i'm thinking that we actually need to backport this change to previous branches.

:(

[https://amplab.cs.berkeley.edu/jenkins/view/RISELab%20Infra/job/spark-branch-2.1-test-maven-hadoop-2.7-ubuntu-testing/2/]
{noformat}
- daysToMillis and millisToDays *** FAILED ***
  9131 did not equal 9130 Round trip of 9130 did not work in tz sun.util.calendar.ZoneInfo[id=""Pacific/Enderbury"",offset=46800000,dstSavings=0,useDaylight=false,transitions=5,lastRule=null] (DateTimeUtilsSuite.scala:554){noformat};;;","09/Aug/18 22:23;srowen;Roger that, will back-port back to 2.1 as best I can.;;;","09/Aug/18 22:26;shaneknapp;word.  let's leave this open until for a bit longer as i continue to test.;;;","09/Aug/18 22:36;shaneknapp;booyah!  i love watching the build queue pile up.  :)

thanks [~srowen]!;;;","09/Aug/18 22:52;shaneknapp;do we care about 2.0 and 1.6?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHS filters wrongly some applications due to permission check,SPARK-24948,13175213,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,mgaido,mgaido,27/Jul/18 16:00,25/Jun/19 10:29,13/Jul/23 08:48,06/Aug/18 21:34,2.3.1,,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,Web UI,,,,0,,,,,"SHS filters the event logs it doesn't have permissions to read. Unfortunately, this check is quite naive, as it takes into account only the base permissions (ie. user, group, other permissions). For instance, if ACL are enabled, they are ignored in this check; moreover, each filesystem may have different policies (eg. they can consider spark as a superuser who can access everything).

This results in some applications not being displayed in the SHS, despite the Spark user (or whatever user the SHS is started with) can actually read their ent logs.

",,apachespark,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28157,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 07 08:50:04 UTC 2018,,,,,,,,,,"0|i3weyf:",9223372036854775807,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,,,,,,,"27/Jul/18 16:08;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21895;;;","07/Aug/18 08:31;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22021;;;","07/Aug/18 08:50;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22022;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datasource partition table should load empty static partitions,SPARK-24937,13174875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,26/Jul/18 14:50,01/Aug/18 20:59,13/Jul/23 08:48,01/Aug/18 20:59,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"How to reproduce:
{code:sql}
spark-sql> CREATE TABLE tbl AS SELECT 1;
spark-sql> CREATE TABLE tbl1 (c1 BIGINT, day STRING, hour STRING)
         > USING parquet
         > PARTITIONED BY (day, hour);
spark-sql> INSERT INTO TABLE tbl1 PARTITION (day = '2018-07-25', hour='01') SELECT * FROM tbl where 1=0;
spark-sql> SHOW PARTITIONS tbl1;
spark-sql> CREATE TABLE tbl2 (c1 BIGINT)
         > PARTITIONED BY (day STRING, hour STRING);
18/07/26 22:49:20 WARN HiveMetaStore: Location: file:/Users/yumwang/tmp/spark/spark-warehouse/tbl2 specified for non-external table:tbl2
spark-sql> INSERT INTO TABLE tbl2 PARTITION (day = '2018-07-25', hour='01') SELECT * FROM tbl where 1=0;
18/07/26 22:49:36 WARN log: Updating partition stats fast for: tbl2
18/07/26 22:49:36 WARN log: Updated size to 0
spark-sql> SHOW PARTITIONS tbl2;
day=2018-07-25/hour=01
spark-sql> 
{code}",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 26 15:35:06 UTC 2018,,,,,,,,,,"0|i3wcvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/18 15:35;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/21883;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with Executing Hive UDF's from Spark 2.2 Onwards,SPARK-24935,13174856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pgandhi,pgandhi,pgandhi,26/Jul/18 13:59,12/Dec/22 18:10,13/Jul/23 08:48,24/Mar/19 23:09,2.2.0,2.3.1,,,,,,,,,,,,,,,,2.3.4,2.4.3,3.0.0,,SQL,,,,0,,,,,"A user of sketches library(https://github.com/DataSketches/sketches-hive) reported an issue with HLL Sketch Hive UDAF that seems to be a bug in Spark or Hive. Their code runs fine in 2.1 but has an issue from 2.2 onwards. For more details on the issue, you can refer to the discussion in the sketches-user list:
[https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/sketches-user/GmH4-OlHP9g/MW-J7Hg4BwAJ]

 

On further debugging, we figured out that from 2.2 onwards, Spark hive UDAF provides support for partial aggregation, and has removed the functionality that supported complete mode aggregation(Refer https://issues.apache.org/jira/browse/SPARK-19060 and https://issues.apache.org/jira/browse/SPARK-18186). Thus, instead of expecting update method to be called, merge method is called here ([https://github.com/DataSketches/sketches-hive/blob/master/src/main/java/com/yahoo/sketches/hive/hll/SketchEvaluator.java#L56)] which throws the exception as described in the forums above.",,apachespark,cloud_fan,gavin_hu,mgaido,pgandhi,rezasafi,tgraves,zanderl,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27907,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 06 16:39:38 UTC 2019,,,,,,,,,,"0|i3wcr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/18 10:09;gurwls223;Sounds non trivial and a regression.;;;","27/Jul/18 10:10;gurwls223;cc [~cloud_fan] and [~lian cheng]];;;","27/Jul/18 14:22;cloud_fan;seems like Hive UDAF can reject partial aggregate?;;;","29/Jul/18 20:56;pgandhi;[~cloud_fan] I too had the same doubt that maybe hive UDAF might still have issues supporting partial aggregation completely though I am not so sure. Would it make sense to add support for complete aggregation mode to ensure backward compatibility? Thank you.;;;","10/Aug/18 15:47;pgandhi;[~cloud_fan] [~lian cheng],

What do you think of my previous comment??;;;","19/Aug/18 19:40;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/22144;;;","15/Feb/19 20:26;pgandhi;Have created a new pull request for this issue here: https://github.com/apache/spark/pull/23778.;;;","21/Feb/19 00:08;zanderl;Glad to see this has been fixed!  What are the chances this will get into 2.4.1?;;;","21/Feb/19 00:25;pgandhi;Hi [~zanderl], thank you for your comment. Will do my best to work on the reviews, rest depends on the reviewers:) ;;;","21/Feb/19 01:25;gavin_hu;Hi [~pgandhi]  ""A user of sketches library..."" That's me! I'm so excited that the issue I reported couple months back has got fixed here. Kudos to you for the great work!

As you may know many users including me are moving towards Spark 2.4.1, getting the fix there will definitely benefit a lot of people. Looking forwards to that. 

 ;;;","21/Feb/19 15:25;pgandhi;Thank you [~gavin_hu] for reporting the issue in the first place. Have sent an email to the Spark dev mailing list requesting them to push the fix for Spark 2.4.1. Will let you know in case of any updates.;;;","01/May/19 20:56;rezasafi;Is there any reason that this wasn't merged into 2.3 line?;;;","06/May/19 16:39;cloud_fan;I have sent https://github.com/apache/spark/pull/24539 to backport it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complex type and binary type in in-memory partition pruning does not work due to missing upper/lower bounds cases,SPARK-24934,13174834,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,26/Jul/18 13:12,12/Dec/22 18:11,13/Jul/23 08:48,30/Jul/18 05:24,2.3.1,2.4.0,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,correctness,,,,"For example, if array is used (where the lower and upper bounds for its column batch are {{null}})), it looks wrongly filtering all data out:

{code}
scala> import org.apache.spark.sql.functions
import org.apache.spark.sql.functions

scala> val df = Seq(Array(""a"", ""b""), Array(""c"", ""d"")).toDF(""arrayCol"")
df: org.apache.spark.sql.DataFrame = [arrayCol: array<string>]

scala> df.filter(df.col(""arrayCol"").eqNullSafe(functions.array(functions.lit(""a""), functions.lit(""b"")))).show()
+--------+
|arrayCol|
+--------+
|  [a, b]|
+--------+


scala> df.cache().filter(df.col(""arrayCol"").eqNullSafe(functions.array(functions.lit(""a""), functions.lit(""b"")))).show()
+--------+
|arrayCol|
+--------+
+--------+
{code}",,apachespark,dvogelbacher,sdicocco,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 16:46:19 UTC 2018,,,,,,,,,,"0|i3wcm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/18 13:16;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21882;;;","26/Jul/18 14:19;dvogelbacher;Thanks for opening and making the pr [~hyukjin.kwon]!;;;","26/Jul/18 14:27;gurwls223;np! BTW, the workaround will be turning off {{spark.sql.inMemoryColumnarStorage.partitionPruning}} although it'd be less performant.;;;","30/Jul/18 13:54;tgraves;what is the real affected versions here?  Since it went into spark 2.3.2 does it affect 2.3.1?;;;","30/Jul/18 16:46;gurwls223;I think this has been a bug from the first place. It at least affects 2.3.1. I manually tested:

{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.1
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql.functions
import org.apache.spark.sql.functions

scala>

scala> val df = Seq(Array(""a"", ""b""), Array(""c"", ""d"")).toDF(""arrayCol"")
df: org.apache.spark.sql.DataFrame = [arrayCol: array<string>]

scala> df.filter(df.col(""arrayCol"").eqNullSafe(functions.array(functions.lit(""a""), functions.lit(""b"")))).show()
+--------+
|arrayCol|
+--------+
|  [a, b]|
+--------+


scala> df.cache().filter(df.col(""arrayCol"").eqNullSafe(functions.array(functions.lit(""a""), functions.lit(""b"")))).show()
+--------+
|arrayCol|
+--------+
+--------+
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The hadoop-provided profile doesn't play well with Snappy-compressed Parquet files,SPARK-24927,13174747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,26/Jul/18 06:53,27/Jul/18 15:59,13/Jul/23 08:48,27/Jul/18 15:59,2.3.1,2.3.2,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,Build,,,,0,,,,,"Reproduction:
{noformat}
wget https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-without-hadoop.tgz
wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz

tar xzf spark-2.3.1-bin-without-hadoop.tgz
tar xzf hadoop-2.7.3.tar.gz

export SPARK_DIST_CLASSPATH=$(hadoop-2.7.3/bin/hadoop classpath)
./spark-2.3.1-bin-without-hadoop/bin/spark-shell --master local
...
scala> spark.range(1).repartition(1).write.mode(""overwrite"").parquet(""file:///tmp/test.parquet"")
{noformat}
Exception:
{noformat}
Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)
  ... 69 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:109)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
  at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
  at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:316)
  at org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)
  at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)
  at org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)
  at org.apache.parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:112)
  at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:93)
  at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:150)
  at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:238)
  at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:121)
  at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:167)
  at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)
  at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:405)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:396)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
  ... 8 more
{noformat}
Root cause:
 # Spark 2.3 [requires snappy-java 1.1.2.6 explicitly|https://github.com/apache/spark/blob/v2.3.1/pom.xml#L163] in the root POM.
 # However, the scope of snappy-java is set to {{$\{hadoop.deps.scope}}}, which is [set to {{provided}}|https://github.com/apache/spark/blob/v2.3.1/assembly/pom.xml#L224] when the {{hadoop-provided}} profile is enabled.

Therefore, snappy-java 1.1.2.6 is not included in the pre-built {{spark-2.3.1-bin-without-hadoop.tgz}} package. Instead, snappy-java 1.0.4.1, which is bundled with Hadoop 2.3.1, is used in the reproduction above and caused the link error.

I think we can simply remove [this line|https://github.com/apache/spark/blob/v2.3.1/pom.xml#L163] to get this issue fixed.",,apachespark,dongjoon,jerryshao,lian cheng,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 26 07:06:10 UTC 2018,,,,,,,,,,"0|i3wc2v:",9223372036854775807,,,,,,,,,,,,,2.3.2,,,,,,,,,,"26/Jul/18 06:54;smilegator;cc [~jerryshao]
;;;","26/Jul/18 07:00;lian cheng;Downgraded from blocker to major, since it's not a regression. Just realized that this issue existed ever since at least 1.6.;;;","26/Jul/18 07:06;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/21879;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala linter rule for sparkContext.hadoopConfiguration,SPARK-24919,13174413,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,25/Jul/18 16:48,20/Aug/18 17:12,13/Jul/23 08:48,26/Jul/18 23:51,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,,,,0,,,,,"In most cases, we should use spark.sessionState.newHadoopConf() instead of sparkContext.hadoopConfiguration, so that the hadoop configurations specified in Spark session
configuration will come into effect.

Add a rule matching spark.sparkContext.hadoopConfiguration or 

spark.sqlContext.sparkContext.hadoopConfiguration to prevent the usage.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23573,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 25 16:53:02 UTC 2018,,,,,,,,,,"0|i3wa0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/18 16:53;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/21873;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHOW CREATE TABLE drops escaping of nested column names,SPARK-24911,13174215,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,24/Jul/18 21:46,10/Sep/18 04:19,13/Jul/23 08:48,25/Jul/18 18:11,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Create a table with quoted nested column - *`b`*:
{code:sql}
create table `test` (`a` STRUCT<`b`:STRING>);
{code}
and show how the table was created:
{code:sql}
SHOW CREATE TABLE `test`
{code}
{code}
CREATE TABLE `test`(`a` struct<b:string>)
{code}

The column *b* becomes unquoted.",,apachespark,maxgekk,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 10 04:19:34 UTC 2018,,,,,,,,,,"0|i3w8sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/18 21:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21803;;;","25/Jul/18 01:26;yumwang;Can we show non printable field delimiter when {{SHOW CREATE TABLE}}. I have reported in https://issues.apache.org/jira/browse/SPARK-23058;;;","10/Sep/18 04:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22377;;;","10/Sep/18 04:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22377;;;","10/Sep/18 04:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22377;;;","10/Sep/18 04:19;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/22377;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark scheduler can hang when fetch failures, executor lost, task running on lost executor, and multiple stage attempts",SPARK-24909,13174181,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,tgraves,24/Jul/18 19:00,17/May/20 17:48,13/Jul/23 08:48,29/Aug/18 23:32,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Scheduler,Spark Core,,,0,,,,,"The DAGScheduler can hang if the executor was lost (due to fetch failure) and all the tasks in the tasks sets are marked as completed. ([https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1265)]

It never creates new task attempts in the task scheduler but the dag scheduler still has pendingPartitions.
{code:java}
8/07/22 08:30:00 INFO scheduler.TaskSetManager: Starting task 55769.0 in stage 44.0 (TID 970752, host1.com, executor 33, partition 55769, PROCESS_LOCAL, 7874 bytes)

18/07/22 08:30:29 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 44 (repartition at Lift.scala:191) as failed due to a fetch failure from ShuffleMapStage 42 (map at foo.scala:27)
18/07/22 08:30:29 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 42 (map at foo.scala:27) and ShuffleMapStage 44 (repartition at bar.scala:191) due to fetch failure
....

18/07/22 08:30:56 INFO scheduler.DAGScheduler: Executor lost: 33 (epoch 18)
18/07/22 08:30:56 INFO schedulerDAGScheduler: Shuffle files lost for executor: 33 (epoch 18)

18/07/22 08:31:20 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[70] at repartition at bar.scala:191), which has no missing parents
18/07/22 08:31:21 INFO cluster.YarnClusterScheduler: Adding task set 44.1 with 59955 tasks

18/07/22 08:31:41 INFO scheduler.TaskSetManager: Finished task 55769.0 in stage 44.0 (TID 970752) in 101505 ms on host1.com (executor 33) (15081/73320)

8/07/22 08:31:41 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(44, 55769) completion from executor 33{code}
 

In the logs above you will see that task 55769.0 finished after the executor was lost and a new task set was started.  The DAG scheduler says ""Ignoring possibly bogus"".. but in the TaskSetManager side it has marked those tasks as completed for all stage attempts. The DAGScheduler gets hung here.  I did a heap dump on the process and can see that 55769 is still in the DAGScheduler pendingPartitions list but the tasksetmanagers are all complete

Note to reproduce this, you need a situation where  you have a shufflemaptask (call it task1) fetching data from an executor where it also has other shufflemaptasks (call it task2) running (fetch from other hosts). the task1 fetching the data has to FetchFail which would cause the stage to fail and the executor to be marked as lost due to the fetch failure.  It restarts a new task set for the new stage attempt, then the shufflemaptask task2 that was running on the executor that was marked Lost finished.  The scheduler ignore that complete event  ""Ignoring possible bogus ..."". This results in a hang because at this point the TaskSetManager has already marked all tasks for all attempts of that stage as completed.

 

Configs needed to be on:
|{{spark.blacklist.application.fetchFailure.enabled=true}}| |
|{{spark.blacklist.application.fetchFailure.enabled=true}}|

spark.files.fetchFailure.unRegisterOutputOnHost=true

spark.shuffle.service.enabled=true",,devaraj,Dhruve Ashar,habren,irashid,tgraves,vanzin,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25263,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 29 23:32:44 UTC 2018,,,,,,,,,,"0|i3w8lj:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"24/Jul/18 19:26;tgraves;Note this may have been introduced as part of SPARK-23433 where we now mark all the partitions in stage attempts as completed.   If the task that completed had been in the latest attempt it would have been removed from pendingPartitions.  If we didn't have SPARK-23433 then I think the stage attempt 44.1 would have went on to run the task again.

 

cc [~irashid];;;","24/Jul/18 20:17;irashid;ugh, yeah I think you're right.  do you have a fix in mind?

we could undo SPARK-23433, so that active taskset doesn't care if an earlier taskset completes the tasks it has (and rethink how to solve that case).

Or we could have the markPartitionCompletedInAllTaskSets take the epoch into account.

Or, we could change that condition in the dagscheduler where its getting hung -- it could resubmit the taskset if it thinks there is still work to be done, but all tasksets are complete (which might be a more stable approach in general), rather than only doing it if {{shuffleStage.pendingPartitions.isEmpty}}.

All of these changes always have a ton of corner cases and past bugs I need to remind myself of, though ...;;;","24/Jul/18 21:16;tgraves;I haven't come up with a fix yet but have been looking at essentially all the things you have mentioned.  will continue working on it, except I'm out tomorrow so will continue thursday.  ;;;","01/Aug/18 14:56;tgraves;this is unfortunately not a straight forward fix, the DAGScheduler doesn't have control over the fine grain things in the taskscheduler/tasksetmanager, so undoing some of that is not possible with the existing api.

one possible thing would be to undo the change in SPARK-19263,  [https://github.com/apache/spark/commit/729ce3703257aa34c00c5c8253e6971faf6a0c8d] and fix that another way as well

Still working on a fix, just making a few notes.;;;","01/Aug/18 15:44;tgraves;looking more I think the fix may actually just be to revert the change from SPARK-19263, so that it always does shuffleStage.pendingPartitions -= task.partitionId.   The change in SPARK-23433, should fix the issue originaly from SPARK-19263.

If we always remove it from the pendingPartitions and the map output isn't there it will resubmit the stage.  SPARK-23433, since its marking all tasks in other stage attempts as complete should make sure no other active stages for that are running.

Need to investigate more and run some tests.

 ;;;","29/Aug/18 23:32;vanzin;Issue resolved by pull request 21976
[https://github.com/apache/spark/pull/21976];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[R] remove spaces to make lintr happy,SPARK-24908,13174156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shaneknapp,shaneknapp,shaneknapp,24/Jul/18 17:22,10/Aug/18 19:52,13/Jul/23 08:48,24/Jul/18 23:15,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Build,,,,0,,,,,"during my travails in porting spark builds to run on our centos worker, i managed to recreate (as best i could) the centos environment on our new ubuntu-testing machine.

while running my initial builds, lintr was crashing on some extraneous spaces in test_basic.R (see:  [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6-ubuntu-test/862/console)]

after removing those spaces, the ubuntu build happily passed the lintr tests.

i then tested this against a modified spark-master-test-sbt-hadoop-2.6 build (see [https://amplab.cs.berkeley.edu/jenkins/view/RISELab%20Infra/job/testing-spark-master-test-with-updated-R-crap/4/),] which scp'ed a copy of test_basic.R in to the repo after the git clone.  everything seems to be working happily.

i will be creating a pull request for this now.

 

attn:  [~felixcheung] [~shivaram] [~ifilonenko]",,apachespark,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 10 19:52:40 UTC 2018,,,,,,,,,,"0|i3w8fz:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"24/Jul/18 17:25;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/21864;;;","24/Jul/18 18:01;shaneknapp;i noticed lintr complaining in my PRB build ([https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/93509/console)] :(

`Error: StartTag: invalid element name [68] Execution halted lintr checks passed.`

so i checked other R-based PRB builds and saw the same thing:

pull request:  https://github.com/apache/spark/pull/21835

associated build:  [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/93402/consoleFull]

i'm guessing that this is a red herring.;;;","10/Aug/18 19:52;srowen;Resolved by https://github.com/apache/spark/pull/21864;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uuid expression should produce different values in each execution under streaming query,SPARK-24896,13173998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,24/Jul/18 04:33,02/Aug/18 22:36,13/Jul/23 08:48,02/Aug/18 22:36,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,SQL,Structured Streaming,,,0,,,,,"Uuid's results depend on random seed given during analysis. Thus under streaming query, we will have the same uuids in each execution. ",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 24 04:41:04 UTC 2018,,,,,,,,,,"0|i3w7gf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/18 04:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21854;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.4.0 Snapshot artifacts has broken metadata due to mismatched filenames,SPARK-24895,13173961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ericfchang,ericfchang,ericfchang,23/Jul/18 23:39,12/Dec/22 18:10,13/Jul/23 08:48,24/Jul/18 22:59,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,,,,0,,,,,"Spark 2.4.0 has Maven build errors because artifacts uploaded to apache maven repo has mismatched filenames:
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce (enforce-banned-dependencies) on project spark_2.4: Execution enforce-banned-dependencies of goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce failed: org.apache.maven.shared.dependency.graph.DependencyGraphBuilderException: Could not resolve following dependencies: [org.apache.spark:spark-mllib-local_2.11:jar:2.4.0-SNAPSHOT (compile), org.apache.spark:spark-network-shuffle_2.11:jar:2.4.0-SNAPSHOT (compile), org.apache.spark:spark-sketch_2.11:jar:2.4.0-SNAPSHOT (compile)]: Could not resolve dependencies for project com.databricks:spark_2.4:pom:1: The following artifacts could not be resolved: org.apache.spark:spark-mllib-local_2.11:jar:2.4.0-SNAPSHOT, org.apache.spark:spark-network-shuffle_2.11:jar:2.4.0-SNAPSHOT, org.apache.spark:spark-sketch_2.11:jar:2.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-mllib-local_2.11:jar:2.4.0-20180723.232411-177 in apache-snapshots ([https://repository.apache.org/snapshots/]) -> [Help 1]
{noformat}
 

If you check the artifact metadata you will see the pom and jar files are 2.4.0-20180723.232411-177 instead of 2.4.0-20180723.232410-177:
{code:xml}
<metadata modelVersion=""1.1.0"">
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-mllib-local_2.11</artifactId>
  <version>2.4.0-SNAPSHOT</version>
  <versioning>
    <snapshot>
      <timestamp>20180723.232411</timestamp>
      <buildNumber>177</buildNumber>
    </snapshot>
    <lastUpdated>20180723232411</lastUpdated>
    <snapshotVersions>
      <snapshotVersion>
        <extension>jar</extension>
        <value>2.4.0-20180723.232411-177</value>
        <updated>20180723232411</updated>
      </snapshotVersion>
      <snapshotVersion>
        <extension>pom</extension>
        <value>2.4.0-20180723.232411-177</value>
        <updated>20180723232411</updated>
      </snapshotVersion>
      <snapshotVersion>
        <classifier>tests</classifier>
        <extension>jar</extension>
        <value>2.4.0-20180723.232410-177</value>
        <updated>20180723232411</updated>
      </snapshotVersion>
      <snapshotVersion>
        <classifier>sources</classifier>
        <extension>jar</extension>
        <value>2.4.0-20180723.232410-177</value>
        <updated>20180723232411</updated>
      </snapshotVersion>
      <snapshotVersion>
        <classifier>test-sources</classifier>
        <extension>jar</extension>
        <value>2.4.0-20180723.232410-177</value>
        <updated>20180723232411</updated>
      </snapshotVersion>
    </snapshotVersions>
  </versioning>
</metadata>
{code}
 
This behavior is very similar to this issue: https://issues.apache.org/jira/browse/MDEPLOY-221

Since 2.3.0 snapshots work with the same maven 3.3.9 version and maven deploy 2.8.2 plugin, it is highly possible that we introduced a new plugin that causes this. 

The most recent addition is the spot-bugs plugin, which is known to have incompatibilities with other plugins: [https://github.com/spotbugs/spotbugs-maven-plugin/issues/21]

We may want to try building without it to sanity check.",,apachespark,ericfchang,kiszk,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24529,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 28 04:22:59 UTC 2018,,,,,,,,,,"0|i3w78f:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"23/Jul/18 23:48;yhuai; 

[~kiszk] and [~hyukjin.kwon] we hit this issue today. Per [https://github.com/spotbugs/spotbugs-maven-plugin/issues/21,] it may be related to spot-bug plugin. We are trying to verify it now.;;;","24/Jul/18 00:16;yhuai;[~kiszk] [~hyukjin.kwon] since this thing is pretty tricky to test it out actually, do you mind if I remove the spotbugs and test out our nightly snapshot build? If this plugin is not the cause, I will add it back. If it is indeed the cause, we can figure out how to fix it. Thanks!;;;","24/Jul/18 01:15;gurwls223;Yes please! don't mind at all.;;;","24/Jul/18 01:19;gurwls223;FWIW, I hit this too. Thanks for analysis.;;;","24/Jul/18 18:00;apachespark;User 'ericfchang' has created a pull request for this issue:
https://github.com/apache/spark/pull/21865;;;","24/Jul/18 22:59;yhuai;[https://github.com/apache/spark/pull/21865] has been merged.;;;","25/Jul/18 00:17;yhuai;[~hyukjin.kwon] [~kiszk] seems this revert indeed fixed the problem :)

 

 ;;;","25/Jul/18 02:25;gurwls223;Thank you [~yhuai]. I couldn't foresee this problem.;;;","27/Jul/18 16:35;kiszk;[~yhuai] Thank you.

BTW, how can I re-enable spotbugs without this problem? Do you have any suggestion? cc: [~hyukjin.kwon];;;","27/Jul/18 16:38;yhuai;[https://github.com/spotbugs/spotbugs-maven-plugin/issues/21] has some info on it. I am wondering if it requires upgrading both the plugin and maven. We probably need to setup a testing jenkins job to make sure everything works before checking in changes.;;;","27/Jul/18 16:47;kiszk;I see. Thank you very much. At first, I will try to make a PR to upgrade a maven.

BTW, I have no idea to make sure maven central repo works well for now.;;;","27/Jul/18 16:57;ericfchang;[~kiszk] for maven, you may need 3.5.2 which includes this fix: https://issues.apache.org/jira/browse/MNG-6240.  It seems others have mentioned upgrading maven alone doesn't always work so I suspect you may need to upgrade spotbugs too as [~yhuai] suggested.

I think the spotbugs error [https://github.com/spotbugs/spotbugs-maven-plugin/issues/21] mentioned some errors on the aether resolver.

I think maven install will give you an error that looks like the first error in the bug description.  Regarding verification of apache repo artifacts, you can check the maven-metadata.xml file by going to a link like this one (this is for spark-core):

[https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/maven-metadata.xml];;;","28/Jul/18 04:22;kiszk;[~ericfchang] Thank you very much for your suggestion. As the first step, I created [a PR|https://github.com/apache/spark/pull/21905] to upgrade maven.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid DNS name due to hostname truncation ,SPARK-24894,13173948,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,dharmesh.kakadia,dharmesh.kakadia,23/Jul/18 22:53,17/May/20 18:23,13/Jul/23 08:48,19/Feb/19 23:20,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,Spark Core,,,1,,,,,"The truncation for hostname happening here [https://github.com/apache/spark/blob/5ff1b9ba1983d5601add62aef64a3e87d07050eb/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L77]  is a problematic and can lead to DNS names starting with ""-"". 

Originally filled here : https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/229

```
{{2018-07-23 21:21:42 ERROR Utils:91 - Uncaught exception in thread kubernetes-pod-allocator io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://kubernetes.default.svc/api/v1/namespaces/default/pods. Message: Pod ""user-archetypes-all-weekly-1532380861251850404-1532380862321-exec-9"" is invalid: spec.hostname: Invalid value: ""-archetypes-all-weekly-1532380861251850404-1532380862321-exec-9"": a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name', or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?'). Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=spec.hostname, message=Invalid value: ""-archetypes-all-weekly-1532380861251850404-1532380862321-exec-9"": a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name', or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?'), reason=FieldValueInvalid, additionalProperties={})], group=null, kind=Pod, name=user-archetypes-all-weekly-1532380861251850404-1532380862321-exec-9, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Pod ""user-archetypes-all-weekly-1532380861251850404-1532380862321-exec-9"" is invalid: spec.hostname: Invalid value: ""-archetypes-all-weekly-1532380861251850404-1532380862321-exec-9"": a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name', or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?'), metadata=ListMeta(resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}). at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:470) at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:409) at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:379) at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:343) at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:226) at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:769) at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:356) at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$$anon$1$$anonfun$3$$anonfun$apply$3.apply(KubernetesClusterSchedulerBackend.scala:140) at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$$anon$1$$anonfun$3$$anonfun$apply$3.apply(KubernetesClusterSchedulerBackend.scala:140) at org.apache.spark.util.Utils$.tryLog(Utils.scala:1922) at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$$anon$1$$anonfun$3.apply(KubernetesClusterSchedulerBackend.scala:139) at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$$anon$1$$anonfun$3.apply(KubernetesClusterSchedulerBackend.scala:138) at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) at scala.collection.mutable.HashMap.foreach(HashMap.scala:99) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$$anon$1.run(KubernetesClusterSchedulerBackend.scala:145) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)}}
```",,dharmesh.kakadia,liyinan926,thi.nguyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 00:16:11 UTC 2020,,,,,,,,,,"0|i3w75j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/18 18:01;liyinan926;[~mcheah]. We need to make sure the truncation leads to a valid hostname.;;;","19/Feb/20 00:16;thi.nguyen;Why is Fix Version 3.0.0? This looks like a bug fix to me, so should've been in a patch version?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HandleNullInputsForUDF rule,SPARK-24891,13173883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,23/Jul/18 18:00,25/Jul/18 04:07,13/Jul/23 08:48,25/Jul/18 02:41,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,"The HandleNullInputsForUDF rule can generate new {{If}} node infinitely, thus causing problems like match of SQL cache missed. ",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 25 04:07:08 UTC 2018,,,,,,,,,,"0|i3w6r3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/18 19:04;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21851;;;","25/Jul/18 04:07;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21869;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dataset.unpersist() doesn't update storage memory stats,SPARK-24889,13173832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,bogomolov,bogomolov,23/Jul/18 14:52,11/Sep/18 17:46,13/Jul/23 08:48,11/Sep/18 17:46,2.3.0,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,Spark Core,,,,0,,,,,"Steps to reproduce:

1) Start a Spark cluster, and check the storage memory value from the Spark Web UI ""Executors"" tab (it should be equal to zero if you just started)

2) Run:
{code:java}
val df = spark.sqlContext.range(1, 1000000000)
df.cache()
df.count()
df.unpersist(true){code}
3) Check the storage memory value again, now it's equal to 1GB

 

Looks like the memory is actually released, but stats aren't updated. This issue makes cluster management more complicated.

!image-2018-07-23-10-53-58-474.png!",,apachespark,bogomolov,devaraj,maropu,riza,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25091,,,,,,,,,,,,,,,,,,,"23/Jul/18 14:53;bogomolov;image-2018-07-23-10-53-58-474.png;https://issues.apache.org/jira/secure/attachment/12932722/image-2018-07-23-10-53-58-474.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 17:46:27 UTC 2018,,,,,,,,,,"0|i3w6fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/18 10:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/22341;;;","11/Sep/18 17:46;vanzin;Issue resolved by pull request 22341
[https://github.com/apache/spark/pull/22341];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the group id for spark-kubernetes-integration-tests,SPARK-24880,13173537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,20/Jul/18 19:52,17/May/20 18:25,13/Jul/23 08:48,20/Jul/18 22:33,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,Kubernetes,Spark Core,,0,,,,,"The correct group id should be `org.apache.spark`. This is causing the nightly build failure: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-maven-snapshots/2295/console

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.8.2:deploy (default-deploy) on project spark-kubernetes-integration-tests_2.11: Failed to deploy artifacts: Could not transfer artifact spark-kubernetes-integration-tests:spark-kubernetes-integration-tests_2.11:jar:2.4.0-20180720.101629-1 from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): Access denied to: https://repository.apache.org/content/repositories/snapshots/spark-kubernetes-integration-tests/spark-kubernetes-integration-tests_2.11/2.4.0-SNAPSHOT/spark-kubernetes-integration-tests_2.11-2.4.0-20180720.101629-1.jar, ReasonPhrase: Forbidden. -> [Help 1]
[ERROR] 
{code}",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 19:54:05 UTC 2018,,,,,,,,,,"0|i3w4m7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 19:54;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/21831;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NPE in Hive partition filter pushdown for `partCol IN (NULL, ....)`",SPARK-24879,13173519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,PenguinToast,PenguinToast,PenguinToast,20/Jul/18 18:40,21/Jul/18 03:03,13/Jul/23 08:48,21/Jul/18 03:03,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,"The following query triggers a NPE:
{code:java}
create table foo (col1 int) partitioned by (col2 int);
select * from foo where col2 in (1, NULL);
{code}
We try to push down the filter to Hive in order to do partition pruning, but the filter converter breaks on a `null`.

Here's the stack:
{code:java}
java.lang.NullPointerException
at org.apache.spark.sql.hive.client.Shim_v0_13$ExtractableLiteral$2$.unapply(HiveShim.scala:601)
at org.apache.spark.sql.hive.client.Shim_v0_13$ExtractableLiterals$2$$anonfun$5.apply(HiveShim.scala:609)
at org.apache.spark.sql.hive.client.Shim_v0_13$ExtractableLiterals$2$$anonfun$5.apply(HiveShim.scala:609)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.spark.sql.hive.client.Shim_v0_13$ExtractableLiterals$2$.unapply(HiveShim.scala:609)
at org.apache.spark.sql.hive.client.Shim_v0_13.org$apache$spark$sql$hive$client$Shim_v0_13$$convert$1(HiveShim.scala:671)
at org.apache.spark.sql.hive.client.Shim_v0_13$$anonfun$convertFilters$1.apply(HiveShim.scala:704)
at org.apache.spark.sql.hive.client.Shim_v0_13$$anonfun$convertFilters$1.apply(HiveShim.scala:704)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.immutable.List.foreach(List.scala:392)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
at scala.collection.immutable.List.flatMap(List.scala:355)
at org.apache.spark.sql.hive.client.Shim_v0_13.convertFilters(HiveShim.scala:704)
at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:725)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:678)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:676)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:676)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:1221)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:1214)
at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:1214)
at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitionsByFilter(ExternalCatalogWithListener.scala:254)
at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionsByFilter(SessionCatalog.scala:955)
at org.apache.spark.sql.hive.execution.HiveTableScanExec.rawPartitions$lzycompute(HiveTableScanExec.scala:172)
at org.apache.spark.sql.hive.execution.HiveTableScanExec.rawPartitions(HiveTableScanExec.scala:164)
at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$11.apply(HiveTableScanExec.scala:190)
at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$11.apply(HiveTableScanExec.scala:190)
at org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2418)
at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:189)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:294)
at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:272)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:846)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:921)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:932)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}",,apachespark,maropu,PenguinToast,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 20:07:05 UTC 2018,,,,,,,,,,"0|i3w4i7:",9223372036854775807,,,,,smilegator,,,,,,,,2.3.2,,,,,,,,,,"20/Jul/18 20:07;apachespark;User 'PenguinToast' has created a pull request for this issue:
https://github.com/apache/spark/pull/21832;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix reverse function for array type of primitive type containing null.,SPARK-24878,13173492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,20/Jul/18 16:41,26/Jul/18 07:07,13/Jul/23 08:48,26/Jul/18 07:07,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"If we use {{reverse}} function for array type of primitive type containing {{null}} and the child array is {{UnsafeArrayData}}, the function returns a wrong result because {{UnsafeArrayData}} doesn't define the behavior of re-assignment, especially we can't set a valid value after we set {{null}}.",,apachespark,cloud_fan,maropu,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 26 07:07:03 UTC 2018,,,,,,,,,,"0|i3w4c7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 16:49;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21830;;;","26/Jul/18 07:07;cloud_fan;Issue resolved by pull request 21830
[https://github.com/apache/spark/pull/21830];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
increase switch to shielding frequent interaction reports with yarn,SPARK-24873,13173411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,hejiefang,hejiefang,20/Jul/18 09:26,12/Dec/22 18:10,13/Jul/23 08:48,21/Jul/18 08:44,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,Spark Shell,YARN,,0,,,,,"There is too much frequent interaction reports when i use spark shell commend which affect my input，so i think it need to increase a switch to shielding frequent interaction reports with yarn

 

!pic.jpg!",,apachespark,hejiefang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 11:02;hejiefang;pic.jpg;https://issues.apache.org/jira/secure/attachment/12932401/pic.jpg",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 21 08:44:05 UTC 2018,,,,,,,,,,"0|i3w3u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 09:43;apachespark;User 'hejiefang' has created a pull request for this issue:
https://github.com/apache/spark/pull/21827;;;","21/Jul/18 04:18;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/21784;;;","21/Jul/18 08:44;gurwls223;Issue resolved by pull request 21784
[https://github.com/apache/spark/pull/21784];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove the symbol “||” of the “OR” operation",SPARK-24872,13173407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hantiantian,hantiantian,hantiantian,20/Jul/18 09:10,10/Apr/19 09:00,13/Jul/23 08:48,10/Apr/19 08:58,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Spark Core,,,,1,,,,,"“||” will perform the function of STRING concat, and it is also the symbol of the ""OR"" operation.

When I want use ""||"" as ""OR"" operation, I find that it perform the function of STRING concat，

  spark-sql> explain extended select * from aa where id==1 || id==2;

   == Parsed Logical Plan ==
    'Project [*]
     +- 'Filter (('id = concat(1, 'id)) = 2)
      +- 'UnresolvedRelation `aa`

   spark-sql> select ""abc"" || ""DFF"" ;

   And the result is ""abcDFF"".

In predicates.scala, ""||"" is the symbol of ""Or"" operation. Could we remove it?

 ",,apachespark,cloud_fan,hantiantian,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 10 08:58:53 UTC 2019,,,,,,,,,,"0|i3w3tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 09:33;apachespark;User 'httfighter' has created a pull request for this issue:
https://github.com/apache/spark/pull/21826;;;","10/Apr/19 08:58;cloud_fan;Issue resolved by pull request 21826
[https://github.com/apache/spark/pull/21826];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache can't work normally if there are case letters in SQL,SPARK-24870,13173356,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eaton,eaton,eaton,20/Jul/18 02:17,24/Jul/18 06:07,13/Jul/23 08:48,24/Jul/18 06:07,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Cache can't work normally if there are case letters in SQL, 
for example:
 sql(""CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"")

sql(""select key, sum(case when Key > 0 then 1 else 0 end) as positiveNum "" +
 ""from src group by key"").cache().createOrReplaceTempView(""src_cache"")
 sql(
 s""""""select a.key
 from
 (select key from src_cache where positiveNum = 1)a
 left join
 (select key from src_cache )b
 on a.key=b.key
 """""").explain

 

The subquery ""select key from src_cache where positiveNum = 1"" on the left of join can use the cache data, but the subquery ""select key from src_cache"" on the right of join cannot use the cache data.",,apachespark,eaton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 02:19:04 UTC 2018,,,,,,,,,,"0|i3w3hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 02:19;apachespark;User 'eatoncys' has created a pull request for this issue:
https://github.com/apache/spark/pull/21823;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AnalysisBarrier to DataFrameWriter ,SPARK-24867,13173327,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,19/Jul/18 22:37,26/Jul/18 01:36,13/Jul/23 08:48,26/Jul/18 00:25,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.3.2,,,,SQL,,,,0,,,,,"
{code}
      val udf1 = udf({(x: Int, y: Int) => x + y})
      val df = spark.range(0, 3).toDF(""a"")
        .withColumn(""b"", udf1($""a"", udf1($""a"", lit(10))))
      df.cache()
      df.write.saveAsTable(""t"")
      df.write.saveAsTable(""t1"")
{code}

Cache is not being used because the plans do not match with the cached plan. This is a regression caused by the changes we made in AnalysisBarrier, since not all the Analyzer rules are idempotent. We need to fix it to Spark 2.3.2
",,apachespark,jerryshao,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 26 01:36:10 UTC 2018,,,,,,,,,,"0|i3w3bj:",9223372036854775807,,,,,,,,,,,,,2.3.2,,,,,,,,,,"19/Jul/18 23:50;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21821;;;","25/Jul/18 01:12;jerryshao;[~smilegator] what's the ETA of this issue?;;;","26/Jul/18 00:26;smilegator;[~jerryshao] This ticket was just resolved. [~lian cheng] might find another serious bug. He will open a JIRA soon. ;;;","26/Jul/18 01:36;jerryshao;I see, thanks! Please let me know when the JIRA is opened.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query plan string representation grows exponentially on queries with recursive cached datasets,SPARK-24850,13172955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,onursatici,onursatici,onursatici,18/Jul/18 15:38,23/Jul/18 16:53,13/Jul/23 08:48,23/Jul/18 16:53,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"As of [https://github.com/apache/spark/pull/21018], InMemoryRelation includes its cacheBuilder when logging query plans. This CachedRDDBuilder includes the cachedPlan, so calling treeString on InMemoryRelation will log the cachedPlan in the cacheBuilder.

Given the sample dataset:
{code:java}
$ cat test.csv
A,B
0,0{code}
If the query plan has multiple cached datasets that depend on each other:
{code:java}
var df_cached = spark.read.format(""csv"").option(""header"", ""true"").load(""test.csv"").cache()
0 to 1 foreach { _ =>
df_cached = df_cached.join(spark.read.format(""csv"").option(""header"", ""true"").load(""test.csv""), ""A"").cache()
}
df_cached.explain

{code}
results in:
{code:java}
== Physical Plan ==
InMemoryTableScan [A#10, B#11, B#35, B#87]
+- InMemoryRelation [A#10, B#11, B#35, B#87], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(2) Project [A#10, B#11, B#35, B#87]
+- *(2) BroadcastHashJoin [A#10], [A#86], Inner, BuildRight
:- *(2) Filter isnotnull(A#10)
: +- InMemoryTableScan [A#10, B#11, B#35], [isnotnull(A#10)]
: +- InMemoryRelation [A#10, B#11, B#35], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(2) Project [A#10, B#11, B#35]
+- *(2) BroadcastHashJoin [A#10], [A#34], Inner, BuildRight
:- *(2) Filter isnotnull(A#10)
: +- InMemoryTableScan [A#10, B#11], [isnotnull(A#10)]
: +- InMemoryRelation [A#10, B#11], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
+- *(1) Filter isnotnull(A#34)
+- InMemoryTableScan [A#34, B#35], [isnotnull(A#34)]
+- InMemoryRelation [A#34, B#35], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
+- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: +- *(2) Project [A#10, B#11, B#35]
: +- *(2) BroadcastHashJoin [A#10], [A#34], Inner, BuildRight
: :- *(2) Filter isnotnull(A#10)
: : +- InMemoryTableScan [A#10, B#11], [isnotnull(A#10)]
: : +- InMemoryRelation [A#10, B#11], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: : +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
: +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
: +- *(1) Filter isnotnull(A#34)
: +- InMemoryTableScan [A#34, B#35], [isnotnull(A#34)]
: +- InMemoryRelation [A#34, B#35], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
+- *(1) Filter isnotnull(A#86)
+- InMemoryTableScan [A#86, B#87], [isnotnull(A#86)]
+- InMemoryRelation [A#86, B#87], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
+- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
+- *(2) Project [A#10, B#11, B#35, B#87]
+- *(2) BroadcastHashJoin [A#10], [A#86], Inner, BuildRight
:- *(2) Filter isnotnull(A#10)
: +- InMemoryTableScan [A#10, B#11, B#35], [isnotnull(A#10)]
: +- InMemoryRelation [A#10, B#11, B#35], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(2) Project [A#10, B#11, B#35]
+- *(2) BroadcastHashJoin [A#10], [A#34], Inner, BuildRight
:- *(2) Filter isnotnull(A#10)
: +- InMemoryTableScan [A#10, B#11], [isnotnull(A#10)]
: +- InMemoryRelation [A#10, B#11], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
+- *(1) Filter isnotnull(A#34)
+- InMemoryTableScan [A#34, B#35], [isnotnull(A#34)]
+- InMemoryRelation [A#34, B#35], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
+- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: +- *(2) Project [A#10, B#11, B#35]
: +- *(2) BroadcastHashJoin [A#10], [A#34], Inner, BuildRight
: :- *(2) Filter isnotnull(A#10)
: : +- InMemoryTableScan [A#10, B#11], [isnotnull(A#10)]
: : +- InMemoryRelation [A#10, B#11], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: : +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
: +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
: +- *(1) Filter isnotnull(A#34)
: +- InMemoryTableScan [A#34, B#35], [isnotnull(A#34)]
: +- InMemoryRelation [A#34, B#35], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
: +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
+- *(1) Filter isnotnull(A#86)
+- InMemoryTableScan [A#86, B#87], [isnotnull(A#86)]
+- InMemoryRelation [A#86, B#87], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
,None)
+- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>

{code}
previously this yielded:
{code:java}
== Physical Plan ==
InMemoryTableScan [A#10, B#11, B#37, B#89]
+- InMemoryRelation [A#10, B#11, B#37, B#89], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
+- *(2) Project [A#10, B#11, B#37, B#89]
+- *(2) BroadcastHashJoin [A#10], [A#88], Inner, BuildRight
:- *(2) Filter isnotnull(A#10)
: +- InMemoryTableScan [A#10, B#11, B#37], [isnotnull(A#10)]
: +- InMemoryRelation [A#10, B#11, B#37], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
: +- *(2) Project [A#10, B#11, B#37]
: +- *(2) BroadcastHashJoin [A#10], [A#36], Inner, BuildRight
: :- *(2) Filter isnotnull(A#10)
: : +- InMemoryTableScan [A#10, B#11], [isnotnull(A#10)]
: : +- InMemoryRelation [A#10, B#11], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
: : +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
: +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
: +- *(1) Filter isnotnull(A#36)
: +- InMemoryTableScan [A#36, B#37], [isnotnull(A#36)]
: +- InMemoryRelation [A#36, B#37], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
: +- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
+- *(1) Filter isnotnull(A#88)
+- InMemoryTableScan [A#88, B#89], [isnotnull(A#88)]
+- InMemoryRelation [A#88, B#89], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
+- *(1) FileScan csv [A#10,B#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:string,B:string>

{code}
This exponential growth can OOM the driver on large query plans with cached datasets.",,apachespark,maropu,onursatici,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 18 15:53:07 UTC 2018,,,,,,,,,,"0|i3w11j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/18 15:53;apachespark;User 'onursatici' has created a pull request for this issue:
https://github.com/apache/spark/pull/21805;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stabilize expression cannonicalization,SPARK-24846,13172896,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hvanhovell,hvanhovell,18/Jul/18 11:11,19/Jul/18 21:27,13/Jul/23 08:48,19/Jul/18 21:27,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,spree,,,,Spark plan canonicalization is can be non-deterministic between different versions of spark due to the fact that {{ExprId}} uses a UUID.,,apachespark,hvanhovell,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 19 21:27:59 UTC 2018,,,,,,,,,,"0|i3w0of:",9223372036854775807,,,,,hvanhovell,,,,,,,,,,,,,,,,,,"18/Jul/18 16:31;apachespark;User 'gvr' has created a pull request for this issue:
https://github.com/apache/spark/pull/21806;;;","19/Jul/18 21:27;hvanhovell;Fixed by gvr's PR. I could not find this user in JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In Spark Thrift Server, CAST AS FLOAT inconsistent with spark-shell or spark-sql ",SPARK-24829,13172594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,17/Jul/18 09:19,12/Dec/22 18:10,13/Jul/23 08:48,27/Jul/18 05:29,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"SELECT CAST('4.56' AS FLOAT)

the result is 4.559999942779541 , it should be 4.56",,apachespark,maropu,toopt4,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24317,,,,,,,SPARK-28620,,,,,,,,,,"18/Jul/18 03:15;zuo.tingbing9;2018-07-18_110944.png;https://issues.apache.org/jira/secure/attachment/12932020/2018-07-18_110944.png","18/Jul/18 03:15;zuo.tingbing9;2018-07-18_111111.png;https://issues.apache.org/jira/secure/attachment/12932021/2018-07-18_111111.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 27 05:29:08 UTC 2018,,,,,,,,,,"0|i3vytb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/18 09:28;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/21789;;;","27/Jul/18 05:29;gurwls223;Fixed in https://github.com/apache/spark/pull/21789;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[K8S][TEST] Kubernetes integration tests don't trace the maven project dependency structure,SPARK-24825,13172500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mcheah,mcheah,mcheah,16/Jul/18 22:30,17/May/20 18:26,13/Jul/23 08:48,18/Jul/18 21:13,2.4.0,,,,,,,,,,,,,,,,,,,,,Kubernetes,Spark Core,Tests,,0,,,,,"The Kubernetes integration tests will currently fail if maven installation is not performed first, because the integration test build believes it should be pulling the Spark parent artifact from maven central. However, this is incorrect because the integration test should be building the Spark parent pom as a dependency in the multi-module build, and the integration test should just use the dynamically built artifact. Or to put it another way, the integration test builds should never be pulling Spark dependencies from maven central.",,apachespark,mcheah,shaneknapp,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 18 21:13:49 UTC 2018,,,,,,,,,,"0|i3vy8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/18 20:20;skonto;To reproduce it try build the test suite:

../../../build/mvn install
 Using `mvn` from path:...spark/build/apache-maven-3.3.9/bin/mvn
 [INFO] Scanning for projects...
 [INFO] 
 [INFO] ------------------------------------------------------------------------
 [INFO] Building Spark Project Kubernetes Integration Tests 2.4.0-SNAPSHOT
 [INFO] ------------------------------------------------------------------------
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/maven-metadata.xml]
 Downloaded: [https://repository.apache.org/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/maven-metadata.xml] (2 KB at 0.4 KB/sec)
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/spark-core_2.11-2.4.0-20180717.095738-170.pom]
 [WARNING] The POM for org.apache.spark:spark-core_2.11:jar:2.4.0-20180717.095738-170 is missing, no dependency information available
 [WARNING] The POM for org.apache.spark:spark-core_2.11:jar:tests:2.4.0-20180717.095737-170 is missing, no dependency information available
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-tags_2.11/2.4.0-SNAPSHOT/maven-metadata.xml]
 Downloaded: [https://repository.apache.org/snapshots/org/apache/spark/spark-tags_2.11/2.4.0-SNAPSHOT/maven-metadata.xml] (2 KB at 0.7 KB/sec)
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-tags_2.11/2.4.0-SNAPSHOT/spark-tags_2.11-2.4.0-20180717.095221-170.pom]
 [WARNING] The POM for org.apache.spark:spark-tags_2.11:jar:tests:2.4.0-20180717.095220-170 is missing, no dependency information available
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/spark-core_2.11-2.4.0-20180717.095738-170.jar]
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/spark-core_2.11-2.4.0-20180717.095737-170-tests.jar]
 Downloading: [https://repository.apache.org/snapshots/org/apache/spark/spark-tags_2.11/2.4.0-SNAPSHOT/spark-tags_2.11-2.4.0-20180717.095220-170-tests.jar]
 Downloaded: [https://repository.apache.org/snapshots/org/apache/spark/spark-tags_2.11/2.4.0-SNAPSHOT/spark-tags_2.11-2.4.0-20180717.095220-170-tests.jar] (9 KB at 5.2 KB/sec)
 Downloaded: [https://repository.apache.org/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/spark-core_2.11-2.4.0-20180717.095737-170-tests.jar] (10438 KB at 577.5 KB/sec)
 [INFO] ------------------------------------------------------------------------
 [INFO] BUILD FAILURE
 [INFO] ------------------------------------------------------------------------
 [INFO] Total time: 26.346 s
 [INFO] Finished at: 2018-07-17T23:15:29+03:00
 [INFO] Final Memory: 14M/204M
 [INFO] ------------------------------------------------------------------------
 [ERROR] Failed to execute goal on project spark-kubernetes-integration-tests_2.11: Could not resolve dependencies for project spark-kubernetes-integration-tests:spark-kubernetes-integration-tests_2.11:jar:2.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-core_2.11:jar:2.4.0-20180717.095738-170 in apache.snapshots ([https://repository.apache.org/snapshots]) -> [Help 1]
 [ERROR] 
 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
 [ERROR] 
 [ERROR] For more information about the errors and possible solutions, please read the following articles:
 [ERROR] [Help 1] [http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptio|http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException]

 

The metadata here show expected version:

[https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/maven-metadata.xml]

 
 <groupId>org.apache.spark</groupId>
 <artifactId>spark-core_2.11</artifactId>
 <version>2.4.0-SNAPSHOT</version>
 <versioning>
 <snapshot>
 <timestamp>20180717.095738</timestamp>
 <buildNumber>170</buildNumber>
 </snapshot>
 <lastUpdated>20180717095738</lastUpdated>
  
 but there is no such jar uploaded.

 

Latest is: [spark-core_2.11-2.4.0-20180717.095737-170.jar|https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.11/2.4.0-SNAPSHOT/spark-core_2.11-2.4.0-20180717.095737-170.jar]

Probably timestamp is wrong here.;;;","17/Jul/18 20:46;shaneknapp;we're trying to get things to work w/o uploading the jar, but instead have it found locally.

 ;;;","17/Jul/18 21:30;skonto;As I mentioned elsewhere one thing that worked for me is modify ./dev/make-distribution.sh to do a clean install instead of clean package. But not sure for the long term for this, although you could purge deps before a build.

Test suite has the following spark deps:

mvn -f pom.xml dependency:resolve | grep spark

[INFO] org.apache.spark:spark-kvstore_2.11:jar:2.4.0-SNAPSHOT:compile
 [INFO] org.apache.spark:spark-network-shuffle_2.11:jar:2.4.0-SNAPSHOT:compile
 [INFO] org.apache.spark:spark-unsafe_2.11:jar:2.4.0-SNAPSHOT:compile
 [INFO] org.apache.spark:spark-core_2.11:jar:2.4.0-SNAPSHOT:compile
 [INFO] org.apache.spark:spark-launcher_2.11:jar:2.4.0-SNAPSHOT:compile
 [INFO] org.apache.spark:spark-tags_2.11:jar:2.4.0-SNAPSHOT:compile
 [INFO] org.apache.spark:spark-tags_2.11:test-jar:tests:2.4.0-SNAPSHOT:compile
 [INFO] org.spark-project.spark:unused:jar:1.0.0:compile
 [INFO] org.apache.spark:spark-network-common_2.11:jar:2.4.0-SNAPSHOT:compile;;;","17/Jul/18 21:33;shaneknapp;yeah, `clean install` isn't probably a good long term solution.  we're still hacking on things, so give us a little bit.  :);;;","17/Jul/18 21:35;skonto;You could remove .m2, downloads should be fast. :) I am hacking too :P

But I suspect also timestamp of published metadata is messed up.

Can we ping people who own the process? [~srowen]?

 ;;;","17/Jul/18 21:41;mcheah;We're looking into this now, this particular phase was built out by myself, [~ssuchter], [~foxish], and [~ifilonenko]. Consulting with some folks from RiseLab now also - [~shaneknapp]. I think we really shouldn't have to maven install, the multi-module build should pick up the other modules properly. We've likely configured the maven reactor incorrectly in the integration test's pom.xml.;;;","17/Jul/18 21:53;shaneknapp;[~vanzin] [~joshrosen] any thoughts?;;;","17/Jul/18 21:55;srowen;To build and test only a child module, you can't just run tests in the child's dir. You run something like ""mvn ... -pl child/pom.xml"" from the parent. Is that the issue?;;;","17/Jul/18 22:57;shaneknapp;[~mcheah] is working on a patch now... ;;;","17/Jul/18 23:37;skonto;Yes [~srowen] is right, this seems to work: 

dev-run-integration-tests.sh:

cd ../../../

./build/mvn -T 1C -Pscala-2.11 -Pkubernetes -pl resource-managers/kubernetes/integration-tests -amd integration-test $\{properties[@]};;;","18/Jul/18 00:03;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/21800;;;","18/Jul/18 21:13;shaneknapp;PR pushed, builds green, and now we have slightly more spammy build logs!  :)

thanks [~mcheah];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HiveExternalCatalogVersionsSuite still flaky; fall back to Apache archive",SPARK-24813,13172281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,15/Jul/18 21:04,17/Jul/18 12:56,13/Jul/23 08:48,16/Jul/18 04:28,2.2.2,2.3.1,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.0,,Tests,,,,0,,,,,"HiveExternalCatalogVersionsSuite is still failing periodically with errors from mirror sites. In fact, the test depends on the Spark versions it needs being available on the mirrors, but older versions will eventually be removed.

The test should fall back to downloading from archive.apache.org if mirrors don't have the Spark release, or aren't responding.

This has become urgent as I helpfully already purged many old Spark releases from mirrors, as requested by the ASF, before realizing it would probably make this test fail deterministically.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23489,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 17 12:56:05 UTC 2018,,,,,,,,,,"0|i3vwvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/18 21:57;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/21776;;;","16/Jul/18 02:17;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/21779;;;","17/Jul/18 12:50;srowen;I think we're also going to need to update the versions of Spark that this test downloads to still try to avoid hitting archive.apache.org, because it's returning 503s. More coming ...;;;","17/Jul/18 12:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/21793;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Last Access Time in the table description is not valid,SPARK-24812,13172271,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,S71955,S71955,S71955,15/Jul/18 18:55,24/Jul/18 18:33,13/Jul/23 08:48,24/Jul/18 18:33,2.2.1,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Last Access Time in the table description is not valid, 

Test steps:

Step 1 -  create a table

Step 2 - Run  command ""DESC FORMATTED table""

 Last Access Time will always displayed wrong date

Wed Dec 31 15:59:59 PST 1969 - which is wrong.

!image-2018-07-16-15-37-28-896.png!

In hive its displayed as ""UNKNOWN"" which makes more sense than displaying wrong date.

Please find the snapshot tested in hive for the same com !image-2018-07-16-15-38-26-717.png! mand

 

Seems to be a limitation as of now, better we can follow the hive behavior in this scenario.

 

 

 

 ",,apachespark,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/18 10:05;S71955;image-2018-07-16-15-37-28-896.png;https://issues.apache.org/jira/secure/attachment/12931759/image-2018-07-16-15-37-28-896.png","16/Jul/18 10:06;S71955;image-2018-07-16-15-38-26-717.png;https://issues.apache.org/jira/secure/attachment/12931760/image-2018-07-16-15-38-26-717.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 15 19:36:04 UTC 2018,,,,,,,,,,"0|i3vwtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/18 19:36;apachespark;User 'sujith71955' has created a pull request for this issue:
https://github.com/apache/spark/pull/21775;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serializing LongHashedRelation in executor may result in data error,SPARK-24809,13172250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,liutang123,liutang123,liutang123,15/Jul/18 10:48,29/Jul/18 20:16,13/Jul/23 08:48,29/Jul/18 20:16,2.0.0,2.1.0,2.2.0,2.3.0,,,,,,,,,,,,,,2.1.4,2.2.3,2.3.2,2.4.0,SQL,,,,0,correctness,,,,"When join key is long or int in broadcast join, Spark will use LongHashedRelation as the broadcast value. Details see SPARK-14419. But, if the broadcast value is abnormal big, executor will serialize it to disk. But, data will lost when serializing.","Spark 2.2.1

hadoop 2.7.1",apachespark,gostop_zlx,java8964,liutang123,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/18 03:39;gostop_zlx;Spark LongHashedRelation serialization.svg;https://issues.apache.org/jira/secure/attachment/12932027/Spark+LongHashedRelation+serialization.svg",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 18 03:40:41 UTC 2018,,,,,,,,,,"0|i3vwov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/18 12:03;apachespark;User 'liutang123' has created a pull request for this issue:
https://github.com/apache/spark/pull/21772;;;","18/Jul/18 03:40;gostop_zlx;[^Spark LongHashedRelation serialization.svg]

I think it's a hidden but critical bug that may cause data error.

 
{code:java}
// code in HashedRelation.scala
private def write(
    writeBoolean: (Boolean) => Unit,
    writeLong: (Long) => Unit,
    writeBuffer: (Array[Byte], Int, Int) => Unit): Unit = {
  writeBoolean(isDense)
  writeLong(minKey)
  writeLong(maxKey)
  writeLong(numKeys)
  writeLong(numValues)
  writeLong(numKeyLookups)
  writeLong(numProbes)

  writeLong(array.length)
  writeLongArray(writeBuffer, array, array.length)
  val used = ((cursor - Platform.LONG_ARRAY_OFFSET) / 8).toInt
  writeLong(used)
  writeLongArray(writeBuffer, page, used)
}
{code}
This write func in HashedRelation.scala will be called when executor didn't have enough memory for the LongToUnsafeRowMap in which the data of broadcast table been saved, however, the value of cursor in executor may not changed after initialization by 
{code:java}
// code placeholder
private var cursor: Long = Platform.LONG_ARRAY_OFFSET
{code}
which makes the value of ""used"" in write func been zero when write to disk, then in the case of deserializing this data in disk will get wrong pointer. Finally, we may get the wrong data from broadcast join.

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There are duplicate words in the title in the DatasetSuite,SPARK-24804,13172180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,hantiantian,hantiantian,hantiantian,14/Jul/18 06:57,18/Jul/18 14:42,13/Jul/23 08:48,18/Jul/18 14:42,2.3.2,,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,,,apachespark,hantiantian,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 18 14:42:10 UTC 2018,,,,,,,,,,"0|i3vw9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/18 07:21;apachespark;User 'httfighter' has created a pull request for this issue:
https://github.com/apache/spark/pull/21767;;;","15/Jul/18 01:28;jerryshao;Please don't set target version or fix version. Committers will help to set when this issue is resolved.

I'm removing the target version of this Jira to avoid blocking the release of 2.3.2;;;","18/Jul/18 14:42;srowen;This is too trivial for a Jira [~hantiantian], but OK for a first contribution.

Resolved by https://github.com/apache/spark/pull/21767;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DriverWrapper should have both master addresses in -Dspark.master,SPARK-24794,13171682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bsikander,bsikander,bsikander,12/Jul/18 09:21,25/Oct/18 13:37,13/Jul/23 08:48,25/Oct/18 13:37,2.2.1,,,,,,,,,,,,,,,,,3.0.0,,,,Deploy,,,,0,,,,,"In standalone cluster mode, one could launch a Driver with supervise mode enabled. Spark launches the driver with a JVM argument -Dspark.master which is set to [host and port of current master|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala#L149].

 

During the life of context, the spark masters can switch due to any reason. After that if the driver dies unexpectedly and comes up it tries to connect with the master which was set initially with -Dspark.master but that master is in STANDBY mode. The context tries multiple times to connect to standby and then just kills itself.

 

*Suggestion:*

While launching the driver process, Spark master should use the [spark.master passed as input|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala#L124] instead of master and port of the current master.

Log messages that we observe:

 
{code:java}
2018-07-11 13:03:21,801 INFO appclient-register-master-threadpool-0 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint []: Connecting to master spark://10.100.100.22:7077..
.....
2018-07-11 13:03:21,806 INFO netty-rpc-connection-0 org.apache.spark.network.client.TransportClientFactory []: Successfully created connection to /10.100.100.22:7077 after 1 ms (0 ms spent in bootstraps)
.....
2018-07-11 13:03:41,802 INFO appclient-register-master-threadpool-0 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint []: Connecting to master spark://10.100.100.22:7077...
.....
2018-07-11 13:04:01,802 INFO appclient-register-master-threadpool-0 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint []: Connecting to master spark://10.100.100.22:7077...
.....
2018-07-11 13:04:21,806 ERROR appclient-registration-retry-thread org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend []: Application has been killed. Reason: All masters are unresponsive! Giving up.{code}",,apachespark,bsikander,Ecaterina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 25 13:37:15 UTC 2018,,,,,,,,,,"0|i3vte7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/18 09:02;Ecaterina;Yes, I also face this problem. Would be nice if somebody could answer this.;;;","19/Jul/18 09:01;apachespark;User 'bsikander' has created a pull request for this issue:
https://github.com/apache/spark/pull/21816;;;","22/Sep/18 19:07;bsikander;Can someone please have a look at this PR?;;;","25/Oct/18 13:37;srowen;Issue resolved by pull request 21816
[https://github.com/apache/spark/pull/21816];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RelationalGroupedDataset.toString throws errors when grouping by UnresolvedAttribute,SPARK-24788,13171546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chorn,chorn,chorn,11/Jul/18 21:24,03/Aug/18 05:43,13/Jul/23 08:48,03/Aug/18 05:43,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"This causes references to the RelationalGroupedDataset to break on the shell because of the toString call:
{code:java}
scala> spark.range(0, 10).groupBy(""id"")
res4: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [id: bigint], value: [id: bigint], type: GroupBy]

scala> spark.range(0, 10).groupBy('id)
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'id
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:105)
  at org.apache.spark.sql.RelationalGroupedDataset$$anonfun$12.apply(RelationalGroupedDataset.scala:474)
  at org.apache.spark.sql.RelationalGroupedDataset$$anonfun$12.apply(RelationalGroupedDataset.scala:473)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.RelationalGroupedDataset.toString(RelationalGroupedDataset.scala:473)
  at scala.runtime.ScalaRunTime$.scala$runtime$ScalaRunTime$$inner$1(ScalaRunTime.scala:332)
  at scala.runtime.ScalaRunTime$.stringOf(ScalaRunTime.scala:337)
  at scala.runtime.ScalaRunTime$.replStringOf(ScalaRunTime.scala:345)
{code}
 

I will create a PR.",,apachespark,chorn,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 06:27:05 UTC 2018,,,,,,,,,,"0|i3vsjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/18 21:30;apachespark;User 'c-horn' has created a pull request for this issue:
https://github.com/apache/spark/pull/21752;;;","02/Aug/18 06:27;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21964;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Events being dropped at an alarming rate due to hsync being slow for eventLogging,SPARK-24787,13171539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,devaraj,sanket991,sanket991,11/Jul/18 20:47,29/Oct/18 00:58,13/Jul/23 08:48,25/Oct/18 21:05,2.3.0,2.3.1,,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,Web UI,,,1,,,,,"[https://github.com/apache/spark/pull/16924/files] updates the length of the inprogress files allowing history server being responsive.

Although we have a production job that has 60000 tasks per stage and due to hsync being slow it starts dropping events and the history server has wrong stats due to events being dropped.

A viable solution is not to make it sync very frequently or make it configurable.",,apachespark,devaraj,hzfeiwang,riza,sanket991,stevel@apache.org,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25645,,SPARK-19531,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 25 21:05:06 UTC 2018,,,,,,,,,,"0|i3vsif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/18 20:48;sanket991;I am happy to work on this... will have a potential solution and a pr up;;;","16/Jul/18 16:21;sanket991;[~vanzin] do you have any suggestions regarding this issue?

[~olegd] I would rather make this configurable or trigger periodically?;;;","13/Aug/18 22:53;vanzin;Is the slowness really caused by the use of hsync vs. hflush? I'd expect the flushing of the data, not the metadata update, to be the expensive part...

In any case, if you have any ideas, feel free to post a PR.;;;","14/Aug/18 14:07;tgraves;Yes it was caused by hsync, hsync has to go to the namenode in addition to the datanode, hflush is datanode only operation.   We saw huge increase in dropped events with this on large jobs, we reverted the change and went back to only hflush and it stopped dropping.

Talked to one of our hdfs experts and he said hsync is expensive.  it might depend on how loaded your hdfs cluster is.;;;","14/Aug/18 16:55;vanzin;In that case it might be good to only use hsync in ""safer"" contexts (i.e. not in event storms like task updates).;;;","15/Aug/18 21:49;stevel@apache.org;yes,, hsync updating the file length is the problem; that is:
{code}
DFSOutputStream.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH))
{code}

will talk to the NN; without that a normal hsync save the data, but will only update the NN when a block is completed.

It's a PITA but the other apps which look for changed files (e.g. YARN ATS) have an algorithm of caching the previous length of the file, re-opening it, trying to seek() to EOF + 1, and if that and or a subsequent read() succeeds, inferring that the file has changed.

See my underreviewed/uncommitted attempt at specifying & documenting this from HADOOP-13327 [outputstream.md|https://github.com/steveloughran/hadoop/blob/s3/HADOOP-13327-outputstream-trunk/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/outputstream.md]

Note some details there on when hsync doesn't, that is: if enough data has been written that you've crossed a block boundary; only the current active block is synced. IMO: bad behaviour.;;;","17/Aug/18 13:58;sanket991;Thanks [~stevel@apache.org] [~vanzin] [~tgraves] it seems we might have to stick with hflush but think of potentially another solution to update the file status changes similar to YARN ATS.

Even if I periodically update it I think the dropped events issue might persist as it hard to have a proper flow control.;;;","16/Oct/18 23:59;devaraj;It seems here the overhead is coming due the force call FileChannel.force in Datanode which is part of the hsync to write the data to the storage device. And the hsync is not making much difference with and without the flag SyncFlag.UPDATE_LENGTH, it might be because the update length is simple call to NameNode to update the length.

I think the hsync change can be reverted, and the history server can get the latest file length using the DFSInputStream.getFileLength() which includes lastBlockBeingWrittenLength, if the cached length is same as FileStatus.getLen() then history server can make additional call to get the latest length using DFSInputStream.getFileLength() and decide whether to update the history log or not.;;;","17/Oct/18 00:01;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/22752;;;","17/Oct/18 00:01;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/22752;;;","25/Oct/18 21:05;vanzin;Issue resolved by pull request 22752
[https://github.com/apache/spark/pull/22752];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.sql.shuffle.partitions=0 should throw exception,SPARK-24783,13171364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,Minskya,Minskya,11/Jul/18 09:56,12/Dec/22 18:10,13/Jul/23 08:48,08/Mar/19 05:10,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"if spark.sql.shuffle.partitions=0 and trying to join tables (not broadcast join)

*result join will be an empty table.*

 ",,kabhwan,langjingxiang,mgaido,Minskya,schahaha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 25 01:25:28 UTC 2019,,,,,,,,,,"0|i3vrfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/19 13:06;langjingxiang;ok 

Let me add a judgement.;;;","18/Feb/19 13:31;kabhwan;Does we have the chance to set the value to 0 or negative value intentionally? If not, the needed change would be trivial: add checkValue in `SHUFFLE_PARTITIONS` to require ""value > 0"".;;;","18/Feb/19 14:51;langjingxiang;yes you are right ,It seems that there is no use of 0 or negative values at present.;;;","08/Mar/19 05:10;gurwls223;Issue resolved by pull request 24008
[https://github.com/apache/spark/pull/24008];;;","25/Dec/19 01:25;schahaha;What's the point of setting it to 0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using a reference from Dataset in Filter/Sort might not work.,SPARK-24781,13171307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,ueshin,ueshin,11/Jul/18 05:07,13/Jul/18 15:26,13/Jul/23 08:48,13/Jul/18 15:26,2.3.1,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,SQL,,,,0,,,,,"When we use a reference from {{Dataset}} in {{filter}} or {{sort}}, which was not used in the prior {{select}}, an {{AnalysisException}} occurs, e.g.,

{code:scala}
val df = Seq((""test1"", 0), (""test2"", 1)).toDF(""name"", ""id"")
df.select(df(""name"")).filter(df(""id"") === 0).show()
{code}

{noformat}
org.apache.spark.sql.AnalysisException: Resolved attribute(s) id#6 missing from name#5 in operator !Filter (id#6 = 0).;;
!Filter (id#6 = 0)
   +- AnalysisBarrier
      +- Project [name#5]
         +- Project [_1#2 AS name#5, _2#3 AS id#6]
            +- LocalRelation [_1#2, _2#3]
{noformat}

If we use {{col}} instead, it works:

{code:scala}
val df = Seq((""test1"", 0), (""test2"", 1)).toDF(""name"", ""id"")
df.select(col(""name"")).filter(col(""id"") === 0).show()
{code}
",,apachespark,felixcheung,jerryshao,maropu,smilegator,ueshin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 13 15:26:50 UTC 2018,,,,,,,,,,"0|i3vr2v:",9223372036854775807,,,,,,,,,,,,,2.3.2,,,,,,,,,,"11/Jul/18 06:30;felixcheung;[~jerryshao];;;","11/Jul/18 06:36;jerryshao;Thanks Felix. Does this have to be in 2.3.2? [~ueshin];;;","11/Jul/18 06:47;ueshin;Yes, I think so. This is a regression from 2.2 to 2.3.;;;","11/Jul/18 06:49;jerryshao;I see. I will wait for this before cutting a new 2.3.2 RC release;;;","11/Jul/18 09:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21745;;;","13/Jul/18 15:26;smilegator;cc [~jerryshao] The fix is merged. Please start the next rc of Spark 2.3.2?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DateTimeUtils.getTimeZone method returns GMT time if timezone cannot be parsed,SPARK-24778,13171236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,redvine,redvine,10/Jul/18 18:47,03/Mar/19 16:06,13/Jul/23 08:48,03/Mar/19 16:06,2.3.1,,,,,,,,,,,,,,,,,3.0.0,,,,SQL,,,,0,,,,,"{{DateTimeUtils.getTimeZone}} calls java's {{Timezone.getTimezone}} method that defaults to GMT if the timezone cannot be parsed. This can be misleading for users and its better to return NULL instead of returning an incorrect value. 

To reproduce: {{from_utc_timestamp}} is one of the functions that calls {{DateTimeUtils.getTimeZone}}. Session timezone is GMT for the following queries.
{code:java}
SELECT from_utc_timestamp('2018-07-10 12:00:00', 'GMT+05:00') -> 2018-07-10 17:00:00 
SELECT from_utc_timestamp('2018-07-10 12:00:00', '+05:00') -> 2018-07-10 12:00:00 (Defaults to GMT as the timezone is not recognized){code}
We could fix it by using the workaround mentioned here: [https://bugs.openjdk.java.net/browse/JDK-4412864].",,maropu,maxgekk,redvine,RenkaiGe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26903,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 03 16:06:40 UTC 2019,,,,,,,,,,"0|i3vqn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/18 21:25;redvine;Workaround could be something like:
{code:java}

TimeZone tz = TimeZone.getTimeZone(timeZoneId);
if (!timeZoneId.equals(tz.getID())) {
   return None
   }
Some(tz) {code}
This would require us to change the return type of {{DateTimeUtils.getTimeZone}}  to {{Option[TimeZone]}} instead of {{TimeZone}}. We'd also have to make appropriate changes to the callers of this method. ;;;","11/Jul/18 02:52;maropu;I think that it is more reasonable to reject the unparsed cases? e.g., in postgresql,
{code:java}
postgres=# SELECT now() AT TIME ZONE 'Asia/Tokyo';
          timezone          
----------------------------
2018-07-11 11:45:12.626255
(1 row)

postgres=# SELECT now() AT TIME ZONE 'unknown';
ERROR:  time zone ""unknown"" not recognized
{code}
Anyway, we might hesitate to change this behaviour in minor releases, so we might hold it until next major releases. cc: [~ueshin];;;","24/Jul/18 23:50;redvine;[~maropu] Sorry, I missed seeing this comment. Failing the query when the timezone is not supported works too. In some cases, Spark returns NULL where the inputs are incorrect. For instance, {{to_date('2018-02-31')}} returns NULL instead of throwing an error. 

 ;;;","13/Feb/19 11:23;RenkaiGe;Since Spark 2.3 dropped support for java versions before Java 8.We can use classes in java.time.* instead of  relevant ones in java.util.*.


I will to work on this issue since I encountered the same problem in my application.;;;","26/Feb/19 07:37;RenkaiGe;I think this issue is already resolved by
https://issues.apache.org/jira/browse/SPARK-26903;;;","03/Mar/19 16:06;maxgekk;The issue has been fixed already by using ZoneId.of for parsing time zone ids.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create table wants to use /user/hive/warehouse in clean clone,SPARK-24758,13170657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,bersprockets,bersprockets,07/Jul/18 21:19,14/Mar/21 03:05,13/Jul/23 08:48,14/Mar/21 03:05,2.4.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,0,,,,,"Got a clean clone of repository:
 - git clone [https://github.com/apache/spark.git] spark_clean
 - cd spark_clean/
 - ./build/sbt -Phive -Phive-thriftserver clean package

Ran spark-sql and tried to create a table:
 - ./bin/spark-sql
 - create table testit as select 1 a, 2 b;

Got error:
{noformat}
18/07/07 13:33:20 WARN HiveMetaStore: Location: file:/user/hive/warehouse/testit specified for non-external table:testit
18/07/07 13:33:20 INFO FileUtils: Creating directory if it doesn't exist: file:/user/hive/warehouse/testit
Error in query: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:file:/user/hive/warehouse/testit is not a directory or unable to create one);
{noformat}
To get things working, it seems you need to do something with dataframewriter (after removing metastore_db, if it exists):
{noformat}
scala> spark.range(0,1).write.saveAsTable(""fred"")
spark.range(0,1).write.saveAsTable(""fred"")
18/07/07 14:08:08 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/07/07 14:08:08 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
scala> 
{noformat}
After that, create table statements work:
{noformat}
spark-sql> create table testit as select 1 a, 2 b;
create table testit as select 1 a, 2 b;
18/07/07 14:14:40 WARN HiveMetaStore: Location: file:<work-copy-dir>/spark-warehouse/testit specified for non-external table:testit
18/07/07 14:14:41 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Time taken: 3.387 seconds
spark-sql> show tables;
show tables;
default fred    false
default testit  false
Time taken: 0.07 seconds, Fetched 2 row(s)
spark-sql>
{noformat}
 ",,bersprockets,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 14 03:05:50 UTC 2021,,,,,,,,,,"0|i3vn4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/18 20:07;bersprockets;This issue was introduced by commit [b83b502c4189c571bda776511c6f7541c6067aae|https://github.com/apache/spark/commit/b83b502c4189c571bda776511c6f7541c6067aae].

One can work around the issue by issuing spark-sql as so:
{noformat}
bin/spark-sql --hiveconf hive.metastore.warehouse.dir=${HOME}/github/spark_clean/spark-warehouse
{noformat};;;","03/Nov/18 05:19;yumwang;The reason is that [SessionState.start(sessionState)|https://github.com/wangyum/spark/blob/b81e3031fd247dfb4b3e02e0a986fb4b19d00f7c/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala#L133] has created {{default}} database. The database location is {{/user/hive/warehouse}}. Then it [will not create default database|https://github.com/apache/spark/blob/f38ea00e83099a5ae8d3afdec2e896e43c2db612/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala#L113-L118] again.
;;;","03/Nov/18 06:07;yumwang;cc [~Qin Yao];;;","14/Mar/21 03:05;bersprockets;Issue no longer present on master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor loss can cause task to not be resubmitted,SPARK-24755,13170625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hthuynh2,mridulm80,mridulm80,07/Jul/18 07:30,19/Jul/18 14:53,13/Jul/23 08:48,19/Jul/18 14:53,2.3.0,,,,,,,,,,,,,,,,,2.3.3,2.4.0,,,Spark Core,,,,1,,,,,"As part of SPARK-22074, when an executor is lost, TSM.executorLost currently checks for ""if (successful(index) && !killedByOtherAttempt(index))"" to decide if task needs to be resubmitted for partition.

Consider following:

For partition P1, tasks T1 and T2 are running on exec-1 and exec-2 respectively (one of them being speculative task)

T1 finishes successfully first.

This results in setting ""killedByOtherAttempt(P1) = true"" due to running T2.
We also end up killing task T2.

Now, exec-1 if/when goes MIA.
executorLost will no longer schedule task for P1 - since killedByOtherAttempt(P1) == true; even though P1 was hosted on T1 and there is no other copy of P1 around (T2 was killed when T1 succeeded).


I noticed this bug as part of reviewing PR# 21653 for SPARK-13343

Essentially, SPARK-22074 causes a regression (which I dont usually observe due to shuffle service, sigh) - and as such the fix is broken IMO.

I dont have a PR handy for this, so if anyone wants to pick it up, please do feel free !
+CC [~XuanYuan] who fixed SPARK-22074 initially.",,apachespark,hthuynh2,mridulm80,riza,tgraves,viirya,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 08 18:44:04 UTC 2018,,,,,,,,,,"0|i3vmxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/18 01:51;hthuynh2;Hi [~mridulm80], since fixing this might affect my change in PR# 21653 for SPARK-13343, if you and [~XuanYuan] don't mind, I can take this. ;;;","08/Jul/18 07:43;mridulm80;Go for it - thanks [~hthuynh2] !;;;","08/Jul/18 13:20;XuanYuan;No problem, thanks [~hthuynh2].
Thanks [~mridulm80] for letting me know, I'll also watch this jira.;;;","08/Jul/18 18:44;apachespark;User 'hthuynh2' has created a pull request for this issue:
https://github.com/apache/spark/pull/21729;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minhash integer overflow,SPARK-24754,13170622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,jiayuanm,jiayuanm,07/Jul/18 05:23,14/Jul/18 20:59,13/Jul/23 08:48,14/Jul/18 20:59,2.1.0,,,,,,,,,,,,,,,,,2.4.0,,,,ML,,,,0,,,,,"Hash computation in MinHashLSHModel has integer overflow bug.

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/MinHashLSH.scala#L69",,apachespark,jiayuanm,kiszk,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 14 20:59:58 UTC 2018,,,,,,,,,,"0|i3vmx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/18 07:11;kiszk;In test cases, we would appreciate it if you will compare values with them by other implementations.;;;","11/Jul/18 19:03;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/21750;;;","14/Jul/18 20:59;srowen;Issue resolved by pull request 21750
[https://github.com/apache/spark/pull/21750];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot filter array<struct> with named_struct,SPARK-24749,13170412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,pin_zhang,pin_zhang,06/Jul/18 05:54,12/Dec/22 18:10,13/Jul/23 08:48,07/Jul/18 03:35,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"1. Create Table

create table arr__int( arr array<struct<a:int>> )stored as parquet;

2. Insert data

insert into arr__int values( array(named_struct('a', 1)));

3. Filter with struct data

select * from arr__int where array_contains (arr, named_struct('a', 1));
Error: org.apache.spark.sql.AnalysisException: cannot resolve 'array_contains(arr__int.`arr`, named_struct('a', 1))' due to data type mismatch: Arguments must be an array followed by a value of same type as the array members; line 1 pos 29;
'Project [*]
+- 'Filter array_contains(arr#6, named_struct(a, 1))
 +- SubqueryAlias arr__int
 +- Relation[arr#6] parquet (state=,code=0)

Caused by schema null is always false in named_struct 

 ",,apachespark,pin_zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 07 03:35:17 UTC 2018,,,,,,,,,,"0|i3vln3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/18 07:29;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21724;;;","07/Jul/18 03:35;gurwls223;Issue resolved by pull request 21724
[https://github.com/apache/spark/pull/21724];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the JavaDirectKafkaWordCount example to support the new API of Kafka,SPARK-24743,13170178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cluo_dev,cluo_dev,cluo_dev,05/Jul/18 06:38,05/Jul/18 14:08,13/Jul/23 08:48,05/Jul/18 14:08,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Examples,,,,0,,,,,"when I ran the example JavaDirectKafkaWordCount as follows:

./bin/spark-submit --class org.apache.spark.examples.streaming.JavaDirectKafkaWordCount --master local --jars /somepath/spark-streaming-kafka-0-10-assembly_2.11-2.3.1.jar examples/jars/spark-examples_2.11-2.3.1.jar kafka-broker:port topic

Then a error happened: 'org.apache.kafka.common.config.ConfigException:Missing required configuration ""bootstrap.servers"" which has no default value'. So I looked into the code and found the example JavaDirectKafkaWordCount  misses some kafka required configs.  ",,apachespark,cluo_dev,koeninger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 05 14:08:02 UTC 2018,,,,,,,,,,"0|i3vk73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/18 08:38;apachespark;User 'cluo512' has created a pull request for this issue:
https://github.com/apache/spark/pull/21717;;;","05/Jul/18 14:08;koeninger;Issue resolved by pull request 21717
[https://github.com/apache/spark/pull/21717];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Field Metadata raises NullPointerException in hashCode method,SPARK-24742,13170168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kupferk,kupferk,kupferk,05/Jul/18 05:59,02/Aug/18 14:22,13/Jul/23 08:48,02/Aug/18 14:22,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"The class org.apache.spark.sql.types.Metadata has a method *putNull* for storing null pointers as Metadata. Unfortunately the hashCode method throws a NullPointerException when there are null values in a Metadata object, rendering the *puNull* method useless.

h2. How to Reproduce
The following code will raise a NullPointerException
{code}
new MetadataBuilder().putNull(""key"").build().hashCode
{code}",,apachespark,kupferk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 14:22:50 UTC 2018,,,,,,,,,,"0|i3vk4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/18 06:00;kupferk;A patch is provided as a pull request in https://github.com/apache/spark/pull/21722;;;","06/Jul/18 05:16;apachespark;User 'kupferk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21722;;;","02/Aug/18 14:22;srowen;Issue resolved by pull request 21722
[https://github.com/apache/spark/pull/21722];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark does not work with Python 3.7.0,SPARK-24739,13170077,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,04/Jul/18 13:47,12/Dec/22 18:10,13/Jul/23 08:48,07/Jul/18 03:38,2.1.3,2.2.2,2.3.1,,,,,,,,,,,,,,,2.3.2,2.4.0,,,PySpark,,,,0,,,,,"Python 3.7 is released in few days ago and our PySpark does not work. For example

{code}
sc.parallelize([1, 2]).take(1)
{code}

{code}
File ""/.../spark/python/pyspark/rdd.py"", line 1343, in __main__.RDD.take
Failed example:
    sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
Exception raised:
    Traceback (most recent call last):
      File ""/.../3.7/lib/python3.7/doctest.py"", line 1329, in __run
        compileflags, 1), test.globs)
      File ""<doctest __main__.RDD.take[2]>"", line 1, in <module>
        sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
      File ""/.../spark/python/pyspark/rdd.py"", line 1377, in take
        res = self.context.runJob(self, takeUpToNumLeft, p)
      File ""/.../spark/python/pyspark/context.py"", line 1013, in runJob
        sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
      File ""/.../spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
        answer, self.gateway_client, self.target_id, self.name)
      File ""/.../spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
        format(target_id, ""."", name), value)
    py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 143.0 failed 1 times, most recent failure: Lost task 0.0 in stage 143.0 (TID 688, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
      File ""/.../spark/python/pyspark/rdd.py"", line 1373, in takeUpToNumLeft
        yield next(iterator)
    StopIteration

    The above exception was the direct cause of the following exception:

    Traceback (most recent call last):
      File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 320, in main
        process()
      File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 315, in process
        serializer.dump_stream(func(split_index, iterator), outfile)
      File ""/.../spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 378, in dump_stream
        vs = list(itertools.islice(iterator, batch))
    RuntimeError: generator raised StopIteration

    	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:309)
    	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:449)
    	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:432)
    	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:263)
    	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
    	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
    	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
    	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
    	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)
    	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)
    	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2071)
    	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2071)
    	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    	at org.apache.spark.scheduler.Task.run(Task.scala:109)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)

    Driver stacktrace:
    	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1607)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1595)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1594)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1594)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
    	at scala.Option.foreach(Option.scala:257)
    	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1828)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1777)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1766)
    	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2031)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2052)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2071)
    	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)
    	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    	at py4j.Gateway.invoke(Gateway.java:282)
    	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
      File ""/.../spark/python/pyspark/rdd.py"", line 1373, in takeUpToNumLeft
        yield next(iterator)
    StopIteration

    The above exception was the direct cause of the following exception:

    Traceback (most recent call last):
      File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 320, in main
        process()
      File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 315, in process
        serializer.dump_stream(func(split_index, iterator), outfile)
      File ""/.../spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 378, in dump_stream
        vs = list(itertools.islice(iterator, batch))
    RuntimeError: generator raised StopIteration

    	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:309)
    	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:449)
    	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:432)
    	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:263)
    	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
    	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
    	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
    	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
    	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)
    	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)
    	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2071)
    	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2071)
    	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    	at org.apache.spark.scheduler.Task.run(Task.scala:109)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	... 1 more
{code}

Should check the behaviour changes or bugs in Python and PySpark.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 07 03:38:32 UTC 2018,,,,,,,,,,"0|i3vjkn:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"04/Jul/18 13:47;gurwls223;I am working on this.;;;","04/Jul/18 16:05;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21714;;;","05/Jul/18 02:23;jerryshao;Do we have to fix it in 2.3.2? I don't think it is even critical. For example Java 9 is out for a long time, but we still don't support Java 9. So this seems not a big problem. ;;;","05/Jul/18 02:26;gurwls223;For this fix particularly, the fix is small and safe. In case of SPARK-19019, it was merged to from 1.6, 2.0, 2.1 and 2.2. I expect the similar situation with this JIRA's case with many stackoverflow questions.;;;","07/Jul/18 03:38;gurwls223;Issue resolved by pull request 21714
[https://github.com/apache/spark/pull/21714];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
--py-files not functional for non local URLs. It appears to pass non-local URL's into PYTHONPATH directly.,SPARK-24736,13169974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,jonathan.weaver,jonathan.weaver,04/Jul/18 02:22,17/May/20 18:25,13/Jul/23 08:48,27/Feb/19 17:49,2.4.0,,,,,,,,,,,,,,,,,3.0.0,,,,Kubernetes,PySpark,Spark Core,,2,,,,,"My spark-submit
bin/spark-submit \
        --master k8s://[https://internal-api-test-k8s-local-7afed8-796273878.us-east-1.elb.amazonaws.com|https://internal-api-test-k8s-local-7afed8-796273878.us-east-1.elb.amazonaws.com/] \
        --deploy-mode cluster \
        --name pytest \
        --conf spark.kubernetes.container.image=[412834075398.dkr.ecr.us-east-1.amazonaws.com/fids/pyspark-k8s:latest|http://412834075398.dkr.ecr.us-east-1.amazonaws.com/fids/pyspark-k8s:latest] \
        --conf [spark.kubernetes.driver.pod.name|http://spark.kubernetes.driver.pod.name/]=spark-pi-driver \
        --conf spark.kubernetes.authenticate.submission.caCertFile=[cluster.ca|http://cluster.ca/] \
        --conf spark.kubernetes.authenticate.submission.oauthToken=$TOK \
        --conf spark.kubernetes.authenticate.driver.oauthToken=$TOK \
--py-files ""[https://s3.amazonaws.com/maxar-ids-fids/screw.zip]"" \
[https://s3.amazonaws.com/maxar-ids-fids/it.py]
 
*screw.zip is successfully downloaded and placed in SparkFIles.getRootPath()*
2018-07-01 07:33:43 INFO  SparkContext:54 - Added file [https://s3.amazonaws.com/maxar-ids-fids/screw.zip] at [https://s3.amazonaws.com/maxar-ids-fids/screw.zip] with timestamp 1530430423297
2018-07-01 07:33:43 INFO  Utils:54 - Fetching [https://s3.amazonaws.com/maxar-ids-fids/screw.zip] to /var/data/spark-7aba748d-2bba-4015-b388-c2ba9adba81e/spark-0ed5a100-6efa-45ca-ad4c-d1e57af76ffd/userFiles-a053206e-33d9-4245-b587-f8ac26d4c240/fetchFileTemp1549645948768432992.tmp
*I print out the  PYTHONPATH and PYSPARK_FILES environment variables from the driver script:*
     PYTHONPATH /opt/spark/python/lib/pyspark.zip:/opt/spark/python/lib/py4j-0.10.7-src.zip:/opt/spark/jars/spark-core_2.11-2.4.0-SNAPSHOT.jar:/opt/spark/python/lib/pyspark.zip:/opt/spark/python/lib/py4j-*.zip:*[https://s3.amazonaws.com/maxar-ids-fids/screw.zip]*
    PYSPARK_FILES [https://s3.amazonaws.com/maxar-ids-fids/screw.zip]
 
*I print out sys.path*
['/tmp/spark-fec3684b-8b63-4f43-91a4-2f2fa41a1914', u'/var/data/spark-7aba748d-2bba-4015-b388-c2ba9adba81e/spark-0ed5a100-6efa-45ca-ad4c-d1e57af76ffd/userFiles-a053206e-33d9-4245-b587-f8ac26d4c240', '/opt/spark/python/lib/pyspark.zip', '/opt/spark/python/lib/py4j-0.10.7-src.zip', '/opt/spark/jars/spark-core_2.11-2.4.0-SNAPSHOT.jar', '/opt/spark/python/lib/py4j-*.zip', *'/opt/spark/work-dir/https', '//[s3.amazonaws.com/maxar-ids-fids/screw.zip|http://s3.amazonaws.com/maxar-ids-fids/screw.zip]',* '/usr/lib/python27.zip', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-linux2', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/lib/python2.7/site-packages']
 
*URL from PYTHONFILES gets placed in sys.path verbatim with obvious results.*
 
*Dump of spark config from container.*
Spark config dumped:
[(u'spark.master', u'k8s://[https://internal-api-test-k8s-local-7afed8-796273878.us-east-1.elb.amazonaws.com|https://internal-api-test-k8s-local-7afed8-796273878.us-east-1.elb.amazonaws.com/]'), (u'spark.kubernetes.authenticate.submission.oauthToken', u'<present_but_redacted>'), (u'spark.kubernetes.authenticate.driver.oauthToken', u'<present_but_redacted>'), (u'spark.kubernetes.executor.podNamePrefix', u'pytest-1530430411996'), (u'spark.kubernetes.memoryOverheadFactor', u'0.4'), (u'spark.driver.blockManager.port', u'7079'), (u'[spark.app.id|http://spark.app.id/]', u'spark-application-1530430424433'), (u'[spark.app.name|http://spark.app.name/]', u'pytest'), (u'[spark.executor.id|http://spark.executor.id/]', u'driver'), (u'spark.driver.host', u'pytest-1530430411996-driver-svc.default.svc'), (u'spark.kubernetes.container.image', u'[412834075398.dkr.ecr.us-east-1.amazonaws.com/fids/pyspark-k8s:latest'|http://412834075398.dkr.ecr.us-east-1.amazonaws.com/fids/pyspark-k8s:latest']), (u'spark.driver.port', u'7078'), (u'spark.kubernetes.python.mainAppResource', u'[https://s3.amazonaws.com/maxar-ids-fids/it.py']), (u'spark.kubernetes.authenticate.submission.caCertFile', u'[cluster.ca|http://cluster.ca/]'), (u'spark.rdd.compress', u'True'), (u'spark.driver.bindAddress', u'100.120.0.1'), (u'[spark.kubernetes.driver.pod.name|http://spark.kubernetes.driver.pod.name/]', u'spark-pi-driver'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.files', u'[https://s3.amazonaws.com/maxar-ids-fids/it.py,https://s3.amazonaws.com/maxar-ids-fids/screw.zip']), (u'spark.kubernetes.python.pyFiles', u'[https://s3.amazonaws.com/maxar-ids-fids/screw.zip']), (u'spark.kubernetes.authenticate.driver.mounted.oauthTokenFile', u'/mnt/secrets/spark-kubernetes-credentials/oauth-token'), (u'spark.submit.deployMode', u'client'), (u'spark.kubernetes.submitInDriver', u'true')]
 ","Recent 2.4.0 from master branch, submitted on Linux to a KOPS Kubernetes cluster created on AWS.

 ",bryanc,holden,ifilonenko,jonathan.weaver,ofrenkel,rvesse,vanzin,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30496,,,,,,SPARK-26933,SPARK-26934,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 17:49:59 UTC 2019,,,,,,,,,,"0|i3vixr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/18 17:10;holden;cc [~ifilonenko];;;","13/Aug/18 20:42;ifilonenko;The URL, until a resource-staging-server is setup will be unable to resolve the file location unless you use SparkFiles.get(file_name)` in your application. As such, using a URL in the --py-files will be unresolved. Thus, remote dependencies won't be supported by --py-files just yet, but we can support local files. ;;;","31/Jan/19 19:37;ofrenkel;Supporting local files with --py-files would be great. By ""local files"" I mean files that are coming from the original machine outside of driver pod and not files available in the driver docker container.;;;","14/Feb/19 17:18;vanzin;This isn't about staging local files. This is about referencing remote files in {{--py-files}}, and Spark generating a wrong {{PYTHONPATH}} out of it. The key part in the description:

{noformat}
 '/opt/spark/work-dir/https', '//s3.amazonaws.com/maxar-ids-fids/screw.zip'
{noformat}

Which is what you'd get when adding the ""https://..."" URL directly into {{PYTHONPATH}}.;;;","19/Feb/19 21:12;vanzin;I'm going to fork this issue into 2: 

- I'll leave this one for the current PR I have opened, which removes duplicated handling for dependencies rom the k8s backend
- I'll open a separate one for the issue where a .zip file provided in {{--py-files}} is not handled correctly by spark-submit; that affects more than just k8s.
- There's a 3rd issue of spark-submit not handling ""local:"" URIs correctly as far as making them available to executors, which isn't exactly related to the issue reported here, but I'll file a bug anyway. ;;;","27/Feb/19 17:49;vanzin;Issue resolved by pull request 23793
[https://github.com/apache/spark/pull/23793];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix containsNull of Concat for array type.,SPARK-24734,13169774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,03/Jul/18 11:20,16/Jul/18 15:17,13/Jul/23 08:48,16/Jul/18 15:17,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,SQL,,,,0,,,,,"Currently {{Concat}} for array type uses the data type of the first child as its own data type, but the children might include an array containing nulls.
We should aware the nullabilities of all children.",,apachespark,cloud_fan,kiszk,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 16 15:17:34 UTC 2018,,,,,,,,,,"0|i3vhpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/18 11:35;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21704;;;","16/Jul/18 15:17;cloud_fan;Issue resolved by pull request 21704
[https://github.com/apache/spark/pull/21704];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to use PythonUDF with literal inputs in filter with data sources,SPARK-24721,13169581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,icexelloss,smilegator,smilegator,02/Jul/18 17:56,28/Aug/18 02:58,13/Jul/23 08:48,28/Aug/18 02:58,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,PySpark,SQL,,,0,,,,,"{code}
import random
from pyspark.sql.functions import *
from pyspark.sql.types import *

def random_probability(label):
    if label == 1.0:
      return random.uniform(0.5, 1.0)
    else:
      return random.uniform(0.0, 0.4999)

def randomize_label(ratio):
    
    if random.random() >= ratio:
      return 1.0
    else:
      return 0.0

random_probability = udf(random_probability, DoubleType())
randomize_label = udf(randomize_label, DoubleType())

spark.range(10).write.mode(""overwrite"").format('csv').save(""/tmp/tab3"")
babydf = spark.read.csv(""/tmp/tab3"")
data_modified_label = babydf.withColumn(
  'random_label', randomize_label(lit(1 - 0.1))
)


data_modified_random = data_modified_label.withColumn(
  'random_probability', 
  random_probability(col('random_label'))
)

data_modified_label.filter(col('random_label') == 0).show()
{code}

The above code will generate the following exception:

{code}
Py4JJavaError: An error occurred while calling o446.showString.
: java.lang.RuntimeException: Invalid PythonUDF randomize_label(0.9), requires attributes from more than one child.
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:166)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:165)
	at scala.collection.immutable.Stream.foreach(Stream.scala:594)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(ExtractPythonUDFs.scala:165)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:116)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:112)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:77)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:325)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:325)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:325)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:325)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:112)
	at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:92)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:119)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:119)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:119)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:109)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:109)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3016)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2216)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2429)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:293)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:226)
	at java.lang.Thread.run(Thread.java:748)
{code}
",,apachespark,bryanc,cloud_fan,icexelloss,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 28 02:58:33 UTC 2018,,,,,,,,,,"0|i3vgif:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"02/Jul/18 17:57;smilegator;cc [~icexelloss] Are you interested in this?;;;","02/Jul/18 21:30;icexelloss;Yep I can take a look;;;","15/Jul/18 19:58;icexelloss;I am currently traveling but will try to take a look when I get back;;;","27/Jul/18 20:25;icexelloss;{code:java}
from pyspark.sql.functions import udf, lit, col

spark.range(1).write.mode(""overwrite"").format('csv').save(""/tmp/tab3"")
df = spark.read.csv('/tmp/tab3')
df2 = df.withColumn('v1', udf(lambda x: x, 'int')(lit(1)))
df2 = df2.filter(df2['v1'] == 0)

df2.explain()
{code}
This is a simpler reproduce;;;","27/Jul/18 21:14;icexelloss;I think the issue is the UDF is being pushed down to the PartitionFilter in FileScan physical node and then ExtractPythonUDFs rule throws the exception (this is the Spark plan before execution the ExtractPythonUDFs rule):
{code:java}
Project [_c0#17, <lambda>(1) AS v1#20]
+- Filter (<lambda>(1) = 0)
   +- FileScan csv [_c0#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tab3], PartitionFilters: [(<lambda>(1) = 0)], PushedFilters: [], ReadSchema: struct<_c0:string>
{code}
I am not familiar with how PartiionFilters pushdown is supposed to work. [~smilegator] and [~cloud_fan] could you guys maybe point me to the right direction? Should we not push down the filter <lambda(1) = 0> to FileScan? Or should we ignore it in the ExtractPythonUDFs rule?

 ;;;","28/Jul/18 01:29;cloud_fan;good catch! I think we should filter out python UDFs when picking partition predicates in `FileSourceStrategy`;;;","14/Aug/18 14:30;apachespark;User 'icexelloss' has created a pull request for this issue:
https://github.com/apache/spark/pull/22104;;;","14/Aug/18 15:26;icexelloss;Updated Jira title to reflect the actual issue;;;","27/Aug/18 15:50;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22244;;;","28/Aug/18 02:58;cloud_fan;Issue resolved by pull request 22104
[https://github.com/apache/spark/pull/22104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split out min retain version of state for memory in HDFSBackedStateStoreProvider,SPARK-24717,13169540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,02/Jul/18 14:02,15/Apr/19 19:40,13/Jul/23 08:48,19/Jul/18 07:08,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Structured Streaming,,,,0,,,,,"HDFSBackedStateStoreProvider has only one configuration for minimum versions to retain of state which applies to both memory cache and files. As default version of ""spark.sql.streaming.minBatchesToRetain"" is set to high (100), which doesn't require strictly 100x of memory, but I'm seeing 10x ~ 80x of memory consumption for various workloads. In addition, in some cases, requiring 2x of memory is even unacceptable, so we should split out configuration for memory and let users adjust to trade-off memory usage vs cache miss.

In normal case, default value '2' would cover both cases: success and restoring failure with less than or around 2x of memory usage, and '1' would only cover success case but no longer require more than 1x of memory. In extreme case, user can set the value to '0' to completely disable the map cache to maximize executor memory.",,apachespark,kabhwan,skonto,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23682,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 15 19:40:05 UTC 2019,,,,,,,,,,"0|i3vg9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/18 14:03;kabhwan;I have a patch for this, but now on top of SPARK-24441. I'll try to decouple and rebase to master branch, and provide a pull request.;;;","02/Jul/18 22:24;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/21700;;;","19/Jul/18 07:08;tdas;Issue resolved by pull request 21700
[https://github.com/apache/spark/pull/21700];;;","21/Sep/18 04:49;kabhwan;Looks like version of this issue wasn't changed while preparing release. Just updated.;;;","15/Apr/19 19:40;skonto;[~tdas] What is the point of having `spark.sql.streaming.minBatchesToRetain` set to 100 by default? 

Wouldnt that create problems with large states when it comes to external storage?

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sbt build brings a wrong jline versions,SPARK-24715,13169448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,dongjoon,dongjoon,02/Jul/18 05:16,12/Dec/22 18:11,13/Jul/23 08:48,02/Jul/18 14:10,2.4.0,,,,,,,,,,,,,,,,,2.4.0,,,,Build,,,,1,,,,,"During SPARK-24418 (Upgrade Scala to 2.11.12 and 2.12.6), we upgrade `jline` version together. So, `mvn` works correctly. However, `sbt` brings old jline library and is hitting `NoSuchMethodError` in `master` branch. Since we use `mvn` mainly, this is dev environment issue.

{code}
$ ./build/sbt -Pyarn -Phadoop-2.7 -Phadoop-cloud -Phive -Phive-thriftserver -Psparkr test:package
$ bin/spark-shell
scala> Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1530385877441).
Spark session available as 'spark'.
Exception in thread ""main"" java.lang.NoSuchMethodError: jline.console.completer.CandidateListCompletionHandler.setPrintSpaceAfterFullCompletion(Z)V
{code}",,apachespark,dongjoon,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25783,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 02 14:10:58 UTC 2018,,,,,,,,,,"0|i3vfov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/18 07:19;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21692;;;","02/Jul/18 14:10;gurwls223;Issue resolved by pull request 21692
[https://github.com/apache/spark/pull/21692];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AppMatser of spark streaming kafka OOM if there are hundreds of topics consumed,SPARK-24713,13169440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuanbo,yuanbo,yuanbo,02/Jul/18 03:12,13/Jul/18 13:38,13/Jul/23 08:48,13/Jul/18 13:38,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Input/Output,,,,0,,,,,We have hundreds of kafka topics need to be consumed in one application. The application master will throw OOM exception after hanging for nearly half of an hour.,,apachespark,koeninger,yuanbo,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 13 13:38:48 UTC 2018,,,,,,,,,,"0|i3vfn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/18 04:04;apachespark;User 'yuanboliu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21690;;;","13/Jul/18 13:38;koeninger;Issue resolved by pull request 21690
[https://github.com/apache/spark/pull/21690];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests will not work with exclude/include tags,SPARK-24711,13169407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,skonto,skonto,skonto,01/Jul/18 15:38,17/May/20 18:23,13/Jul/23 08:48,05/Jul/18 13:38,2.3.1,,,,,,,,,,,,,,,,,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,,"I tried to exclude some tests when adding mine and I got errors of the form:

[INFO] BUILD FAILURE

[INFO] ------------------------------------------------------------------------
 [INFO] Total time: 6.798 s
 [INFO] Finished at: 2018-07-01T18:34:13+03:00
 [INFO] Final Memory: 36M/652M
 [INFO] ------------------------------------------------------------------------
 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.20.1:test (default-test) on project spark-kubernetes-integration-tests_2.11: There are test failures.
 [ERROR] 
 [ERROR] Please refer to /home/stavros/Desktop/workspace/OSS/spark/resource-managers/kubernetes/integration-tests/target/surefire-reports for the individual test results.
 [ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.
 [ERROR] There was an error in the forked process
 [ERROR] Unable to load category: noDcos

 

This will not happen if maven surfire plugin is disabled as stated here: [http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin]

I will create a PR shortly.",,apachespark,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 05 13:38:39 UTC 2018,,,,,,,,,,"0|i3vffr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/18 10:57;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/21697;;;","05/Jul/18 13:38;srowen;Issue resolved by pull request 21697
[https://github.com/apache/spark/pull/21697];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
